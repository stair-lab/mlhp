{
  "hash": "7c61ea231aec6139172852d7330b443c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Alignment of Policy with Preferences\nformat: html\nfilters:\n  - pyodide\nexecute:\n  engine: pyodide\n  pyodide:\n    auto: true\n---\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n<iframe\n  src=\"https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/\"\n  style=\"width:45%; height:225px;\"\n></iframe>\n[Fullscreen Part 1](https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/){.btn .btn-outline-primary .btn role=\"button\"}\n[Fullscreen Part 2](https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/){.btn .btn-outline-primary .btn role=\"button\"}\n\n:::\n\n## Dueling Bandit\n\nThe multi-armed bandit (MAB) problem involves a gambler deciding which lever to pull on an MAB \nmachine to maximize the winning rate, despite not knowing which machine is the most rewarding. \nThis scenario highlights the need to balance exploration (trying new machines to discover potential \nhigher rewards) and exploitation (using current knowledge to maximize gains). MAB algorithms \naddress this dilemma by making decisions under uncertainty to achieve the best possible outcomes \nbased on gathered data. At the core of the MAB problem is a set of actions, or 'arms,' denoted by \n$\\mathcal{A} = \\{1, 2, \\ldots, K\\}$, where $K$ signifies the total number of arms. For each round $t$, \nthe agent selects an arm $a_t \\in \\mathcal{A}$ and receives a reward $r_t$, sampled from an arm-specific,\nunknown probability distribution. The expected reward of pulling arm $a$ is represented as $\\mu_a = \\mathbb{E}[r_t | a]$.\n\nThe multi-armed bandit framework can be extended in various ways to model more complex scenarios. \nIn the infinite-armed bandit problem, the set of possible arms $\\mathcal{A}$ is either very large \nor infinite. This introduces significant challenges in exploration, as the agent cannot afford to \nexplore each arm even once. Algorithms for infinite-armed bandits typically assume some regularity \nor structure of the reward function across arms to make the problem tractable. The contextual bandit \nproblem extends the bandit framework by incorporating observable external states or contexts that \ninfluence the reward distributions of arms. The agent's task is to learn policies that map contexts \nto arms to maximize reward. This model is particularly powerful for personalized recommendations, where \nthe context can include user features or historical interactions. In dueling bandit problems, the agent \nchooses two arms to pull simultaneously and receives feedback only on which of the two is better, not \nthe actual reward values. This pairwise comparison model is especially useful in scenarios where absolute \nevaluations are difficult, but relative preferences are easier to determine, such as in ranking systems.\n\nContextual bandits extend the multi-armed bandits by making decisions conditional on the state of the \nenvironment and previous observations. The benefit of such a model is that observing the environment \ncan provide additional information, potentially leading to better rewards and outcomes. In each \niteration, the agent is presented with the context of the environment, then decides on an action \nbased on the context and previous observations. Finally, the agent observes the action's outcome and \nreward. Throughout this process, the agent aims to maximize the expected reward.\n\nIn many real-world contexts, one may not have a real-valued reward (or at least a reliable one) \nassociated with a decision. Instead, we may only have observations indicating which of a set of \nbandits was optimal in a given scenario. The assumption is that within these observations of \npreferred choices among a set of options, there is an implicit reward or payoff encapsulated in that \ndecision. Consider the following examples:\n\n1.  **Dietary preferences**: When providing food recommendations to humans, it is often not possible \n    to quantify an explicit reward from recommending a specific food item. Instead, we can offer meal \n    options and observe which one the person selects.\n\n2.  **Video recommendation**: Websites like YouTube and TikTok recommend specific videos to users. It is \n    typically not feasible to measure the reward a person gains from watching a video. However, we can infer \n    that a user preferred one video over another. From these relative preference observations, we can develop \n    a strategy to recommend videos they are likely to enjoy.\n\n3.  **Exoskeleton gait optimization**: Tucker et al. (2020) created a framework that uses human-evaluated \n    preferences for an exoskeleton gait algorithm to develop an optimal strategy for the exoskeleton to assist\n    a human in walking. A human cannot reliably produce a numerical value for how well the exoskeleton helped \n    them walk but can reliably indicate which option performed best according to their preferences.\n\nGenerally, we assume access to a set of actions. A noteworthy assumption is that any observations we make\nare unbiased estimates of the payoff. This means that if we observe a human preferred one option over another \n(or several others), the preferred option had a higher implicit reward or payoff than the alternatives. In the \ncase of dietary preferences, this may mean that a human liked the preferred option; in the case of video \nrecommendations, a user was more entertained, satisfied, or educated by the video they selected than the other options.\n\nThe overarching context is that we do not have direct or reliable access to rewards. We may not have a \nreward at all (for some decisions, it may be impossible to define a real value to the outcome), or it may be \nnoisy (for example, if we ask a human to rate their satisfaction on a scale of 1 to 10). We use relative \ncomparisons to evaluate the best of multiple options in this case. Our goal is to minimize total regret in \nthe face of noisy comparisons. Humans may not always provide consistent observations (since human decision-making \nis not guaranteed to be consistent). However, we can still determine an optimal strategy with the observed \ncomparisons. We aim to minimize the frequency of sub-optimal decisions according to human preferences. In practice, \nmany formulations of bandits can allow for infinitely many bandits (for example, in continuous-value and high-dimensional \nspaces). However, this situation can be intractable when determining an optimal decision strategy. With \ninfinite options, how can we always ensure we have chosen the best? We will constrain our bandits to a \ndiscrete space to enable efficient exploration. We will assume that we have $k$ bandits, $b_i, i \\in [1, k]$, \nand our task is to choose the one that will minimize regret.\n\nWith the framework outlined, we now define our approach more formally. This method was introduced by \n [@YUE20121538], and proofs for the guarantees and derivations of parameters can be found in their work.\n\nTo determine the optimal action, we will compare pairwise to ascertain the probability that an action $b_i$ is \npreferred over another $b_j$, where $i \\ne j$. Concretely, we assume access to a function $\\epsilon$ that helps \ndetermine this probability; in practice, this can be done with an oracle, such as asking a human which of two \noptions they prefer: $$P(b_i > b_j) = \\varepsilon(b_i, b_j) + \\frac{1}{2}.$$ With this model, three basic properties \ngovern the values provided by $\\epsilon$:\n$$\\epsilon(b_i, b_j) = -\\epsilon(b_j, b_i), \\epsilon(b_i, b_i) = 0, \\epsilon(b_i, b_j) \\in \\left(-\\frac{1}{2}, \\frac{1}{2} \\right).$$\n\nWe assume there is a total ordering of bandits, such that $b_i \\succ b_j$ implies $\\epsilon(b_i, b_j) > 0$. We \nimpose two constraints to properly model comparisons:\n\n-   **Strong Stochastic Transitivity**: We must maintain our total ordering of bandits, and as such, the comparison \n    model also respects this ordering:\n    $$b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\ge \\text{max}\\{\\epsilon(b_i, b_j), \\epsilon(b_j, b_k)\\}.$$ {#eq-stochastic-transitivity}\n\n-   **Stochastic Triangle Inequality**: We also impose a triangle inequality, which captures the condition that the probability \n    of a bandit winning (or losing) a comparison will exhibit diminishing returns as it becomes increasingly superior (or inferior) \n    to the competing bandit:\n    $$b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\le \\epsilon(b_i, b_j) + \\epsilon(b_j, b_k).$$ {#eq-triangle-inequality}\n\nThese assumptions may initially seem limiting; however, common models for comparisons satisfy these constraints. \nFor example, the Bradley-Terry Model follows $P(b_i > b_j) = \\frac{\\mu_i}{\\mu_i + \\mu_j}$. \nThe Gaussian model with unit variance also satisfies these constraints: $P(b_i > b_j) = P(X_i - X_j > 0)$, \nwhere $X_i - X_j \\sim N(\\mu_i - \\mu_j, 2)$.\n\nTo accurately model the preferences between bandits in our framework of pairwise bandit comparisons and regret, \nwe must track certain parameters in our algorithm. First, we will maintain a running empirical estimate of the probability \nof bandit preferences based on our observations. It is important to note that we do not have direct access to an $\\epsilon$ \nfunction. Instead, we must present two bandits to a human, who selects a winner. To do this, we define:\n$$\\hat{P}_{i, j} = \\frac{\\# b_i\\ \\text{wins}}{\\# \\text{comparisons between}\\ i \\text{and}\\ j}.$$\n\nWe will also compute confidence intervals at each timestep for each of the entries in $\\hat{P}$ as\n$$\\hat{C}_t = \\left( \\hat{P}_t - c_t, \\hat{P}_t + c_t \\right),$$\nwhere $c_t = \\sqrt{\\frac{4\\log(\\frac{1}{\\delta})}{t}}$. Note that $\\delta = \\frac{1}{TK^2}$, where $T$ is the \ntime horizon and $K$ is the number of bandits.\n\nPreviously, we discussed approaches for finding the best action in a specific context. Now, we consider changing contexts, \nwhich means there is no longer a static hidden preference matrix $P$. Instead, at every time step, there is a preference \nmatrix $P_C$ depending on context $C$. We consider a context $C$ and a preference matrix $P_C$ to be chosen by nature as a \nresult of the given environment (Yue et al., 2012). The goal of a contextual bandits algorithm is to find a policy $\\pi$ that \nmaps contexts to a Von Neumann winner distribution over our bandits. That is, our policy $\\pi$ should map any context to some \ndistribution over our bandits such that sampling from that distribution is preferred to a random action for that context.\n\n### Regret\n\nThe agent aims to pick a sequence of arms $(a_1, a_2, \\ldots, a_T)$\nacross a succession of time steps $t = 1$ to $t = T$ to maximize the\ntotal accumulated reward. Formally, the strategy seeks to maximize the\nsum of the expected rewards:\n$\\max_{a_1, \\ldots, a_T} \\mathbb{E} \\left[\\sum_{t=1}^{T} r_t\\right]$.\nRegret is defined as the difference between the cumulative reward that\ncould have been obtained by always pulling the best arm (in hindsight,\nafter knowing the reward distributions) and the cumulative reward\nactually obtained by the algorithm. Formally, if $\\mu^*$ is the expected\nreward of the best arm and $\\mu_{a_t}$ is the expected reward of the arm\nchosen at time $t$, the regret after $T$ time steps is given by\n$R(T) = T \\cdot \\mu^* - \\sum_{t=1}^{T} \\mu_{a_t}$. The objective of a\nbandit algorithm is to minimize this regret over time, effectively\nlearning to make decisions that are as close as possible to the\ndecisions of an oracle that knows the reward distributions beforehand.\nLow regret indicates an algorithm that has often learned to choose\nwell-performing arms, balancing the exploration of unknown arms with the\nexploitation of arms that are already known to perform well. Thus, an\nefficient bandit algorithm exhibits sub-linear regret growth, meaning\nthat the average regret per round tends to zero as the number of rounds\n$T$ goes to infinity: $\\lim_{T \\to \\infty} \\frac{R(T)}{T} = 0$.\nMinimizing regret is a cornerstone in the design of bandit algorithms,\nand its analysis helps in understanding the long-term efficiency and\neffectiveness of different bandit strategies.\n\nAs previously discussed, our goal is to select the bandit that minimizes\na quantity that reflects regret or the cost of not selecting the optimal\nbandit at all times. We can leverage our comparison model to define a\nquantity for regret over some time horizon $T$, which is the number of\ndecisions we make (selecting what we think is the best bandit at each\niteration). Assuming we know the best bandit $b^*$ (and we know that\nthere *is* a best bandit, since there is a total ordering of our\ndiscrete bandits), we can define two notions of regret:\n\n-   Strong regret: aims to capture the fraction of users who would\n    prefer the optimal bandit $b^*$ over the *worse* of the options\n    $b_1, b_2$ we provide at a given\n    step:$R_T = \\sum_{t = 1}^T \\text{max} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}$\n\n-   Weak regret: aims to capture the fraction of users who would prefer\n    the optimal bandit $b^*$ over the *better* of the options $b_1, b_2$\n    we provide at a given\n    step:$\\tilde{R}_T = \\sum_{t = 1}^T \\text{min} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}$\n\nThe best bandit described in our regret definition is called a\n**Condorcet Winner**. This is the strongest form of winner. It's the\naction **$A_{i}$** which is preferred to each other action **$A_j$**\nwith $p > 0.5$ in a head-to-head election. While the above introduced\nnotions of regret assume an overall best bandit to exist, there might be\nsettings, where no bandit wins more than half head-to-head duels. A set\nof actions without a Condorcet winner is described by the following\npreference matrix, where each entry $\\Delta_{jk}$ is\n$p(j \\succ k) - 0.5$, the probability that action $j$ is preferred over\naction $k$ minus 0.5. There is no Condorcet winner as there is no action\nthat is preferred with $p > 0.5$ over all other actions. Imagine, you\nwant to find the best pizza to eat (=action). There may not be a pizza\nthat wins more than half of the head-to-head duels against every other\npizza.\n\nHowever, we might still have an intuition of the best pizza. Therefore\nSui et al., 2018 introduce the concepts of different $\\textit{winners}$\nin dueling bandit problems [@advancements_dueling]. In this example, we\nmight define the best pizza as the most popular one. We call the Pizza\nreceiving the most votes in a public vote the **Borda Winner**, or\nformally, Borda winner\n$j = \\arg\\max_{i \\in A, i \\neq j} \\left(\\sum p(j \\succ i)\\right)$. In\ncontrast to the Condorcet Winner setting, there is always guaranteed to\nbe one or more (in the case of a tie) Borda winners for a set of\nactions. However - if there is a Condorcet Winner, this might not\nnecessarily be the same as a Borda Winner: In our Pizza example, a\nPepperoni Pizza might win more than half of its head-to-head duels,\nwhile the Cheese-Pizza is still the most popular in a public poll.\n\nA more generic concept of winner is the **Von Neumann Winner**, which\ndescribes a probability distribution rather than a single bandit winner.\nA Von Neumann winner simply prescribes a probability distribution $W$\nsuch that sampling from this distribution 'beats' an action from the\nrandom uniform distribution with $p > 0.5$. In our pizza example, this\nwould correspond to trusting a friend to order whichever Pizza he likes,\nbecause this may still be preferred to ordering randomly. Formally, $W$\nis a Von Neumann if $(j \\sim W, k \\sim R) [p(p(j \\succ k) > 0.5) > 0.5]$\nwhere $R$ describes the uniform probability distribution over our\nactions. The concept of a Von Neumann winner is useful in contextual\nbandits, which will be introduced later. In these settings, the\npreference matrix depends on different context, which may have different\nBorda winners, just as different parties may vote for different pizzas.\n\n::: {#fig-condorcet_violation}\n         A        B           C         D       E      F\n  --- ------- ---------- ----------- ------- ------- ------\n   A     0     **0.03**   **-0.02**   0.06    0.10    0.11\n   B   -0.03      0       **0.03**    0.05    0.08    0.11\n   C            -0.03         0       0.04    0.07    0.09\n   D   -0.06    -0.05       -0.04       0     0.05    0.07\n   E   -0.10    -0.08       -0.07     -0.05     0     0.03\n   F   -0.11    -0.11       -0.09     -0.07   -0.03    0\n\n  : Violation of Condorcet Winner. Highlighted entries are different\n  from Table 1. No Condorcet winner exists as no arm could beat every\n  other arm.\n:::\n\nNext, we introduce two performance measures for the planner. The\n**asymptotic ex-post regret** is defined as\n$$\\text{Regret}(\\mu_1, \\ldots \\mu_K) = T\\cdot \\max_i \\mu_i - \\sum_{i=1}^T E[\\mu_{I_t}].$$\n\nIntuitively, this represents the difference between the reward achieved\nby always taking the action with the highest possible reward and the\nexpected welfare of the recommendation algorithm (based on the actions\nit recommends at each timestep).\n\nWe also define a weaker performance measure, the **Bayesian regret**,\nwhich is defined as\n$$\\text {Bayesian regret}=E_{\\mu_1, \\ldots, \\mu_K \\sim \\text {Prior}}\\left[\\operatorname{Regret}\\left(\\mu_1, \\ldots, \\mu_K\\right)\\right]$$\n\nWith a Bayesian optimal policy, we would like either definition of\nregret to vanish as $T\\to \\infty$; we are considering \"large-market\noptimal\\\" settings where there are many short-lived, rather than a few\nlong-term, users. Note the fact that ex-post regret is prior-free makes\nit robust to inaccuracies on the prior.\n\n### Acquisition Functions\n\nVarious strategies have been developed to balance the\nexploration-exploitation trade-off. These strategies differ in selecting\narms based on past experiences and rewards.\n\n#### Classical Acquisition Functions\n\n**Uniform** acquisition function is the most straightforward approach where\neach arm is selected uniformly randomly over time. This strategy does\nnot consider the past rewards and treats each arm equally promising\nregardless of the observed outcomes. It is a purely explorative strategy\nthat ensures each arm is sampled enough to estimate its expected reward,\nbut it does not exploit the information to optimize rewards. In\nmathematical terms, if $N_t(a)$ denotes the number of times arm $a$ has\nbeen selected up to time $t$, the Uniform Strategy would ensure that\n$N_t(a) \\approx \\frac{t}{K}$ for all arms $a$ as $t$ grows large:\n$P(a_t = a) = \\frac{1}{K}$\n\nThe **Epsilon Greedy** is a popular method that introduces a\nbalance between exploration and exploitation. With a small probability\n$\\epsilon$, it explores by choosing an arm at random, and with a\nprobability $1 - \\epsilon$, it exploits by selecting the arm with the\nhighest estimated reward so far. This strategy incrementally favors\nactions that have historically yielded higher rewards, but still allows\nfor occasional exploration to discover better options potentially. The\nparameter $\\epsilon$ is chosen based on the desired exploration level,\noften set between 0.01 and 0.1. $$P(a_t = a) =\n\\begin{cases} \n\\frac{\\epsilon}{K} + 1 - \\epsilon & \\text{if } a = \\arg\\max_{a'} \\hat{\\mu}_{a'} \\\\\n\\frac{\\epsilon}{K} & \\text{otherwise}\n\\end{cases}$$\n\n**Upper Confidence Bound** (UCB) acquisition function takes a more\nsophisticated approach to the exploration-exploitation dilemma. It\nselects arms based on both the estimated rewards and the uncertainty or\nvariance associated with those estimates. Specifically, it favors arms\nwith high upper confidence bounds on the estimated rewards, which is a\nsum of the estimated mean and a confidence interval that decreases with\nthe number of times the arm has been played. This ensures that arms with\nless certainty (those played less often) are considered more often,\nnaturally balancing exploration with exploitation as the uncertainty is\nreduced over time.\n\n$$P(a_t = a) =\n\\begin{cases} \n1 & \\text{if } a = \\arg\\max_{a'} \\left( \\hat{\\mu}_{a'} + \\sqrt{\\frac{2 \\ln t}{N_t(a')}} \\right) \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n\n#### Interleaved Filter\n\nThis algorithm tries to find the best bandit (Condorcet Winner) in a\ndiscrete, limited bandit-space via pairwise comparisons of the bandits.\nWe will now introduce the algorithm for the Interleaved Filter as\nprovided in [@YUE20121538] to solve a dueling bandit setup. It starts\nwith a randomly defined *best bandit* $\\hat{b}$ and iteratively compares\nit to set $W$ containing the remaining bandits $b$ resulting in winning\nprobabilities $\\hat{P}_{\\hat{b},b}$ and confidence interval\n$\\hat{C}_{\\hat{b},b}$. If a bandit $b$ is *confidently worse* than\n$\\hat{b}$, it is removed from $W$. If a bandit $b'$ is *confidently\nbetter* than $\\hat{b}$, it is set as new *best bandit* $\\hat{b}$ and\nbandit $\\hat{b}$ as well as every other bandit $b$ *worse* than\n$\\hat{b}$ are removed from $W$. This is done, until $W$ is empty,\nleaving the final $\\hat{b}$ as the predicted best bandit.\n\n:::: algorithm\n::: algorithmic\n**input:** $T$, $B=\\{b_1, \\dots, b_k\\}$ $\\delta \\gets 1/(TK^2)$ \nChoose $\\hat{b} \\in B$ randomly\n$W \\gets \\{b_1, \\dots, b_k\\} \\backslash \\{\\hat{b}\\}$ $\\forall b \\in W$,\nmaintain estimate $\\hat{P}_{\\hat{b},b}$ of $P(\\hat{b} > b)$ according to\n(6) $\\forall b \\in W$, maintain $1 - \\delta$ confidence interval\n$\\hat{C}_{\\hat{b},b}$ of $\\hat{P}_{\\hat{b},b}$ according to (7), (8)\ncompare $\\hat{b}$ and $b$ update $\\hat{P}_{\\hat{b},b}$,\n$\\hat{C}_{\\hat{b},b}$ $W \\gets W \\backslash \\{b\\}$\n\n$W \\gets W \\backslash \\{b\\}$ $\\hat{b} \\gets b'$,\n$W \\gets W \\backslash \\{b'\\}$ $\\forall b \\in W$, reset\n$\\hat{P}_{\\hat{b},b}$ and $\\hat{C}_{\\hat{b},b}$ $\\hat{T} \\gets$ Total\nComparisons Made $(\\hat{b}, \\hat{T})$\n:::\n::::\n\nParameter Initialization\n\n:   In lines 1-6 of the algorithm, we take the inputs and first compute\n    the value $\\delta$ which is used to compute our confidence\n    intervals. We select an initial guess of an optimal bandit $\\hat{b}$\n    by uniformly sampling from all bandits $\\mathcal{B}$. We also keep a\n    running set of bandit candidates $W$, which is initialized to be\n    $\\mathcal{B} \\setminus \\{\\hat{b}\\}$. At this point, we also\n    initialize our empirical estimates for $\\hat{P}, \\hat{C}$.\n\n    Next, we will repeat several steps until our working set of bandit\n    candidates $W$ is empty.\n\nUpdate Estimates Based on Comparisons\n\n:   The first step at each iteration (lines 8-11) is to look at all\n    candidates in $W$, and compare them to our current guess $\\hat{b}$\n    using an oracle (e.g. by asking a human which of $\\hat{b}$ or\n    $b \\in W$ is preferred). With this new set of wins and comparisons,\n    we update our estimates of $\\hat{P}, \\hat{C}$.\n\nPrune Suboptimal Bandits\n\n:   In lines 12-13, with updated comparison win probabilities and\n    corresponding confidence intervals, we can remove bandit candidates\n    from $W$ that we are *confident* $\\hat{b}$ is better than. The\n    intuition here is that we are mostly sure that our current best\n    guess is better than some of the candidates, and we don't need to\n    consider those candidates in future iterations.\n\nCheck for Better Bandits from Candidate Set\n\n:   Now that our candidate set of bandits may be smaller, in lines 15-21\n    we check if there are any bandits $b'$ that we are *confident* are\n    better than our current best guess. If we do find such a candidate,\n    we remove bandits which $\\hat{P}$ indicates $b$ is *likely* worse\n    than $\\hat{b}$. Note that in this step, we do not require the\n    probability to be outside the confidence interval, since we already\n    found one we believe to be significantly closer to optimal than our\n    current best guess.\n\n    Once we remove the candidates *likely* worse than $\\hat{b}$, we\n    crown $b'$ as the new best guess, e.g. $\\hat{b} := b'$.\n    Consequently, we remove $b'$ from $W$ and reset our empirical win\n    counters $\\hat{P}, \\hat{C}$.\n\nWith this algorithm defined, let us look at some provisions of the\nmethod with respect to identifying the optimal strategy. Note that the\nproofs and derivations for these quantities are provided in\n[@YUE20121538].\n\nFirst, the method guarantees that for the provided time horizon $T$, the\nalgorithm returns the correct bandit with probability\n$P \\ge 1 - \\frac{1}{T}$. It is interesting and useful to note that if\none has a strict requirement for the probability of identifying the\ncorrect bandit, one can compute the time horizon $T$ that guarantees\nthis outcome at that probability. Furthermore, a time horizon of 1\nleaves no probabilistic guarantee of a successful outcome, and\nincreasing $T$ has diminishing returns. Second, in the event that the\nalgorithm returns an incorrect bandit, the maximal regret incurred is\nlinear with respect to $T$, e.g. $\\mathcal(O)(T)$. This is also a useful\nprovision as it allows us to estimate the overall cost in the worst case\noutcome. Based on these two provisions, we can compute the expected\ncumulative regret from running the Interleaved Filter algorithm, which\nis:\n$$\\mathbb{E}\\left[R_T\\right] \\le \\left(1 - \\frac{1}{T}\\right) \\mathbb{E}\\left[ R_T^{IF} \\right] + \\frac{1}{T}\\mathcal{O}(T) \\\\\n= \\mathcal{O}\\left(\\mathbb{E}\\left[ R_T^{IF} \\right] + 1\\right)$$\n\nInterestingly, the original work shows that these bounds hold for both\nstrong and weak regret. As demonstrated, the Interleaved Filter\nalgorithm [\\[fig-if\\]](#fig-if){reference-type=\"ref\" reference=\"fig-if\"}\nprovides a robust method to ascertain the optimal bandit or strategy\ngiven a set of options and only noisy comparisons. In most real-world\nscenarios for modeling human preferences, it is not possible to observe\na real-world reward value, or at least a reliable one and as such this\nmethod is a useful way to properly model human preferences.\n\nFurthermore, the algorithm provides strong guarantees for the\nprobability of selecting the correct bandit, maximal regret, and the\nnumber of comparisons required. It is even more impressive that the\nmethod can do so without severely limiting constraints; as demonstrated,\nthe most commonly used models satisfy the imposed constraints.\n\nAs we look to model human preferences, we can certainly leverage this\nmethod for k-armed dueling bandits to identify the best strategy to\nsolve human-centric challenges, from video recommendation to meal\nselection and exoskeleton-assisted walking.\n\n#### Dueling Bandit Gradient Descent\n\nThis algorithm tries to find the best bandit in a continuous\nbandit-space. Here, the set of all bandits is regarded as an\nInformation-Retrieval (IR) system with infinite bandits uniquely defined\nby $w$. We will cover the *Dueling Bandit Gradient Descent* algorithm\nfrom Yue and Joachims 2009 [@IR]. Yue and Joachims use the dueling\nbandits formulation for online IR optimization. They propose a retrieval\nsystem parameterized by a set of continuous variables lying in $W$, a\n$d$-dimensional unit-sphere. The DBGD algorithm adapts the current\nparameters $w_t$ of IR system by comparison with slightly altered\nparameters $w_t'$ both querying query $q_t$. Only if the IR outcome\nusing $w_t'$ is preferred, the parameters are changed in their\ndirection. We will now discuss the algorithm more detailed.\n\n:::: algorithm\n::: algorithmic\n**input:** $\\gamma$, $\\delta$, $w_1$ \n\nSample unit vector $u_t$ uniformly\n\n$w_t' \\gets P_W(w_t + \\delta u_t)$ \n\nCompare $w_t$ and $w_t'$\n\n$w_{t+1} \\gets P_W(w_t + \\gamma u_t)$ \n\n$w_{t+1} \\gets w_t$\n:::\n::::\n\nWe first choose exploration step length $\\delta$, exploitation step\nlength $\\gamma$, and starting point (in unit-sphere) $w_1$. Choose a\nquery and sample a random unit vector $u_t$. We duel $w_t$ and $w_t'$,\nwhere $w_t$ is our current point in the sphere, and $w_t'$ is our\nexploratory comparison, which is generated by taking a random step of\nlength $\\delta$, such that $w_t' = w_t + \\delta u_t$. The objective of\nthis duel is to ascertain the binary preference of users with respect to\nthe results yielded by the IR systems parameterized by $w_t$ and $w_t'$\nrespectively, taking query $q_t$ as an input. The parameters that get\nthe majority of the votes in the head to head win. If $w_t$ wins, then\nwe keep the parameters for the next iteration. If $w_t'$ wins the duel,\nwe update our parameters in the direction of $u_t$ by taking a step of\nlength $\\gamma$. Note that the algorithm describes projection operation\n$P_W(\\overrightarrow{v})$. Since $u_t$ is chosen randomly,\n$w_t + \\delta u_t$ or $w_t + \\gamma u_t$ could exist outside of the unit\nsphere where all possible parameter configurations lie. In this case, we\nsimply project the point back onto the sphere using said projection\n$P_W(\\overrightarrow{v})$.\n\nYue and Joachims show that this algorithm has sublinear regret in $T$,\nthe number of iterations. We note that the algorithm assumes that there\nexists a hidden reward function $R(w)$ that maps system parameters $w_t$\nto a reward value which is smooth and strictly concave over the input\nspace $W$.\n\nLastly, we would also like to give motivation behind $\\delta$ and\n$\\gamma$ being different values. We need a $\\delta$ that is sufficiently\nlarge that the comparison between a system parameterized by $w_t$ and\n$w_t'$ is meaningful. On the other hand, we may wish to take a smaller\nstep in the direction of $w_t'$ during our update step, as during a\nduel, we only score $w_t$ against $w_t'$ over the results on one query\n$q_t$. Having $\\delta > \\gamma$ allows us to get reward signal from\nmeaningfully different points while also updating our belief of the best\npoint $w_{\\text{best}}$ gradually.\n\n#### Sparring EXP4 {#sparring-exp4 .unnumbered}\n\nZoghi et al. 2015 propose one algorithm for this problem --- sparring\nEXP4, which duels two traditional EXP4 - algorithms. The (traditional)\nEXP4 algorithm solves the traditional contextual bandits --- the case\nwhere we can directly observe a reward for a choice of bandit given a\ncontext. The EXP4 algorithm embeds each bandit as a vector. When the\nalgorithm sees the context (called 'advice' in this formulation), it\nproduces a probability distribution over the choices based on an\nadjusted softmax function on the inner product between the context and\nthe bandit vectors. The probability function is different from a softmax\nas we assign some minimum probability that any action gets chosen to\nenforce exploration. A reward is then observed for the choice and\npropagated back through the embedding of the chosen bandit.\n\nSparring EXP4 runs two instances of the EXP4 algorithm against each\nother. Each EXP4 instance samples an action given a context, and then\nthese choices are 'dueled' against each other. Instead of directly\nobserving a reward, as for traditional EXP4, we instead observe two\nconverse reward --- a positive reward for the choice that won the duel\nand a negative reward to the choice that lost. The reward is\nproportional to the degree to which the bandit wins the duel, i.e. how\nlikely the bandit is to be preferred over the other when users are\nqueried for binary preferences. Like in traditional EXP4, the reward or\nnegative reward is then propagated back through the representations of\nthe bandits.\n\n#### Feel-good Thompson sampling\n\nThis algorithm is a solution for the contextual dueling bandit setting,\nand tries to minimize cumulative average regret (= find WHAT WINNER?!Von\nNeumann???):\n$$\\text{Regret}(T) := \\sum_{t=1}^{T} \\left[ r_{*}(x_t, a_{t}^{*}) - \\frac{r_{*}(x_t, a_{t}^{1}) + r_{*}(x_t, a_{t}^{2})}{2} \\right],$$\nwhere $r_{*}(x_t, a_{t})$ is the true, hidden reward function of a\ncontext $x_t$ and action $a_t$. Thompson sampling is an iterative\nprocess of receiving preference over two actions, each maximizing a\ndifferent approximation of the reward function based on past data and\nadding this new information to the data.\n\nFinding good approximations of the reward function at time $t$ is done\nby sampling two reward function parameters $\\theta_t^{j=1}$ and\n$\\theta_t^{j=2}$ from a posterior distribution based on all previous\ndata $p_j(\\cdot \\mid S_{t-1})$. This posterior distribution is\nproportional to the multiplication of the prior and the likelihood\nfunction, which is a Gaussian in standard Thompson sampling. In\nFeel-Good Thompson sampling, an additional term called \\\"Feel-good\nexploration\\\" encourages parameters $\\theta$ with a large maximum reward\nin previous rounds. This change to the likelihood function may increase\nprobabilities in uncertain areas, thus exploring those regions. All\nthat's left is to select an action maximizing each reward function\napproximation and receive a preference $y_t$ on one of them to add the\nnew information to the dataset[@fgts_cdb].\n\n:::: algorithm\n::: algorithmic\nInitialize $S_0 = \\varnothing$. Receive prompt $x_t$ and action space\n$\\mathcal{A}_t$. Sample model parameter $\\theta_t^j$ from the posterior\ndistribution $p^j(\\cdot \\mid S_{t-1})$ Select response\n$a_t^j = \\arg\\max_{a \\in \\mathcal{A}_t} \\langle \\theta_t^j, \\phi(x_t, a) \\rangle$.\nReceive preference $y_t$. Update dataset\n$S_t \\leftarrow S_{t-1} \\cup \\{(x_t, a_t^1, a_t^2, y_t)\\}$.\n:::\n::::\n\n### Applications\n\nThere are many applications where contextual bandits are used. Many of\nthese applications can utilize human preferences. One particular\napplication illustrates the benefits a contextual bandit would have over\na multi-armed bandit: a website deciding which app to show someone\nvisiting the website. A multi-armed bandit might decide to show someone\nan ad for a swimsuit because the swimsuit ads have gotten the most user\nclicks (which indicates human preference). A contextual bandit might\nchoose differently, however. A contextual bandit will also take into\naccount the context, which in this case might mean information about the\nuser (location, previously visited pages, and device information). If it\ndiscovers the user lives in a cold environment, for example, it might\nsuggest a sweater ad for the user instead and get a better chance of a\nclick. There are many more examples of where contextual bandits can be\napplied. They can be applied in other web applications, such as to\noptimize search results, medical applications, such as how much of a\nmedication to prescribe based on a patient's history, and gaming\napplications, such as basing moves off of the state of a chess board to\ntry to win. In each of the above examples, human feedback could have\nbeen introduced during training and leveraged to learn a reward\nfunction.\n\nWe explored different versions of bandits that address the\nexploration-exploitation trade-off in various real-world scenarios.\nThese models have been employed across various fields, including but not\nlimited to healthcare, finance, dynamic pricing, and anomaly detection.\nThis section provides a deep dive into some real-world applications,\nemphasizing the value and advancements achieved by incorporating bandit\nmethodologies. The content of this section draws upon the findings from\nthe survey cited in reference [@bouneffouf2020survey].\n\nIn healthcare, researchers have been applying bandits to address\nchallenges in clinical trials and behavioral\nmodeling [@bouneffouf2017bandit; @bastani2020online]. One of the\nexamples is drug dosing. Warfarin, an oral anticoagulant, has\ntraditionally been administered using fixed dosing protocols. Physicians\nwould then make subsequent adjustments based on the patient's emerging\nsymptoms. Nonetheless, inaccuracies in the initial dosage---whether too\nlow or too high---can lead to serious complications like strokes and\ninternal bleeding. In a pivotal study, researchers\nin [@bastani2020online] modeled the Warfarin initial dosing as a\ncontextual bandit problem to assign dosages to individual patients\nappropriately based on their medication history. Their contributions\ninclude the adaptation of the LASSO estimator to the bandit setting,\nachieving a theoretical regret bound of $O({s_0}^2 \\log^2(dT)$, where\n$d$ represents the number of covariates, $s_0 << d$ signifies the number\nof pertinent covariates, and $T$ indicates the total number of users.\nAdditionally, they conducted empirical experiments to validate the\nrobustness of their methodology.\n\nWithin the finance sector, bandits have been instrumental in reshaping\nthe landscape of portfolio optimization. Portfolio optimization is an\napproach to designing a portfolio based on the investor's return and\nrisk criteria, which fits the exploration-exploitation nature of the\nbandit problems. [@shen2015portfolio] utilized multi-armed bandits to\nexploit correlations between the instruments. They constructed\northogonal portfolios and integrated them with the UCB policy to achieve\na cumulative regret bound of $\\frac{8n}{\\Delta*} \\ln(m) + 5n$, where\n$n$, $m$, and $\\Delta*$ denotes the number of available assets, total\ntime steps, and the gap between the best-expected reward and the\nexpected reward. On the other hand, [@huo2017risk] focused on\nrisk-awareness online portfolio optimization by incorporating a compute\nof the minimum spanning tree in the bipartite graph, which encodes a\ncombination of financial institutions and assets that helps diversify\nand reduce exposure to systematic risk during the financial crisis.\n\nDynamic pricing, also known as demand-based pricing, refers to the\nstrategy of setting flexible prices for products or services based on\ncurrent market demands. The application of bandits in dynamic pricing\noffers a systematic approach to making real-time pricing decisions while\nbalancing the trade-off between exploring new price points and\nexploiting known optimal prices. [@misra2019dynamic] proposed a policy\nwhere the company has only incomplete demand information. They derived\nan algorithm that balances immediate and future profits by combining\nmulti-armed bandits with partial identification of consumer demand from\neconomic theory.\n\nare essential components of numerous online platforms, guiding users\nthrough vast content landscapes to deliver tailored suggestions. These\nsystems are instrumental in platforms like e-commerce sites, streaming\nplatforms, and social media networks. However, the challenge of\neffectively recommending items to users is non-trivial, given the\ndynamic nature of user preferences and the vast amount of content\navailable.\n\nOne of the most significant challenges in recommendation systems is the\n\\\"cold start\\\" problem. This issue arises when a new user joins a\nplatform, and the system has limited or no information about the user's\npreferences. Traditional recommendation algorithms struggle in such\nscenarios since they rely on historical user-item interactions. As\ndiscussed in [@zhou2017large], the bandit setting is particularly\nsuitable for large-scale recommender systems with a vast number of\nitems. By continuously exploring user preferences and exploiting known\ninteractions, bandit-based recommender systems can quickly adapt to new\nusers, ensuring relevant recommendations in a few interactions. The\ncontinuous exploration inherent in bandit approaches also means that as\na user's preferences evolve, the system can adapt, ensuring that\nrecommendations remain relevant. Recommending content that is up to date\nis also another important aspect of a recommendation system.\nIn [@bouneffouf2012a], the concept of \\\"freshness\\\" in content is\nexplored through the lens of the bandit problem. The Freshness-Aware\nThompson Sampling algorithm introduced in this study aims to manage the\nrecommendation of fresh documents according to the user's risk of the\nsituation.\n\nDialogue systems, often termed conversational agents or chatbots, aim to\nsimulate human-like conversations with users. These systems are deployed\nacross various platforms, including customer support, virtual\nassistants, and entertainment applications, and they are crucial for\nenhancing user experience and engagement. Response selection is\nfundamental to creating a natural and coherent dialogue flow.\nTraditional dialogue systems rely on a predefined set of responses or\nrules, which can make interactions feel scripted and inauthentic.\nIn [@liu2018customized], the authors proposed a contextual multi-armed\nbandit model for online learning of response selection. Specifically,\nthey utilized bidirectional LSTM to produce the distributed\nrepresentations of a dialogue context and responses and customized the\nThompson sampling method.\n\nTo create a more engaging and dynamic interaction, there's a growing\ninterest in developing pro-active dialogue systems that can initiate\nconversations without user initiation. [@perez2018contextual] proposed a\nnovel approach to this challenge with contextual bandits. By introducing\nmemory models into the bandit framework, the system can recall past\ninteractions, making its proactive responses more contextually relevant.\nTheir contributions include the Contextual Attentive Memory Network,\nwhich implements a differentiable attention mechanism over past\ninteractions.\n\n[@upadhyay2019a] addressed the challenge of orchestrating multiple\nindependently trained dialogue agents or skills in a unified system.\nThey attempted online posterior dialogue orchestration, defining it as\nselecting the most suitable subset of skills in response to a user's\ninput, which studying a context-attentive bandit model that operates\nunder a skill execution budget, ensuring efficient and accurate response\nselection.\n\nAnomaly detection refers to the task of identifying samples that behave\ndifferently from the majority. In [@ding2019interactive], the authors\ndelve into anomaly detection in an interactive setting, allowing the\nsystem to actively engage with human experts through a limited number of\nqueries about genuine anomalies. The goal is to present as many true\nanomalies to the human expert as possible after a fixed query budget is\nused up. They applied the multi-armed contextual bandit framework to\naddress this issue. This algorithm adeptly integrates both nodal\nattributes and node dependencies into a unified model, efficiently\nmanaging the exploration-exploitation trade-off during anomaly queries.\n\nThere are many challenges associated with contextual bandits. The first\nchallenge is that each action only reveals the reward for that\nparticular action. Therefore, the algorithm has to work with incomplete\ninformation. This leads to the dilemma of exploitation versus\nexploration: when should the algorithm choose the best-known option\nversus trying new options for potentially better outcomes? Another\nsignificant challenge for contextual bandits is using context\neffectively. The context the environment gives needs to be explored to\nfigure out which action is best for each context.\n\nThe overarching goal in systems designed for recommending options of\nhigh value to users is to achieve an optimal balance between exploration\nand exploitation. This dual approach is crucial in environments where\nuser preferences and needs are dynamic and diverse. Exploration refers\nto the process of seeking out new options, learning about untried\npossibilities, and gathering fresh information that could lead to\nhigh-value recommendations. In contrast, exploitation involves utilizing\nexisting knowledge and past experiences to recommend the best options\ncurrently known. This balance is key to maintaining a system that\ncontinuously adapts to changing user preferences while ensuring the\nreliability of its recommendations.\n\nA key observation in such systems is the dual role of users as both\nproducers and consumers of information. Each user's experience\ncontributes valuable data that informs future recommendations for\nothers. For instance, platforms like Waze, Netflix, and Trip Advisor\nrely heavily on user input and feedback. Waze uses real-time traffic\ndata from drivers to recommend optimal routes; Netflix suggests movies\nand shows based on viewing histories and ratings; Trip Advisor relies on\ntraveler reviews to guide future tourists. In these examples, the\nbalance between gathering new information (exploration) and recommending\nthe best-known options (exploitation) is dynamically managed to enhance\nuser experience and satisfaction. This approach underscores the\nimportance of user engagement in systems where monetary incentives are\nnot (or can not be) the primary driver.\n\nRecommendation systems often face the challenge of overcoming user\nbiases that can lead to a narrow exploration of options. Users come with\npreconceived notions and preferences, which can cause them to overlook\npotentially valuable options that initially appear inferior or unaligned\nwith their interests. This predisposition can significantly limit the\neffectiveness of recommendation systems, as users might miss out on\nhigh-value choices simply due to their existing biases.\n\nTo counteract this, it is crucial for recommendation systems to actively\nincentivize exploration among users. One innovative approach to achieve\nthis is through the strategic use of **information asymmetry**. By\ncontrolling and selectively presenting information, these systems can\nguide users to explore options they might not typically consider. This\nmethod aims to reveal the true potential of various options by nudging\nusers out of their comfort zones and encouraging a broader exploration\nof available choices. An important note here is that the system is not\nlying to users - it only selectively reveals information it has.\n\nThe concept of incentivizing exploration becomes even more complex when\nconsidering different types of users. For instance, systems often\nencounter short-lived users who have little to gain from contributing to\nthe system's learning process, as their interactions are infrequent or\nbased on immediate needs. Similarly, some users may operate under a\n'greedy' principle, primarily seeking immediate gratification rather\nthan contributing to the long-term accuracy and effectiveness of the\nsystem. In such scenarios, managing information asymmetry can be a\npowerful tool. By selectively revealing information, recommendation\nsystems can create a sense of novelty and interest, prompting even the\nmost transient or self-interested users to engage in exploration,\nthereby enhancing the system's overall knowledge base and recommendation\nquality.\n\n## Preferential Bayesian Optimization\n\nThe traditional Bayesian optimization (BO) problem is described as\nfollows. There is a black-box objective function\n$g: \\mathcal{X} \\rightarrow \\Re$ defined on a bounded subset\n$\\mathcal{X} \\subseteq \\Re^q$ such that direct queries to the function\nare expensive or not possible. However, we would like to solve the\nglobal optimization problem of finding\n$\\mathbf{x}_{\\min }=\\arg \\min _{\\mathbf{x} \\in \\mathcal{X}} g(\\mathbf{x})$.\nThis is highly analogous to modeling human preferences, since it is the\ncase that direct access to a human's latent preference function is not\npossible but we would still like to find its optimum, such as in A/B\ntests or recommender systems.\n\nWe approach this problem for human preferences with *Preferential\nBayesian Optimization* (PBO), as the key difference is that we are able\nto query the preference function through pairwise comparisons of data\npoints, i.e. *duels*. This is a form of indirect observation of the\nobjective function, which models real-world scenarios closely: we\ncommonly need to to optimize a function via data about preferences. With\nhumans, it has been demonstrated that we are better at evaluating\ndifferences rather than absolute magnitudes [@kahneman_tversky_1979] and\ntherefore PBO models can be applied in various contexts.\n\n### Problem statement\n\nThe problem of finding the optimum of a latent preference function\ndefined on $\\mathcal{X}$ can be reduced to determining a sequence of\nduels on $\\mathcal{X} \\times \\mathcal{X}$. From each duel\n$\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\in$\n$\\mathcal{X} \\times \\mathcal{X}$ we obtain binary feedback $\\{0,1\\}$\nindicating whether or not $\\mathbf{x}$ is preferred over\n$\\mathbf{x}^{\\prime}$ ($g(\\mathbf{x}) < g(\\mathbf{x}^{\\prime})$). We\nconsider that $\\mathbf{x}$ is the winner of the duel if the output is\n$\\{1\\}$ and that $\\mathbf{x}^{\\prime}$ wins the duel if the output is\n$\\{0\\}$. The aim is to find $\\mathbf{x}_{\\min }$ by reducing as much as\npossible the number of queried duels.\n\nThe key idea in PBO is to learn a preference function in the space of\nduels using a Gaussian process. We define a joint reward\n$f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)$ on each\nduel which is never directly observed. Instead, the feedback we obtain\nafter each pair is a binary output $y \\in$ $\\{0,1\\}$ indicating which of\nthe two inputs is preferred. One definition of f we will use (though\nothers are possible) is\n$f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})$.\nThe more $\\mathbf{x}^{\\prime}$ is preferred over $\\mathbf{x}$, the\nbigger the reward.\n\nWe define the model of preference using a Bernoulli likelihood, where\n$p\\left(y=1 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)$\nand\n$p\\left(y=0 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)$\nfor some inverse link function $\\pi: \\Re \\times \\Re \\rightarrow[0,1]$.\n$\\pi_f$ has the property that\n$\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)=1-\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)$.\nA natural choice for $\\pi_f$ is the logistic function\n$$\\label{eq:bernoulli_pref}\n\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{1}{1+e^{-f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)}},$$\nbut others are possible. Therefore we have that for any duel\n$\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]$ in which\n$g(\\mathbf{x}) \\leq g\\left(\\mathbf{x}^{\\prime}\\right)$ it holds that\n$\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5$.\n$\\pi_f$ is a preference function that maps each query\n$\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]$ to the probability of\nhaving a preference on the left input $\\mathbf{x}$ over the right input\n$\\mathbf{x}^{\\prime}$.\n\nWhen we marginalize over the right input $\\mathbf{x}^{\\prime}$ of $f$\n(is this correct?), the global minimum of $f$ in $\\mathcal{X}$ coincides\nwith $\\mathbf{x}_{\\min }$. We also introduce the definition of the\n*Copeland score function* for a point $\\mathbf{x}$ as\n$$S(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\mathbb{I}_{\\left\\{\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5\\right\\}} d \\mathbf{x}^{\\prime}$$\nwhere\n$\\operatorname{Vol}(\\mathcal{X})=\\int_{\\mathcal{X}} d \\mathbf{x}^{\\prime}$\nis a normalizing constant that bounds $S(\\mathbf{x})$ in the interval\n$[0,1]$. If $\\mathcal{X}$ is a finite set, the Copeland score is simply\nthe proportion of duels that a certain element $\\mathbf{x}$ will win\nwith probability larger than 0.5. A soft variant we will use instead of\nthe Copeland score is the *soft-Copeland score*, defined as\n$$\\label{eq:soft-copeland}\nC(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d \\mathbf{x}^{\\prime}$$\nwhere the probability function $\\pi_f$ is integrated over $\\mathcal{X}$.\nThis score aims to capture the average probability of $\\mathbf{x}$ being\nthe winner of a duel.\n\nWe define the *Condorcet winner* $\\mathbf{x}_c$ as the point with\nmaximal soft-Copeland score. Note that this corresponds to the global\nminimum of $f$, since the defining integral takes maximum value for\npoints $\\mathbf{x} \\in \\mathcal{X}$ where\n$f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=$\n$g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})>0$ or all\n$\\mathbf{x}^{\\prime}$, occurring only if $\\mathbf{x}_c$ is a minimum of\n$f$. Therefore, if the preference function $\\pi_f$ can be learned by\nobserving the results of duels then our optimization problem of finding\nthe minimum of $f$ can be solved by finding the Condorcet winner of the\nCopeland score.\n\n### Acquisition Functions\n\nWe describe several acquisition functions for sequential learning of the\nCondorcet winner. Our dataset\n$\\mathcal{D}=\\left\\{\\left[\\mathbf{x}_i, \\mathbf{x}_i^{\\prime}\\right], y_i\\right\\}_{i=1}^N$\nrepresents the $N$ duels that have been performed so far. We aim to\ndefine a sequential policy\n$\\alpha\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] ; \\mathcal{D}_j, \\theta\\right)$\nfor querying duels, where $\\theta$ is a vector of model\nhyper-parameters, in order to find the minimum of the latent function\n$g$ as quickly as possible. Using Gaussian processes (GP) for\nclassification with our dataset $\\mathcal{D}$ allows us to perform\ninference over $f$ and $\\pi_f$.\n\n#### Pure Exploration {#pure-exploration .unnumbered}\n\nThe output variable $y_{\\star}$ of a prediction follows a Bernoulli\ndistribution with probability given by the preference function $\\pi_f$.\nTo carry out exploration as a policy, one method is to search for the\nduel where GP is most uncertain about the probability of the outcome\n(has the highest variance of $\\sigma\\left(f_{\\star}\\right)$ ), which is\nthe result of transforming out epistemic uncertainty about $f$, modeled\nby a GP, through the logistic function. The first order moment of this\ndistribution coincides with the expectation of $y_{\\star}$ but its\nvariance is $$\\begin{aligned}\n\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\right] & =\\int\\left(\\sigma\\left(f_{\\star}\\right)-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star} \\\\\n& =\\int \\sigma\\left(f_{\\star}\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star}-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]^2\n\\end{aligned}$$ which explicitly takes into account the uncertainty over\n$f$. Hence, pure exploration of duels space can be carried out by\nmaximizing\n$$\\alpha_{\\mathrm{PE}}\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\mid \\mathcal{D}_j\\right)=\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\left|\\left[\\mathbf{x}_{\\star}, \\mathbf{x}_{\\star}^{\\prime}\\right]\\right| \\mathcal{D}_j\\right] .$$\n\nNote that in this case, duels that have been already visited will have a\nlower chance of being visited again even in cases in which the objective\ntakes similar values in both players. In practice, this acquisition\nfunctions requires computation of an intractable integral, that we\napproximate using Monte-Carlo.\n\n#### Principled Optimistic Preferential Bayesian Optimization (POP-BO) {#principled-optimistic-preferential-bayesian-optimization-pop-bo .unnumbered}\n\nIn a slightly modified problem setup\n[@xu2024principledpreferentialbayesianoptimization], the algorithm tries\nto solve for the MLE $\\hat{g}$ and its confidence set $\\mathcal{B}_g$\nwhere $g$ is the ground truth black-box function. Assumptions include\nthat $g$ is a member of a reproducing kernel Hilbert space (RKHS)\n$\\mathcal{H}_k$ for some kernel function\n$k: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$, and\n$\\|g\\|_k \\leq B$ so that\n$\\mathcal{B}_g = \\left\\{\\tilde{g} \\in \\mathcal{H}_k \\mid\\|\\tilde{g}\\|_k \\leq B\\right\\}$.\nSimilarly defining\n$f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})$,\nwe model the preference function with a Bernoulli distribution as in\nEquation\n[\\[eq:bernoulli_pref\\]](#eq:bernoulli_pref){reference-type=\"ref\"\nreference=\"eq:bernoulli_pref\"} and also assume that probabilities follow\nthe Bradley-Terry model, i.e.\n$$\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{e^{g(\\mathbf{x})}}{e^{g(\\mathbf{x})}+e^{g\\left(\\mathbf{x^{\\prime}}\\right)}}$$\n\nThe update rule for MLE $\\hat{g}$ is (equation 8,6,5) $$\\begin{aligned}\n\\hat{g}_t^{\\text {MLE }}&:= \\arg \\underset{\\tilde{g} \\in \\mathcal{B}^t_g}{\\max}\\ell_t(\\tilde{g}) \\\\\n\\ell_t(\\tilde{g}) &:= \\log \\prod_{\\tau=1}^t y_\\tau \\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])+\\left(1-y_\\tau\\right)\\left(1-\\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])\\right) \\\\\n&=\\sum_{\\tau=1}^t \\log \\left(\\frac{e^{\\tilde{g}(\\mathbf{x_\\tau})} y_\\tau+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\left(1-y_\\tau\\right)}{e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}}\\right) \\\\\n&=\\sum_{\\tau=1}^t\\left(\\tilde{g}(\\mathbf{x_\\tau}) y_\\tau+\\tilde{g}(\\mathbf{x_\\tau^\\prime})\\left(1-y_\\tau\\right)\\right)-\\sum_{\\tau=1}^t \\log \\left(e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\right)\n\\end{aligned}$$\n\n(Eq 22 shows how to represent this as a convex optimisation problem so\nthat it can be solved)\n\nThe update rule for the confidence set $\\mathcal{B}_f^{t+1}$ is, (eq 9,\n10?)\n\n$$\\begin{aligned}\n&\\forall \\epsilon, \\delta > 0 \\\\\n&\\mathcal{B}_g^{t+1}:=\\left\\{\\tilde{g} \\in \\mathcal{B}_g \\mid \\ell_t(\\tilde{g}) \\geq \\ell_t\\left(\\hat{g}_t^{\\mathrm{MLE}}\\right)-\\beta_1(\\epsilon, \\delta, t)\\right\\}\n\\end{aligned}$$ where\n$$\\beta_1(\\epsilon, \\delta, t):=\\sqrt{32 t B^2 \\log \\frac{\\pi^2 t^2 \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{6 \\delta}}+ C_L \\epsilon t=\\mathcal{O}\\left(\\sqrt{t \\log \\frac{t \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}+\\epsilon t\\right),$$\nwith $C_L$ a constant independent of $\\delta, t$ and $\\epsilon$.\n$\\epsilon$ is typically chosen to be $1 / T$, where T is the running\nhorizon of the algorithm. This satisfies the theorem that,\n$$\\mathbb{P}\\left(g \\in \\mathcal{B}_g^{t+1}, \\forall t \\geq 1\\right) \\geq 1-\\delta .$$\n\nIntuitively, the confidence set $\\mathcal{B}_g^{t+1}$ includes the\nfunctions with the log-likelihood value that is only 'a little worse'\nthan the maximum likelihood estimator, and the theorem states that\n$\\mathcal{B}_g^{t+1}$ contains the ground-truth function $g$ with high\nprobability.\n\nInner level optimization in Line 4 of the algorithm can also be\nrepresented as a convex optimisation problem so that it can be solved,\nEq 24, 25. The outer optimisation can be solved using grid search or Eq\n26 for medium size problems.\n\n:::: algorithm\n::: algorithmic\nGiven the initial point $\\mathbf{x_0} \\in \\mathcal{X}$ and set\n$\\mathcal{B}_g^1 = \\mathcal{B}_g$ Set the reference point\n$\\mathbf{x_t^{\\prime}} = \\mathbf{x_{t-1}}$ Compute\n$\\mathbf{x_t} \\in \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\tilde{g} \\in \\mathcal{B}_g^t} (\\tilde{g}(\\mathbf{x}) - \\tilde{g}(\\mathbf{x_t^{\\prime}}))$,\nwith the inner optimal function denoted as $\\tilde{g}_t$ Obtain the\noutput of the duel $y_t$ and append the new data point to\n$\\mathcal{D}_t$ Update the maximum likelihood estimator\n$\\hat{g}_t^{\\mathrm{MLE}}$ and the posterior confidence set\n$\\mathcal{B}_g^{t+1}$.\n:::\n::::\n\n#### qEUBO: Decision-Theoretic EUBO {#qeubo-decision-theoretic-eubo .unnumbered}\n\nqEUBO [@astudillo2023qeubodecisiontheoreticacquisitionfunction] derives\nan acquisition function that extends duels to $q>2$ options which we\ncall *queries*. Let\n$X=\\left(\\mathbf{x_1}, \\ldots, \\mathbf{x_q}\\right) \\in \\mathcal{X}^q$\ndenote a query containing two points or more, and let\n$g: \\mathcal{X} \\rightarrow \\Re$ be the latent preference function. Then\nafter $n$ user queries, we define the *expected utility of the best\noption* (qEUBO) as\n$$\\mathrm{qEUBO}_n(X)=\\mathbb{E}_n\\left[\\max \\left\\{g\\left(x_1\\right), \\ldots, g\\left(x_q\\right)\\right\\}\\right].$$\n\nWe now show that qEUBO is one-step Bayes optimal, meaning that each step\nchooses the query that maximises the expected utility received by the\nhuman. For a query $X \\in \\mathcal{X}^q$, let\n$$V_n(X)=\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)] \\mid X_{n+1}=X\\right] .$$\nThen $V_n$ defines the expected utility received if an additional query\n$X_{n+1}=X$ is performed, and maximizing $V_n$ is one-step Bayes\noptimal. Since $\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x)]$ does not\ndepend on $X_{n+1}$, we can also equivalently maximize\n$$\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)]-\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x)] \\mid X_{n+1}=X\\right],$$\nwhich takes the same form as the knowledge gradient acquisition function\n[@wu2018parallelknowledgegradientmethod] in standard Bayesian\noptimization.\n\n$V_n$ involves a nested stochastic optimization task, while qEUBO is a\nmuch simpler policy. When human responses are noise-free, we are able to\nuse qEUBO as a sufficient policy due to the following theorem:\n\n::: theorem\n$$\\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} \\mathrm{qEUBO}_n(X) \\subseteq \\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} V_n(X) .$$\n:::\n\n::: proof\n*Proof.* For a query $X \\in \\mathcal{X}^q$, let\n$x^{+}(X, i) \\in \\operatorname{argmax}_{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x) \\mid(X, i)]$\nand define $X^{+}(X)=$ $\\left(x^{+}(X, 1), \\ldots, x^{+}(X, q)\\right)$.\n\n**Claim 1** $V_n(X) \\leq \\mathrm{qEUBO}_n\\left(X^{+}(X)\\right) .$ We see\nthat $$\\begin{aligned}\nV_n(X) & =\\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[g\\left(x^{+}(X, i)\\right) ] \\\\\n& \\leq \\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[\\max _{i=1, \\ldots, q} g(x^{+}(X, i))] \\\\\n& =\\mathbb{E}_n\\left[\\max _{i=1, \\ldots, q} g\\left(x^{+}(X, i)\\right)\\right] \\\\\n& =\\mathrm{qEUBO}_n\\left(X^{+}(X)\\right),\n\\end{aligned}$$ as claimed.\n\n**Claim 2** $\\mathrm{qEUBO}_n(X) \\leq V_n(X) .$ For any given\n$X \\in \\mathbb{X}^q$ we have\n$$\\mathbb{E}_n\\left[f\\left(x_{r(X)}\\right) \\mid(X, r(X))\\right] \\leq \\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x) \\mid(X, r(X))] .$$\nSince $f\\left(x_{r(X)}\\right)=\\max _{i=1, \\ldots, q} f\\left(x_i\\right)$,\ntaking expectations over $r(X)$ on both sides obtains the required\nresult.\n\nNow building on the arguments above, let\n$X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)$\nand suppose for contradiction that\n$X^* \\notin \\operatorname{argmax}_{X \\in \\mathbb{X}^q} V_n(X)$. Then,\nthere exists $\\widetilde{X} \\in \\mathbb{X}^q$ such that\n$V_n(\\widetilde{X})>V_n\\left(X^*\\right)$. We have $$\\begin{aligned}\n\\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) & \\geq V_n(\\tilde{X}) \\\\\n& >V_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) .\n\\end{aligned}$$\n\nThe first inequality follows from (1). The second inequality is due to\nour supposition for contradiction. The third inequality is due to (2).\nFinally, the fourth inequality holds since\n$X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)$.\nThis contradiction concludes the proof. \n:::\n\nTherefore a sufficient condition for following one-step Bayes optimality\nis by maximizing $\\text{qEUBO}_n$.\n\nIn experiments that were ran comparing qEUBO to other state-of-the-art\nacquisition functions, qEUBO consistently outperformed on most problems\nand was closely followed by qEI and qTS. These results also extended to\nexperiments with multiple options when $q>2$. In fact, there is faster\nconvergence in regret when using more options in human queries. \\[Prove\nTheorem 3: Regret analysis\\]\n\n#### qEI: Batch Expected Improvement {#qei-batch-expected-improvement .unnumbered}\n\n$$\\begin{aligned}\n\\mathrm{qEI}= & \\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\max _{i \\in[1, \\ldots, q]}\\left(\\mu_{\\min }-y_i\\right)\\right)_{+}\\right] \\\\\n= & \\sum_{i=1}^q \\mathbb{E}_{\\mathbf{y}}\\left(\\mu_{\\min }-y_i \\mid y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) \\\\\n& p\\left(y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) .\n\\end{aligned}$$\n\n#### qTS: Batch Thompson Sampling {#qts-batch-thompson-sampling .unnumbered}\n\n:::: algorithm\n::: algorithmic\nInitial data\n$\\mathcal{D}_{\\mathcal{I}(1)}=\\{(\\mathbf{x}_i, y_i)\\}_{i \\in \\mathcal{I}(1)}$\nCompute current posterior\n$p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})$ Sample\n$\\boldsymbol{\\theta}$ from\n$p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})$ Select\n$k \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]$\nCollect $y_k$ by evaluating $f$ at $\\mathbf{x}_k$\n$\\mathcal{D}_{\\mathcal{I}(t+1)} \\leftarrow \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{(\\mathbf{x}_k, y_k)\\}$\n:::\n::::\n\n:::: algorithm\n::: algorithmic\nInitial data\n$\\mathcal{D}_{\\mathcal{I}(1)}=\\{\\mathbf{x}_i, y_i\\}_{i \\in \\mathcal{I}(1)}$,\nbatch size $S$ Compute current posterior\n$p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})$ Sample\n$\\boldsymbol{\\theta}$ from\n$p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})$ Select\n$k(s) \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]$\n$\\mathcal{D}_{\\mathcal{I}(t+1)} = \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{\\mathbf{x}_{k(s)}, y_{k(s)}\\}_{s=1}^S$\n:::\n::::\n\n### Regret Analysis\n\n#### qEUBO Regret {#qeubo-regret .unnumbered}\n\nWith the definition of Bayesian simple regret, we have that qEUBO\nconverges to zero at a rate of $o(1/n)$, i.e.\n\n::: theorem\n$$\\label{th:quebo_regret}\n\\mathbb{E}\\left[f\\left(x^*\\right)-f\\left(\\widehat{x}_n^*\\right)\\right]=o(1 / n)$$\n:::\n\nwhere $x^*=\\operatorname{argmax}_{x \\in \\mathrm{X}} f(x)$ and\n$\\widehat{x}_n^* \\in \\operatorname{argmax}_{x \\in \\mathrm{X}} \\mathbb{E}_n[f(x)]$.\n\nThis theorem holds under the following assumptions:\n\n1.  **$f$ is injective** $\\mathbf{P}(f(x)=f(y))=0$ for any\n    $x, y \\in \\mathbb{X}$ with $x \\neq y$.\n\n2.  **$f$ represents the preferred option** $\\exists a>1 / 2$ s.t.\n    $\\mathbf{P}\\left(r(X) \\in \\operatorname{argmax}_{i=1, \\ldots, 2} f\\left(x_i\\right) \\mid f(X)\\right) \\geq a \\forall$\n    $X=\\left(x_1, x_2\\right) \\in \\mathbb{X}^2$ with $x_1 \\neq x_2$\n    almost surely under the prior on $f$.\n\n3.  **Expected difference in utility is proportional to probability of\n    greater utility** $\\exists \\Delta \\geq \\delta>0$ s.t.\n    $\\forall \\mathcal{D}^{(n)} \\text{and} \\forall x, y \\in \\mathbb{X}$\n    (potentially depending on $\\mathcal{D}^{(n)}$),\n    $$\\delta \\mathbf{P}^{(n)}(f(x)>f(y)) \\leq \\mathbb{E}^{(n)}\\left[\\{f(x)-f(y)\\}^{+}\\right] \\leq \\Delta \\mathbf{P}^{(n)}(f(x)>f(y))$$\n    almost surely under the prior on $f$.\n\nFurther lemmas leading to a proof of Theorem\n[\\[th:quebo_regret\\]](#th:quebo_regret){reference-type=\"ref\"\nreference=\"th:quebo_regret\"} is given in\n[@astudillo2023qeubodecisiontheoreticacquisitionfunction] Section B.\n\n#### qEI Regret {#qei-regret .unnumbered}\n\nThe following theorem shows that, under the same assumptions used for\nqEUBO regret, simple regret of qEI can fail to converge to 0.\n\n::: theorem\nThere exists a problem instance (i.e., $\\mathbb{X}$ and Bayesian prior\ndistribution over f) satisfying the assumptions described in Theorem\n[\\[th:quebo_regret\\]](#th:quebo_regret){reference-type=\"ref\"\nreference=\"th:quebo_regret\"} such that if the sequence of queries is\nchosen by maximizing qEI, then\n$\\mathbb{E}\\left[f\\left(x^*\\right)-\\right.$\n$\\left.f\\left(\\widehat{x}_n^*\\right)\\right] \\geq R$ for all $n$, for a\nconstant $R>0$.\n:::\n\n::: proof\n*Proof.* Let $X = \\{1, 2, 3, 4\\}$ and consider the functions\n$f_i:X \\rightarrow R$, for $i=1,2,3,4$, given by $f_i(1) = -1$ and\n$f_i(2) = 0$ for all $i$, and $$\\begin{aligned}\n    f_1(x) = \\begin{cases}\n    1, &\\ x=3\\\\\n    \\frac{1}{2}, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_2(x) = \\begin{cases}\n    \\frac{1}{2}, &\\ x=3\\\\\n    1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_3(x) = \\begin{cases}\n    -\\frac{1}{2}, &\\ x=3\\\\\n    -1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_4(x) = \\begin{cases}\n    -1, &\\ x=3\\\\\n    -\\frac{1}{2}, &\\ x=4\n    \\end{cases}.\n\\end{aligned}$$\n\nLet $p$ be a number with $0 < p < 1/3$ and set $q=1-p$. We consider a\nprior distribution on $f$ with support $\\{f_i\\}_{i=1}^4$ such that\n$$\\begin{aligned}\np_i = Pr(f=f_i) = \n    \\begin{cases}\n        p/2, i =1,2,\\\\\n        q/2, i=3,4.\n    \\end{cases}\n\\end{aligned}$$ We also assume the user's response likelihood is given\nby $Pr(r(X)=1\\mid f(x_1) > f(x_2)) = a$ for some $a$ such that\n$1/2 < a < 1$,\n\nLet $D^{(n)}$ denote the set of observations up to time $n$ and let\n$p_i^{(n)} = Pr(f=f_i \\mid \\mathbb{E}^{(n)})$ for $i=1,2,3,4$. We let the\ninitial data set be $\\mathcal{D}^{(0)} = \\{(X^{(0)}, r^{(0)})\\}$, where\n$X^{(0)}= (1,2)$. We will prove that the following statements are true\nfor all $n\\geq 0$.\n\n1.  $p_i^{(n)} > 0$ for $i=1,2,3,4$.\n\n2.  $p_1^{(n)} < \\frac{1}{2}p_3^{(n)}$ and\n    $p_2^{(n)} < \\frac{1}{2}p_4^{(n)}$.\n\n3.  $\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}$.\n\n4.  $\\arg \\max_{X\\in\\mathcal{X}^2}\\text{qEI}^{(n)}(X) = \\{(3, 4)\\}$.\n\nWe prove this by induction over $n$. We begin by proving this for $n=0$.\nSince $f_i(1) < f_i(2)$ for all $i$, the posterior distribution on $f$\ngiven $\\mathcal{D}^{(0)}$ remains the same as the prior; i.e., $p_i^{(0)} = p_i$\nfor $i=1,2,3,4$. Using this, statements 1 and 2 can be easily verified.\nNow note that $\\mathbb{E}^{(0)}[f(1)]=-1$, $\\mathbb{E}^{(0)}[f(2)]=0$, and\n$\\mathbb{E}^{(0)}[f(3)] = \\mathbb{E}^{(0)}[f(4)] = \\frac{3}{2}(p - q)$. Since $p < q$,\nit follows that $\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}$; i.e., statement\n3 holds. Finally, since $\\max_{x\\in\\{1,2\\}}\\mathbb{E}^{(0)}[f(x)] = 0$, the qEI\nacquisition function at time $n=0$ is given by\n$\\text{qEI}^{(0)}(X) = \\mathbb{E}^{(0)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]$. A direct\ncalculation can now be performed to verify that statement 4 holds. This\ncompletes the base case.\n\nNow suppose statements 1-4 hold for some $n\\geq 0$. Since\n$X^{(n+1)} = (3, 4)$, the posterior distribution on $f$ given\n$D^{(n+1)}$ is given by $$\\begin{aligned}\np_i^{(n+1)} \\propto \\begin{cases}\n                        p_i^{(n)}\\ell, \\ i=1,3,\\\\\n                         p_i^{(n)} (1 - \\ell), \\ i=2,4,\n                        \\end{cases}\n\\end{aligned}$$ where\n$$\\ell = a I\\{r^{(n+1)} = 1\\} + (1-a)I\\{r^{(n+1)} = 2\\}.$$ Observe that\n$0< \\ell < 1$ since $0 < a < 1$. Thus, $\\ell > 0$ and $1-\\ell > 0$.\nSince $p_i^{(n)} > 0$ by the induction hypothesis, it follows from this\nthat $p_i^{(n+1)} > 0$ for $i=1,2,3,4$. Moreover, since\n$p_i^{(n+1)} \\propto p_i^{(n)}\\ell$ for $i=1,3$ and\n$p_1^{(n)} < \\frac{1}{2}p_3^{(n)}$ by the induction hypothesis, it\nfollows that $p_1^{(n+1)} < \\frac{1}{2}p_3^{(n+1)}$. Similarly,\n$p_2^{(n+1)} < \\frac{1}{2}p_4^{(n+1)}$. Thus, statements 1 and 2 hold at\ntime $n+1$.\n\nNow observe that $$\\begin{aligned}\n    \\mathbb{E}^{(n+1)}[f(3)] &= p_1^{(n+1)} + \\frac{1}{2}p_2^{(n+1)} - \\frac{1}{2}p_3^{(n+1)} - p_4^{(n+1)}\\\\\n    &= \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(\\frac{1}{2}p_2^{(n+1)} - p_4^{(n+1)}\\right)\\\\\n    &\\leq \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(p_2^{(n+1)} - \\frac{1}{2}p_4^{(n+1)}\\right)\\\\\n    &\\leq 0,\n\\end{aligned}$$ where the last inequality holds since\n$p_1^{(n+1)} < \\frac{1}{2}p_3^{(n+1)}$ and\n$p_2^{(n+1)} < \\frac{1}{2}p_4^{(n+1)}$. Similarly, we see that\n$\\mathbb{E}^{(n+1)}[f(4)] \\leq 0$. Since $\\mathbb{E}^{(n+1)}[f(1)]=-1$ and\n$\\mathbb{E}^{(n+1)}[f(2)]=0$, it follows that\n$\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n+1)}[f(x)]=\\{2\\}$; i.e., statement 3 holds at\ntime $n+1$.\n\nSince $\\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(0)}[f(x)] = 0$, the qEI acquisition function at\ntime $n+1$ is given by\n$\\text{qEI}^{(n+1)}(X) = \\mathbb{E}^{(n+1)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]$. Since\n$f(1) \\leq f(x)$ almost surely under the prior for all $x\\in\\mathcal{X}$, there\nis always a maximizer of qEI that does not contain $1$. Thus, to find\nthe maximizer of qEI, it suffices to analyse its value at the pairs\n$(2, 3)$, $(3,4)$ and $(4,2)$. We have\n$$\\text{qEI}^{(n+1)}(2, 3) = p_1^{(n+1)} + 1/2 p_2^{(n+1)},$$\n$$\\operatorname{qEI}^{(n+1)}(3, 4) = p_1^{(n+1)} + p_2^{(n+1)}$$ and\n$$\\operatorname{qEI}^{(n+1)}(4, 2) = 1/2p_1^{(n+1)} + p_2^{(n+1)}.$$\nSince $p_1^{(n+1)} > 0$ and $p_2^{(n+1)} > 0$, it follows that\n$\\arg \\max_{X \\in X^2}\\text{qEI}^{(n+1)}(X) = \\{(3, 4)\\}$, which concludes\nthe proof by induction.\n\nFinally, since $\\arg \\max_{x\\in X}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}$ for all $n$, the\nBayesian simple regret of qEI is given by $$\\begin{aligned}\n    \\mathbb{E}\\left[f(x^*) - f(2)\\right] &= \\sum_{i=1}p_i\\left(\\max_{x\\in X}f_i(x) - f_i(2)\\right)\\\\\n    &= p\n\\end{aligned}$$ for all $n$. \n:::\n\n#### POP-BO Regret {#pop-bo-regret .unnumbered}\n\nCommonly used kernel functions within the RKHS are:\n\n1.  Linear: $$k(x, \\bar{x})=x^{\\top} \\bar{x} .$$\n\n2.  Squared Exponential (SE):\n    $$k(x, \\bar{x})=\\sigma_{\\mathrm{SE}}^2 \\exp \\left\\{-\\frac{\\|x-\\bar{x}\\|^2}{l^2}\\right\\},$$\n    where $\\sigma_{\\mathrm{SE}}^2$ is the variance parameter and $l$ is\n    the lengthscale parameter.\n\n3.  Matrn:\n    $$k(x, \\bar{x})=\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right)^\\nu K_\\nu\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right),$$\n    where $\\rho$ and $\\nu$ are the two positive parameters of the kernel\n    function, $\\Gamma$ is the gamma function, and $K_\\nu$ is the\n    modified Bessel function of the second kind. $\\nu$ captures the\n    smoothness of the kernel function.\n\nWith the definition of Bayesian simple regret, we have the following\ntheorem defining the regret bound:\n\n::: theorem\nWith probability at least $1-\\delta$, the cumulative regret of POP-BO\nsatisfies,\n$$R_T=\\mathcal{O}\\left(\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}} T}\\right),$$\nwhere\n$$\\beta_T=\\beta(1 / T, \\delta, T)=\\mathcal{O}\\left(\\sqrt{T \\log \\frac{T \\mathcal{N}\\left(\\mathcal{B}_f, 1 / T,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}\\right).$$\n:::\n\nThe guaranteed convergence rate is characterised as:\n\n::: theorem\n[]{#th: popbo_converge label=\"th: popbo_converge\"} Let $t^{\\star}$ be\ndefined as in Eq. (19). With probability at least $1-\\delta$,\n$$f\\left(x^{\\star}\\right)-f\\left(x_{t^{\\star}}\\right) \\leq \\mathcal{O}\\left(\\frac{\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}}}}{\\sqrt{T}}\\right)$$\n:::\n\nTheorem\n[\\[th: popbo_converge\\]](#th: popbo_converge){reference-type=\"ref\"\nreference=\"th: popbo_converge\"} highlights that by minimizing the known\nterm\n$2\\left(2 B+\\lambda^{-1 / 2} \\sqrt{\\beta\\left(\\epsilon, \\frac{\\delta}{2}, t\\right)}\\right) \\sigma_t^{f f^{\\prime}}\\left(\\left(x_t, x_t^{\\prime}\\right)\\right)$,\nthe reported final solution $x_{t^{\\star}}$ has a guaranteed convergence\nrate.\n\nFurther kernel-specific regret bounds for POP-BO are calculated as\nfollows:\n\n::: theorem\nSetting $\\epsilon=1 / T$ and running our POP-BO algorithm in Alg. 1,\n\n1.  If $k(x, y)=\\langle x, y\\rangle$, we have,\n    $$R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4}\\right) .$$\n\n2.  If $k(x, y)$ is a squared exponential kernel, we have,\n    $$R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4(d+1)}\\right) .$$\n\n3.  If $k(x, y)$ is a Matrn kernel, we have,\n    $$\\left.R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4} T^{\\frac{d}{\\nu}\\left(\\frac{1}{4}+\\frac{d+1}{4+2(d+1)^d / \\nu}\\right.}\\right)\\right).$$\n:::\n\n\n## Exercises\n### Question 1: Preferential Bayesian Optimization (30 points) {#sec-question-1-preferential-bayesian-optimization-30-points .unnumbered}\n\n**Preferential Bayesian Optimization (PBO)** is a variant of Bayesian\nOptimization (BO) designed to handle scenarios where feedback is\nprovided in terms of preferences between alternatives rather than\nexplicit numeric evaluations. Suppose you are optimizing an unknown\nfunction $f$ over a space $\\mathcal{X}$, but instead of receiving\nfunction values, you only receive pairwise comparisons between different\npoints in the input space. That is, given two points\n$x_1, x_2 \\in \\mathcal{X}$, you receive feedback in the form of a\npreference: $x_1 \\succ x_2$ implies $f(x_1) > f(x_2)$.\n\nThe **Gaussian Process (GP)** framework is used to model $f$, and the\noptimization is guided by this model. Let $p(x_1 \\succ x_2 | f)$ be the\nprobability that $x_1$ is preferred over $x_2$, which can be modeled\nusing a Bradley-Terry or Thurstone model based on the GP prior.\n\nUsing the paper \"Preferential Bayesian Optimization\"\n(<https://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf>),\nanswer the following:\n\n(a) **Modeling Preferences (6 points)**\n\n    (i) **Likelihood Derivation (Written, 2 points):** Given two points\n        $x_1$ and $x_2$ and their corresponding latent function values\n        $f(x_1)$ and $f(x_2)$, derive the likelihood of a preference\n        $x_1 \\succ x_2$ using the Bradley-Terry model. Your solution\n        here.\n\n    (ii) **Incorporating into GP (Written, 2 points):** Explain how this\n         likelihood can be incorporated into the GP framework to model\n         preferences probabilistically. Specifically, describe how the\n         covariance function of the GP affects the joint distribution of\n         preferences and discuss any assumptions made regarding the\n         smoothness or structure of $f$.\n\n    (iii) **Posterior Update (Written, 2 points):** Write out an\n          expression for the posterior mean and variance at new query\n          points by using the posterior predictive distribution based on\n          previously observed preferences (no need to simplify since\n          it's intractable analytically). Suggest an approach that can\n          be used to approximate the mean and variance.\n\n(b) **Acquisition Function Adaptation (6 points)**\n\n    (i) **Expected Improvement (EI) for Preferences (Written, 2\n        points):** Explain how the Expected Improvement (EI) acquisition\n        function is adapted in the context of PBO to handle preferences\n        rather than absolute function values. Please read the paper for\n        this.\n\n    (ii) **EI Computation for Pairwise Comparisons (Written, 2\n         points):** Derive the expression for EI when dealing with\n         pairwise comparisons. Show how the computation of EI differs\n         from the standard BO setting and discuss how uncertainty in the\n         GP model is used in this context.\n\n    (iii) **Selection Strategy (Written, 2 points):** Describe how the\n          acquisition function uses the pairwise preference data to\n          select the next query point. Provide a rigorous justification\n          for this selection strategy in terms of maximizing expected\n          information gain.\n\n(c) **Exploration-Exploitation Balance in PBO (6 points)**\n\n    (i) **Exploration Mechanism (Written, 2 points):** Explain how\n        exploration is handled in the PBO framework. Describe how\n        uncertainty in the preference model (the GP posterior)\n        influences the selection of new points for evaluation.\n\n    (ii) **Uncertainty Quantification (Written, 2 points):** Define how\n         the variance in the GP posterior represents uncertainty in the\n         model and show how this uncertainty is updated as new\n         preferences are observed.\n\n    (iii) **Empirical Validation (Written, 2 points):** Design an\n          experiment to empirically validate the balance between\n          exploration and exploitation in PBO. Describe the setup,\n          including the objective function, the experimental conditions,\n          and the evaluation metric for measuring the quality of\n          exploration-exploitation balance.\n\n(d) **Scalability and Practical Considerations (6 points)**\n\n    (i) **Challenges in Preference Feedback (Written, 2 points):**\n        Discuss the challenges associated with preference feedback in\n        real-world applications, such as inconsistency in user\n        preferences and potential biases.\n\n    (ii) **GP Scalability (Written, 2 points):** Explain how the\n         scalability of the GP model affects the performance of PBO,\n         especially as the number of observations increases. Include a\n         discussion on computational complexity and possible solutions.\n\n    (iii) **Extensions for Large-Scale Problems (Written, 2 points):**\n          Propose potential extensions or modifications to improve the\n          applicability of PBO to large-scale optimization problems. For\n          example, discuss the feasibility of sparse GPs or other\n          approximation techniques and evaluate their potential impact\n          on PBO performance.\n\n(e) **Empirical Experimentation (6 points)**\n\n    (i) **Copeland Score (Coding, 2 points):** Implement\n        `compute_max_copeland_score` in\\\n        `pbo/forrester_duel.py`.\n\n    (ii) **Copeland Acquisition (Coding, 4 points):** Implement\n         `copeland_acquisition`. Run `forrester_duel.py` and briefly\n         discuss any patterns you observe in the chosen duels (black Xs\n         on the heatmap).\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Define the Forrester function\ndef forrester_function(x):\n    \"\"\"\n    Evaluates the Forrester function at the given input.\n    \n    Args:\n    - x (float or numpy.ndarray): Input value(s) in the range [0, 1].\n    \n    Returns:\n    - float or numpy.ndarray: Evaluated Forrester function value(s).\n    \"\"\"\n    return (6 * x - 2)**2 * np.sin(12 * x - 4)\n\n# Sigmoid function for probabilistic preferences\ndef sigmoid(x):\n    \"\"\"\n    Computes the sigmoid function for the given input.\n    \n    Args:\n    - x (float or numpy.ndarray): Input value(s).\n    \n    Returns:\n    - float or numpy.ndarray: Sigmoid-transformed value(s).\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# Simulate duel outcome probabilistically\ndef simulate_duel_outcome(x, x_prime):\n    \"\"\"\n    Simulates the outcome of a duel between two candidates based on probabilistic preferences.\n    \n    Args:\n    - x (float): First candidate's input value.\n    - x_prime (float): Second candidate's input value.\n    \n    Returns:\n    - int: 1 if x wins, 0 otherwise.\n    \"\"\"\n    prob = sigmoid(forrester_function(x_prime) - forrester_function(x))  # Probability x beats x'\n    return np.random.choice([1, 0], p=[prob, 1 - prob])\n\n# Compute the Soft Copeland score for all candidates (vectorized)\ndef compute_max_copeland_score(candidates, gp, landmarks):\n    \"\"\"\n    Computes the maximum Copeland score for given candidates using predicted win probabilities.\n    \n    Args:\n    - candidates (numpy.ndarray): Array of candidate points.\n    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.\n    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.\n    \n    Returns:\n    - tuple: Maximum Copeland score and the best candidate.\n    \"\"\"\n    # YOUR CODE HERE (~6 lines)\n        # 1. Generate all pairs between candidates and landmarks.\n        # 2. Get win probabilities and average\n        # 3. Return appropriate maximum and best candidate.\n    pass \n    # END OF YOUR CODE\n\n# Acquisition function with GP retraining and maximum Copeland score for each outcome\ndef copeland_acquisition(x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score):\n    \"\"\"\n    Computes the acquisition value for a candidate pair by simulating outcomes and retraining the GP.\n    \n    Args:\n    - x (float): First value of duel.\n    - x_prime (float): Second value of duel.\n    - x_candidates (numpy.ndarray): Array of candidate points to evaluate soft Copeland on.\n    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.\n    - train_X (numpy.ndarray): Current training input pairs.\n    - train_y (numpy.ndarray): Current training labels.\n    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.\n    - max_copeland_score (float): Maximum copeland score prior to acquiring any new pair\n    \n    Returns:\n    - float: Acquisition value for the given pair (x, x_prime).\n    \"\"\"\n    # YOUR CODE HERE (~14-16 lines)\n        # 1. Predict dueling probabilities\n        # 2. Simulate adding (x, x') with y=1 (x beats x') and fit GP \n        # 3. Simulate adding (x, x') with y=0 (x' beats x) and fit GP \n        # 4. Compute expected improvement in max Copeland score\n        # 5. Return weighted acquisition value\n    pass\n    # END OF YOUR CODE\n\nif __name__ == \"__main__\":\n    # Initialization\n    np.random.seed(42)\n    kernel = C(28.0, constant_value_bounds='fixed') * RBF(length_scale=0.15, length_scale_bounds='fixed')\n    gp = GaussianProcessClassifier(kernel=kernel)\n\n    # Generate initial training data (random pairs)\n    train_X = np.array([[0, 0], [0, 0]]) #np.random.uniform(0, 1, (10, 2))  # 20 random dueling pairs [x, x']\n    train_y = np.array([simulate_duel_outcome(pair[0], pair[1]) for pair in train_X])\n\n    # Fixed landmark points and their function values\n    landmarks = np.linspace(0, 1, 30)  # 10 fixed landmarks\n\n    # Generate candidate pairs for optimization\n    x_candidates = np.linspace(0, 1, 30)  # Reduced grid for efficiency\n    X, X_prime = np.meshgrid(x_candidates, x_candidates)\n    candidate_pairs = np.c_[X.ravel(), X_prime.ravel()]\n\n    # Optimization loop\n    n_iterations = 20\n    for iteration in range(n_iterations):\n        # Retrain the GP with current training data\n        gp.fit(train_X, train_y)\n\n        # Compute global maximum Copeland score\n        max_copeland_score, condorcet_winner = compute_max_copeland_score(x_candidates, gp, landmarks)\n        print(f\"Condorcet winner iteration {iteration}: {condorcet_winner} with soft-Copeland score {max_copeland_score}\")\n\n        # Evaluate acquisition values for all candidate pairs\n        acquisition_values = np.zeros(len(candidate_pairs))\n        for idx, (x, x_prime) in tqdm(enumerate(candidate_pairs), total=len(candidate_pairs)):\n            acquisition_values[idx] = copeland_acquisition(\n                x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score\n            )\n\n        # Select the pair with the highest acquisition value\n        best_idx = np.argmax(acquisition_values)\n        next_x, next_x_prime = candidate_pairs[best_idx]\n\n        # Simulate the actual outcome of the duel\n        outcome = simulate_duel_outcome(next_x, next_x_prime)\n\n        # Update training data with the new duel outcome\n        train_X = np.vstack([train_X, [next_x, next_x_prime]])\n        train_y = np.append(train_y, outcome)\n\n    # Generate heatmaps\n    x = np.linspace(0, 1, 100)\n    X, X_prime = np.meshgrid(x, x)\n    pairs = np.c_[X.ravel(), X_prime.ravel()]\n\n    # Ground Truth Preference Probabilities\n    gt_preferences = np.array([\n        sigmoid(forrester_function(x_prime) - forrester_function(x))\n        for x, x_prime in pairs\n    ]).reshape(X.shape)\n\n    # GP-Predicted Preferences\n    gp_predictions = gp.predict_proba(pairs)[:, 1].reshape(X.shape)\n\n    # Plot Ground Truth Preference Heatmap\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.contourf(X, X_prime, gt_preferences, levels=50, cmap='jet')\n    plt.colorbar(label=\"Ground Truth Preference Probability\")\n    plt.title(\"Ground Truth Preference Heatmap\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"x'\")\n\n    print(f'Chosen duels: {train_X[-n_iterations:]}')\n\n    # Plot GP-Predicted Preference Heatmap\n    plt.subplot(1, 2, 2)\n    plt.contourf(X, X_prime, gp_predictions, levels=50, cmap='jet')\n    plt.colorbar(label=\"GP-Predicted Preference Probability\")\n    plt.scatter(train_X[-n_iterations:, 0], train_X[-n_iterations:, 1], c='black', label=\"Last Iterations\", s=30, marker='x')\n    plt.title(\"GP-Predicted Preference Heatmap\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"x'\")\n\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n:::\n\n### Question 2: Linear Dueling Bandit (30 points) {#sec-question-2-linear-dueling-bandit-30-points .unnumbered}\n\nIn the linear dueling bandit problem, feedback is provided through\npairwise comparisons between actions, rather than direct rewards.\nConsider a finite set of $K$ actions, each represented by a feature\nvector $x_1, x_2, \\dots, x_K \\in \\mathbb{R}^d$. Let the unknown\npreference scores be $f(x_i) = \\theta^\\top x_i$ and\n$f(x_j) = \\theta^\\top x_j$, where $\\theta \\in \\mathbb{R}^d$ is an\nunknown parameter vector. The goal is to identify the best action by\niteratively comparing pairs of actions while minimizing cumulative\nregret. Using qEUBO from <https://arxiv.org/pdf/2303.15746>, complete\nthe following:\n\n(a) **Acquisition Functions for Regret Minimization (Written, 10\n    points)**: Write out the expression for the acquisition function\n    Expected Improvement discussed in Q1 and qEUBO in the context of the\n    linear dueling bandit. Discuss conditions under which each\n    acquisition function could outperform the others in minimizing\n    cumulative regret.\n\n(b) **Experimental Evaluation of Acquisition Functions (Written +\n    Coding, 10 points)**: Benchmark the performance of the two\n    acquisition functions experimentally.\n\n    (i) Finish implementing the acquisition functions in a linear\n        dueling bandit simulation with $K = 10$ and $d = 5$, using\n        synthetic data by completing the function\n        `calculate_regret_from_gp` in `linear_dueling/run.py`.\n\n    (ii) Measure and compare cumulative regret over $T = 200$ rounds for\n         each acquisition function.\n\n    (iii) Report and analyze the empirical regret curves, discussing any\n          notable performance differences.\n\n(c) **Effect of Dimensionality on Regret (Written + Coding, 10\n    points)**: Analyze how increasing feature dimensionality impacts\n    regret.\n\n    (i) Experimentally evaluate the regret for different values of $d$\n        (e.g., $d = 5, 10, 20$) while keeping $K$ constant.\n\n    (ii) Plot the regret against $d$ and explain any observed trends.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom __future__ import annotations\n\nfrom typing import Optional\nimport itertools\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\nfrom tqdm import tqdm\nfrom botorch.acquisition.preference import qExpectedUtilityOfBestOption\nfrom botorch.acquisition.logei import qLogExpectedImprovement\nfrom botorch.fit import fit_gpytorch_mll\nfrom botorch.models.gpytorch import GPyTorchModel\nfrom botorch.utils.sampling import draw_sobol_samples\nfrom botorch.sampling import SobolQMCNormalSampler\nfrom botorch.posteriors.gpytorch import GPyTorchPosterior\nfrom gpytorch.distributions import base_distributions\nfrom gpytorch.likelihoods import Likelihood\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import Kernel, RBFKernel, ScaleKernel\nfrom gpytorch.mlls.variational_elbo import VariationalELBO\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.priors.torch_priors import GammaPrior\nfrom gpytorch.variational import (\n    CholeskyVariationalDistribution,\n    UnwhitenedVariationalStrategy,\n    VariationalStrategy,\n)\n\n\nclass PreferentialSoftmaxLikelihood(Likelihood):\n    r\"\"\"\n    Implements the softmax likelihood used for GP-based preference learning.\n\n    .. math::\n        p(\\mathbf y \\mid \\mathbf f) = \\text{Softmax} \\left( \\mathbf f \\right)\n\n    :param int num_alternatives: Number of alternatives (i.e., q).\n    \"\"\"\n\n    def __init__(self, num_alternatives):\n        super().__init__()\n        self.num_alternatives = num_alternatives\n        self.noise = torch.tensor(1e-4)  # This is only used to draw RFFs-based\n        # samples. We set it close to zero because we want noise-free samples\n        self.sampler = SobolQMCNormalSampler(\n            sample_shape=torch.Size([512]))  # This allows for\n        # SAA-based optimization of the ELBO\n\n    def _draw_likelihood_samples(\n        self, function_dist, *args, sample_shape=None, **kwargs\n    ):\n        function_samples = self.sampler(\n            GPyTorchPosterior(function_dist)).squeeze(-1)\n        return self.forward(function_samples, *args, **kwargs)\n\n    def forward(self, function_samples, *params, **kwargs):\n        function_samples = function_samples.reshape(\n            function_samples.shape[:-1]\n            + torch.Size(\n                (\n                    int(function_samples.shape[-1] / self.num_alternatives),\n                    self.num_alternatives,\n                )\n            )\n        )  # Reshape samples as if they came from a multi-output model (with `q` outputs)\n        num_alternatives = function_samples.shape[-1]\n\n        if num_alternatives != self.num_alternatives:\n            raise RuntimeError(\"There should be %d points\" %\n                               self.num_alternatives)\n\n        res = base_distributions.Categorical(\n            logits=function_samples)  # Passing the\n        # function values as logits recovers the softmax likelihood\n        return res\n\n\nclass VariationalPreferentialGP(GPyTorchModel, ApproximateGP):\n    def __init__(\n        self,\n        queries: Tensor,\n        responses: Tensor,\n        use_withening: bool = True,\n        covar_module: Optional[Kernel] = None,\n    ) -> None:\n        r\"\"\"\n        Args:\n            queries: A `n x q x d` tensor of training inputs. Each of the `n` queries is constituted\n                by `q` `d`-dimensional decision vectors.\n            responses: A `n x 1` tensor of training outputs. Each of the `n` responses is an integer\n                between 0 and `q-1` indicating the decision vector selected by the user.\n            use_withening: If true, use withening to enhance variational inference.\n            covar_module: The module computing the covariance matrix.\n        \"\"\"\n        self.queries = queries\n        self.responses = responses\n        self.input_dim = queries.shape[-1]\n        self.q = queries.shape[-2]\n        self.num_data = queries.shape[-3]\n        train_x = queries.reshape(\n            queries.shape[0] * queries.shape[1], queries.shape[2]\n        )  # Reshape queries in the form of \"standard training inputs\"\n        train_y = responses.squeeze(-1)  # Squeeze out output dimension\n        bounds = torch.tensor(\n            [[0, 1] for _ in range(self.input_dim)], dtype=torch.double\n        ).T  # This assumes the input space has been normalized beforehand\n        # Construct variational distribution and strategy\n        if use_withening:\n            inducing_points = draw_sobol_samples(\n                bounds=bounds,\n                n=2 * self.input_dim,\n                q=1,\n                seed=0,\n            ).squeeze(1)\n            inducing_points = torch.cat([inducing_points, train_x], dim=0)\n            variational_distribution = CholeskyVariationalDistribution(\n                inducing_points.size(-2)\n            )\n            variational_strategy = VariationalStrategy(\n                self,\n                inducing_points,\n                variational_distribution,\n                learn_inducing_locations=False,\n            )\n        else:\n            inducing_points = train_x\n            variational_distribution = CholeskyVariationalDistribution(\n                inducing_points.size(-2)\n            )\n            variational_strategy = UnwhitenedVariationalStrategy(\n                self,\n                inducing_points,\n                variational_distribution,\n                learn_inducing_locations=False,\n            )\n        super().__init__(variational_strategy)\n        self.likelihood = PreferentialSoftmaxLikelihood(\n            num_alternatives=self.q)\n        self.mean_module = ConstantMean()\n        scales = bounds[1, :] - bounds[0, :]\n\n        if covar_module is None:\n            self.covar_module = ScaleKernel(\n                RBFKernel(\n                    ard_num_dims=self.input_dim,\n                    lengthscale_prior=GammaPrior(3.0, 6.0 / scales),\n                ),\n                outputscale_prior=GammaPrior(2.0, 0.15),\n            )\n        else:\n            self.covar_module = covar_module\n        self._num_outputs = 1\n        self.train_inputs = (train_x,)\n        self.train_targets = train_y\n\n    def forward(self, X: Tensor) -> MultivariateNormal:\n        mean_X = self.mean_module(X)\n        covar_X = self.covar_module(X)\n        return MultivariateNormal(mean_X, covar_X)\n\n    @property\n    def num_outputs(self) -> int:\n        r\"\"\"The number of outputs of the model.\"\"\"\n        return 1\n\n\n# Objective function for pairwise comparisons\ndef f(x):\n    \"\"\"\n    Computes the preference score for a given action.\n\n    Args:\n        x (torch.Tensor): A feature vector of dimension `d`.\n\n    Returns:\n        torch.Tensor: The computed preference score.\n    \"\"\"\n    return x @ theta_true\n\n# Simulate pairwise comparisons\n\n\ndef simulate_comparison(x1, x2):\n    \"\"\"\n    Simulates a pairwise comparison between two actions based on their preference scores.\n\n    Args:\n        x1 (torch.Tensor): Feature vector of the first action.\n        x2 (torch.Tensor): Feature vector of the second action.\n\n    Returns:\n        torch.Tensor: The feature vector of the preferred action.\n    \"\"\"\n    prob_x1 = torch.sigmoid(f(x1) - f(x2))\n    return x1 if torch.rand(1).item() < prob_x1 else x2\n\n# Function to fit a Variational GP model\n\n\ndef fit_variational_gp(train_X, train_Y):\n    \"\"\"\n    Fits a Variational Gaussian Process (GP) model to the given training data.\n\n    Args:\n        train_X (torch.Tensor): Training feature pairs of shape [n, 2, d].\n        train_Y (torch.Tensor): Training preferences of shape [n, 1].\n\n    Returns:\n        VariationalPreferentialGP: A fitted GP model.\n    \"\"\"\n    queries = train_X.reshape(train_X.shape[0], 2, d)\n    responses = train_Y\n    return fit_model(queries, responses)\n\n\ndef fit_model(queries, responses):\n    \"\"\"\n    Internal helper to train a VariationalPreferentialGP.\n\n    Args:\n        queries (torch.Tensor): Training feature pairs.\n        responses (torch.Tensor): Training responses (preferences).\n\n    Returns:\n        VariationalPreferentialGP: Trained GP model.\n    \"\"\"\n    model = VariationalPreferentialGP(queries, responses)\n    model.train()\n    model.likelihood.train()\n    mll = VariationalELBO(\n        likelihood=model.likelihood,\n        model=model,\n        num_data=2 * model.num_data,\n    )\n    fit_gpytorch_mll(mll)\n    model.eval()\n    model.likelihood.eval()\n    return model\n\n# Acquisition function definition\n\n\ndef get_acquisition_functions(gp):\n    \"\"\"\n    Returns acquisition functions (qLogEI and qEUBO) for a given GP model.\n\n    Args:\n        gp (VariationalPreferentialGP): The fitted GP model.\n\n    Returns:\n        tuple: qLogExpectedImprovement and qExpectedUtilityOfBestOption acquisition functions.\n    \"\"\"\n    with torch.no_grad():\n        posterior = gp.posterior(gp.train_inputs[0])\n        best_f = posterior.mean.squeeze(-1).max()\n\n    qLogEI = qLogExpectedImprovement(model=gp, best_f=best_f)\n    qEUBO = qExpectedUtilityOfBestOption(pref_model=gp)\n    return qLogEI, qEUBO\n\n# Evaluate acquisition function on pairs\n\n\ndef evaluate_acquisition_on_pairs(acq_function, arms):\n    \"\"\"\n    Computes acquisition values for all possible pairs of arms.\n\n    Args:\n        acq_function: The acquisition function to evaluate.\n        arms (torch.Tensor): All available arms (feature vectors).\n\n    Returns:\n        tuple: A list of pairs and their corresponding acquisition values.\n    \"\"\"\n    pairs = list(itertools.combinations(arms, 2))\n    pair_values = []\n    with torch.no_grad():\n        for x1, x2 in pairs:\n            pair = torch.stack([x1, x2]).unsqueeze(0)\n            pair_values.append(acq_function(pair))\n    return pairs, torch.tensor(pair_values)\n\n# Regret calculation\n\n\ndef calculate_regret_from_gp(gp, actions):\n    \"\"\"\n    Computes the regret for the current GP model.\n\n    Args:\n        gp (VariationalPreferentialGP): The fitted GP model.\n        actions (torch.Tensor): Feature vectors of arms.\n\n    Returns:\n        torch.Tensor: The calculated regret.\n    \"\"\"\n    # YOUR CODE HERE (~6 lines)\n    # Compare the ground truth optimal arm to the GP's believed best arm\n    # Hint: To find GP believed best arm in expectation, use gp.posterior which returns with a mean property.\n    pass\n    # END OF YOUR CODE\n\n\nif __name__ == \"__main__\":\n    # Set default tensor precision\n    torch.set_default_dtype(torch.double)\n\n    # Problem settings\n    torch.manual_seed(55)\n    K = 30  # Number of arms (discrete choices)\n    d = 2   # Dimensionality of feature vectors\n    T = 100  # Number of rounds (iterations)\n    bounds = torch.tensor([[0.0] * d, [1.0] * d])  # Bounds for action space\n\n    # Generate random actions (feature vectors)\n    actions = torch.rand(K, d)\n\n    # Ground-truth preference parameter (unknown to the model)\n    theta_true = torch.ones(d)\n\n    # Generate initial observations\n    n_initial = 5\n    indices = torch.randint(0, K, (n_initial, 2))\n    train_X_logei = actions[indices]  # Shape: [n_initial, 2, d]\n    train_X_qeubo = train_X_logei.clone()\n    train_X_random = train_X_logei.clone()\n    train_Y_logei = torch.tensor([[0.0 if simulate_comparison(x1, x2).equal(x1) else 1.0]\n                                  for x1, x2 in train_X_logei])\n    train_Y_qeubo = train_Y_logei.clone()\n    train_Y_random = train_Y_logei.clone()\n\n    # Optimization loop\n    cumulative_regret_logei = []\n    cumulative_regret_qeubo = []\n    cumulative_regret_random = []\n\n    for t in tqdm(range(T)):\n        # Fit GP models\n        gp_logei = fit_variational_gp(train_X_logei, train_Y_logei)\n        gp_qeubo = fit_variational_gp(train_X_qeubo, train_Y_qeubo)\n        gp_random = fit_variational_gp(train_X_random, train_Y_random)\n\n        # Define acquisition functions\n        qLogEI, _ = get_acquisition_functions(gp_logei)\n        _, qEUBO = get_acquisition_functions(gp_qeubo)\n\n        # Evaluate acquisition functions\n        pairs_logei, acq_values_logei = evaluate_acquisition_on_pairs(\n            qLogEI, actions)\n        pairs_qeubo, acq_values_qeubo = evaluate_acquisition_on_pairs(\n            qEUBO, actions)\n\n        # Select pairs based on acquisition values\n        best_pair_idx_logei = torch.argmax(acq_values_logei)\n        best_pair_idx_qeubo = torch.argmax(acq_values_qeubo)\n        x1_logei, x2_logei = pairs_logei[best_pair_idx_logei]\n        x1_qeubo, x2_qeubo = pairs_qeubo[best_pair_idx_qeubo]\n\n        # Random pair selection\n        random_indices = torch.randint(0, K, (2,))\n        x1_random = actions[random_indices[0]]\n        x2_random = actions[random_indices[1]]\n\n        # Simulate comparisons\n        selected_logei = simulate_comparison(x1_logei, x2_logei)\n        selected_qeubo = simulate_comparison(x1_qeubo, x2_qeubo)\n        selected_random = simulate_comparison(x1_random, x2_random)\n\n        # Update training data\n        train_X_logei = torch.cat(\n            [train_X_logei, torch.stack([x1_logei, x2_logei]).unsqueeze(0)])\n        train_Y_logei = torch.cat([train_Y_logei, torch.tensor(\n            [[0.0 if selected_logei.equal(x1_logei) else 1.0]])])\n        train_X_qeubo = torch.cat(\n            [train_X_qeubo, torch.stack([x1_qeubo, x2_qeubo]).unsqueeze(0)])\n        train_Y_qeubo = torch.cat([train_Y_qeubo, torch.tensor(\n            [[0.0 if selected_qeubo.equal(x1_qeubo) else 1.0]])])\n        train_X_random = torch.cat(\n            [train_X_random, torch.stack([x1_random, x2_random]).unsqueeze(0)])\n        train_Y_random = torch.cat([train_Y_random, torch.tensor(\n            [[0.0 if selected_random.equal(x1_random) else 1.0]])])\n\n        # Calculate regrets\n        regret_logei = calculate_regret_from_gp(gp_logei, actions)\n        regret_qeubo = calculate_regret_from_gp(gp_qeubo, actions)\n        regret_random = calculate_regret_from_gp(gp_random, actions)\n\n        print(f'Regret LogEI: {regret_logei}')\n        print(f'Regret qEUBO: {regret_qeubo}')\n        print(f'Regret Random: {regret_random}')\n\n        cumulative_regret_logei.append(regret_logei)\n        cumulative_regret_qeubo.append(regret_qeubo)\n        cumulative_regret_random.append(regret_random)\n\n    # Plot cumulative regret\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_logei), dim=0), label='qLogEI')\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_qeubo), dim=0), label='qEUBO')\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_random), dim=0), label='Random')\n    plt.xlabel('Round')\n    plt.ylabel('Cumulative Regret')\n    plt.legend()\n    plt.title('Comparison of qLogEI, qEUBO, and Random Sampling')\n    plt.show()\n```\n:::\n\n\n:::\n\n\n### Question 3: Multi-Objective Thompson Sampling in Linear Contextual Bandits (30 points) {#sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points .unnumbered}\n\nThompson Sampling (TS) is commonly used for reward maximization in\nmulti-armed bandit problems, optimizing for the expected reward across\nactions. However, in many real-world scenarios, other objectives, such\nas the interpretability or reusability of learned parameters, are\nequally valuable. This is particularly relevant when modeling unknown\nreward functions with parameters that might offer insights or inform\nfuture experiments. A purely reward-focused Thompson Sampling approach\nmay result in increased false positive rates due to aggressive\nexploitation, whereas a pure exploration approach---such as those used\nin active learning---might better suit the goal of parameter learning.\n\nAssume a multi-objective setting where the goal is to not only maximize\nthe cumulative reward but also to accurately learn the parameters of the\nreward function itself in a linear contextual bandit setting. Let each\narm be represented by a feature vector $x \\in \\mathbb{R}^d$, with\nrewards generated by an unknown linear model\n$r = \\theta^\\top x + \\epsilon$, where\n$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Given these considerations,\nanswer the following:\n\n(a) **Theoretical Analysis of Multi-Objective Thompson Sampling (8\n    points)**\n\n    (i) **(Written, 3 points).** Define a cumulative regret objective\n        that balances maximizing the expected reward and minimizing the\n        parameter estimation error $\\|\\theta - \\hat{\\theta}\\|_2$.\n        Explain how this multi-objective regret differs from the\n        single-objective regret typically used in linear bandits.\n\n    (ii) **(Written, 3 points).** Derive the expected regret bounds for\n         Thompson Sampling in the single-objective case and describe the\n         additional challenges posed when extending these bounds to the\n         multi-objective case.\n\n    (iii) **(Written, 2 points).** Suppose you were to use a pure\n          exploration approach for parameter estimation. Provide an\n          upper bound for the parameter error\n          $\\|\\theta - \\hat{\\theta}\\|_2$ over $T$ rounds.\n\n(b) **Acquisition Strategies for Multi-Objective Optimization (8\n    points)**\n\n    (i) **(Written, 3 points).** Explain how to adapt the Upper\n        Confidence Bound (UCB) acquisition function to balance\n        exploration and exploitation for parameter learning alongside\n        reward maximization. Discuss the effect of tuning parameters on\n        exploration.\n\n    (ii) **(Written + Coding, 3 points).** Implement a Thompson Sampling\n         acquisition strategy that alternates between reward\n         maximization and parameter-focused exploration using a\n         multi-objective UCB. Implement the `select_arm` function of\n         `multi_obj_thompson/bandit.py`.\n\n    (iii) **(Written, 2 points).** Describe the impact of this\n          alternating acquisition strategy on false positive rates and\n          regret in comparison to standard Thompson Sampling.\n\n(c) **Posterior Distribution Analysis (8 points)**\n\n    (i) **(Written, 2 points).** Given a prior distribution for $\\theta$\n        and observed rewards, derive the posterior distribution of\n        $\\theta$ at each time step in the context of multi-objective\n        Thompson Sampling. Explain any assumptions needed for\n        computational tractability.\n\n    (ii) **(Coding, 4 points).** Implement a Bayesian update for the\n         posterior of $\\theta$ following each observation. Do this in\n         `update`.\n\n    (iii) **(Written, 2 points).** Explain how this posterior update\n          accommodates both exploration for parameter estimation and\n          exploitation for reward maximization.\n\n(d) **Empirical Evaluation (6 points)**\n\n    (i) **(Coding, 3 points).** Design and conduct an experiment\n        comparing standard Thompson Sampling, pure exploration, and your\n        multi-objective TS algorithm. Run this experiment on a synthetic\n        dataset with $d = 5$ features and $K = 10$ arms by executing\n        `run.py`.\n\n    (ii) **(Written, 3 points).** Report and interpret the results by\n         comparing the cumulative reward and parameter estimation error\n         across methods. Provide insights on the trade-offs observed and\n         any patterns in the rate of regret reduction.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass MultiObjectiveThompsonSamplingBandit:\n    \"\"\"\n    A class that implements a multi-objective Thompson sampling bandit.\n\n    Attributes:\n    - d (int): Dimension of the feature vector x.\n    - lambda_prior (float): Regularization parameter for the prior covariance matrix.\n    - sigma_noise (float): Standard deviation of the noise in rewards.\n    - mu (np.array): Prior mean of theta (initialized as a zero vector).\n    - Sigma (np.array): Prior covariance of theta (initialized as a scaled identity matrix).\n    \"\"\"\n\n    def __init__(self, d, lambda_prior=1.0, sigma_noise=1.0):\n        \"\"\"\n        Initializes the bandit with a prior on theta and noise variance.\n\n        Parameters:\n        - d (int): Dimension of the feature vector x.\n        - lambda_prior (float): Regularization parameter for the prior covariance matrix.\n        - sigma_noise (float): Standard deviation of the noise in rewards.\n        \"\"\"\n        self.d = d\n        self.lambda_prior = lambda_prior\n        self.sigma_noise = sigma_noise\n\n        # Initialize prior mean and covariance matrix\n        self.mu = np.zeros(d)  # Prior mean of theta\n        self.Sigma = lambda_prior * np.eye(d)  # Prior covariance of theta\n\n    def select_arm(self, arms, mode):\n        \"\"\"\n        Selects an arm (action) based on the specified mode.\n\n        Parameters:\n        - arms (np.array): A 2D NumPy array of shape (K, d) representing the feature vectors of K arms.\n        - mode (str): Selection mode, either 'exploit' (reward maximization) or 'explore' (focus on reducing uncertainty in theta).\n\n        Returns:\n        - selected_arm (np.array): The feature vector of the selected arm.\n        - arm_index (int): The index of the selected arm.\n        \"\"\"\n        # Sample a belief for theta from the current posterior\n        theta_sample = np.random.multivariate_normal(self.mu, self.Sigma)\n\n        # Generate reward noise for the arms\n        reward_noise = np.random.normal(0, self.sigma_noise, size=len(arms))\n\n        if mode == 'exploit':\n            # YOUR CODE HERE (~2 lines)\n                # 1. Compute expected rewards with noise\n                # 2. Select the arm with the highest expected reward\n                pass \n            # END OF YOUR CODE\n        elif mode == 'explore':\n            # Compute posterior covariance norms to evaluate exploration potential for each arm\n            posterior_cov_norms = []\n            for x in arms:\n                x = x.reshape(-1, 1)  # Reshape to column vector\n\n                # Find posterior covariance hypothetically and get its norm\n                # YOUR CODE HERE (~4 lines)\n                pass\n                # END OF YOUR CODE\n\n                posterior_cov_norms.append(norm)\n\n            # Select the arm that minimizes the posterior covariance norm\n            arm_index = np.argmin(posterior_cov_norms)\n\n        else:\n            raise ValueError(\"Mode must be either 'exploit' or 'explore'.\")\n\n        return arms[arm_index], arm_index, posterior_cov_norms if mode == 'explore' else None\n\n    def update(self, x_t, r_t):\n        \"\"\"\n        Updates the posterior distribution of theta given a new observation.\n\n        Parameters:\n        - x_t (np.array): Feature vector of the selected arm at time t.\n        - r_t (float): Observed reward at time t.\n        \"\"\"\n        x_t = x_t.reshape(-1, 1)  # Reshape to column vector\n\n        # YOUR CODE HERE (~4 lines)\n        # Obtain mu_new and Sigma_new of theta posterior. This requires doing some math!\n        pass\n        # END OF YOUR CODE\n\n        # Update internal state\n        self.mu = mu_new.flatten()\n        self.Sigma = Sigma_new\n\nif __name__ == '__main__':\n    # Number of features (dimension) and arms\n    d = 5  # Feature dimension\n    K = 10  # Number of arms\n\n    # Generate random arms (feature vectors)\n    np.random.seed(42)\n    arms = np.random.randn(K, d)\n\n    # True theta (unknown to the bandit)\n    theta_true = np.random.randn(d)\n\n    # Initialize the bandit\n    bandit = MultiObjectiveThompsonSamplingBandit(d)\n\n    # Number of rounds\n    T = 1000\n\n    # Lists to store results\n    regrets = []  # Store the regret at each round\n    theta_errors = []  # Store the error between estimated and true theta\n\n    # Simulation loop\n    for t in range(T):\n        # Alternate between 'exploit' and 'explore' modes\n        mode = 'exploit' if t % 2 == 0 else 'explore'\n\n        # Select an arm based on the current mode\n        x_t, arm_index, _ = bandit.select_arm(arms, mode=mode)\n\n        # Observe the reward with noise\n        r_t = theta_true @ x_t + np.random.normal(0, bandit.sigma_noise)\n\n        # Update the bandit with the new observation\n        bandit.update(x_t, r_t)\n\n        # Compute regret (difference between optimal reward and received reward)\n        optimal_reward = np.max(arms @ theta_true)  # Best possible reward\n        regret = optimal_reward - (theta_true @ x_t)  # Regret for this round\n        regrets.append(regret)\n\n        # Compute parameter estimation error (distance between true and estimated theta)\n        theta_error = np.linalg.norm(theta_true - bandit.mu)\n        theta_errors.append(theta_error)\n\n    # Final estimates after all rounds\n    mu_estimate, Sigma_estimate = bandit.mu, bandit.Sigma\n\n    # Print results\n    print(\"Estimated theta:\", mu_estimate)\n    print(\"True theta:\", theta_true)\n    print(\"Cumulative regret:\", np.sum(regrets))\n    print(\"Final covariance norm:\", np.linalg.norm(Sigma_estimate))\n\n    # Visualization of results\n\n    # Plot cumulative regret over time\n    plt.figure()\n    plt.plot(np.cumsum(regrets))\n    plt.title('Cumulative Regret over Time')\n    plt.xlabel('Rounds')\n    plt.ylabel('Cumulative Regret')\n    plt.show()\n\n    # Plot estimation error over time\n    plt.figure()\n    plt.plot(theta_errors)\n    plt.title('Theta Estimation Error over Time')\n    plt.xlabel('Rounds')\n    plt.ylabel('Estimation Error (L2 Norm)')\n    plt.show()\n```\n:::\n\n\n:::\n\n### Question 4: Mechanism Design in Preference Learning (30 points) {#sec-question-4-mechanism-design-in-preference-learning-30-points .unnumbered}\n\nIn mechanism design, a central challenge is optimizing resource\nallocation while accounting for user preferences, which may be private\nand complex. This problem can be addressed using learning techniques to\ninfer user preferences, thereby enabling the designer to make informed\npricing and allocation decisions. Consider a scenario where a designer\nallocates a divisible resource $B$ among $N$ players, each with a\nprivate, continuous, concave utility function $U_i(x_i)$ over their\nallocated share $x_i$, where $x = [x_1, x_2, \\dots, x_N]$ denotes the\nallocation vector. The designer aims to maximize social welfare while\nensuring full resource utilization.\n\n(a) **Modeling User Preferences (7 points)**:\n\n    (i) **(Written, 1 point)** Provide a realistic scenario in which we\n        estimate a utility function through eliciting preferences in the\n        context of the mechanism.\n\n    (ii) **(Written, 3 point)** Explain how elliptical slice sampling\n         can be used with a GP in order to estimate a utility function\n         through preferences.\n\n    (iii) **(Written, 3 point)** How can the elliptical slice posterior\n          samples be used to obtain the mean of the posterior predictive\n          for test points? (Hint: Read page $44$ of\n          <https://gaussianprocess.org/gpml/chapters/RW.pdf>.)\n\n(b) **Optimization with Learned Preferences (10 points)**:\n\n    (i) **(Written, 3 point)** Formulate the designer's optimization\n        problem, maximizing social welfare $\\sum_{i=1}^N U_i(x_i)$\n        subject to the constraint $\\sum_{i=1}^N x_i \\leq B$.\n\n    (ii) **(Written, 4 point)** Using the Lagrange multiplier method,\n         derive the conditions that must be met for optimal allocation\n         and pricing.\n\n    (iii) **(Written, 3 point)** As an alternative approach to Lagrange\n          multipliers, explain how projected gradient descent (PGD) can\n          be used to solve the designer's optimization problem.\n\n(c) **Benchmarking Learning and Allocation Efficiency (13 points)**:\n\n    (i) **(Coding, 3 point)** Implement `preference_loglik` in the file\n        `gp_mechanism/preference_gp.py`.\n\n    (ii) **(Coding, 3 point)** Implement `predictive_function`.\n\n    (iii) **(Coding, 3 point)** Implement `optimize_allocations` inside\n          `gp_mechanism/run.py`.\n\n    (iv) **(Written, 4 point)** Compare GP-approximated utility\n         allocations through PGD, exact utility allocations through PGD,\n         and the optimal Lagrange-based allocation done by hand with\n         each other for your choice of utility functions $U_i$. Make\n         sure your utilities are continuous and concave.\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom typing import Callable\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch  # Import PyTorch\nfrom tqdm import tqdm\n\n\nclass EllipticalSliceSampler:\n    def __init__(self,\n                 prior_cov: np.ndarray,\n                 loglik: Callable):\n        \"\"\"\n        Initializes the Elliptical Slice Sampler.\n\n        Args:\n        - prior_cov (np.ndarray): Prior covariance matrix.\n        - loglik (Callable): Log-likelihood function.\n        \"\"\"\n        self.prior_cov = prior_cov\n        self.loglik = loglik\n\n        self._n = prior_cov.shape[0]  # Dimensionality of the space\n        # Cache Cholesky decomposition\n        self._chol = np.linalg.cholesky(prior_cov)\n\n        # Initialize state and cache previous states\n        self._state_f = self._chol @ np.random.randn(self._n)\n\n    def _indiv_sample(self):\n        \"\"\"\n        Main algorithm for generating an individual sample using Elliptical Slice Sampling.\n        \"\"\"\n        f = self._state_f  # Previous state\n        # Sample from prior for the ellipse\n        nu = self._chol @ np.random.randn(self._n)\n        log_y = self.loglik(f) + np.log(np.random.uniform()\n                                        )  # Log-likelihood threshold\n\n        theta = np.random.uniform(0., 2 * np.pi)  # Initial proposal angle\n        theta_min, theta_max = theta - 2 * np.pi, theta  # Define bracketing interval\n\n        # Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.\n        while True:\n            # YOUR CODE HERE (~10 lines)\n            # Generate a new sample point based on the current angle.\n            f_prime = f * np.cos(theta) + nu * np.sin(theta)\n\n            # Check if the proposed point meets the acceptance criterion.\n            if self.loglik(f_prime) > log_y:  # Accept the sample\n                self._state_f = f_prime\n                return\n\n            else:  # If not accepted, adjust the bracket and select a new angle.\n                if theta < 0:\n                    theta_min = theta\n                else:\n                    theta_max = theta\n                theta = np.random.uniform(theta_min, theta_max)\n            # END OF YOUR CODE\n\n    def sample(self,\n               n_samples: int,\n               n_burn: int = 500) -> np.ndarray:\n        \"\"\"\n        Generates samples using Elliptical Slice Sampling.\n\n        Args:\n        - n_samples (int): Total number of samples to return.\n        - n_burn (int): Number of initial samples to discard (burn-in).\n\n        Returns:\n        - np.ndarray: Array of samples after burn-in.\n        \"\"\"\n        samples = []\n        for i in tqdm(range(n_samples), desc=\"Sampling\"):\n            self._indiv_sample()\n            if i > n_burn:\n                # Store sample post burn-in\n                samples.append(self._state_f.copy())\n\n        return np.stack(samples)\n\n\ndef squared_exponential_cov_torch(X1, X2, length_scale=1.0, variance=1.0):\n    \"\"\"\n    Squared Exponential (RBF) Covariance Function using PyTorch tensors.\n\n    Args:\n        X1 (torch.Tensor): First set of input points.\n        X2 (torch.Tensor): Second set of input points.\n        length_scale (float): Length scale of the kernel.\n        variance (float): Variance (amplitude) of the kernel.\n\n    Returns:\n        torch.Tensor: Covariance matrix between X1 and X2.\n    \"\"\"\n    X1 = X1.reshape(-1, 1)\n    X2 = X2.reshape(-1, 1)\n    dists = torch.sum(X1**2, dim=1).reshape(-1, 1) + \\\n        torch.sum(X2**2, dim=1) - 2 * torch.mm(X1, X2.T)\n    return variance * torch.exp(-0.5 * dists / length_scale**2)\n\n\ndef generate_preferences(x_pairs, utility_fn):\n    \"\"\"\n    Generates preference labels based on the Bradley-Terry model.\n\n    Args:\n        x_pairs (np.array): Array of preference pairs, shape [n_pairs, 2].\n        utility_fn (function): Ground truth utility function.\n\n    Returns:\n        np.array: Preference labels (1 if the first item in the pair is preferred, 0 otherwise).\n    \"\"\"\n    preference_labels = []\n    for x1, x2 in x_pairs:\n        u1, u2 = utility_fn(x1), utility_fn(x2)\n        prob = np.exp(u1) / (np.exp(u1) + np.exp(u2))\n        preference_labels.append(1 if np.random.rand() < prob else 0)\n    return np.array(preference_labels)\n\n\ndef create_predictive_function(ground_truth_utility, num_pairs=3000, n_samples=100, n_burn=50, length_scale=2.0, variance=0.5):\n    \"\"\"\n    Creates a predictive function to compute the posterior predictive mean of a Gaussian Process.\n\n    Args:\n        ground_truth_utility (function): The ground truth utility function for generating preferences.\n        num_pairs (int): Number of random preference pairs to generate.\n        n_samples (int): Number of samples for Elliptical Slice Sampling.\n        n_burn (int): Number of burn-in samples for Elliptical Slice Sampling.\n        length_scale (float): Length scale for the Squared Exponential Kernel.\n        variance (float): Variance (amplitude) of the Squared Exponential Kernel.\n\n    Returns:\n        function: A predictive function that computes the posterior predictive mean.\n    \"\"\"\n    # Generate random preference pairs\n    np.random.seed(42)\n    x_pairs = np.random.uniform(0, 10, size=(num_pairs, 2))\n    X_flat = x_pairs.flatten()\n\n    # Generate preference labels\n    preference_labels = generate_preferences(x_pairs, ground_truth_utility)\n\n    # Convert X_flat to PyTorch tensor\n    X_flat_torch = torch.tensor(X_flat, dtype=torch.float32)\n\n    # GP Prior (using PyTorch)\n    K_torch = squared_exponential_cov_torch(\n        X_flat_torch, X_flat_torch, length_scale=length_scale, variance=variance)\n    # Add jitter for numerical stability\n    K_torch += 1e-2 * torch.eye(len(X_flat_torch))\n    prior_cov = K_torch.numpy()  # Convert back to numpy for the sampler\n\n    # Log-likelihood function\n    def preference_loglik(f):\n        \"\"\"\n        Computes the log-likelihood of the preferences under the Bradley-Terry model.\n\n        Args:\n            f (np.array): Latent utility values.\n\n        Returns:\n            float: Log-likelihood of the given latent utilities.\n        \"\"\"\n        log_likelihood = 0.0\n        for (x1, x2), label in zip(x_pairs, preference_labels):\n            idx1 = np.where(X_flat == x1)[0][0]\n            idx2 = np.where(X_flat == x2)[0][0]\n            f1, f2 = f[idx1], f[idx2]\n\n            # YOUR CODE HERE (~4 lines)\n            # Add datapoint log likelihood using Bradley-Terry model\n            pass\n            # END OF YOUR CODE\n        return log_likelihood\n\n    # Elliptical Slice Sampling\n    sampler = EllipticalSliceSampler(\n        prior_cov=prior_cov, loglik=preference_loglik)\n    posterior_samples = sampler.sample(n_samples=n_samples, n_burn=n_burn)\n    posterior_mean = np.mean(posterior_samples, axis=0)\n\n    # Convert posterior_mean to PyTorch tensor\n    posterior_mean_torch = torch.tensor(posterior_mean, dtype=torch.float32)\n\n    # Compute K_inv using PyTorch\n    K_inv_torch = torch.inverse(K_torch)\n\n    # Define the predictive function\n    def predictive_function(x):\n        \"\"\"\n        Predicts the utility for new input points.\n\n        Args:\n            x (torch.Tensor): Input points to predict utilities for.\n\n        Returns:\n            torch.Tensor: Predicted expected utilities.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise ValueError('Predictive function must take in torch.tensor')\n        x = x.reshape(-1, 1)\n        X_flat_torch_reshaped = X_flat_torch.reshape(-1, 1)\n\n        # YOUR CODE HERE (~2 lines)\n        # Implement equation (3.21) on page 44 of https://gaussianprocess.org/gpml/chapters/RW.pdf\n        pass\n        # END OF YOUR CODE\n\n    return predictive_function\n\n\nif __name__ == \"__main__\":\n    # Ground truth utility function\n    def ground_truth_utility(x): return np.sin(x)\n\n    # Create the predictive function\n    predictive_fn = create_predictive_function(ground_truth_utility)\n\n    # Test the predictive function\n    X_test = torch.linspace(0, 10, 50).reshape(-1, 1)  # Test points\n    posterior_means = predictive_fn(\n        X_test).detach().numpy()  # Predicted posterior means\n\n    # Ground truth utilities\n    ground_truth_utilities = ground_truth_utility(X_test.numpy())\n\n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.title(\"GP Posterior Predictive Mean (Utility Approximation)\")\n    plt.plot(X_test.numpy(), posterior_means,\n             label=\"Posterior Predictive Mean\", color=\"red\")\n    plt.scatter(X_test.numpy(), ground_truth_utilities,\n                label=\"Ground Truth Utility\", color=\"blue\", alpha=0.5)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Utility\")\n    plt.legend()\n    plt.show()\n```\n:::\n\n\n:::\n\n::: {.callout-note title=\"code\"}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport torch\nfrom preference_gp import create_predictive_function\n\n# Feel free to play around with continuous, concave utility functions!\ndef utility_1(x):\n    \"\"\"\n    Utility function 1: 3 * log(x + 1)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 3 * torch.log(x + 1)\n\ndef utility_2(x):\n    \"\"\"\n    Utility function 2: 5 * log(x + 2)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 5 * torch.log(x + 2)\n\ndef utility_3(x):\n    \"\"\"\n    Utility function 3: 8 * log(x + 3)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 8 * torch.log(x + 3)\n\ndef project(x, B):\n    \"\"\"\n    Projects the allocation vector `x` onto the feasible set {z | sum(z) = B, z >= 0}.\n    This ensures that the allocations respect the resource constraint.\n\n    Args:\n        x (torch.Tensor): Current allocation vector.\n        B (float): Total available resource.\n\n    Returns:\n        torch.Tensor: Projected allocation vector.\n    \"\"\"\n    with torch.no_grad():\n        # Sort x in descending order\n        sorted_x, _ = torch.sort(x, descending=True)\n        \n        # Compute cumulative sum adjusted by B\n        cumulative_sum = torch.cumsum(sorted_x, dim=0) - B\n        \n        # Find the threshold (water-filling algorithm)\n        rho = torch.where(sorted_x - (cumulative_sum / torch.arange(1, len(x) + 1, dtype=torch.float32)) > 0)[0].max().item()\n        theta = cumulative_sum[int(rho)] / (rho + 1)\n        \n        # Compute the projected allocation\n        return torch.clamp(x - theta, min=0)\n\ndef optimize_allocations(utilities, B, learning_rate, num_iterations):\n    \"\"\"\n    Optimizes the allocation of resources to maximize the total utility.\n\n    Args:\n        utilities (list): List of utility functions or GP-based predictive functions.\n        B (float): Total available resource.\n        learning_rate (float): Step size for gradient ascent.\n        num_iterations (int): Number of optimization iterations.\n\n    Returns:\n        torch.Tensor: Final resource allocations.\n    \"\"\"\n    # Initialize resource allocations equally\n    x = torch.tensor([1.0] * len(utilities), requires_grad=True)\n\n    # Optimization loop\n    for iteration in range(num_iterations):\n        # YOUR CODE HERE (~6 lines)\n        # 1. Compute total utility and backprop\n        # 2. Update x directly with x.grad\n        # 3. Project onto convex constraint set since we are using Projected Gradient Descent (PGD)\n        pass\n        # END OF YOUR CODE\n        \n        # Log progress every 10 iterations or at the last iteration\n        if iteration % 10 == 0 or iteration == num_iterations - 1:\n            print(f\"Iteration {iteration}: Total Utility = {total_utility.item():.4f}, Allocations = {x.data.numpy()}\")\n    \n    return x\n\nif __name__ == \"__main__\":\n    # Generate GP models for each utility\n    gp_1 = create_predictive_function(lambda x: utility_1(torch.tensor(x)).numpy())\n    gp_2 = create_predictive_function(lambda x: utility_2(torch.tensor(x)).numpy())\n    gp_3 = create_predictive_function(lambda x: utility_3(torch.tensor(x)).numpy())\n\n    # Combine utility GPs into a list for optimization\n    utilities = [gp_1, gp_2, gp_3]  # Use [utility_1, utility_2, utility_3] for exact utility functions\n\n    # Resource constraint and optimization settings\n    B = 10  # Total available resource\n    learning_rate = 0.1  # Gradient ascent step size\n    num_iterations = 2000  # Number of iterations\n\n    # Optimize allocations\n    final_allocations = optimize_allocations(utilities, B, learning_rate, num_iterations)\n\n    # Final results\n    print(\"\\nFinal allocations:\")\n    print(final_allocations.data.numpy())\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "chap3_files/figure-pdf"
    ],
    "filters": []
  }
}