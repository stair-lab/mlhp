[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning from Human Preferences",
    "section": "",
    "text": "Fullscreen Slide\nMachine learning is increasingly shaping various aspects of our lives, from education and healthcare to scientific discovery. A key challenge in developing trustworthy intelligent systems is ensuring they align with human preferences. Learning from human feedback offers a promising solution to this challenge. This book introduces the foundations and practical applications of machine learning from human preferences. Instead of manually predefining the learning goal, the book presents preference-based learning that incorporates human feedback to guide the learning process, drawing insights from related fields such as economics, psychology, and human-computer interaction. By the end of this book, readers will be equipped with the key concepts and tools needed to design systems that effectively align with human preferences.\nThe book is intended for researchers, practitioners, and students who are interested in intergrating machine learning with human-centered application. We assume some basic knowledge of probability and statistics, but provides sufficient background and references for the readers to follow the main ideas. The book also provides illustrative program examples and datasets. The field of machine learning from human preference is a vibrant area of research and practice with many open challenges and opportunities, and we hope that this book will inspire readers to further explore and advance this exciting field. The book is divided into 4 chapters:\n\nChapter 1 lays the foundation for probabilistic modeling of preferences and decisions. It covers key model assumptions (e.g., bounded rationality), along with methods for preference data collection (e.g., pairwise comparisons), and models to interpret the data (e.g., Bradley-Terry model). The chapter also explore preference aggregation through the lense of social choice theory.\nChapter 2 examines various functional form and learning of reward modeling via examples from the field of language modeling and robotics. We discuss the challenges in learning multimodal rewards, meta-reward learning, human biases in reward models, and strategies to leverage both rational and irrational behavior.\nChapter 3 focused on a process where the end goal is to elicit the utility function. Since human is involved in the data collection, we concentrate on active learning methods that elicit the most preference information with a minimal amount of human query. Various strategies are explored, including reducing the learner’s variance, exploiting ambiguity and domain knowledge in ranking, with examples from robotics and machine learning systems.\nChapter 4 focuses on process where preference is a signal guiding decision. We discuss dueling bandits, an algorithm for decision making from pairwise preferences, as well as reinforcement learning from human feedback (RLHF) to align language models. Moreover, addressing the challenges of modeling uncertainty in reward models emerges as a crucial area for improvement. We also discuss decision making setting beyond a single reward frameworks to accommodate the oversight of diverse objectives. Embracing multi-objective is pivotal to representing the multifaceted goals of varied stakeholders within AI systems.\nChapter 5 analyzes principles from human-computer interaction (HCI) for systems that learn from humans, like cognitive constraints and user experience. We discuss ethical issues that arise in interaction models and approaches for designing preference elicitation systems considering fairness, privacy, and other socio-technical factors. We tackle challenges around aligning learned models with values from diverse expert and non-expert stakeholders. Ethical considerations in learning from human preference are addressed. We discuss the potential benefits and risks of learning from human preferences and how to address them responsibly and fairly. We also raise questions about the selection and protection of human participants and the possible consequences of exploiting or manipulating human responses.\n\nThis book covers various topics, from the statistical foundations and strategies for interactively querying humans to applications for eliciting preference. We review the relevant foundations in statistics, psychology, economics and other disciplines and explore their applications to various domains, such as natural language and robotics, and more. We adopt the machine learning perspective for modeling, estimating, and evaluating the learning processes. This book is used at Stanford University for a quarter-long class CS329H: Machine Learning from Human Preferences. We include the lecture slides and homework at the begining and end of each chapter, respectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "src/chap2.html",
    "href": "src/chap2.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 The Construction of Preference\nFullscreen Part 1 Fullscreen Part 2\nHuman preference modeling aims to capture humans’ decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one should understand to create such systems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#sec-foundations",
    "href": "src/chap2.html#sec-foundations",
    "title": "2  Background",
    "section": "",
    "text": "2.1.1 Axiom 1. Construction of Choices Set: Luce’s Choice Axiom (Luce, 1959)\nPreference models model the preferred choices amongst a set of items. Preference models must enumerate the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are distinct (mutually exclusive) choices. A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. We assume that if a new item is added to the choice set, the relative probabilities of choosing between the original items remain unchanged. This is known as the Independence of Irrelevant Alternatives (IIA) property from Luce’s axiom of choices (Luce 1977).\n\n\n2.1.2 Axiom 2. Preference Centers around Utility: Reciprocity (Block & Marschak, 1960)\nPreference models are centered around the notion of reward, a scalar quantity representing the benefit or value an individual attains from selecting a given choice. We assume that the underlying reward mechanism of a human preference model captures the final decision output from a human. We use the notation \\(u_{i,j}\\) as the reward of person \\(i\\) choosing item \\(j\\). The reward is a random variable, decomposing into true reward \\(u_{i,j}^*\\) and a random noise \\(\\epsilon_{i,j}\\): \\(u_{i,j} = u_{i,j}^* + \\epsilon_{i,j}\\). McFadden (1974) posits that reward can further be decomposed into user-specific reward \\(\\theta_i\\) and item-specific reward \\(z_j\\): \\(u_{i,j}^* = \\theta_i + z_j\\). This decomposition indicates that for a single user, only the relative difference in reward matters to predict the choice among items, and the scale of rewards is important when comparing across users.\n\n\n2.1.3 Axiom 3. Preference captures decision-making: Wins as a Sufficient Statistic (Bühlmann & Huber, 1963)\nHuman preferences are classified into two categories: revealed preferences and stated preferences. Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models that represent relationships between input decision attributes that may have little interpretability but enable powerful models of human preference. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences. Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model), which are reasonable assumptions for how a human would consider a set of items. This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.\n\n\n2.1.4 Axiom 4. Rationality: The Transitivity of odds (Good, 1955)\nThe preference model assumes that humans are rational. Perfect rationality posits that individuals make decisions that maximize their reward, assuming they have complete information and the cognitive ability to process this information to make optimal choices. Numerous studies have shown that this assumption frequently fails to describe actual human behavior. Bounded rationality acknowledges that individuals operate within the limits of their information and cognitive capabilities (Simon 1972). Here, decisions are influenced by noise, resulting in probabilistic choice behavior: while individuals aim to maximize their reward, noise can lead to deviations from perfectly rational choices (Miljkovic 2005). Instead of deterministic reward maximization, the decision maker will choose an item with probability proportional to the reward they receive for that item. This probabilistic model can be operationalized with Boltzmann distribution. Utility of person \\(i\\) on item \\(j\\) is computed by a function \\(f_i: e_j \\rightarrow \\mathbb{R}\\), where \\(e_j \\in \\mathbb{R}^d\\) is an embedding of item \\(j\\). The probability of item \\(j\\) being preferred by person \\(i\\) over all other alternatives in the choice set \\(\\mathcal{C}\\) is\n\\[\np_{ij} =  p_i(j \\succ j': j' \\neq j \\forall j' \\in \\mathcal{C}) = Z_i^{-1} \\exp \\circ f_i(e_j) \\text{ where } Z_i = \\sum_{j' \\in \\mathcal{C}} \\exp \\circ f_i(e_{j'})\n\\]\nOne can extend the above model in various ways. For example, the above model does not account for similar actions. Consider the following example when choosing a mode of transportation: car and train, with no particular preference for either choice. The preferred probability is 50% for either item. However, if we have 99 cars and one train in the choice set, we would have a 99% probability of choosing a car. To address this issue, various extensions have been proposed. For example, we can introduce a similarity metric to cluster items. We want a metric that acts more as a distance in the feature space with the following properties: Identity (an item is most similar to itself), symmetric (the similarity of item \\(j\\) to \\(j'\\) is the same as that of \\(j'\\) to \\(j\\)), and positive semidefinite (similarity metric is non-negative). Under this extension, the probablity of item \\(j\\) being preferred over all other alternatives by person \\(i\\) is \\(p_{ij} / w_j, \\text{ where } w_j = \\sum_{j' \\in \\mathcal{C}} s(e_j, e_{j'})\\). This de-weights similar items, which is the desired effect for human decision-making.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#preference-model",
    "href": "src/chap2.html#preference-model",
    "title": "2  Background",
    "section": "2.2 Models of Preferences and Decisions",
    "text": "2.2 Models of Preferences and Decisions\nNext, we explore ways humans can express their preferences, including accept-reject sampling, pairwise sampling, rank-order sampling, rating-scale sampling, best-worst scaling, and multiple-choice samples. We will understand the process of collecting data through simulation and, when appropriate, discuss the real-world application of these models. Each item \\(i\\) is represented by a \\(d=2\\) dimensional vector \\(x^i\\). There is only one user in the simulation, and they have a latent reward function \\(f\\) that they use to compute the latent reward of an item from the features. Here, the latent reward function is the Ackley function .\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nimport numpy as np\nnp.random.seed(0)\n\ndef ackley(X, a=20, b=0.2, c=2*np.pi):\n    \"\"\"\n    Compute the Ackley function.\n    Parameters:\n      X: A NumPy array of shape (n, d) where each row is a d-dimensional point.\n      a, b, c: Parameters of the Ackley function.\n    Returns:\n      A NumPy array of function values.\n    \"\"\"\n    X = np.atleast_2d(X)\n    d = X.shape[1]\n    sum_sq = np.sum(X ** 2, axis=1)\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n    term2 = -np.exp(np.sum(np.cos(c * X), axis=1) / d)\n    return term1 + term2 + a + np.e\n\nWe next define a function to visualize the surface:\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nccmap = LinearSegmentedColormap.from_list(\"ackley\", [\"#f76a05\", \"#FFF2C9\"])\nplt.rcParams.update({\n    \"font.size\": 14,\n    \"axes.labelsize\": 16,\n    \"xtick.labelsize\": 14,\n    \"ytick.labelsize\": 14,\n    \"legend.fontsize\": 14,\n    \"axes.titlesize\": 16,\n})\nplt.rcParams['text.usetex'] = True\n\ndef draw_surface():\n    inps = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(inps, inps)\n    grid = np.column_stack([X.ravel(), Y.ravel()])\n    Z = ackley(grid).reshape(X.shape)\n    \n    plt.figure(figsize=(6, 5))\n    contour = plt.contourf(X, Y, Z, 50, cmap=ccmap)\n    plt.contour(X, Y, Z, levels=15, colors='black', linewidths=0.5, alpha=0.6)\n    plt.colorbar(contour, label=r'$f(x)$', ticks=[0, 3, 6])\n    plt.xlim(-2, 2)\n    plt.ylim(-2, 2)\n    plt.xticks([-2, 0, 2])\n    plt.yticks([-2, 0, 2])\n    plt.xlabel(r'$x_1$')\n    plt.ylabel(r'$x_2$')\n\n\n2.2.1 Item-wise Model\nOne method for data collection is accept-reject sampling, where the user considers one item at a time and decides if they like it. Below is an example survey using accept-reject sampling:\n\n\nWe will use a simulation to familiarize ourselves with accept-reject sampling. On the surface below, blue and red points correspond to accept or reject points.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nd = 2\nn_items = 800\nitems = np.random.randn(n_items, d)*0.5 + np.ones((n_items, d))*0.5\nrewards = ackley(items)\ny = (rewards &gt; rewards.mean())\ndraw_surface()\nplt.scatter(items[:, 0], items[:, 1], c=y, cmap='coolwarm', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nThe binary choice model centers around one item. The model predicts, for that item, after observing user choices in the past, whether that item will be chosen. We use binary variable \\(y \\in \\{0, 1\\}\\) to represent whether the user will pick that choice in the next selection phase. We denote \\(P = p(y = 1)\\). We can formally model \\(y\\) as a function of the reward of the positive choice: \\(y = \\mathbb{I}[U&gt;0]\\). We explore two cases based on the noise distribution. \\(\\psi\\) is the logistic function or the standard normal cumulative distribution function if noise follows logistic distribution and the standard normal distribution, respectively: \\[\np(u_{i,j} &gt; 0) = p(u_{i,j}^* + \\epsilon &gt; 0) = 1 - p( \\epsilon &lt; -u_{i,j}^*) = \\psi(u_{i,j}^*).\n\\]\nA generalization of accept-reject sampling is rating-scale sampling. Rating-scale sampling, such as the Likert scale, is a method in which participants rate items on a fixed-point scale (e.g., 1 to 5, “Strongly Disagree” to “Strongly Agree”) to measure levels of preference towards items (Harpe 2015). Participants can also mark a point on a continuous rating scale to indicate their preference or attitude. Commonly used in surveys, product reviews, and psychological assessments, this method provides a more nuanced measure than discrete scales. Rating-scale sampling is simple for participants to understand and use, provides rich data on the intensity of preferences, and is flexible enough for various measurements (e.g., agreement, satisfaction). However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in clustering responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants’ preferences or attitudes.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nfrom matplotlib.colors import LinearSegmentedColormap\nlikert_cmap = LinearSegmentedColormap.from_list(\"likert_scale\", [\"red\", \"blue\"], N=5)\nnormalized = (rewards - rewards.min()) / (rewards.max() - rewards.min())\nratings = np.round(normalized * 4).squeeze()\n\ndraw_surface()\nscatter = plt.scatter(items[:, 0], items[:, 1], c=ratings, cmap=likert_cmap, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nSuppose we have a single example with attributes \\(z_i\\) and wish to know which of \\(J\\) rating scales an individual will choose from. We can define \\(J - 1\\) parameters, which act as thresholds on the reward computed by \\(u_i = u_{i,j}^*\\) to classify the predicted choice between these items. For example, if there are three predefined items, we can define parameters \\(a, b \\in \\mathbb{R}\\) such that \\[\ny_i =\n\\begin{cases}\n    1 & u &lt; a \\\\\n    2 & a \\le u &lt; b \\\\\n    3 & \\text{else}\n\\end{cases}\n\\]\nBy assuming the noise distribution to be either logistic or standard normal, we have \\[\n\\begin{split}\n    p(y_i = 1) & = p(u &lt; a) = p(u_{i,j}^* + \\epsilon &lt; a) = \\psi(a-u_{i,j}^*) \\\\\n    p(y_i = 2) & = p(a \\le u &lt; b) = p(a - u_{i,j}^* \\le \\epsilon &lt; b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)  - \\psi(u_{i,j}^*-a) \\\\\n    p(y_i = 3) & = p(u &gt; b) = p(u_{i,j}^* + \\epsilon &gt; b ) = p( \\epsilon &gt; b - u_{i,j}^*) = \\psi(b-u_{i,j}^*)\n\\end{split}\n\\]\nHaving the model, we next explore the estimation of model parameters. A common approach for parameter estimation is maximum likelihood (Casella and Berger 1990; Bock et al. 2015). The likelihood of a model is the probability of the observed data given the model parameters; intuitively, we wish to maximize this likelihood, as that would mean that our model associates observed human preferences with high probability. Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the binary choice model with logistic noise is\n\\[\\mathcal{L}(z, Y; \\beta) = \\prod_{i = 1}^J p(y = y_i | z_i; \\beta) = \\prod_{i = 1}^J \\frac{1}{1 + \\exp^{-u_{i,j}^*}}\\]\nThis objective can be optimized with a gradient-based method, such as gradient descent (Ruder 2016). Gradient descent operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to minimize the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a learning rate) to move the parameters in a direction that minimizes the objective. In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration; learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers, which reduce noise by using moving averages of gradients or provide better estimates of the best direction in which to update the gradients.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions import Bernoulli\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Number of users and items\nnum_users = 50\nnum_items = 100\n\n# Generate user-specific and item-specific rewards\ntheta = torch.randn(num_users, device=device, requires_grad=True)\nz = torch.randn(num_items, device=device, requires_grad=True)\n\n# Generate observed choices using logistic function\nprobs = torch.sigmoid(theta[:, None] - z[None, :])\ndata = Bernoulli(probs=probs).sample()\n\n# Mask out a fraction of the response matrix\nmask = torch.rand_like(data) &gt; 0.2  # 80% observed, 20% missing\ndata_masked = data.clone()\ndata_masked[~mask] = float('nan')\n\n# Initialize parameters for EM algorithm\ntheta_est = torch.randn(num_users, device=device, requires_grad=True)\nz_est = torch.randn(num_items, device=device, requires_grad=True)\n\n# Optimizer\noptimizer = optim.LBFGS([theta_est, z_est], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    optimizer.zero_grad()\n    probs_est = torch.sigmoid(theta_est[:, None] - z_est[None, :])\n    loss = -(Bernoulli(probs=probs_est).log_prob(data) * mask).mean()\n    loss.backward()\n    return loss\n\n# EM Algorithm\npbar = tqdm(range(100))\nfor iteration in pbar:\n    if iteration &gt; 0:\n        previous_theta = theta_est.clone()\n        previous_z = z_est.clone()\n        previous_loss = loss.clone()\n    \n    loss = optimizer.step(closure)\n    \n    if iteration &gt; 0:\n        d_loss = (previous_loss - loss).item()\n        d_theta = torch.norm(previous_theta - theta_est, p=2).item()\n        d_z = torch.norm(previous_z - z_est, p=2).item()\n        grad_norm = torch.norm(optimizer.param_groups[0][\"params\"][0].grad, p=2).item()\n        grad_norm += torch.norm(optimizer.param_groups[0][\"params\"][1].grad, p=2).item()\n        pbar.set_postfix({\"grad_norm\": grad_norm, \"d_theta\": d_theta, \"d_z\": d_z, \"d_loss\": d_loss})\n        if d_loss &lt; 1e-5 and d_theta &lt; 1e-5 and d_z &lt; 1e-5 and grad_norm &lt; 1e-5:\n            break\n\n# Compute AUC ROC on observed and inferred data\nfrom torchmetrics import AUROC\nauroc = AUROC(task=\"binary\")\nprobs_final = torch.sigmoid(theta_est[:, None] - z_est[None, :])\ntrain_probs = probs_final[mask]\ntest_probs = probs_final[~mask]\ntrain_labels = data[mask]\ntest_labels = data[~mask]\nauc_train = auroc(train_probs, train_labels)\nauc_test = auroc(test_probs, test_labels)\nprint(f\"train auc: {auc_train}\")\nprint(f\"test auc: {auc_test}\")\n\ntrain auc: 0.7983196973800659\ntest auc: 0.7844991683959961\n\n\n\n\n2.2.2 Pairwise Model\nIn pairwise sampling, participants compare two items to determine which is preferred. One of the major advantages of this method is the low cognitive demand for raters. Its disadvantage is the limited amount of information content elicited by a sample. Below is a survey based on pairwise sampling:\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nn_pairs = 10000\npair_indices = np.random.randint(0, n_items, size=(n_pairs, 2))\n# Exclude pairs where both indices are the same\nmask = pair_indices[:, 0] != pair_indices[:, 1]\npair_indices = pair_indices[mask]\n\nscores = np.zeros(n_items, dtype=int)\nwins = rewards[pair_indices[:, 0]] &gt; rewards[pair_indices[:, 1]]\n\n# For pairs where the first item wins:\n#   - Increase score for the first item by 1\n#   - Decrease score for the second item by 1\nnp.add.at(scores, pair_indices[wins, 0], 1)\nnp.add.at(scores, pair_indices[wins, 1], -1)\n\n# For pairs where the second item wins or it's a tie:\n#   - Decrease score for the first item by 1\n#   - Increase score for the second item by 1\nnp.add.at(scores, pair_indices[~wins, 0], -1)\nnp.add.at(scores, pair_indices[~wins, 1], 1)\n\n# Determine preferred and non-preferred items based on scores\npreferred = scores &gt; 0\nnon_preferred = scores &lt; 0\n\ndraw_surface()\nplt.scatter(items[preferred, 0], items[preferred, 1], c='blue', label='Preferred', alpha=0.5)\nplt.scatter(items[non_preferred, 0], items[non_preferred, 1], c='purple', label='Non-preferred', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe Bradley-Terry model compares the reward of choice over all others (Bradley and Terry 1952) in the set of \\(J\\) choices \\(i \\in \\{1, 2, \\dots, J\\}\\). Each choice can also have its unique random noise variable representing the unobserved factor. However, we can also choose to have all choices’ unobserved factors follow the same distribution (e.g., independent and identically distributed, IID). The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: \\(\\epsilon \\sim \\mathcal{N}(0, \\Sigma)\\). If \\(\\Sigma\\) is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the IID assumption. In the case of the extreme value distribution, we model the probability of a user preferring choice \\(i\\), which we denote as \\(P_i = Z^{-1}\\exp(u_{i,j}^*)\\) where \\(Z = \\sum_{j = 1}^{J} \\exp(u_{i,j}^*)\\).\nWe can model an open-ended ranking of the available items with the Plackett-Luce model, in which we jointly model the full sequence of choice ordering (Plackett 1975). The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of \\(J\\) choices \\(\\{y_1, \\dots, y_J\\}\\), we factorize the joint probability into conditionals. Each conditional follows the Bradley-Terry model: \\[\np(y_1, \\dots, y_J) = p(y_1) p(y_2 | y_1) ... p(y_J | y_{1:{J - 1}}) = \\prod_{i = 1}^J \\frac{\\exp(u_{i,j}^*)}{\\sum_{j \\ge i} \\exp(u_{i,j}^*)}\n\\]\nPairwise sampling has proven useful in aligning large language models (LLM) with human preference. An LLM, such as GPT-4, Llama 3.2, and BERT, typically refers to a large and pre-trained neural network that serves as the basis for various downstream tasks. They are pre-trained on a massive corpus of text data, learning to understand language and context. They are capable of multiple language-related tasks such as text classification, language generation, and question answering. A LLM should be aligned to respond correctly based on human preferences. A promising approach is to train LLMs using reinforcement learning (RL) with the reward model (RM) learned from human preference data, providing a mechanism to score the quality of the generated text. This approach, known as RL from human feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.\nWe discuss the reward model used in the Llama2 model. The Llama2 RM (Touvron et al. 2023) is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping \\(L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^V\\), where \\(D\\) is the embedding dimension from the transformer decoder stack and \\(V\\) is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps \\(L: \\mathbb{R}^D \\rightarrow \\mathbb{R}^1\\). It’s important to initialize the RM from the LLM it’s meant to evaluate. The RM will have the same “knowledge” as the LLM. This is particularly useful for evaluation objectives such as “Does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be helpful to have the RM know more. In addition, the RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs. An RM is trained with paired preferences (prompt history, accepted response, rejected response). Prompt history is a multiturn history of user prompts and model generations; the accepted response is the preferred final model generation by an annotator, and the rejected response is the unpreferred response. The RM is trained with maximum likelihood under the Bradley-Terry model with an optional margin term m(r):\n\\[p(y_c \\succ y_r | x) = \\sigma(r_\\theta(x,y_c) - r_\\theta(x,y_r) - m(r))\\]\nThe margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate. Margins were designed primarily based on the sigmoid function, which is used to normalize the raw reward model score flattens out beyond the range of \\([-4, 4]\\). Thus, the maximum possible margin is eight. A small regularization term is often added to center the score distribution on 0. We consider two variants of preference rating-based margin. When the preference rating-based margin is small, outcomes are rated as “Significantly Better” (1), “Better” (2 out of 3), and “Slightly Better” (1 out of 3), and “Negligibly Better or Unsure” (0 out of 3). In contrast, when the margin is large, outcomes are rated as “Significantly Better” (3), “Better” (2), and “Slightly Better” (1), and “Negligibly Better or Unsure” (0 out of 3).\n\n\n2.2.3 List-wise Model\nMultiple-choice sampling involves participants selecting one item from a set of alternatives. Multiple-choice sampling is simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one item from many. It is beneficial in complex choice scenarios, such as modes of transportation, where choices are not independent (Bolt and Wollack 2009). Multiple-choice sampling often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other items. In rank-order sampling, participants rank items from most to least preferred. Used in voting, market research, and psychology, it provides rich preference data but is more complex and cognitively demanding than pairwise comparisons, especially for large item sets. Participants may also rank inconsistently (Ragain and Ugander 2019). In Best-worst scaling (BWS), participants are presented with items and asked to identify the most and least preferred items. The primary objective of BWS is to discern the relative importance or preference of items, making it widely applicable in various fields such as market research, health economics, and social sciences (Campbell and Erdem 2015). BWS provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in rewards that are easy to interpret. However, BWS also has limitations, including potential scale interpretation differences among participants and design challenges to avoid biases, such as the order effect or the context in which items are presented.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#function-class",
    "href": "src/chap2.html#function-class",
    "title": "2  Background",
    "section": "2.3 The Utility Function Class",
    "text": "2.3 The Utility Function Class\n\n2.3.1 Parametric and Nonparametric Function Class\nThe reward of the item can take parametric form, such as \\(z_j = f_{\\theta}(x_j)\\). It can also take the nonparametric form, which is commonly used in the ideal point model, where the reward of an item \\(j\\) is calculated by the distance from the item to the human in some embedding space(Huber 1976). Given vector representation \\(e_i\\) of choice \\(i\\) and a vector \\(v_n\\) representing an individual \\(n\\), we can use a distance function \\(K\\) to model a stochastic reward function with the unobserved factors following a specified distribution: \\(u_{n, i} = K(e_i, v_n) + \\epsilon_{n, i}\\). The intuition is that vectors exist in a shared \\(n\\)-dimensional space, and as such, we can use geometry to match choices whose representations are closest to that of a given individual (Jamieson and Nowak 2011; Tatli, Nowak, and Vinayak 2022) when equipped with a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations share a common embedding space. In some contexts, this can be a robust bias to add to the model (Greiner 2005), but it is a key factor one must consider before employing such a model, and it is a key design choice for modeling.\n\n\n2.3.2 Unimodal and Multimodal Function Class\nSo far, we have considered learning from data from one person with a particular set of preferences or a group with similar preferences, but this is not always the case. Consider a scenario where a user turns left at an intersection (Myers et al. 2021). What would they do if they saw a car speeding down the road approaching them? Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, the model can make an appropriate decision. However, what if the model was given data from both aggressive and timid drivers and does not know which data corresponds to which type of driver? A naive preference learning approach would result in a model trying to find a policy close enough to both driving patterns. The group label is often unobserved because it is expensive to obtain or a data point cannot be cleanly separated into any group (e.g., a more timid driver can be aggressive when they are in a hurry).\nMyers et al. (2022) formulates this problem as learning a mixture of \\(M\\) linear reward functions on the embedding space, where \\(M\\) is given. The reward of item \\(j\\) given by the expert \\(i\\) is given by: \\(f_i(e_j) = w^\\top_i e_j,\\) where \\(w_m\\) is a vector of parameters corresponding to the \\(m\\)-th expert’s preferences. An unknown distribution over the reward parameters exists, and we can represent this distribution with convex mixing coefficients \\(\\alpha = [\\alpha_1, ..., \\alpha_M]\\). Consider a robot that performs the following trajectories and asks a user to rank all the trajectories. The robot will be given back a set of trajectory rankings from M humans, and the objective is to learn the underlying reward function. Given the ranking \\((j_1 \\succ ... \\succ j_K | m)\\) of expert \\(m\\) and define \\(\\theta = \\{w_{1:M}, \\alpha_{1:M}\\}\\), the probability of item \\(j\\) being preferred by \\(m\\) over all other alternatives is\n\\[p(j_1 \\succ ... \\succ j_K | \\theta) = \\sum_{i = 1}^M \\alpha_i \\prod_{j = 1}^K  p_{ij}\\]\nThen the parameters posterior is \\(p(\\theta | Q_{1:T}, x_{1:T}) \\propto p(\\theta) \\prod_t p(x_t | Q_{\\leq t}, \\theta) = p(\\theta) \\prod_t p(x_t | \\theta, Q_t)\\). The first proportionality is from the Bayes rule and the assumption that the queries at timestamp \\(t\\) are conditionally independent of the parameters given history. This assumption is reasonable because the previous queries & rankings ideally give all the information to inform the choice of the next set. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters. The prior distribution is dependent on the use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a \\(M - 1\\) simplex to ensure that they add up to 1. Then, we can use maximum likelihood estimation to compute the parameters with the simplified posterior.\nAnother example setting multimodal preference is negotiations (Kwon et al. 2021). Let’s say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of ?fig-negotiation, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob’s proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting (Kwon et al. 2021) show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice’s utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning (Kwon et al. 2021).\n\n\n2.3.3 Single Objective and Multi-Objective Utility\nThe industry has centered around optimizing for two primary reward signals: helpfulness and harmlessness (safety). There are also other axes, such as factuality, reasoning, tool use, code, and multilingualism, but these are out of scope for us. The Llama2 paper collected preference data from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals. Two main approaches can be taken for RLHF in this context. Train a unified reward model that integrates both datasets or train two separate reward models, one for each quality, and optimize the LLM toward both. Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained in both. The chosen solution was item 2, where two RMs are used to train the LLM piecewise. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where \\(R_s\\), \\(R_h\\), and \\(R_c\\) are the safety, helpfulness, and combined reward, respectively. \\(g\\) and \\(p\\) are the model generation and the user prompt:\n\\[\n\\begin{aligned}\n    R_c(g \\mid p) =\n    \\begin{cases}\n        R_s(g \\mid p) & \\text{if } \\text{is\\_safety}(p) \\text{ or } R_s(g \\mid p) &lt; 0.15 \\\\\n        R_h(g \\mid p) & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\]\n\n\n2.3.4 Pretraining\n\nRL often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL aims to solve this by learning from human feedback, but this often demands a highly impractical number of queries or leads to oversimplified reward functions that don’t hold up in real-world tasks. As discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries to address the impractical requirement of human queries. (Hejna III and Sadigh 2023) proposes pre-training models on previous tasks with the meta-learning method MAML (Finn, Abbeel, and Levine 2017), and then the meta-trained model can adapt to new tasks with fewer queries. We consider settings where a state is denoted as \\(s\\in S\\), and action is denoted as \\(a\\in A\\), for state space \\(S\\) and action space \\(A\\). The reward function \\(r: S\\times A \\to \\mathbb{R}\\) is unknown and needs to be learned from eliciting human preferences. There are multiple tasks, each with its own reward function and transition probabilities. The reward model is parameterized by \\(\\psi\\). We denote \\(\\hat{r}_\\psi(s, a)\\) to be a learned estimate of an unknown ground-truth reward function \\(r(s, a)\\), parameterized by \\(\\psi\\). Accordingly, a reward model determines an RL policy \\(\\phi\\) by maximizing the accumulated rewards. The preferences is learned via pair. For each pre-training task, there is a dataset \\(D\\) consists of binary preference between pair of trajectory. Bradley-Terry model is used to predict the preferred trajectory.\nTo efficiently approximate the reward function \\(r_\\text{new}\\) for a new task with minimal queries, Hejna III and Sadigh (2023) utilizes a pre-trained reward function \\(\\hat{r}_\\psi\\) that can be quickly fine-tuned using just a few preference comparisons by leveraging the common structure across tasks by pre-training on data from prior tasks. Although any meta-learning method is compatible, (Hejna III and Sadigh 2023) opts for Model Agnostic Meta-Learning (MAML) due to its simplicity. With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference-based RL during an online adaptation phase. Given a pre-trained reward model \\(\\psi\\), the the active few-shot adaption iterates between finding informative pair of trajectory to query human and update reward model and corresponding policy with new data. Informative pair is selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize \\(\\mathbb{V}(p(e_j \\succ e_{j'}))\\) are selected each time feedback is collected.\nThe experiment tests the proposed method on the Meta-World benchmark (Yu et al. 2020). Three baselines compared with the proposed method are (1) Soft-Actor Critic (SAC) trained from ground truth rewards, representing performance upper bound, PEBBLE (Lee, Smith, and Abbeel 2021), which does not use information from prior tasks, and (3) Init, which initializes the reward model with the pretrained weights from meta learning but instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE. The results show that the proposed method outperforms all of the baseline methods. There are still some drawbacks. For example, many of the queries the model picks to elicit human preference are almost identical. Moreover, despite the improved query complexity, an impractical number of queries still need to be made. In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch if the new task is too out-of-distribution. Designing a method that automatically balances between using the prior information or training from scratch is an important future direction.\nZhou et al. (2019) studies a related problem by asking the question, “How can we efficiently learn both from expert demonstrations and from trials where we only get binary feedback from a human?” This paper seeks to learn new tasks with the following general problem setting: We only get one expert demonstration of the target task; after seeing the expert demonstration, robots try to solve the task 1 or more times; then the user (or some pre-defined reward function) annotates each trial as a success/failure; the agent learns from both the demos and the annotated trials to perform well on the target task. A task \\(i\\) is described by the tuple \\(\\{S, A, r_i, P_i\\}\\). \\(S\\) and \\(A\\) represents all possible states and action, respectively. \\(r_i\\) is the reward function \\(r_i : S \\times A \\to \\mathbb{R}\\), and \\(P_i\\) is the transition dynamics function. \\(S\\) and \\(A\\) are shared across tasks. Learning occurs in 3 phases. During the watch phase, we give the agent \\(K=1\\) demonstrations of the target tasks and all demonstrations are successful. In the Try phase, we use the agent learned during the Watch phase to attempt the task for \\(L\\) trials. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is \\(L\\) trajectories and corresponding feedback. After completing the trials, the agent must learn from both the original expert demonstrations and the trials to solve the target task.\nFirst, we are given a dataset of expert demonstrations containing multiple demos for each task and the dataset contains hundreds of tasks. Importantly, no online interaction is needed for training, and this method trains only with supervised learning. This section describes how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop. What we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations via meta-imitation learning. Given the demonstrations \\(\\{d_{i,k}\\}\\) for task \\(i\\), we sample another different demonstration coming from the same task \\(d_i^{\\text{test}}\\), where \\(d_i^{\\text{test}}\\) is an example of optimal behavior given the demonstrations. The policy is obtained by imitating actions taken on \\(d_i^{\\text{test}}\\) via maximum likelihood:\n\\[\\mathcal{L}^\\text{watch}(\\theta, \\mathcal{D}_i^*) = \\mathbb{E}_{\\{d_{i,k}\\} \\sim \\mathcal{D}_i^*} \\mathbb{E}_{\\{d_{i,k}^{\\text{test}}\\} \\sim \\mathcal{D}_i^*  \\{d_{i,k}\\}} \\mathbb{E}_{(s_t, a_t) \\sim d_i^{\\text{test}}} \\log \\pi_\\theta^{\\text{watch}} (a_t | s_t, \\{d_{i,k}\\})\\]\nThis corresponds to imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved? In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together. On the Try phase when the agent is given a set of demonstrations \\(\\{d_{i,k}\\}\\), we deploy the policy \\(\\pi_\\theta^{\\text{watch}}(a | s, \\{d_{i,k}\\})\\) to collect \\(L\\) trials. There is no training involved in the Try phase; we simply condition the policy on the given demonstrations. During the Watch phase, the objective was to train a policy conditioned on demonstrations \\(\\pi_\\theta^{\\text{watch}}(a | s, \\{d_{i,k}\\})\\). The authors of Watch, Try, Learn uses a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. We want to learn \\(\\pi_\\phi^{\\text{watch}}(a | s, \\{d_{i,k}\\}, \\{\\mathbf{\\tau}_{i, l}\\})\\). To train the policy, we again use meta-imitation learning, where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters \\(\\phi\\) to minimize the following loss: \\[\\mathcal{L}^{\\text{learn}}(\\phi, \\mathcal{D}_i, \\mathcal{D}_i^*) = \\mathbb{E}_{(\\{d_{i,k}\\}, \\{\\mathbf{\\tau}_{i,l}\\}) \\sim \\mathcal{D}_i} \\mathbb{E}_{\\{d_{i,k}^{\\text{test}}\\} \\sim \\mathcal{D}_i^* \\{d_{i,k}\\}} \\mathbb{E}_{(s_t, a_t) \\sim d_i^{\\text{test}}} \\big[- \\log \\pi_\\theta^{\\text{learn}} (a_t | s_t, \\{d_{i,k}\\}, \\{\\tau_{i,l}\\}) \\big]\\]\nThree baselines are considered: (1) behavior cloning is simple imitation learning based on maximum log-likelihood training using data from all tasks, (2) meta-imitation learning corresponds to simply running the policy from the Watch step without using any trial data. We only condition on the set of expert demonstrations, but no online trials, and (3) behavior cloning + SAC pretrains a policy with behavior cloning on all data, and follow that with RL fine-tuning for the specific target task, using the maximum-entropy algorithm SAC (Haarnoja et al. 2018). The proposed approach significantly outperforms baselines on every task family: it is far superior to behavior cloning and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families.\n\n\n2.3.5 Others Consideration\nOne key challenge is managing the bias and variance trade-off. Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example, if the two sets of vectors follow different distributions, each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final reward value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data. On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the output. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters and learn correlations in the data that do not contribute to learning human preferences but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.\nAnother important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world. Individual models may prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models that fit only one person can tremendously overfit the training distribution and capture noise in the data, which is not truly representative of human preferences. On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population. Choosing the appropriate scope for a model is crucial. It must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.\nWhen training or using a reward model, LLM Distribution Shift is an important factor to consider. With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution. In addition, RM and LLM are coupled: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM. Last but not least, training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap2.html#exercises",
    "href": "src/chap2.html#exercises",
    "title": "2  Background",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nQuestion 1: Choice Modeling (15 points)\nWe discussed discrete choice modeling in the context of reward being a linear function. Suppose we are deciding between \\(N\\) choices and that the reward for each choice is given by \\(U_i=\\beta_i\\mathbf{x}+\\epsilon_i\\) for \\(i=1, 2, \\cdots, N\\). We view \\(\\mathbf{x}\\) as the data point that is being conditioned on for deciding which choice to select, and \\(\\beta_i\\) as the weights driving the linear reward model. The noise \\(\\epsilon_i\\) is i.i.d. sampled from a type of extreme value distribution called the Gumbel distribution. The standard Gumbel distribution is given by the density function \\(f(x)=e^{-(x+e^{-x})}\\) and cumulative distribution function \\(F(x)=e^{-e^{-x}}.\\) Fix \\(i\\). Our objective is to calculate \\(p(U_i\\,\\, \\text{has max reward})\\).\n\n(Written, 2 points). Set \\(U_i=t\\) and compute \\(p(U_j&lt;t)\\) for \\(j\\neq i\\) in terms of \\(F\\). Use this probability to derive an integral for \\(p(U_i\\,\\,  \\text{has max reward})\\) over \\(t\\) in terms of \\(f\\) and \\(F\\). Example of solution environment.\n(Written, 4 points). Compute the integral derived in part (a) with the appropriate \\(u\\)-substitution. You should arrive at multi-class logistic regression!\n\nNext, you will implement logistic regression to predict preferred completions. We will use the preference dataset from RewardBench. Notice the provided data/chosen_embeddings.pt and data/rejected_embeddings.pt files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token’s final hidden embedding. Let \\(e_1\\) and \\(e_2\\) be two hidden embeddings with \\(e_1\\succ e_2\\). We assume reward is a linear function of embedding \\(u_j=w^\\top e_j\\) and use the Bradley-Terry model to predict the preferred item. We can view maximum likelihood across the preference dataset with this model as logistic regression on \\(e_1-e_2\\) and all labels being \\(1\\). Here, we are given a dataset \\(X\\) with \\(N\\) rows of datapoints and \\(D\\) features per datapoint. The weights of the model are parametrized by \\(w\\), a \\(d\\)-dimensional column vector. Given binary labels \\(y\\) of shape \\(N\\) by \\(1\\), the negative log likelihood function and the corresponding gradient is\n\\[p(y, X| w)=-\\frac{1}{N}(y^\\top \\log(\\sigma(X^\\top w)) + (1-y)^\\tau \\log(1-\\sigma(X^\\top w))), \\quad \\nabla_w p(y, X | w)=\\frac{1}{N}X^T(\\sigma(X^\\top w)-y),\\]\nwhere \\(\\sigma\\) is the sigmoid function and is applied element-wise along with \\(\\log\\). As usual, we use maximum likelihood to learn the parameter.\n\n(Coding, 5 points). Implement the functions train and the predict_probs in LogisticRegression class. The starter code is provided below.\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n(Written, 4 points). Open the notebook rewardbench_preferences.ipynb and run all the cells. Make sure to tune the learning_rate and num_iterations. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve \\(\\approx 90\\%\\) expected accuracy on validation. You may add loss reporting to the train function to verify your model is improving over time.\n\n\n\nQuestion 2: Revealed and Stated Preferences (20 points)\nAlice and Bob are running for president. For \\(R\\) voters, we can access their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an unknown probability \\(z\\) of voting for Alice among the population. The aim of this question is to estimate \\(z\\) through maximum likelihood estimation by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability \\(\\frac{z+1}{2}\\) for reasons of “political correctness.”\n\n(Written, 5 points). Suppose there are \\(R_A\\) revealed preferences for Alice, \\(R_B\\) revealed preferences for Bob, \\(S_A\\) stated preferences for Alice, and \\(S_B\\) stated preferences for Bob. Note \\(R=R_A+R_B\\). Compute the log-likelihood of observing such preferences in terms of \\(z, R_A, R_B, S_A, S_B\\).\n(Coding, 1 point). Implement the short function stated_prob in the file voting/simulation.py.\n(Coding, 5 points). Implement the class VotingSimulation.\n(Coding, 7 points). Implement your derived expression from part (a) in the log_likelihoods function.\n(Written, 2 points). Finally, implement the average_mae_mle method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate \\(\\hat{z}\\) (i.e., \\(|\\hat{z}-z|\\)) as the number of voters surveyed increases. Open voting/visualize_sim.ipynb and run the cells to get a plot of MAE vs. voters surveyed averaged across \\(100\\) simulations. Attach the plot to this question and briefly explain what you notice.\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nQuestion 3: Probabilistic Multi-modal Preferences (25 points)\nSuppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with \\(30000\\) examples of the form \\((m_1, m_2, \\text{user id})\\) where \\(m_1\\) and \\(m_2\\) are movies with \\(m_1\\succ m_2\\). The preferences come from \\(600\\) distinct users with \\(50\\) examples per user. Each movie has a \\(10\\)-dimensional feature vector \\(m\\), and each user has a \\(10\\)-dimensional weight vector \\(u\\). Given movie features \\(m_1, m_2\\) and user weights \\(u\\), the user’s preference between the movies is given by a Bradley-Terry reward model: \\[P(m_1\\succ m_2)=\\frac{e^{u\\cdot m_1}}{e^{u\\cdot m_1} + e^{u\\cdot m_2}}=\\frac{1}{1+e^{u\\cdot (m_2-m_1)}}=\\sigma(u\\cdot (m_1-m_2)).\\]\nYou realize that trying to estimate the weights for each user with only \\(50\\) examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a multi-modal preferences approach. For simplicity, you assume a model where a proportion \\(p\\) of the users have weights \\(w_1\\) and the other \\(1-p\\) have weights \\(w_2\\). In this setting, each user belongs to one of two groups: users with weights \\(w_1\\) are part of Group 1, and users with weights \\(w_2\\) are part of Group 2.\n\n(Written, 3 points). For a datapoint \\((m_1, m_2)\\) with label \\(m_1\\succ m_2\\), compute the data likelihood \\(P(m_1\\succ m_2 | p, w_1, w_2)\\) assuming \\(p, w_1, w_2\\) are given.\n(Written, 3 points). As a follow up, use the likelihood to simplify the posterior distribution of \\(p, w_1, w_2\\) after updating on \\((m_1, m_2)\\) leaving terms for the priors unchanged.\n(Written, 4 points). Assume priors \\(p\\sim B(1, 1)\\), \\(w_1\\sim\\mathcal{N}(0, \\mathbf{I})\\), and \\(w_2\\sim\\mathcal{N}(0, \\mathbf{I})\\) where \\(B\\) represents the Beta distribution and \\(\\mathcal{N}\\) represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to Markov Chain Monte Carlo (MCMC) approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain \\(\\{x_t\\}_{t=1}^\\infty\\) so that \\(\\lim_{t\\to\\infty}x_t\\) act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that \\(x_{t+1}\\) only depends on \\(x_t\\) for all \\(t\\ge 1\\).\n\nThe most basic version of MCMC is known as Metropolis-Hastings. Assume \\(\\pi\\) is the target distribution we wish to sample from where \\(\\pi(z)\\) represents the probability density at point \\(z\\). Metropolis-Hastings constructs the approximating Markov chain \\(x_t\\) as follows: a proposal \\(P\\) for \\(x_{t+1}\\) is made via sampling from a chosen distribution \\(Q(\\,\\cdot\\,| x_t)\\) (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by\n\\[A= \\min \\left( 1, \\frac{\\pi(P)Q(x_t | P)}{\\pi(x_t)Q(P | x_t)} \\right). \\text{ That is } x_{t+1}=\\begin{cases} P & \\text{with probability } A, \\\\ x_t & \\text{with probability } 1 - A. \\end{cases}\\] To extract our samples from \\(\\pi\\), we run the Markov chain for \\(N\\) timesteps and disregard the first \\(T&lt;N\\) timesteps in what is called the burn-in or mixing time (i.e., our final samples are \\(x_{T+1}, x_{T+2},\\cdots, x_{N}\\)). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution \\(\\pi\\) – initial elements of the chain will not be a good approximation of \\(\\pi\\) and depend more on the choice of initialization \\(x_1\\). To build some intuition, suppose we have a biased coin that turns heads with probability \\(p_{\\text{heads}}\\). We observe \\(12\\) coin flips to have \\(9\\) heads (H) and \\(3\\) tails (T). If our prior for \\(p_{\\text{H}}\\) was \\(B(1, 1)\\), then our posterior will be \\(B(1+9, 1+3)=B(10, 4)\\). The Bayesian update is given by\n\\[p(p_{\\text{H}}|9\\text{H}, 3\\text{T}) = \\frac{p(9\\text{H}, 3\\text{T} | p_{\\text{H}})B(1, 1)(p_{\\text{H}})}{\\int_0^1 P(9\\text{H}, 3\\text{T} | p_{\\text{H}})B(1, 1)(p_{\\text{H}}) dp_{\\text{H}}} =\\frac{p(9\\text{H}, 3\\text{T} | p_{\\text{H}})}{\\int_0^1 p(9\\text{H}, 3\\text{T} | p_{\\text{H}})  dp_{\\text{H}}}.\\]\nFind the acceptance probablity \\(A\\) in the setting of the biased coin assuming the proposal distribution \\(Q(\\cdot|x_t)=x_t+N(0,\\sigma)\\) for given \\(\\sigma\\). Notice that this choice of \\(Q\\) is symmetric, i.e., \\(Q(x_t|P)=Q(P|x_t)\\). In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!\n\n(Written + Coding, 6 points). Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in multimodal_preferences/biased_coin.py. Attach a histogram of your MCMC samples overlayed on top of the true posterior \\(B(10, 4)\\) by running python biased_coin.py.\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n(Coding, 9 points). Implement Metropolis-Hastings in the movie setting inside multimodal_preferences/movie_metropolis.py. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running python movie_metropolis.py. You should be able to achieve a \\(90\\%\\) success rate with most fraction_accepted values above \\(0.1\\). Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "src/chap3.html",
    "href": "src/chap3.html",
    "title": "3  Learning",
    "section": "",
    "text": "3.1 The Supervised Learning Problem\nDesigning a good utility function (or reward function) by hand for a complex AI or robotics task is notoriously difficult and error-prone. Instead of manually specifying what is “good” behavior, we can learn a utility function from human preferences. In this chapter, we explore how an agent can infer a human’s underlying utility function (their preferences or reward criteria) from various forms of feedback. We discuss both supervised learning and Bayesian approaches to utility learning, and examine techniques motivated by robotics—learning from demonstrations, physical corrections, trajectory evaluations, and pairwise comparisons. Throughout, we include mathematical formulations and code examples to illustrate the learning process.\nSupervised learning approaches treat human feedback as labeled data to directly fit a utility function. The core idea is to assume there exists a true utility function \\(u^*(x)\\) (over states, outcomes, or trajectories \\(x\\)) that explains a human’s choices. We then choose a parameterized model \\(u_\\theta(x)\\) and adjust \\(\\theta\\) so that \\(u_\\theta\\) agrees with the human-provided preferences.\nA common feedback format is pairwise comparisons: the human is shown two options (outcomes or trajectories) \\(A\\) and \\(B\\) and indicates which is preferred. We can model the probability that the human prefers \\(A\\) over \\(B\\) using a logistic or Bradley–Terry model:\n\\[\nP(A \\succ B \\mid u_\\theta) \\;=\\; \\sigma\\!\\Big(u_\\theta(A) - u_\\theta(B)\\Big)\\,,\n\\]\nwhere \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) is the sigmoid function. This implies the human is more likely to prefer \\(A\\) if \\(u_\\theta(A)\\) is much larger than \\(u_\\theta(B)\\).\nAt the heart of learning from human preferences lies a latent utility function — a function that assigns numerical value to states, trajectories, or outcomes according to a human’s (possibly unspoken) preferences. The goal of a learning algorithm is to infer this function from observed feedback, which may come in the form of demonstrations, ratings, rankings, or pairwise comparisons. But how exactly do we represent and update our belief about this hidden utility function?\nTwo major paradigms in statistical learning provide different answers: point estimation and posterior estimation.\nIn point estimation, we seek a single “best guess” for the utility function — typically a function \\(u_\\theta(x)\\) from a parameterized family (e.g. linear models, neural nets), with parameters \\(\\theta \\in \\mathbb{R}^d\\). Given data \\(\\mathcal{D}\\) from human feedback (e.g. preferences), we choose the parameter \\(\\hat{\\theta}\\) that best explains the observed behavior. Formally:\n\\[\n\\hat{\\theta} = \\arg\\max_\\theta \\; p(\\mathcal{D} \\mid \\theta)\n\\]\nThis is maximum likelihood estimation (MLE): we pick the parameters that make the observed data most probable under our model. Once \\(\\hat{\\theta}\\) is selected, we treat \\(u_{\\hat{\\theta}}\\) as the agent’s utility function, and optimize or sample behavior accordingly. This approach is straightforward and computationally efficient. It is the foundation of most supervised learning methods (like logistic regression or deep learning), and it provides a natural interpretation: we’re directly finding the utility function that agrees with the human feedback. However, it discards uncertainty: it assumes the data is sufficient to pin down a single utility function, which may not be true in practice.\nIn contrast, posterior estimation takes a fully Bayesian view. Instead of committing to one estimate, we maintain a distribution over utility functions. That is, we place a prior \\(p(\\theta)\\) over parameters (or over functions \\(u\\) more generally), and update this to a posterior after observing data \\(\\mathcal{D}\\):\n\\[\np(\\theta \\mid \\mathcal{D}) \\;=\\; \\frac{p(\\mathcal{D} \\mid \\theta)\\, p(\\theta)}{p(\\mathcal{D})}\n\\]\nThis posterior expresses our uncertainty over which utility functions are compatible with the human feedback. From this distribution, we can make predictions (e.g., using the posterior mean utility), quantify confidence, or even actively select new queries to reduce uncertainty (active learning). For instance, if we model utilities with a Gaussian Process (GP), then the posterior over \\(u(x)\\) is also a GP after observing comparisons or evaluations. If we use a neural network for \\(u_\\theta(x)\\), we can approximate the posterior with ensembles, variational inference, or MCMC. Posterior estimation is especially valuable when human feedback is sparse, noisy, or ambiguous — as is often the case in real-world preference learning. It allows the agent to reason about what it doesn’t know and to take cautious or exploratory actions accordingly.\nThe next two sections instantiate these two perspectives. In Section 4.1, we explore point estimation via supervised learning — treating preference data as labeled examples and fitting a utility model. In Section 4.2, we shift to posterior estimation with Bayesian methods like Gaussian processes and Bayesian neural networks, which model both our current estimate and the uncertainty around it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#point-estimation-via-maximum-likelihood",
    "href": "src/chap3.html#point-estimation-via-maximum-likelihood",
    "title": "3  Learning",
    "section": "3.2 Point Estimation via Maximum Likelihood",
    "text": "3.2 Point Estimation via Maximum Likelihood\nGiven a dataset of comparisons \\(D=\\{(A_i, B_i, y_i)\\}\\) (with \\(y_i=1\\) if \\(A_i\\) was preferred and \\(0\\) if \\(B_i\\) was preferred), we can fit \\(\\theta\\) by maximizing the likelihood of the human’s choices. Equivalently, we minimize a binary cross-entropy loss:\n\\[\n\\mathcal{L}(\\theta) = -\\sum_{i} \\Big[\\,y_i \\log \\sigma(u_\\theta(A_i)\\!-\\!u_\\theta(B_i)) + (1-y_i)\\log(1-\\sigma(u_\\theta(A_i)\\!-\\!u_\\theta(B_i)))\\Big]\\,,\n\\]\noften with a regularization term to prevent overfitting. This is a straightforward supervised learning problem – essentially logistic regression – on pairwise difference features.\nExample: Suppose a human’s utility for an outcome can be described by a quadratic function (unknown to the learning algorithm). We collect some pairwise preferences and then train a utility model \\(u_\\theta(x)\\) to predict those preferences. The code below simulates this scenario:\n\nimport numpy as np\n\n# True utility function (unknown to learner), e.g. u*(x) = -(x-5)^2 + constant \ndef true_utility(x):\n    return -(x-5)**2  # (peak at x=5)\n\n# Generate synthetic pairwise preference data\nnp.random.seed(42)\nn_pairs = 20\nX1 = np.random.uniform(0, 10, size=n_pairs)  # 20 random x-values\nX2 = np.random.uniform(0, 10, size=n_pairs)  # 20 more random x-values\n# Determine preferences according to true utility\nprefs = (true_utility(X1) &gt; true_utility(X2)).astype(int)  # 1 if X1 preferred, else 0\n\n# Parametric model for utility: u_theta(x) = w0 + w1*x + w2*x^2  (quadratic form)\n# Initialize weights\nw = np.zeros(3)\nlr = 0.01       # learning rate\nreg = 1e-3      # L2 regularization strength\nfor epoch in range(1000):\n    # Compute predictions via logistic model\n    util_diff = (w[0] + w[1]*X1 + w[2]*X1**2) - (w[0] + w[1]*X2 + w[2]*X2**2)\n    pred = 1 / (1 + np.exp(-util_diff))      # σ(w·(phi(X1)-phi(X2)))\n    # Gradient of cross-entropy loss\n    grad = np.array([0.0, 0.0, 0.0])\n    error = pred - prefs  # (sigma - y)\n    # Features for X1 and X2\n    phi1 = np.vstack([np.ones(n_pairs), X1, X1**2]).T\n    phi2 = np.vstack([np.ones(n_pairs), X2, X2**2]).T\n    phi_diff = phi1 - phi2\n    # Gradient: derivative of loss w.rt w = (sigma - y)*φ_diff (averaged) + reg\n    grad = phi_diff.T.dot(error) / n_pairs + reg * w\n    # Update weights\n    w -= lr * grad\n\nprint(\"Learned weights:\", w)\n\nLearned weights: [ 0.          2.74417195 -0.22129969]\n\n\nAfter training, we can compare the learned utility function \\(u_\\theta(x)\\) to the true utility \\(u^*(x)\\). Below we plot the two functions:\n\nimport matplotlib.pyplot as plt\n\n# Plot true vs learned utility curves\nxs = np.linspace(0, 10, 200)\ntrue_vals = true_utility(xs)\nlearned_vals = w[0] + w[1]*xs + w[2]*xs**2\n\nplt.figure(figsize=(6,4))\nplt.plot(xs, true_vals, label=\"True Utility\", linewidth=3)\nplt.plot(xs, learned_vals, label=\"Learned Utility\", linestyle=\"--\", linewidth=3)\nplt.xlabel(\"State x\")\nplt.ylabel(\"Utility\")\nplt.title(\"True vs. Learned Utility Function\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe learned curve closely matches the true utility up to an arbitrary scaling factor (utility is only defined up to affine transform when inferred from comparisons). The algorithm successfully recovered a utility function that orders states almost the same as the true utility \\(u^*(x)\\). In general, learning from comparisons can infer the relative utility of options (which item is preferred), although the absolute scale of \\(u_\\theta\\) is unidentifiable without further assumptions. Supervised learning on preferences has been widely used for ranking problems and preference-based reward learning.\nIn standard preference learning, we often learn a utility function and then use it to define a policy. However, in some settings—especially those involving large models like language models—it is more effective to directly learn a policy that aligns with human preferences, bypassing the intermediate reward model. One such method is Direct Preference Optimization (DPO), which offers a simple, stable way to align a policy to preference data through supervised learning.\nTo understand DPO, consider the following setting:\n\nWe are given a reference policy \\(\\pi_{\\text{ref}}\\), such as a pre-trained language model.\nWe want to learn a new policy \\(\\pi_\\theta\\) that improves upon \\(\\pi_{\\text{ref}}\\) by better reflecting human preferences.\nOur data consists of pairwise comparisons: for each prompt \\(x\\), a human expresses a preference between two outputs \\(y_+ \\succ y_-\\), where \\(y_+\\) is the preferred response.\n\nRather than learning an explicit reward function \\(R(x, y)\\) and using it to optimize the policy via reinforcement learning, DPO treats this as a classification problem: we want to encourage the policy to assign higher likelihood to the preferred response.\nTo formalize this, we define a preference score: \\[\ns_\\theta(x, y_+, y_-) = \\log \\pi_\\theta(y_+ \\mid x) - \\log \\pi_\\theta(y_- \\mid x)\n\\] This is the difference in log-likelihood between the preferred and dispreferred outputs. We can then define the DPO loss as a logistic regression objective: \\[\n\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\log \\sigma\\left(s_\\theta(x, y_+, y_-)\\right)\n\\] where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function.\nThis loss encourages \\(\\pi_\\theta\\) to assign greater probability mass to \\(y_+\\) than \\(y_-\\), pushing the policy toward outputs that align with human preferences. Because this is a differentiable, supervised loss, it can be optimized with standard gradient-based techniques, without needing to sample from the environment or estimate advantages, as in traditional RL.\nAlthough DPO does not explicitly define or optimize a reward function, we can interpret it as doing so implicitly. Suppose we define a reward function: \\[\nR_\\theta(y \\mid x) = \\log \\pi_\\theta(y \\mid x) - \\log \\pi_{\\text{ref}}(y \\mid x)\n\\] This reward encourages \\(\\pi_\\theta\\) to move away from \\(\\pi_{\\text{ref}}\\) in directions that increase the probability of preferred outputs. Under this formulation, the DPO objective can be interpreted as optimizing this reward difference directly from preferences.\nTo understand why this implicit reward leads to a stable and interpretable policy, we can connect DPO to the principle of maximum entropy. This principle says that, among all distributions that satisfy certain constraints (e.g., achieving a particular expected reward), we should prefer the one with maximum entropy—that is, the most uncertain or uncommitted distribution consistent with our knowledge.\nFormally, consider the space \\(\\mathcal{P}\\) of distributions over responses \\(y\\), and a reward function \\(R(y)\\). The maximum entropy distribution that satisfies a reward constraint is the solution to:\n\\[\np^*(y) = \\arg\\max_{p \\in \\mathcal{P}} H(p) \\quad \\text{subject to} \\quad \\mathbb{E}_p[R(y)] \\geq \\rho\n\\]\nThe solution to this constrained optimization problem is a Boltzmann distribution: \\[\np^*(y) \\propto \\exp\\left(\\frac{R(y)}{\\tau}\\right)\n\\] for some temperature \\(\\tau &gt; 0\\), where \\(\\tau\\) controls how deterministic the distribution is. As \\(\\tau \\to 0\\), the distribution concentrates on the highest-reward outputs; as \\(\\tau \\to \\infty\\), it becomes uniform.\nNow suppose our reference policy \\(\\pi_{\\text{ref}}(y \\mid x)\\) already represents a reasonable starting point. Then the optimal policy \\(\\pi_\\theta\\) can be viewed as a reward-weighted version of this reference policy:\n\\[\n\\pi_\\theta(y \\mid x) \\propto \\pi_{\\text{ref}}(y \\mid x) \\cdot \\exp\\left(R_\\theta(y \\mid x)\\right)\n\\]\nThis form ensures that \\(\\pi_\\theta\\) remains close to \\(\\pi_{\\text{ref}}\\) (via the KL term), while still assigning more mass to high-reward (preferred) outputs. Importantly, this form arises naturally from maximum entropy inference when the reference distribution is used as a baseline.\nDPO thus combines reward maximization with entropy regularization, encouraging the learned policy to prefer outcomes favored by human feedback while preserving diversity and stability. It sidesteps the challenges of explicitly learning a reward model or tuning complex RL pipelines, offering a direct and scalable method for preference-based alignment.\nIn practice, DPO has been shown to achieve similar or better alignment performance compared to reinforcement learning from human feedback (RLHF) while being more stable and easier to implement. It avoids the need to sample from the model during training or tune delicate hyperparameters of RL. Conceptually, DPO demonstrates that if we structure our utility model cleverly (here, as the log-ratio of policy and reference), we can extract an optimal policy in closed-form and learn utilities via supervised learning.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy.special import logsumexp\n\n# --- Setup: 1D input x, discrete actions y ---\nnp.random.seed(0)\nx = 5.0  # fixed input\nY = np.linspace(-4, 4, 100)  # discrete action space\nn_actions = len(Y)\n\n# --- True reward function (unknown to learner) ---\ndef true_reward(x, y):\n    return -((y - np.sin(x))**2)  # reward peak near y = sin(x)\n\nR_true = true_reward(x, Y)\n\n# --- Reference policy: fixed Gaussian-like distribution ---\ndef ref_policy(y):\n    logits = -0.5 * (y / 2.0)**2  # log probs of N(0, 2^2)\n    return np.exp(logits - logsumexp(logits))\n\npi_ref = ref_policy(Y)\n\n# --- Preference data from reward samples ---\ndef sample_preference(x, Y, R_fn, temperature=1.0):\n    logits = R_fn(x, Y) / temperature\n    probs = np.exp(logits - logsumexp(logits))\n    sampled = np.random.choice(len(Y), size=2, replace=False, p=probs)\n    y_plus, y_minus = sampled if R_fn(x, Y[sampled[0]]) &gt; R_fn(x, Y[sampled[1]]) else sampled[::-1]\n    return y_plus, y_minus\n\nn_pairs = 100\npair_indices = [sample_preference(x, Y, true_reward) for _ in range(n_pairs)]\n\n# --- DPO loss and gradient ---\ndef dpo_loss_and_grad(theta, y_pos_idx, y_neg_idx, pi_ref):\n    logits = theta + np.log(pi_ref + 1e-8)\n    logp_pos = logits[y_pos_idx] - logsumexp(logits)\n    logp_neg = logits[y_neg_idx] - logsumexp(logits)\n    s = logp_pos - logp_neg\n    sigma = 1 / (1 + np.exp(-s))\n    loss = -np.log(sigma + 1e-8)\n    softmax = np.exp(logits - logsumexp(logits))\n    grad = - (1 - sigma) * (np.eye(n_actions)[y_pos_idx] - np.eye(n_actions)[y_neg_idx]) + sigma * softmax\n    return loss, grad\n\n# --- Training loop with history tracking ---\ntheta = np.zeros(n_actions)\nlr = 0.05\nn_steps = 100\nhistory = []\n\nfor step in range(n_steps):\n    total_grad = np.zeros_like(theta)\n    for y_pos_idx, y_neg_idx in pair_indices:\n        _, grad = dpo_loss_and_grad(theta, y_pos_idx, y_neg_idx, pi_ref)\n        total_grad += grad\n    theta -= lr * total_grad / n_pairs\n    logits_snapshot = theta + np.log(pi_ref + 1e-8)\n    pi_snapshot = np.exp(logits_snapshot - logsumexp(logits_snapshot))\n    history.append(pi_snapshot)\n\n# --- Animation setup ---\nfig, ax = plt.subplots(figsize=(7, 4))\nline_true, = ax.plot(Y, R_true, 'k--', label='True Reward')\nline_ref, = ax.plot(Y, pi_ref, 'g-', label='Reference Policy')\nline_learned, = ax.plot([], [], 'b-', label='Learned Policy')\n\n# Add preference pair indicators\npref_lines = [ax.axvline(Y[idx], color='blue', linestyle=':', alpha=0.3) for idx, _ in pair_indices]\npref_lines += [ax.axvline(Y[idx], color='red', linestyle=':', alpha=0.3) for _, idx in pair_indices]\n\nax.set_ylim(-0.025, 0.025)\nax.set_title(\"DPO Policy Evolution\")\nax.set_ylabel(\"Probability\")\nax.set_xlabel(\"y\")\nax.legend()\n\ndef update(frame):\n    pi_snapshot = history[frame]\n    line_learned.set_data(Y, pi_snapshot)\n    ax.set_title(f\"DPO Policy Evolution (Step {frame + 1})\")\n    return [line_learned]\n\nfrom IPython.display import HTML\nfrom matplotlib import rc\nrc('animation', html='jshtml')\n\nani = animation.FuncAnimation(fig, update, frames=n_steps, interval=100, blit=True)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThe pairwise logistic approach can be extended to other feedback types. If humans provide numeric ratings or scores for options, one can treat utility learning as a regression problem: fit \\(u_\\theta(x)\\) to predict those scores (perhaps with a suitable bounded output or ordinal regression if scores are ordinal). If humans rank multiple options at once, algorithms like RankNet or RankSVM generalize the pairwise approach to listwise ranking losses. All these methods boil down to defining a loss that penalizes disagreements between the predicted utility order and the human-provided preferences, then optimizing \\(\\theta\\) to minimize that loss.\nSupervised learning of utility is powerful due to its simplicity, but it typically provides point estimates of \\(u_\\theta\\). Next, we consider Bayesian approaches that maintain uncertainty over the utility function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#posterior-estimation",
    "href": "src/chap3.html#posterior-estimation",
    "title": "3  Learning",
    "section": "3.3 Posterior Estimation",
    "text": "3.3 Posterior Estimation\nWhen feedback data is sparse, as is common in preference learning, it can be advantageous to model uncertainty over the utility function. Bayesian approaches place a prior on the utility function and update a posterior as human feedback is observed. This yields not only a best-guess utility function but also a measure of confidence or uncertainty, which is valuable for active learning (deciding which queries to ask next) and for safety (knowing when the learned reward might be wrong).\nA popular Bayesian approach assumes that the human’s utility function can be modeled as a Gaussian Process (GP) – a distribution over functions. A GP prior is defined by a mean function (often taken to be 0 for convenience) and a kernel (covariance function) \\(k(x,x')\\) which encodes assumptions about the smoothness or structure of the utility function. For example, one might assume \\(u(x)\\) is a smooth function of state, and choose a radial basis function (RBF) kernel \\(k(x,x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2/(2\\ell^2))\\) with some length-scale \\(\\ell\\).\nAfter observing some preference data, Bayes’ rule gives a posterior over the function \\(u(x)\\). In the case of pairwise comparisons, the likelihood of a comparison \\((A \\succ B)\\) given an underlying utility function \\(u\\) can be modeled via the same logistic function: \\(P(A \\succ B \\mid u) = \\sigma(u(A)-u(B))\\). Combining this likelihood with the GP prior is analytically intractable (due to the non-Gaussian logistic likelihood), but one can use approximation techniques (Laplace approximation or MCMC) to obtain a posterior GP. The result is a Gaussian process preference model that can predict the utility of any new option with an uncertainty interval.\nIf we have direct evaluations of utility (e.g., the human provides a numeric reward for some states), the GP inference is simpler – it reduces to standard GP regression. However, in many real-world scenarios, humans are better at making relative judgments than assigning absolute utility values. This change in feedback type transforms the inference problem fundamentally. Instead of having a Gaussian likelihood (as in standard GP regression), we now have a non-Gaussian likelihood, typically modeled using a probit or logistic function. The observed data no longer provide direct samples of the latent utility function, but instead impose constraints on the relative ordering of latent values.\nDue to this non-Gaussian likelihood, exact Bayesian inference is no longer tractable: the posterior over the latent utility function given the pairwise data does not have a closed-form expression. The GP prior is still Gaussian, but the posterior becomes non-Gaussian and multi-modal, particularly as the number of comparisons grows.\nTo address this, we must turn to approximate inference methods. One common and computationally efficient choice is the Laplace approximation, which approximates the true posterior with a Gaussian centered at the maximum a posteriori (MAP) estimate. This involves: 1. Finding the mode of the posterior (i.e., the most probable utility values given the data), 2. Approximating the curvature of the log-posterior around this mode using the Hessian (second derivative), 3. Using this local curvature to construct a Gaussian approximation.\nWhile not exact, this method works well in practice, especially when the posterior is unimodal and the number of comparison pairs is moderate. Other alternatives such as variational inference or sampling-based methods (e.g., Hamiltonian Monte Carlo) can yield more accurate results but often require more complex implementation and computational resources.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\n# --- True latent utility function ---\ndef true_u(x):\n    return np.sin(x) + 0.1 * x\n\n# --- RBF Kernel function ---\ndef rbf_kernel(x1, x2, length_scale=0.8, sigma_f=1.0):\n    x1, x2 = np.atleast_2d(x1).T, np.atleast_2d(x2).T\n    sqdist = (x1 - x2.T) ** 2\n    return sigma_f**2 * np.exp(-0.5 * sqdist / length_scale**2)\n\n# --- Generate synthetic preference data ---\nnp.random.seed(42)\nnum_pairs = 10\nX_candidates = np.linspace(0, 10, 100)\ntrue_utilities = true_u(X_candidates)\n\n# Sample preference pairs\nidx_pairs = np.random.choice(len(X_candidates), size=(num_pairs, 2), replace=True)\nX_pref_pairs = []\nfor i, j in idx_pairs:\n    xi, xj = X_candidates[i], X_candidates[j]\n    if true_utilities[i] &gt; true_utilities[j]:\n        X_pref_pairs.append((xi, xj))\n    else:\n        X_pref_pairs.append((xj, xi))\nX_pref_pairs = np.array(X_pref_pairs)\n\n# --- Unique x values and indexing ---\nX_all = np.unique(X_pref_pairs.flatten())\nn = len(X_all)\nx_to_idx = {x: i for i, x in enumerate(X_all)}\n\n# --- GP prior kernel matrix ---\nlength_scale = 0.8\nsigma_f = 1.0\nsigma_noise = 1e-6\nK = rbf_kernel(X_all, X_all, length_scale, sigma_f) + sigma_noise * np.eye(n)\n\n# --- Negative log-posterior function ---\ndef neg_log_posterior(f):\n    prior_term = 0.5 * f.T @ np.linalg.solve(K, f)\n    lik_term = 0.0\n    for xi, xj in X_pref_pairs:\n        fi, fj = f[x_to_idx[xi]], f[x_to_idx[xj]]\n        delta = (fi - fj) / np.sqrt(2)\n        lik_term -= np.log(norm.cdf(delta) + 1e-6)\n    return prior_term + lik_term\n\n# --- MAP estimation of latent utilities ---\nf_init = np.zeros(n)\nres = minimize(neg_log_posterior, f_init, method=\"L-BFGS-B\")\nf_map = res.x\n\n# --- Laplace approximation: compute W (Hessian of neg log likelihood) ---\nW = np.zeros((n, n))\nfor xi, xj in X_pref_pairs:\n    i, j = x_to_idx[xi], x_to_idx[xj]\n    fi, fj = f_map[i], f_map[j]\n    delta = (fi - fj) / np.sqrt(2)\n    phi = norm.pdf(delta)\n    Phi = norm.cdf(delta) + 1e-6\n    w = (phi / Phi)**2 + delta * phi / Phi\n    w /= 2  # adjust for sqrt(2)\n    W[i, i] += w\n    W[j, j] += w\n    W[i, j] -= w\n    W[j, i] -= w\n\n# --- Posterior covariance approximation ---\nL = np.linalg.cholesky(K)\nK_inv = np.linalg.solve(L.T, np.linalg.solve(L, np.eye(n)))\nH = K_inv + W\nH_inv = np.linalg.inv(H)\n\n# --- Prediction at test points ---\nX_test = np.linspace(0, 10, 200)\nK_s = rbf_kernel(X_all, X_test, length_scale, sigma_f)\nK_ss_diag = np.diag(rbf_kernel(X_test, X_test, length_scale, sigma_f))\n\n# Posterior mean and variance\nposterior_mean = K_s.T @ K_inv @ f_map\ntemp = np.linalg.solve(H, K_s)\nposterior_var = K_ss_diag - np.sum(K_s * temp, axis=0)\nposterior_std = np.sqrt(np.maximum(posterior_var, 0))\n\n# --- Visualization ---\nplt.figure(figsize=(8, 4))\nplt.plot(X_test, true_u(X_test), 'k--', label=\"True utility\")\nplt.plot(X_test, posterior_mean, 'b-', label=\"Posterior mean\")\nplt.fill_between(X_test,\n                 posterior_mean - 1.96 * posterior_std,\n                 posterior_mean + 1.96 * posterior_std,\n                 color='blue', alpha=0.2, label=\"95% CI\")\nplt.scatter(X_all, [true_u(x) for x in X_all], c='red', marker='x', label=\"Observed x\")\nplt.title(\"GP Preference Learning (Laplace Approximation, 100 Pairs)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Utility\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nGaussian Process posterior for a utility function (blue mean with 95% confidence band) after observing 5 points of noisy utility data (red ×). The true utility function (black dashed) is non-trivial. The GP correctly captures the function’s value around observed regions and expresses high uncertainty in the unobserved middle region. In practice, this uncertainty could guide an algorithm to query more feedback in the region \\(x\\approx [4,7]\\) to reduce ambiguity.\nGaussian processes are a flexible way to learn utility functions. They naturally handle irregular data and provide principled uncertainty estimates. GP-based preference learning has been applied to tasks like interactive Bayesian optimization, where an algorithm seeks to find the maximum of \\(u(x)\\) by iteratively querying a human which of two options is better (Christiano et al. 2023).\nInstead of GPs, one can use Bayesian neural networks or ensemble methods to model uncertainty in \\(u_\\theta(x)\\). For instance, a neural network can be trained on preference data, and techniques like Monte Carlo dropout or deep ensembles can provide uncertainty estimates for its predictions. These approaches scale to high-dimensional inputs (where GPs may be less practical) while still capturing epistemic uncertainty about the utility.\nOne principled way to capture uncertainty in Bayesian neural networks is via Markov Chain Monte Carlo (MCMC) methods, which seek to approximate the posterior distribution over model parameters given the data. In this setting, we place a prior over the neural network weights, \\(p(\\theta)\\), and define a likelihood function based on observed preferences—typically using a probabilistic choice model such as the Bradley-Terry or probit model. Given a dataset \\(\\mathcal{D} = \\{(x_i, x_j) : x_i \\succ x_j\\}\\), the posterior is defined as\n\\[\np(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) \\cdot p(\\theta),\n\\]\nwhere \\(p(\\mathcal{D} \\mid \\theta)\\) is the likelihood of observing the pairwise comparisons under the utility function \\(u_\\theta(x)\\), and \\(p(\\theta)\\) is the prior over the parameters.\nUnlike Gaussian processes, for which posterior inference is tractable in closed form under Gaussian likelihoods, inference in BNNs with non-Gaussian likelihoods is generally intractable. This is due to the non-conjugate nature of the neural network likelihood and the high-dimensional, nonlinear structure of the weight space. As a result, approximate inference methods are required.\nMCMC provides a general-purpose approach to approximate sampling from the posterior. The key idea is to construct a Markov chain whose stationary distribution is the target posterior. One of the most widely used algorithms is the Metropolis-Hastings (MH) algorithm. Given a current state \\(\\theta_t\\), a new proposal \\(\\theta'\\) is generated from a proposal distribution \\(q(\\theta' \\mid \\theta_t)\\), and accepted with probability\n\\[\nA = \\min\\left(1, \\frac{p(\\mathcal{D} \\mid \\theta') \\, p(\\theta') \\, q(\\theta_t \\mid \\theta')}{p(\\mathcal{D} \\mid \\theta_t) \\, p(\\theta_t) \\, q(\\theta' \\mid \\theta_t)}\\right).\n\\]\nWhen the proposal distribution is symmetric, i.e., \\(q(\\theta' \\mid \\theta_t) = q(\\theta_t \\mid \\theta')\\), the acceptance probability simplifies to a ratio of posterior densities. Over time, the chain yields samples \\(\\theta^{(1)}, \\dots, \\theta^{(T)} \\sim p(\\theta \\mid \\mathcal{D})\\), which can be used to compute posterior predictive estimates for the utility function:\n\\[\n\\mathbb{E}[u(x)] \\approx \\frac{1}{T} \\sum_{t=1}^T u_{\\theta^{(t)}}(x),\n\\]\nwith corresponding uncertainty estimates captured via the variance of the predictions across samples.\nMCMC methods are particularly appealing for preference learning because they directly quantify epistemic uncertainty in the utility function, which is crucial for downstream tasks such as decision-making, active learning, and safe exploration. Furthermore, MCMC makes no restrictive assumptions on the form of the posterior and can be used with non-convex and multi-modal distributions that arise from complex neural network architectures.\nHowever, MCMC also faces significant computational challenges in practice. First, the convergence of the Markov chain can be slow, especially in high-dimensional parameter spaces. Second, naive random-walk proposals (as in the basic Metropolis-Hastings algorithm) may suffer from low acceptance rates and poor mixing. More advanced MCMC methods such as Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampling (NUTS) can help address these issues by using gradient information to propose more efficient moves through the parameter space.\nDespite these limitations, MCMC remains a valuable tool for principled Bayesian inference in preference modeling, particularly in settings where uncertainty quantification is critical and computational cost is acceptable. In lower-dimensional settings or as a pedagogical tool, even simple MH-based approaches can offer intuitive and effective approximations to the posterior over preference functions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom IPython.display import HTML\nfrom matplotlib import rc\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, gaussian_kde\nfrom sklearn.decomposition import PCA\n\n# --- True latent utility function ---\ndef true_u(x):\n    return np.sin(x) + 0.1 * x\n\n# --- Generate synthetic preference data ---\nnp.random.seed(0)\nX = np.linspace(0, 10, 40)\ny_true = true_u(X)\n\n# Create pairwise comparisons\npairs = []\nfor _ in range(50):\n    i, j = np.random.choice(len(X), 2, replace=False)\n    if y_true[i] &gt; y_true[j]:\n        pairs.append((X[i], X[j], 1))  # x_i preferred over x_j\n    else:\n        pairs.append((X[j], X[i], 1))\n\n# --- Define a deep neural network: 3 hidden layers ---\ndef init_deep_params(hidden_dims=[4, 4, 4]):\n    params = {}\n    layer_dims = [1] + hidden_dims + [1]\n    for i in range(len(layer_dims) - 1):\n        W_key = f\"W{i+1}\"\n        b_key = f\"b{i+1}\"\n        params[W_key] = np.random.randn(layer_dims[i+1], layer_dims[i]) * 0.1\n        params[b_key] = np.zeros((layer_dims[i+1], 1))\n    return params\n\ndef deep_forward(x, params):\n    x = x.reshape(1, 1)\n    num_layers = len(params) // 2\n    h = x\n    for i in range(1, num_layers):\n        h = np.tanh(params[f\"W{i}\"] @ h + params[f\"b{i}\"])\n    out = params[f\"W{num_layers}\"] @ h + params[f\"b{num_layers}\"]\n    return out.squeeze()\n\ndef deep_utility(x, params):\n    return np.array([deep_forward(np.array([xi]), params) for xi in x])\n\n# --- Log likelihood (Bradley-Terry) ---\ndef deep_log_likelihood(params, pairs):\n    ll = 0.0\n    for xi, xj, _ in pairs:\n        ui = deep_forward(np.array([xi]), params)\n        uj = deep_forward(np.array([xj]), params)\n        ll += np.log(norm.cdf((ui - uj) / np.sqrt(2)) + 1e-6)\n    return ll\n\n# --- Gaussian prior on weights ---\ndef deep_log_prior(params):\n    lp = 0.0\n    for v in params.values():\n        lp -= 0.5 * np.sum(v**2)\n    return lp\n\n# --- Proposal distribution ---\ndef deep_propose(params, sigma=0.1):\n    new_params = {}\n    for k, v in params.items():\n        new_params[k] = v + np.random.randn(*v.shape) * sigma\n    return new_params\n\n# --- Metropolis-Hastings sampling ---\ndef deep_mh(init_params, pairs, num_iters=2000, burn_in=500):\n    samples = []\n    current = init_params\n    current_lp = deep_log_likelihood(current, pairs) + deep_log_prior(current)\n\n    for i in range(num_iters):\n        proposal = deep_propose(current)\n        proposal_lp = deep_log_likelihood(proposal, pairs) + deep_log_prior(proposal)\n        accept_prob = np.exp(proposal_lp - current_lp)\n        if np.random.rand() &lt; accept_prob:\n            current = proposal\n            current_lp = proposal_lp\n        if i &gt;= burn_in:\n            samples.append(current)\n\n    return samples\n\n# --- Run MCMC ---\ndeep_samples = deep_mh(init_deep_params(), pairs, num_iters=40000, burn_in=1000)\n\n# --- Posterior predictions ---\nX_test = np.linspace(0, 10, 200)\ndeep_preds = np.array([deep_utility(X_test, s) for s in deep_samples])\ndeep_mean = deep_preds.mean(axis=0)\ndeep_std = deep_preds.std(axis=0)\n\n# --- Plot results ---\nplt.figure(figsize=(8, 4))\nplt.plot(X_test, true_u(X_test), 'k--', label='True utility')\nplt.plot(X_test, deep_mean, 'b-', label='Posterior mean')\nplt.fill_between(X_test, deep_mean - 1.96 * deep_std, deep_mean + 1.96 * deep_std,\n                 color='blue', alpha=0.2, label='95% CI')\nplt.title(\"3-layer Bayesian Neural Network via MCMC on Preference Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Utility\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Flatten and PCA\ndef flatten_params(params):\n    return np.concatenate([v.flatten() for v in params.values()])\n\nflat_samples = np.array([flatten_params(p) for p in deep_samples])\npca = PCA(n_components=2)\nproj_samples = pca.fit_transform(flat_samples)\n\n# Density heatmap from KDE\nkde = gaussian_kde(proj_samples.T)\nxmin, xmax = proj_samples[:, 0].min(), proj_samples[:, 0].max()\nymin, ymax = proj_samples[:, 1].min(), proj_samples[:, 1].max()\nxx, yy = np.meshgrid(np.linspace(xmin, xmax, 100), np.linspace(ymin, ymax, 100))\nzz = kde(np.vstack([xx.ravel(), yy.ravel()])).reshape(xx.shape)\n\n# Downsample to 80 frames\nn_frames = 80\nidx = np.linspace(0, len(proj_samples) - 1, n_frames).astype(int)\nproj_subset = proj_samples[idx]\n\n# Animation with heatmap\nfig, ax = plt.subplots(figsize=(6, 5))\nax.contourf(xx, yy, zz, levels=30, cmap=\"Blues\", alpha=0.5)\n\nline, = ax.plot([], [], 'r-o', markersize=3, alpha=0.8, label=\"Chain trajectory\")\nstart_point = ax.plot([], [], 'go', label='Start')[0]\nend_point = ax.plot([], [], 'ko', label='Current')[0]\n\nax.set_xlim(xmin - 0.2, xmax + 0.2)\nax.set_ylim(ymin - 0.2, ymax + 0.2)\nax.set_title(\"Posterior exploration via Markov chain simulation\")\nax.set_xlabel(\"PCA1\")\nax.set_ylabel(\"PCA2\")\nax.legend(loc=\"upper left\")\n\ndef update(frame):\n    line.set_data(proj_subset[:frame+1, 0], proj_subset[:frame+1, 1])\n    start_point.set_data([proj_subset[0, 0]], [proj_subset[0, 1]])\n    end_point.set_data([proj_subset[frame, 0]], [proj_subset[frame, 1]])\n    return line, start_point, end_point\n\nani = animation.FuncAnimation(fig, update, frames=n_frames, interval=80, blit=True)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother Bayesian approach is Bayesian Inverse Reinforcement Learning (IRL), where a prior is placed on the parameters of a reward function and Bayes’ rule is used to update this distribution given demonstrations or preferences (iliad2019learning?). Early work like Ramachandran & Amir (2007) treated IRL as Bayesian inference, using MCMC to sample likely reward functions consistent with demonstrations. Such methods yield a posterior over reward functions, reflecting ambiguity when multiple rewards explain the human’s behavior.\nIn summary, Bayesian utility learning methods acknowledge that with limited human feedback, many possible utility functions might be compatible with the data. They keep track of this ambiguity, which is crucial for making cautious decisions and for actively gathering more feedback.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#case-study-learning-from-human-feedback-in-robotics",
    "href": "src/chap3.html#case-study-learning-from-human-feedback-in-robotics",
    "title": "3  Learning",
    "section": "Case Study: Learning from Human Feedback in Robotics",
    "text": "Case Study: Learning from Human Feedback in Robotics\nThus far, we discussed preference learning in general terms. We now focus on robotics, where an agent must learn a reward/utility function that captures the human’s objectives for a sequential decision-making task. Robotics brings additional challenges: the utility often depends on a trajectory of states and actions, and feedback can come in multiple forms. We outline several key forms of human feedback for robot learning and how to learn from them:\n\nLearning from demonstrations – inferring utility from expert demonstrations of the task.\nLearning from physical corrections – updating utility when a human physically intervenes in the robot’s behavior.\nLearning from trajectory evaluations – using human-provided scores or critiques of full trajectories.\nLearning from pairwise trajectory comparisons – inferring reward from which of two trajectories a human prefers.\n\nThese are not mutually exclusive; in practice, combinations can be very powerful (iliad2019learning?). We describe each mode and how utility functions can be derived.\n\nLearning from Demonstrations (Inverse Reinforcement Learning)\nIn Learning from Demonstrations, also known as Inverse Reinforcement Learning, the human provides examples of desired behavior (e.g. teleoperating a robot to show how to perform a task). The assumption is that the demonstrator is approximately optimizing some latent reward function \\(R^*(s,a)\\) (or utility for trajectories). IRL algorithms then search for a reward function \\(R_\\theta\\) under which the given demonstrations \\(\\tau_{demo}\\) have high expected return (iliad2019learning?).\nOne classic approach is Maximum Margin IRL, which finds a reward function that makes the return of the demonstration trajectories higher than that of any other trajectories by a large margin. Another is Maximum Entropy IRL, which models the demonstrator as noisily optimal (Boltzmann-rational) (iliad2019learning?). In MaxEnt IRL, the probability of a trajectory \\(\\tau\\) under reward parameters \\(\\theta\\) is modeled as:\n\\[\nP(\\tau \\mid \\theta) = \\frac{\\exp\\{R_\\theta(\\tau)\\}}{\\displaystyle \\sum_{\\tau'} \\exp\\{R_\\theta(\\tau')\\}} \\,,\n\\]\nwhere \\(R_\\theta(\\tau) = \\sum_{t} r_\\theta(s_t, a_t)\\) is the cumulative reward of \\(\\tau\\). The IRL algorithm then adjusts \\(\\theta\\) to maximize the likelihood of the human demonstrations (while often using techniques to approximate the denominator, since summing over all trajectories is intractable). The end result is a reward function \\(R_\\theta(s,a)\\) that rationalizes the demonstrations.\nKey challenge: unless demonstrations are optimal and cover the space well, IRL might recover an ambiguous or incorrect reward. In robotics, humans often have difficulty providing flawless demonstrations (due to hard-to-use interfaces or limited expertise) (iliad2019learning?). For example, users teleoperating a robot arm might move jerkily or only accomplish part of the task (iliad2019learning?). This makes sole reliance on demonstrations problematic. Nonetheless, demonstration data can provide a strong prior: it shows at least one way to succeed (or partial preferences for certain behaviors).\n\n\nLearning from Preferences and Rankings of Trajectories\nWhen high-quality demonstrations are hard to obtain, preference queries on trajectories are a viable alternative (iliad2019learning?). In preference-based learning for robotics, the robot (or algorithm) presents two (or more) trajectories of the task outcome, and the human chooses which one is better. Each such comparison provides a bit of information about the true underlying reward. By asking many queries, the algorithm can home in on the reward function that explains the human’s choices (iliad2019learning?).\nA concrete example is an agent learning to do a backflip in simulation (Christiano et al. 2023). The agent initially performs random flails. The system then repeatedly shows the human two video clips of the agent’s behavior and asks which is closer to a proper backflip. From these comparisons, a reward model is learned that assigns higher value to behaviors more like backflips (Christiano et al. 2023). The agent then uses reinforcement learning to optimize this learned reward, gradually performing better backflips. This process continues, with the human being asked comparisons on trajectories where the algorithm is most uncertain (to maximally inform the reward model) (Christiano et al. 2023).\n(Learning from human preferences | OpenAI) Framework for learning from human preferences in robotics: a reward predictor (utility function) is learned from human feedback on trajectory comparisons, and an RL algorithm uses this learned reward to improve the policy (Christiano et al. 2023). The loop is iterative: as the policy improves, new queries focus on areas of uncertainty to refine the reward model.\nSuch preference-based reward learning has enabled complex skills without explicitly programmed rewards (Christiano et al. 2023). Notably, Christiano et al. (2017) showed that an agent can learn Atari game policies and robotic manipulations from a few hundred comparison queries, achieving goals that are hard to specify but easy to judge (Christiano et al. 2023). Preferences are often easier for humans than demonstrations: choosing between options is simpler than generating one from scratch (iliad2019learning?). However, preference learning can be slow if each query only yields one bit of information. Active learning and combining preferences with other feedback can greatly improve efficiency (iliad2019learning?).\n\n\nLearning from Trajectory Evaluations (Critiques and Ratings)\nSometimes humans provide feedback in the form of evaluative scores or critiques on full trajectories (or partial trajectories). For example, after a robot finishes an attempt at a task, the human might give a reward signal (e.g. +1/-1, or a rating 1–5 stars, or say “too slow” vs “good job”). This is the premise of the TAMER framework (Training an Agent via Evaluative Reinforcement) and related approaches, where a human’s scalar reward signals are directly treated as the reward function for the agent in reinforcement learning.\nFrom a utility learning perspective, such feedback can be used to directly fit a utility model \\(u_\\theta\\) that predicts the human’s rating for a given trajectory. For instance, if a human provides a score \\(H(\\tau)\\) for trajectory \\(\\tau\\), one can treat it as a training target for \\(u_\\theta(\\tau)\\) (possibly under a regression loss). However, because humans are inconsistent and may not precisely quantify their preferences, it’s often useful to model \\(H(\\tau)\\) as a noisy realization of the underlying utility, rather than a perfect label. A Bayesian approach could treat \\(H(\\tau)\\) as a noisy observation of \\(u(\\tau)\\) and update a posterior for \\(u\\). Alternatively, classification approaches can be used (e.g. treat trajectories into “liked” vs “disliked” based on thresholded ratings).\nA challenge with trajectory-level feedback is credit assignment: the human’s single score must be attributed to the entire sequence of actions. Algorithms like COACH (Continuous cOaching of Automated Control Handlers) address this by allowing humans to give feedback at intermediate steps, thereby guiding the agent which specific part of the behavior was good or bad. In either case, learning from trajectory evaluations turns the human into a reward function provider, and the learning algorithm’s job is to infer the latent reward function that the human’s evaluations are trying to convey.\n\n\nLearning from Physical Corrections\nRobots that physically collaborate with humans can receive physical corrections: the human may push the robot or otherwise intervene to adjust its behavior. Such corrections provide insight into the human’s desired utility. For example, if a household robot is carrying a fragile object too recklessly and the human physically slows it down or re-routes it, that indicates the human’s reward favors safety over speed at that moment.\nLearning from physical corrections can be formalized in different ways. One approach is to treat a correction as a demonstration on a small segment: the human’s intervention suggests a better action or trajectory than what the robot was doing. This can be converted into a comparison: “the trajectory after correction is preferred over the original trajectory” for that time segment. The robot can then update its reward function \\(\\theta\\) to satisfy \\(R_\\theta(\\text{human-corrected behavior}) &gt; R_\\theta(\\text{robot’s initial behavior})\\). Repeated corrections yield a dataset of such pairwise preferences, focused on the states where the robot was wrong (losey2021corrections?).\nAnother approach is to infer the human’s intent through the sequence of corrections. Research by Losey et al. (2021) formalized learning from sequences of physical corrections, noting that each correction is not independent: a series of pushes might only make sense in aggregate (losey2021corrections?). By analyzing the cumulative effect of multiple interventions, the algorithm can deduce the underlying objective more accurately (e.g. the human consistently steers the robot away from the table edges, implying a high negative reward for collisions). Their algorithm introduced an auxiliary reward term to capture the human’s trade-off: they will correct the robot if the immediate mistake is worth fixing relative to long-term performance (losey2021corrections?). The conclusion was that reasoning over the sequence of corrections improved learning of the human’s objective (losey2021corrections?).\nPhysical corrections are intuitive for humans – we often instinctively guide others or objects when they err. For the robot, interpreting this guidance requires converting it into constraints or examples for the utility function. It is a powerful signal because it is active: the human is not just telling preferences but directly imparting the desired direction of change.\n\n\nCombining Multiple Feedback Types\nEach feedback modality has strengths and weaknesses. Demonstrations provide a lot of information but can be hard to perform; preferences are easy for humans but yield information slowly; corrections are very informative locally but require physical interaction; trajectory evaluations are straightforward but coarse. Combining these modes can lead to faster and more robust reward learning (iliad2019learning?). For example, the DemPref algorithm (iliad2019learning?) first uses demonstrations to get an initial rough reward model, then uses preference queries to refine it quickly (iliad2019learning?). In user studies, such combined approaches learned better rewards with fewer queries than using either alone (iliad2019learning?).\nIn practical robot learning systems, one might start by asking for a demonstration. If the demo is suboptimal, the system can then ask preference questions on alternative attempts to clarify the true goal. During actual execution, if the human intervenes, the robot updates its reward function on the fly to avoid repeating the mistake. This interactive reward learning loop continues until the robot’s behavior aligns with human intent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap3.html#summary",
    "href": "src/chap3.html#summary",
    "title": "3  Learning",
    "section": "3.4 Summary",
    "text": "3.4 Summary\nLearning utility functions from human preferences enables value alignment: aligning an AI system’s objectives with what humans actually want, rather than what we think we want in abstract. We covered how supervised learning can extract utilities from comparisons or scores, and how Bayesian methods like Gaussian processes and Bayesian neural nets can capture uncertainty in our inferences. In robotics, we saw that feedback can come in many forms – demonstrations, comparisons, corrections, evaluations – each providing a unique window into the human’s utility function. By intelligently combining these signals, robots can efficiently learn complex reward functions that would be extremely difficult to hand-code.\nKey takeaways and best practices include:\n\nUse the right feedback for the problem: If optimal examples are available, demonstrations jump-start learning. If not, pairwise preferences or scalar critiques might be easier to obtain.\nModel uncertainty: Knowing what the system doesn’t know (via a Bayesian model) allows for smart query selection and avoids overconfidently optimizing the wrong objective.\nIterate with the human: Preference learning is fundamentally an interactive process. An agent can query a human in ambiguous cases and continuously refine the utility estimate (Christiano et al. 2023).\nValidate the learned utility: Once a reward is learned, testing the robot’s policy and having humans verify or correct it is crucial. Even a few manual corrections can reveal if the learned reward misses a key aspect, allowing further refinement.\nBe aware of scaling and bias: Human feedback can be noisy or biased. Techniques like DPO suggest ways to simplify learning and avoid instability, but one should monitor for issues like reward hacking or unintended solutions, intervening with additional feedback as needed.\n\nLearning from human preferences is a rich area of ongoing research. It lies at the intersection of machine learning, human-computer interaction, and ethics. As AI systems become more advanced, the importance of teaching them our utility functions (and not mis-specified proxies) grows. The methods discussed in this chapter are building blocks toward AI that truly understands and pursues what humans value, acquired through learning with humans in the loop rather than in isolation. By mastering these techniques, we move closer to AI and robots that can be trusted to make decisions aligned with human preferences and well-being.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Learning</span>"
    ]
  },
  {
    "objectID": "src/chap4.html",
    "href": "src/chap4.html",
    "title": "4  Elicitation",
    "section": "",
    "text": "4.1 The Active Learning Problem\nFullscreen - AL Fullscreen - ME\nAcquiring labeled data is expensive. Active learning (AL) is a learning paradigm that aims to reduce the amount of labeled data required to train a model to achieve high accuracy. AL algorithms iteratively select an input datapoint for an oracle (e.g., a human annotator) to label such that when the label is observed, the model improves the most. Two primary setups in AL is pool-based and stream-based. In pool-based AL, the model selects samples from a large unlabeled pool of data. For example, a model for text classification selects the most uncertain texts from a large pool to ask a human annotator to label. In stream-based AL, the model receives samples sequentially (one sample at a time) and decides whether to label them. The data is gone if the decision maker decides not to label it. In AL, a model is trained on the current dataset, and a set of candidate points is evaluated for potential inclusion. AL selects one of these points to add to the dataset based on an “acquisition function” defined with respect to the current model to estimate the value of each candidate point for improving model performance. The dataset is updated with the newly queried point, and the cycle repeats until the budget is exhausted or a predefined reliability criterion is met.\nAL has successfully enhance various real-world systems. For example, AL can improve the computer vision models used in autonomous vehicles (Jarl et al. 2021). Probing a model to understand what type of data it would benefit from is more practical. In robotics, autonomous agents may query humans when unsure how to act when facing new situations (Taylor, Berrueta, and Murphey 2021). Here, collecting data often incurs significant financial and time costs because physical robot arm worns out over time. In meteorology, AL can help decide where to place additional sensors for weather predictions (Singh, Nowak, and Ramanathan 2006). Sensor placement involves deploying teams to remote locations and expensive construction for an extra data point. Choosing these locations and allocating resources wisely is of interest to governments and businesses. AL could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream tasks (Margatina et al. 2023). Here, it might be difficult to fully describe a targeted NLP task. Often, instead of defining a task via a dataset of examples, it may be easier for a human to interact with the LLM for a specific use case, identify gaps in the model, and address those using AL.\nTypically, in robotic, robots learn by observing human demonstrations. However, expert demonstrations are often limited, and training a supervised learning model would require vast amounts of demonstration data, which is difficult to obtain at scale. Demonstrations tend to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task. AL algorithms can be employed in preference learning tasks, where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive labeled data or reducing the high cost of annotations.\nMotivating by the pairwise preference setting, we consider a binary classification problem. The model is trained on a small labeled dataset \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N\\), where \\(x_i\\) represents the input data and \\(y_i\\) is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while maximizing the model’s performance. Here, the value of a datapoint is in how much it helps identify the underlying model, and this notion of informativeness is often quantify with uncertainty. Two primary types of uncertainty are often considered: epistemic and aleatoric uncertainty. Epistemic uncertainty, or model uncertainty, arises from a lack of knowledge and can be reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence due to insufficient or incomplete information in its training set. On the other hand, aleatoric uncertainty, or data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real data-generating process. AL often focuses on selecting data that reduce the epistemic uncertainty.\nThere are several method for quantify model uncertainty. Bayesian methods, such as Bayesian neural networks and Gaussian processes, offer a principled way of estimating uncertainty of parameter posterior distribution by iteratively updating a prior distribution over model. Exact posterior computation can become computationally prohibitive, especially for complex likelihood function, and approximated Bayesian computation is proposed to address this. For example, ensemble methods involve training multiple models and combining their predictions to provide an estimate of uncertainty. Ensemble methods are relatively easy to implement, but they are noisy and still somewhat expensive. Conformal prediction methods also provide a framework for estimating uncertainty by offering a measure of confidence in predictions based on the conformity of a given instance with the training data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#estimating-the-value-of-additional-data-with-acquisition-function",
    "href": "src/chap4.html#estimating-the-value-of-additional-data-with-acquisition-function",
    "title": "4  Elicitation",
    "section": "4.2 Estimating the Value of Additional Data with Acquisition Function",
    "text": "4.2 Estimating the Value of Additional Data with Acquisition Function\nUncertainty quantification plays a vital role in acquisition functions, which are central to AL strategies. These functions determine which samples are most valuable to label by evaluating their utility based on the model’s current uncertainty estimates. Common acquisition functions include uncertainty sampling (Zhu et al. 2010), which selects samples the model is least confident about, query-by-committee (Beluch et al. 2018), which utilizes a set of models to choose the most uncertain samples, and Bayesian AL by Disagreement (BALD) (Houlsby et al. 2011), which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, acquisition functions guide the AL process, improving the model’s efficiency in learning from limited data. Other acquisition functions that can be employed include:\n\nExpected model change (Cai, Zhang, and Zhou 2013): This approach focuses on labeling points that would have the most impact on changing the current model parameters.\nExpected error reduction (Mussmann et al. 2022): Points that would most effectively reduce the model’s generalization error are labeled using this strategy.\nVariance reduction (Cohn, Ghahramani, and Jordan 1996): This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model’s predictions, it aims to improve overall performance.\n\nUncertainty sampling (Zhu et al. 2010) selects data points for which the model exhibits the greatest uncertainty, focusing labeling efforts on ambiguous samples where additional information is likely to yield the greatest benefit. Several acquisition strategies fall under uncertainty sampling, including entropy sampling, margin sampling, and least confidence sampling. Entropy sampling measures value of addition data by the entropy of the predicted probability distribution: \\(\\alpha(x) = - \\sum_{y} p(y|x) \\log p(y|x)\\). Margin sampling focuses on the difference between the two highest predicted probabilities for a sample: \\(\\alpha(x) = p(y_1|x) - p(y_2|x)\\), where \\(y_1\\) and \\(y_2\\) are two most likely classes. Least confidence sampling measures value of additional data by the lowest predicted probability for its most likely class: \\(\\alpha(x) = 1 - p(y_{\\text{max}}|x)\\), where \\(y_{\\text{max}}\\) is the class with the highest probability. Consider a binary classification problem with three candidate \\(x_1, x_2, x_3\\). The code below demonstrate that uncertainty sampling methods yield the same conclusion of selecting \\(x_1\\).\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nQuery-by-Committee (Beluch et al. 2018) is selects samples for labeling based on the level of disagreement among members of a committee. Several acquisition functions can be employed under this framework to quantify the disagreement. The vote entropy measures the uncertainty based on how often the committee members vote for each class. The acquisition function is defined as \\(\\alpha(x) = \\mathbb{H}\\left[V(y)/C\\right]\\), where \\(V(y)\\) is the number of votes for class \\(y\\) and \\(C\\) is the number of committee members. Consensus Entropy measures the entropy of the average probability distribution across committee members. It is given by \\(\\alpha(x) = \\mathbb{H}[p_C(y|x)]\\), where \\(p_C(y|x)\\) is the average probability distribution for sample \\(x\\) across all committee members. The KL divergence quantifies the disagreement by comparing the probability distribution of each committee member to the average distribution. The acquisition function is given by \\(\\alpha(x) = \\frac{1}{C} \\sum_{c=1}^{C} D_{KL}[p_C(y|x) || p_C(y|x)]\\), where \\(p_C(y|x)\\) is the probability distribution of committee member \\(c\\) and \\(p_C(y|x)\\) is the average distribution across the committee. As an example, consider a binary classification problem with three candidate \\(x_1\\), \\(x_2\\), and \\(x_3\\) and three committee members. Numerical result below show that all acquisition functions selects \\(x_1\\).\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBayesian AL by Disagreement (BALD) (Houlsby et al. 2011) selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:\n\\[\n\\begin{aligned}\n&\\mathbb{I}(\\theta; y|x, \\mathcal{D}) = \\mathbb{H}[p(y|x, \\mathcal{D})] - \\mathbb{E}_{p(\\theta | \\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]] \\\\\n&\\mathbb{H}[p(y|x, \\mathcal{D})] = \\mathbb{H}\\left[\\int_{\\theta} p(y|x, \\theta, \\mathcal{D}) p(\\theta | \\mathcal{D}) d\\theta\\right] \\approx \\mathbb{H}\\left[\\frac{1}{N}\\sum_{i=1}^{N} p(y|x, \\theta_i, \\mathcal{D})\\right] = \\mathbb{H}\\left[\\overline{p}(y|x, \\mathcal{D})\\right] \\\\\n&\\mathbb{E}_{p(\\theta|\\mathcal{D})} [\\mathbb{H}[p(y|x, \\theta, \\mathcal{D})]] = \\mathbb{E}_{p(\\theta|\\mathcal{D})} \\left[ - \\sum_{y} p(y|x, \\theta, \\mathcal{D}) \\log p(y|x, \\theta, \\mathcal{D}) \\right] \\approx - \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{y} p(y|x, \\theta_i, \\mathcal{D}) \\log p(y|x, \\theta_i, \\mathcal{D}) \\right)\n\\end{aligned}\n\\]\nWhen there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized. As an example, consider a binary classification problem with two classes, \\(y_1\\) and \\(y_2\\). We have two samples, \\(x_1\\) and \\(x_2\\). BALD selects \\(x_1\\) for labeling.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAL by Variance Reduction (Cohn, Ghahramani, and Jordan 1996) is an algorithm designed to select the next data point for labeling based on the anticipated reduction in the model’s variance. The objective is to identify the point \\(x \\sim p(x)\\) that, when labeled \\(y_x\\), will most effectively decrease the model’s variance. The expected error at a given input \\(x\\) is \\(\\mathbb{E}_{\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)} (\\hat{y} - y)^2\\). \\(\\hat{y}\\) represents the model’s prediction, and \\(y\\) denotes the true label at \\(x\\). Using bias-variance decomposition (Geman, Bienenstock, and Doursat 1992), the expected error is decomposed as \\[\\begin{aligned}\n\\mathbb{E} (\\hat{y} - y)^2 = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[y|x]) + (\\mathbb{E}[y|x] - y)]^2 = \\mathbb{E} [(y - \\mathbb{E}[y|x])^2] + 2\\mathbb{E} [(\\hat{y} - \\mathbb{E}[y|x])(\\mathbb{E}[y|x] - y)] + \\mathbb{E}(\\hat{y} - \\mathbb{E}[y|x])^2\n\\end{aligned}\\] where the expectation is taken over \\(\\hat{y} \\sim p(\\hat{y} | \\mathcal{D}; x), y \\sim p(y|x)\\). The first term represents the variance of the true label \\(y\\), the second term evaluates to zero since \\(\\mathbb{E}_{\\hat{y}, y}[\\mathbb{E}[y|x] - y] = 0\\), and the third term accounts for the variance of the model’s prediction \\(\\hat{y}\\): \\[\\mathbb{E}(\\hat{y} - \\mathbb{E}[y|x])^2 = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[\\hat{y}] + \\mathbb{E}[\\hat{y}] - \\mathbb{E}[y|x])^2] = \\mathbb{E}[(\\hat{y} - \\mathbb{E}[\\hat{y}])^2] + (\\mathbb{E}[\\hat{y}] - \\mathbb{E}[y|x])^2\\]\nHence, \\[\\mathbb{E} (\\hat{y} - y)^2 = \\mathbb{E}_{y} [(y - \\mathbb{E}[y|x])^2] + (\\mathbb{E}_{\\hat{y}} [\\hat{y} - \\mathbb{E}[y|x]] )^2 + \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\\]\nHere, the first term signifies the variance of the true label, which remains constant for a given \\(x\\). The second term captures how much the average model prediction deviates from the expected true label. The third term quantifies the model’s uncertainty at \\(x\\). Cohn, Ghahramani, and Jordan (1996) denotes the uncertainty term as \\(\\sigma^2_{\\hat{y}} (x | \\mathcal{D}) = \\mathbb{E}_{\\hat{y}} [(\\hat{y} - \\mathbb{E}_{\\hat{y}}[\\hat{y}])^2]\\). The acquisition function is \\(\\mathbb{E}_{p(x)} [\\sigma^2_{\\hat{y}} (x | \\tilde{\\mathcal{D}})]\\). One could rely on empirical measure like a loss on test labelled data to gauge model improvement, which can help decide the termination of data acquisition. The size of the data set and its relationship to the loss is tied to the model complexity. To evaluate the performance of variance reduction strategy, Cohn, Ghahramani, and Jordan (1996) studies the Arm2D problem. Arm2D is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles \\(\\mathbf{\\theta_1}, \\mathbf{\\theta_2}\\). In this analysis, the two models are the Gaussian mixture model and locally-weighted regression (LOESS). The results shown that the variance of the learner decreases because the authors selected points to minimize expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both models as the dataset size increases. This is a notable outcome because the expected learner variance for these models can be computed accurately and efficiently relative to a new point. When integrated into the general AL loop, this significantly enhances model performance. In the case of the locally-weighted regression model (?fig-empirical:regress), it is surprising that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. However, when AL by variance reduction is applied, using expected learner variance as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#active-preference-learning-with-ideal-point-model",
    "href": "src/chap4.html#active-preference-learning-with-ideal-point-model",
    "title": "4  Elicitation",
    "section": "4.3 Active Preference Learning with Ideal Point Model",
    "text": "4.3 Active Preference Learning with Ideal Point Model\nFor any \\(n\\) elements to be ranked, there are \\(n!\\) possible orderings that can result in the correct complete ranking. Given that a lower bound on sorting is \\(n\\log n\\), obtaining a guaranteed true rating over \\(n\\) items requires \\(n\\log n\\) pairwise comparisons if those comparisons are chosen at random. This number can be quite high and costly in many applications, especially since most ranking information comes from humans. The more comparisons they have to make, the more money and time is spent. This process can also be inefficient, as some comparisons provide more value to the learning process than others, making some comparisons a waste. This inefficiency can be detrimental in fields like psychology and market research, where comparisons are heavily utilized, and a faster process could offer significant benefits. The reason the lower bound on the number of comparisons is \\(n\\log n\\) is that it assumes no prior information about the underlying space and field, so comparisons are chosen at random. However, leveraging the structures within the comparison space can provide more information about which comparisons are most valuable. For example, (G. and Nowak 2011) discusses how eye doctors have a wide range of options when assigning prescriptions for glasses, yet patients do not see them making many comparisons before deciding on the best option. This is because eye doctors incorporate domain knowledge into the process and only ask clients for comparisons when necessary. Applying similar knowledge in the ranking field leads to an AL approach that selects data based on the relevance of a comparison query toward finding the final \\(\\sigma(\\Theta)\\).\nG. and Nowak (2011) explores AL within data that can be embedded in a \\(d\\)-dimensional embedding space, where comparisons between two different items divide the space into halves, with one object being superior in each half. By leveraging such geometry, the paper develops a geometric AL approach. Let \\(\\theta\\) be the item representation in the embedding space. For each ranking \\(\\sigma\\), there is a reference point \\(r_{\\sigma} \\in \\mathbb{R}^d\\), such that if \\(\\theta_{i} \\succ \\theta_{j}\\), \\(||\\theta_i - r_{\\sigma}|| &lt; ||\\theta_j - r_{\\sigma}||\\). In other words, object \\(i\\) is closer to the reference point \\(r_{\\sigma}\\) than object \\(j\\). \\(\\Sigma_{n,d}\\) is the set of all possible rankings of the \\(n\\) items that satisfy the above embedding distances condition. Not all rankings will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions. For every ranking \\(\\sigma\\), there is \\(M_n(\\sigma)\\), the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random, \\(\\mathbb{E}[M_n(\\sigma)] = n\\log n\\), and it can be reduced by incorporating geometry. \\(q_{i,j}\\) is the query of comparison between items \\(i\\) and \\(j\\).\nAs an example, G. and Nowak (2011) studies a 2D space with three items: \\(\\theta_1\\), \\(\\theta_2\\), and \\(\\theta_3\\). There are pairwise queries \\(q_{1,3}\\), \\(q_{2,3}\\), and \\(q_{1,2}\\) between them, denoted by solid lines equidistant from the two items they compare. These lines split the \\(R^2\\) space into halves, with each half closer to one of the two items. The paper colors the side of the worse object for each query in dark grey and takes the intersection of these halves, resulting in the dark grey region in the image. This region indicates \\(\\Sigma_{n,2}\\) since all points follow the embedding conditions. Specifically, for every point \\(r\\) in the dark grey area, \\(||\\theta_3 - r|| &lt; ||\\theta_2 - r|| &lt; ||\\theta_1 - r||\\), meaning \\(\\theta_3 &lt; \\theta_2 &lt; \\theta_1\\). Thus, every point \\(r\\) is one of the \\(r_\\sigma\\) representing their respective rankings \\(\\sigma \\in \\Sigma_{n,2}\\). In other words, the paper aims to have the reference points and dark grey region closest to the worst object and furthest from the best object.\nThe authors also denote the label for each query \\(q_{i,j}\\), such as label \\(y_{i,j} = 1\\{q_{i,j}\\}\\) (for example, \\(y_{1,2} = 0, y_{3,2} = 1\\)). This allows for deciding how to label new queries represented by dashed and dotted lines, depending on which items each query compares. Focusing on the dotted line, called \\(q_{i,4}\\), where \\(i={1,2,3}\\), and considering potential locations of \\(\\theta_4\\), the line must be equidistant from one of the three items in the picture and \\(\\theta_4\\), meaning \\(\\theta_4\\) can be placed in three different locations. If the query performed is \\(q_{2,4}\\), then \\(\\theta_4\\) will be closer to the dark grey area than \\(\\theta_2\\), thus \\(y_{2,4} = 0\\). However, if \\(q_{1,4}\\) or \\(q_{3,4}\\) are performed, \\(\\theta_4\\) will be further from the dark grey area than \\(\\theta_1\\) or \\(\\theta_3\\), meaning \\(y_{1,4} = y_{3,4} = 1\\). In this case, the labels are contradictory and depend on which object they are compared with, making such a query \\(q_{i,4}\\) ambiguous.\nIn contrast, the authors analyze the dashed line, called \\(q_{i,5}\\), where \\(i={1,2,3}\\), and consider potential locations of \\(\\theta_5\\). Since the line must be equidistant from one of the three items in the picture and \\(\\theta_5\\), it can be placed in three different locations. If one of the three potential queries is performed, \\(\\theta_5\\) will be closer to the dark grey area than \\(\\theta_1\\), \\(\\theta_2\\), and \\(\\theta_3\\), meaning \\(y_{1,5} = y_{2,5} = y_{3,5} = 0\\). In this case, all labels are the same regardless of which object is used, meaning such a query will not be contradictory, as all agree on the label. The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease the total \\(M_n(\\sigma)\\). Intuitively, if there is contradictory information about a query, it needs to be erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain space agree on the query’s label, that information can be used without asking a human, incorporating the knowledge of the embedding distances. Lastly, to consider the general case of the \\(R^d\\) space, rather than discussing halves of the image, it is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of \\(1\\) to the query and the half-space assigning a label of \\(0\\). If both half-spaces exist, they have conflicting information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.\nIt is important to demonstrate that the number of comparisons decreases. Specifically, (G. and Nowak 2011) shows that this algorithm has \\(E[M_n(\\sigma)] = O(d\\log n)\\), where \\(d\\) is the dimension of the space and \\(d &lt; n\\), which improves on the \\(O(n\\log n)\\) baseline. The proof can be studied in detail in the paper itself, but at a high level, it starts by reasoning about the probability of a query being ambiguous and a comparison being requested from a human, thus representing \\(M_n = \\Sigma_{k=1}^{n-1}\\Sigma_{i=1}^k 1\\{Requestq_{i,k+1}\\}\\). For that, the authors define \\(Q(i,j)\\), which represents the number of different rankings that exist for \\(i\\) elements in \\(j\\)-dimensional space (e.g., \\(Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!\\)). In that case, \\(|\\Sigma_{n,d}| = Q(n,d)\\). Further, using recurrence relations for \\(Q(i,j)\\), the authors derive that \\(|\\Sigma_{n,d}| = Q(n,d) = O(n^{2d})\\), which is omitted here. Analogously, the authors define \\(P(i,j)\\), which represents the number of rankings in \\(\\Sigma_{n,d}\\) that will still be possible with the addition of a new element \\(i+1\\) to the ranking items. \\(P(i,j)\\) estimates how much of the dark grey area will still exist after making a query for \\(i+1\\). As indicated there, the dotted line ambiguous query did not change the dark grey a rea at all (\\(P(n,d) = Q(n,d)\\)), whereas the dashed non-ambiguous query would cut a piece from it (\\(P(n,d) &lt; Q(n,d)\\)). Thus, \\(Request q_{i,k+1} = P(k,d) / Q(k,d)\\), so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to obtain more useful information. With this in mind, the authors derive that \\(E[M_n(\\sigma)] = O(d\\log n)\\), showing that fewer queries are needed for effective ranking.\nThe issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) (G. and Nowak 2011), which uses majority voting for every query to indicate the ground truth of the query’s label. However, the authors consider that a group of people can still give incorrect or divided responses. If the votes for each answer are almost equal in number, the authors push that query to the end of the algorithm to see if it can become a non-ambiguous query with more information learned. If it does not, an odd number of voters is used to determine the final ranking.\n\n\n\nTable 4.1: Statistics for the Robust Query Selection Algorithm (RQSA) (G. and Nowak 2011) and the baseline of conducting all comparisons. \\(y\\) serves as a noisy ground truth, \\(\\tilde{y}\\) is the result of all comparisons, and \\(\\hat{y}\\) is the output of the RQSA.\n\n\n\n\n\nDimension\n\n2\n3\n\n\n\n\n% of queries\nmean\n14.5\n18.5\n\n\n\nstd\n5.3\n6\n\n\nAverage error\n\\(d(\\bar{y}, y)\\)\n0.23\n0.21\n\n\n\n\\(d(\\bar{y}, y)\\)\n0.31\n0.29\n\n\n\n\n\n\nWith regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in Table 4.1. The ground truth labels came from humans, indicated by \\(y\\) in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons (\\(\\tilde{y}\\)) and the Robust Query Selection Algorithm (RQSA) (\\(\\hat{y}\\)). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by \\(8\\%\\) compared to the baseline, which indicates that AL that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of Table 4.1, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy.\n\nUser Information as Domain Knowledge for Active Learning\nAn alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two items. Prior studies have shown (Amershi et al. 2014) that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies (Guillory and Bilmes 2011) it was determined that presenting users with an option of indifference between the two items can remove those problems. Moreover, in connection to AL, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required. For this problem, the following terms are defined:\n\n\\(c\\) - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using AL.\n\\(H\\) - a set of hypotheses over the possible cost functions, where for each \\(h \\in H\\) there is a cost function \\(c_h\\) associated with it.\n\\(h^*\\) - a true hypothesis that the model needs to determine, which has cost \\(c_{h^*}\\) associated with it\n\\(t(x,y)\\) - a test performed to compare items \\(x\\) and \\(y\\) (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to \\(H\\) as more information is learned.\n\\(o(x,y)\\) - observation or result of \\(t(x,y)\\), where \\(o(x,y) \\in \\{x&lt;y, x&gt;y\\}\\)\n\\(S = \\{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\\}\\) - a sequence of \\(m\\) pairs of tests and observations\n\\(w(H|S)\\) - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area and \\(Q(i,j)\\)). This means that if \\(h \\in H\\) is inconsistent with user responses received, it is removed from \\(H\\).\n\nWith the key terms defined, let’s consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. \\(w(H|S)\\) is the sum of the weights of all hypotheses that are still consistent with the evidence: \\(w(H|S) = \\sum_{h \\in H} w(h | S)\\). Each \\(w(h|S)\\) is a probability of the evidence’s existence given such hypothesis: \\(w(h|S) = p(S|h)\\). Such probability comes from the test-observation pairs since they compose the set \\(S\\). Moreover, each test is independent of other tests, which gives \\(p(S|h) = \\prod_{(t,o) \\in S} p((t,o) | h)\\). In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as: \\[\\begin{aligned}\n    p((t, o = x) | h) =\n    \\begin{cases}\n        1 & c_h(x) &lt; c_h(y)\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}\\]\nUsers are not perfect evaluators. Prior work (Amershi et al. 2014) has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users’ responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results (Guillory and Bilmes 2011), they don’t accurately reflect the real world, which gave rise to the idea of creating query-based noise. Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items \\(x\\) and \\(y\\), if \\(c_{h^*}(x) \\simeq c_{h^*}(y)\\) then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if \\(c_{h^*}(x) &gt;&gt; c_{h^*}(y)\\), the user will certainly select \\(y\\) and the other way around, which is where the noise is not needed. Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule (Shepard 1957) when talking about comparisons. This rule previously gave rise to a logistic model based on the noise (Viappiani and Boutilier 2010) where the probability of observation for a given test is \\(p((t, o = x) | h) \\propto exp(-\\gamma * c_h(x))\\)\n\n\n\n\n\n\nFigure 4.1: User response model in the noiseless setting\n\n\n\n\n\n\n\n\n\nFigure 4.2: User response with Luce Sheppard noise model\n\n\n\nFigure 4.1, Figure 4.2 demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user’s response.\nWe will now discuss the functionality of CLAUS, which is an algorithm designed by (Holladay et al. 2016) that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as \\(\\epsilon\\) and it is associated with each \\(c_h\\), so now every hypothesis \\(h\\) is defined over a pair of \\((c_h, \\epsilon_h)\\). It is important to note that the goal is to still learn and maintain our objective on \\(c\\), \\(\\epsilon\\) is only necessary to model the users’ responses. The uncertainty relates to the cost function as \\(|c_h(x) - c_h(y)| &lt; \\epsilon_h\\). This means that the user is uncertain between items \\(x\\) and \\(y\\) and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user’s preference towards one item, which will not be real and will skew the cost functions. This causes modifications of the problem set-up:\n\nFor test \\(t(x,y)\\) the observation will be \\(o(x,y) \\in \\{x&lt;y, x&gt;y, \\tilde{xy}\\}\\), where \\(\\tilde{xy}\\) is the uncertain response.\nThe probability distribution over the user’s response (?eq-prob_base) will now be defined as:\n\n\\[\\begin{aligned}\n    p((t, o = x) | h) =\n    \\begin{cases}\n        1 & c_h(x) &lt; c_h(y) - \\epsilon_h\\\\\n        0 & else\n    \\end{cases}, \\quad\n    p((t, o = \\tilde{xy}) | h) =\n    \\begin{cases}\n        1 & |c_h(x) - c_h(y)|^2 &lt; \\epsilon_h^2\\\\\n        0 & else\n    \\end{cases}\n\\end{aligned}\\]\nThis means the user confidently selects \\(x\\) when it is better than \\(y\\) by more than \\(\\epsilon\\), but if the squared difference of the cost functions of two items is negligible by \\(\\epsilon\\) user will choose the indifferent option.\n\nFinally this also updates the noise model: \\[\\begin{aligned}\n&p((t, o = x) | h) \\propto \\exp(-\\gamma * [c_h(x) - c_h(y)]) \\\\\n&p((t, o = \\tilde{xy}) | h) \\propto exp(-1/\\epsilon_h^2 * [c_h(x) - c_h(y)]^2)\n\\end{aligned}\\]\n\nRather than predicting a specific pair \\((c_h, \\epsilon_h)\\), the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (?fig-equiv_c), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about \\(c\\) and \\(\\epsilon\\) that distinguishes between the distinct \\(h\\), finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).\n\n\n\nTable 4.2: Performance of GBS and CLAUS with different labels for the uncertainty\n\n\n\n\n\nCategory\nAccuracy\nQuery Count\n\n\n\n\nGBS - About Equal\n\\(94.15 \\pm 0.52\\)\n\\(36.02 \\pm 0.03\\)\n\n\nGBS - Not Sure\n\\(\\textbf{94.66} \\pm \\textbf{0.55}\\)\n\\(35.95 \\pm 0.04\\)\n\n\nCLAUS - About Equal\n\\(91.56 \\pm 0.84\\)\n\\(\\textbf{25.93} \\pm \\textbf{0.41}\\)\n\n\nCLAUS - Not Sure\n\\(90.86 \\pm 0.74\\)\n\\(26.98 \\pm 0.47\\)\n\n\n\n\n\n\nThe first performance evaluation is done on the number of queries and confirms that it decreases. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of \\(\\epsilon\\) on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal \\(\\epsilon \\simeq 0.07\\), after which higher \\(\\epsilon\\) does not provide any benefit.\nLastly, the authors considered the performance difference, which is indicated in Table 4.2. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as “About Equal” or “Not Sure” as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in Table 4.2. The performance of CLAUS is lower by \\(3\\%\\) on average, showing that a smaller number of queries can still lead to a performance loss. However, the second column of Table 4.2 supports the information, as it also shows that 10 fewer queries were conducted on average.\nAL can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent’s learning within a dynamic system.\nA dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by \\(f_{HR}\\), so that we have \\(x^{t+1} = f_{HR}(x^t, u_R, u_H)\\). At a given time step \\(t\\), we have \\(x_t\\), \\(u_R^t\\), and \\(u_H^t\\). This can be encapsulated into a single \\(d\\) dimensional feature vector that the authors denote as \\(\\phi\\). The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as \\(r_H\\), this means we can write \\(r_H\\) as \\(r_H(x^t, u_R^t, u_H^t) = w^{\\intercal}\\phi(x^t, u_R^t, u_H^t)\\). Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory:\n\\[R_{H}(x^0, u_R, u_H) = \\sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t) \\quad \\Phi = \\sum \\phi(x^t, u_R^t, u_H^t) \\quad R_H(traj) = w\\cdot\\Phi(traj)\\]\nFirst, the scale of \\(w\\) does not matter because we only care about the relative rewards produced with \\(w\\) (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e. which one has a higher preference reward). This means we can constrain \\(||w|| &lt;= 1\\), so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let \\(I_t = +1\\) if the human prefers trajectory \\(A\\). According to Bradley-Terry model, \\(p(A \\succ B|w) = \\sigma(R_H(traj_A) - R_H(traj_B))\\). Let \\(\\psi = \\Phi(traj_a) - \\Phi(traj_b). Then f_{\\psi} (w) = p(I_t|w) = \\sigma(I_t w^{\\intercal}\\psi)\\). We can update \\(p(w)\\) everytime we get a result from a human preference query using Bayes’ rule: \\(p(w|I_t) &lt;- p(w) \\cdot p(I_t|w)\\) via Markov chain Monte Carlo method. This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy \\(H(I|w)\\). We want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights): \\[\\max_{x^0, u_R, u_H^A, u_H^B} \\min\\{\\mathbb{E}[1-f_{\\psi}(w)], \\mathbb{E}[1 - f_{-\\psi}(w)]\\}\\]\nTo do so, we sample \\(w_1, ... w_m\\) from \\(p(w)\\), approximating the distribution \\(p(w)\\) as \\(p(w) = \\frac{1}{M} \\sum \\delta (w_i).\\) We can now approximate the expectation expression as \\(E[1 - f_{\\psi}(w)] = \\frac{1}{M} (\\sum 1 - f_{\\psi}(w_i))\\), and now we can optimize the expression to generate a synthetic query. The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human – one at a time. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries:\n\\[\\max_{\\xi_{ib+1_A}, \\xi_{ib+1_B}, ... , \\xi_{ib+b_A}, \\xi_{ib+b_B}} \\mathbb{H}(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)\\]\nWe could consider optimizing this in the greedy fashion. This would mean just synthetically generating \\(b\\) independent queries. The drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries, such as Medioid Selection where we have to cluster \\(B\\) greedy vectors into \\(b &lt; B\\) groups and pick one vector from each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. The authors test both the non-batched and variety of batched learning algorithms on multiple environments. When graphed over \\(N\\) the non-batched AL approach does in the same ball-park of performance as the batched approaches. However, over time, we see that learning is a much slower process when not-batched.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#sec-metric-elicitation",
    "href": "src/chap4.html#sec-metric-elicitation",
    "title": "4  Elicitation",
    "section": "4.4 Case Study 2: Performance Metric Elicitation",
    "text": "4.4 Case Study 2: Performance Metric Elicitation\nIn binary classification problems, selecting an appropriate performance metric that aligns with the real-world task is crucial. The problem of metric elicitation aims to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive). Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process of devising a metric that best matches the preferences of practitioners or users. This is achieved by querying an “oracle” who provides feedback on proposed potential metrics through pairwise comparisons. Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.\nThe motivation for the pairwise comparison aspect of metric elicitation (Hiranandani et al. 2019a) stems from a rich history of literature in psychology, economics, and computer science (Samuelson 1938; Mas-Colell 1977; Varian 2006; Braziunas and Boutilier 2012; Tamburrelli and Margara 2014), demonstrating that humans are often ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately capture binary metrics such as accuracy, \\(F_\\beta\\), and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to introduce two binary-search procedures that can approximate the oracle’s performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.\nFirst, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics. In this context, \\(X \\in \\mathcal{X}\\) represents an input random variable, while \\(Y \\in \\{0, 1\\}\\) denotes the output random variable. We learn from a dataset of size \\(n\\), denoted by \\(\\{(x, y)_i\\}^n_{i=1}\\), which is generated independently and identically distributed (i.i.d.) from some distribution \\(\\mathbb{P}(X, Y)\\). The conditional probability of the positive class, given some sample \\(x\\), is denoted by \\(\\eta(\\vec{x}) = \\mathbb{P}(Y=1 | X=x)\\). The marginal probability of the positive class is represented by \\(\\zeta = \\mathbb{P}(Y=1)\\). The set of all potential classifiers is \\(\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\{0,1\\}\\}\\). The confusion matrix for a classifier \\(h\\) is \\(C(h, \\mathbb{P}) \\in \\mathbb{R}^{2 \\times 2}\\), where \\(C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j)\\) for \\(i, j \\in \\{0,1\\}\\). These entries represent the false positives, true positives, false negatives, and true negatives, ensuring that \\(\\sum_{i,j}C_{ij}=1\\). The set of all confusion matrices is denoted by \\(\\mathcal{C}\\). Since \\(FN(h, \\mathbb{P}) = \\zeta - TP(h, \\mathbb{P})\\) and \\(FP(h, \\mathbb{P}) = 1 - \\zeta - TN(h, \\mathbb{P})\\), \\(\\mathcal{C}\\) is actually a 2-dimensional space, not a 4-dimensional space.\nAny hyperplane in the \\((tp, tn)\\) space is given by \\(\\ell := a \\cdot tp + b \\cdot tn = c\\), where \\(a, b, c \\in \\mathbb{R}\\). Given a classifier \\(h\\), we define a performance metric \\(\\phi : [0, 1]^{2 \\times 2} \\rightarrow \\mathbb{R}\\). The value \\(\\phi(C(h))\\), which represents the performance of a classifier with respect to a certain metric, is referred to as the utility of the classifier \\(h\\). We assume, without loss of generality, that a higher value of \\(\\phi\\) indicates a better performance metric for \\(h\\). Our focus is to recover some metric \\(\\phi\\) using comparisons between confusion matrices \\(C(h)\\), determined by classifiers \\(h\\), which approximates the oracle’s “ground-truth” metric \\(\\phi^*\\). Next, we introduce two classes of performance metrics—Linear Performance Metrics (LPM) and Linear-Fractional Performance Metrics (LFPM)—for which we will present two elicitation algorithms.\nAn LPM, given constants \\(\\{a_{11}, a_{01}, a_{10}, a_{00}\\} \\in \\mathbb{R}^{4}\\), is defined as \\(\\phi(C) = a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN = m_{11} TP + m_{00} TN + m_{0}\\), where \\(m_{11} = (a_{11} - a_{10})\\), \\(m_{00} = (a_{00} - a_{01})\\), and \\(m_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)\\). This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. One example of an LPM is weighted accuracy, defined as \\(WA = w_1TP + w_2TN\\), where adjusting \\(w_1\\) and \\(w_2\\) controls the relative importance of different types of misclassification. An LFPM, defined by constants \\(\\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\\} \\in \\mathbb{R}^{8}\\), is given by: \\[\\phi(C) = \\frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN} = \\frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},\\] where \\(p_{11} = (a_{11} - a_{10})\\), \\(p_{00} = (a_{00} - a_{01})\\), \\(q_{11} = (b_{11} - b_{10})\\), \\(q_{00} = (b_{00} - b_{01})\\), \\(p_{0} = a_{10} \\zeta + a_{01} (1 - \\zeta)\\), and \\(q_{0} = b_{10} \\zeta + b_{01} (1 - \\zeta)\\). This parametrization also simplifies the elicitation process by reducing the number of variables. Common LFPMs include the \\(F_\\beta\\) score and Jaccard similarity, defined as:\n\\[F_{\\beta} = \\frac{TP}{\\frac{TP}{1+\\beta^{2}} - \\frac{TN}{1+\\beta^{2}} + \\frac{\\beta^{2} \\zeta + 1 - \\zeta}{1+\\beta^{2}}}, \\quad JAC = \\frac{TP}{1 - TN}. \\tag{4.1}\\]\nSetting \\(\\beta = 1\\) gives the F1 score, which is widely used as a classification metric. Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about \\(\\mathcal{C}\\). Particularly, we will assume that \\(g(t) = \\mathbb{P}[\\eta(X) \\geq t]\\) is continuous and strictly decreasing for \\(t \\in [0, 1]\\); essentially, \\(\\eta\\) has positive density and zero probability.\nAdditionally, \\(\\mathcal{C}\\) is convex, closed, and contained within the rectangle \\([0, \\zeta] \\times [0, 1-\\zeta]\\), and is rotationally symmetric around its center, \\((\\frac{\\zeta}{2}, \\frac{1-\\zeta}{2})\\), where the axes represent the proportion of true positives and negatives. The only vertices of \\(\\mathcal{C}\\) are \\((0, 1-\\zeta)\\) and \\((\\zeta, 0)\\), corresponding to predicting all \\(0\\)’s or all \\(1\\)’s on a given dataset. Therefore, \\(\\mathcal{C}\\) is strictly convex, and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix. Next, recall that an LPM is represented in terms of three parameters (\\(\\phi = m_{11}TP + m_{00}TN + m_0\\)). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of \\(\\mathcal{C}\\). We first note that this point is independent of \\(m_0\\). Additionally, we only care about the relative weightings of \\(m_{11}\\) and \\(m_{00}\\), not their actual values—they are scale invariant. Therefore, we can parametrize the space of LPMs as \\(\\varphi_{LPM} = \\{\\mathbf{m} = (\\cos \\theta, \\sin \\theta) : \\theta \\in [0, 2\\pi]\\}\\), where \\(\\cos \\theta\\) corresponds to \\(m_{00}\\) and \\(\\sin \\theta\\) corresponds to \\(m_{11}\\). As we already know, we can recover the Bayes classifier given \\(\\mathbf{m}\\), and it is unique, corresponding to one point on the boundary of \\(\\mathcal{C}\\) due to its convexity. The supporting hyperplane at this point is defined as \\(\\bar{\\ell}_{\\mathbf{m}} := m_{11} \\cdot tp + m_{00} \\cdot tn = m_{11} \\overline{TP}_{\\mathbf{m}} + m_{00} \\overline{TN}_{\\mathbf{m}}\\). We note that if \\(m_{00}\\) and \\(m_{11}\\) have opposite signs, then \\(\\bar{h}_m\\) is the trivial classifier predicting all 1’s or all 0’s, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices. Additionally, the boundary \\(\\partial \\mathcal{C}\\) can be split into upper and lower boundaries (\\(\\partial \\mathcal{C}_{+}, \\partial \\mathcal{C}_{-}\\)), corresponding to \\(\\theta \\in (0, \\pi/2)\\) and \\(\\theta \\in (\\pi, 3\\pi/2)\\) respectively (and whether \\(m_{00}, m_{11}\\) are positive or negative). We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric \\(\\phi\\), we define:\n\nThe Bayes utility as \\(\\bar{\\tau} := \\sup_{h \\in \\mathcal{H}} \\phi(C(h)) = \\sup_{C \\in \\mathcal{C}} \\phi(C)\\); this is the highest achievable utility (using the metric \\(\\phi\\)) over all classifiers $h \\(\\mathcal{H}\\) for a given problem.\nThe Bayes classifier as \\(\\bar{h} := \\arg \\max_{h \\in \\mathcal{H}} \\phi(C(h))\\); this is the classifier \\(h\\) corresponding to the Bayes utility.\nThe Bayes confusion matrix as \\(\\bar{C} := \\arg \\max_{C \\in \\mathcal{C}} \\phi(C)\\); this is the confusion matrix corresponding to the Bayes utility and classifier.\n\nSimilarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing “\\(\\sup\\)” with “\\(\\inf\\)”; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem. We also have the following useful proposition:\n\n\n\n\n\n\nproposition\n\n\n\n\n\n\nProposition 4.1 Let \\(\\phi \\in \\varphi_{LPM}\\). Then\n\\[\\bar{h}(x) = \\left\\{\\begin{array}{lr}\n\\unicode{x1D7D9} \\left[\\eta(x) \\geq \\frac{m_{00}}{m_{11} + m_{00}}\\right], & m_{11} + m_{00} \\geq 0 \\\\\n\\unicode{x1D7D9} \\left[\\frac{m_{00}}{m_{11} + m_{00}} \\geq \\eta(x)\\right], & \\text { o.w. }\n\\end{array}\\right\\} \\tag{4.2}\\]\nis a Bayes optimal classifier with respect to \\(\\phi\\). The inverse Bayes classifier is given by \\(\\underline{h} = 1 - \\bar{h}\\).\n\n\n\n\nThis is a simple derivation based on the fact that we only get rewards from true positives and true negatives. Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.\nWe will now formalize the problem of metric elicitation. Given two classifiers \\(h\\) and \\(h'\\) (or equivalently, two confusion matrices \\(C\\) and \\(C'\\)), we define an oracle query as the function:\n\\[\\Gamma\\left(h, h^{\\prime}\\right)=\\Omega\\left(C, C^{\\prime}\\right)=\\unicode{x1D7D9}\\left[\\phi(C)&gt;\\phi\\left(C^{\\prime}\\right)\\right]=: \\unicode{x1D7D9} \\left[C \\succ C^{\\prime}\\right], \\tag{4.3}\\]\nwhich represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:\n\n\n\n\n\n\ndefinition\n\n\n\n\n\n\nDefinition 4.1 Suppose the true (oracle) performance metric is \\(\\phi\\). The goal is to recover a metric \\(\\hat{\\phi}\\) by querying the oracle for as few pairwise comparisons of the form \\(\\Omega\\left(C, C^{\\prime}\\right)\\) so that \\(\\|\\phi - \\hat{\\phi}\\|_{--} &lt; \\kappa\\) for a sufficiently small \\(\\kappa &gt; 0\\) and for any suitable norm \\(\\|\\cdot\\|_{--}\\).\n\n\n\n\nIn practice, we do not have access to the true probability distribution or the population, which would provide the true values of \\(C\\) and \\(C'\\). However, we can subtly alter this problem description to use \\(\\hat{C}\\) and \\(\\hat{C}^{\\prime}\\), which are derived from our dataset of \\(n\\) samples:\n\n\n\n\n\n\ndefinition\n\n\n\n\n\n\nDefinition 4.2 Suppose the true (oracle) performance metric is \\(\\phi\\). The aim is to recover a metric \\(\\hat{\\phi}\\) by querying the oracle for as few pairwise comparisons of the form \\(\\Omega\\left(\\hat{C}, \\hat{C}^{\\prime}\\right)\\) so that \\(\\|\\phi - \\hat{\\phi}\\|_{--} &lt; \\kappa\\) for a sufficiently small \\(\\kappa &gt; 0\\) and for any suitable norm \\(\\|\\cdot\\|_{--}\\).\n\n\n\n\nAs is common in theoretical ML research, we solve the population problem and then consider ways to extend this to practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the confusion matrices from a portion of the dataset we have access to.\n\n4.4.1 Linear Performance Metric Elicitation\nFor LPM elicitation, we need one more proposition.\n\n\n\n\n\n\nproposition\n\n\n\n\n\n\nProposition 4.2 For a metric \\(\\psi\\) (quasiconvex and monotone increasing in TP/TN) or \\(\\phi\\) (quasiconcave and monotone increasing), and parametrization \\(\\rho^+\\)/\\(\\rho^-\\) of upper/lower boundary, composition \\(\\psi \\circ \\rho^-\\) is quasiconvex and unimodal on [0, 1], and \\(\\phi \\circ \\rho^+\\) is quasiconcave and unimodal on [0, 1].\n\n\n\n\nQuasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding \\(\\phi\\)’s. We first note that to maximize a quasiconcave metric, in which \\(\\phi\\) is monotonically increasing in \\(TP\\) and \\(TN\\), we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of \\(\\mathcal{C}\\). We thus set our initial search range to be \\([0, \\pi/2]\\) and repeatedly divide it into four regions. Then, we calculate the resulting confusion matrix on the 5 resulting boundaries of these regions and query the oracle \\(4\\) times. We repeat this in each iteration of the binary search until a maximizer is found.\n\n\n\n\n\n\nremark\n\n\n\n\n\n\nRemark 4.1. In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical binary search must be used. To illustrate this, consider the two distributions in Figure 4.3:\n\n\n\n\n\n\n\n\n\n\nFigure 4.3\n\n\n\nFor both the symmetric and skewed distributions, if we were to divide the search range into two portions and compare \\(A\\), \\(C\\), and \\(E\\), we would find that \\(C &gt; A\\) and \\(C &gt; E\\). In both cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at \\(C\\) itself (as in the first case). Therefore, we must make comparisons between all five points \\(A, B, C, D, and E\\). This allows us to correctly restrict our search range to \\([B, D]\\) in the first case and \\([C, E]\\) in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Quasiconcave Metric Maximization} \\begin{algorithmic} \\State \\textbf{input:} $\\epsilon &gt; 0$ and oracle $\\Omega$ \\State \\textbf{initialize:} $\\theta_a = 0, \\theta_b = \\frac{\\pi}{2}$ \\While{$|\\theta_b - \\theta_a| &gt; \\epsilon$} \\State set $\\theta_c = \\frac{3\\theta_a+\\theta_b}{4}$, $\\theta_d = \\frac{\\theta_a+\\theta_b}{2}$, and $\\theta_e = \\frac{\\theta_a+3\\theta_b}{4}$ \\State obtain $h\\theta_a, h\\theta_c, h\\theta_d, h\\theta_e, h\\theta_b$ using Proposition 1 \\State Compute $C\\theta_a, C\\theta_c, C\\theta_d, C\\theta_e, C\\theta_b$ using (1) \\State Query $\\Omega(C\\theta_c, C\\theta_a), \\Omega(C\\theta_d, C\\theta_c), \\Omega(C\\theta_e, C\\theta_d)$, and $\\Omega(C\\theta_b, C\\theta_e)$ \\If{$q_{i,j}$ is ambiguous} \\State request $q_{i,j}$'s label from reference \\Else \\State impute $q_{i,j}$'s label from previously labeled queries \\EndIf \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta &lt; \\theta' &lt; \\theta''$} \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$ \\EndIf \\If{$C\\theta' \\succ C\\theta'' \\succ C\\theta'''$ for consecutive $\\theta &lt; \\theta' &lt; \\theta''$} \\State assume the default order $C\\theta \\prec C\\theta' \\prec C\\theta''$ \\EndIf \\If{$C\\theta_a \\succ C\\theta_c$} \\State Set $\\theta_b = \\theta_d$ \\ElsIf{$C\\theta_a \\prec C\\theta_c \\succ C\\theta_d$} \\State Set $\\theta_b = \\theta_d$ \\ElsIf{$C\\theta_c \\prec C\\theta_d \\succ C\\theta_e$} \\State Set $\\theta_a = \\theta_c$ \\State Set $\\theta_b = \\theta_e$ \\ElsIf{$C\\theta_d \\prec C\\theta_e \\succ C\\theta_b$} \\State Set $\\theta_a = \\theta_d$ \\Else \\State Set $\\theta_a = \\theta_d$ \\EndIf \\EndWhile \\State \\textbf{output:} $\\vec{m}, C$, and $\\vec{l}$, where $\\vec{m} = m_l(\\theta_d), C = C\\theta_d$, and $\\vec{l} := (\\vec{m}, (tp, tn)) = (\\vec{m}, C)$ \\end{algorithmic} \\end{algorithm}\n\n\nTo elicit LPMs, we run Algorithm 1, querying the oracle in each iteration, and set the elicited metric \\(\\hat{m}\\) (which is the maximizer on \\(\\mathcal{C}\\)) to be the slope of the resulting hyperplane, since the metric is linear.\n\n\n\n\n\n\nremark\n\n\n\n\n\n\nRemark 4.2. To find the minimum of a quasiconvex metric, we flip all instances of \\(\\prec\\) and \\(\\succ\\), and use an initial search range of \\([\\pi, 3\\pi/2]\\); we use this algorithm, which we refer to as Algorithm 2, in our elicitation of LFPMs.\n\n\n\n\nNext, we provide a Python implementation of Algorithm 1.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\ndef get_m(theta):\n    \"\"\"\n    Inputs: \n    - theta: the value that parametrizes m\n    Outputs:\n    - m_0 and m_1 for the LPM\n    \"\"\"\n\n    return (math.cos(theta), math.sin(theta))\n\ndef lpm_elicitation(epsilon, oracle):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon &gt; 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    Outputs:\n    - estimate for m, which is used to compute the LPM as described above\n    \"\"\"\n\n    a = 0\n    b = math.pi/2\n    while (b - a &gt; epsilon):\n        c = (3 * a + b) / 4\n        d = (a + b) / 2\n        e = (a + 3 * b) / 4\n\n        m_a, m_b, m_c, m_d, m_e = (get_m(x) for x in [a,b,c,d,e]) # using definition of m\n        c_a, c_b, c_c, c_d, c_e = (get_c(x) for x in [m_a, m_b, m_c, m_d, m_e]) # compute classifier from m's then calculate confusion matrices\n        \n        response_ac = oracle(c_a, c_c)\n        response_cd = oracle(c_c, c_d)\n        response_de = oracle(c_d, c_e)\n        response_eb = oracle(c_e, c_b)\n\n        # update ranges to keep the peak\n        if response_ac:\n            b = d\n        elif response_cd:\n            b = d\n        elif response_de:\n            a = c\n            b = e\n        elif response_eb:\n            a = d\n        else:\n            a = d\n    return get_m(d), get_c(d)\n\n\n\n\n\n\n4.4.2 Linear-Fractional Performance Metric Elicitation\nNow, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption: Let \\(\\phi \\in \\varphi_{L F P M}\\). We assume \\(p_{11}, p_{00} \\geq 0, p_{11} \\geq q_{11}, p_{00} \\geq q_{00},\\) \\(p_{0}=0, q_{0}=\\) \\(\\left(p_{11}-q_{11}\\right) \\zeta+\\left(p_{00}-q_{00}\\right)(1-\\zeta)\\), and \\(p_{11}+p_{00}=1\\).\nThese assumptions guarantee that the LFPM \\(\\phi\\) which we are trying to elicit is monotonically increasing in \\(TP\\) and \\(TN\\), just as in the LPM elicitation case. We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.\nThe general idea of the algorithm is to use Algorithm 1 to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM \\(\\phi^*\\) with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.\nMore formally, suppose that the true metric is \\[\\phi^{*}(C)=\\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}. \\tag{4.4}\\] Then, let \\(\\bar{\\tau}\\) and \\(\\underline{\\tau}\\) represent the maximizer and minimizer of \\(\\phi\\) over \\(\\mathcal{C}\\), respectively. There exists a hyperplane \\[\\begin{aligned}\n\\bar{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}\\right) t n=\\bar{\\tau}^{*} q_{0}^{*},\n\\end{aligned}\\] which touches \\(\\mathcal{C}\\) at \\(\\left(\\overline{T P}^{*}, \\overline{T N}^{*}\\right)\\) on \\(\\partial \\mathcal{C}_{+}\\). Correspondingly, there also exists a hyperplane \\(\\underline{\\ell}_{f}^{*}:=\\left(p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}\\right) t p+\\left(p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}\\right) \\operatorname{tn}=\\underline{\\tau}^{*} q_{0}^{*}\\), which touches \\(\\mathcal{C}\\) at \\(\\left(\\underline{TP}^{*}, \\underline{T N}^{*}\\right)\\) on \\(\\partial \\mathcal{C}_{-}\\). While we are unable to obtain Equation 4.4 and ?eq-eq3.49 directly, we can use Algorithm 1 to get a hyperplane \\[\\bar{\\ell}:=\\bar{m}_{11} t p+\\bar{m}_{00} t n= \\bar{m}_{11} \\overline{T P}^{*}+\\bar{m}_{00} \\overline{T N}^{*} = \\bar{C}_{0}, \\tag{4.5}\\] which is equivalent to \\(\\bar{\\ell}_{f}^{*}\\) (Equation 4.4) up to a constant multiple. From here, we can obtain the system of equations\n\\[p_{11}^{*}-\\bar{\\tau}^{*} q_{11}^{*}=\\alpha \\bar{m}_{11}, p_{00}^{*}-\\bar{\\tau}^{*} q_{00}^{*}=\\alpha \\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{*}=\\alpha \\bar{C}_{0}, \\tag{4.6}\\] where \\(\\alpha &gt; 0\\) (we know it is \\(\\geq0\\) due to our assumptions earlier and because \\(\\bar{m}\\) is positive, but if it is equal to \\(0\\) then \\(\\phi^*\\) would be constant. So, our resulting system of equations is \\[\\begin{aligned}\n    p_{11}^{\\prime}-\\bar{\\tau}^{*} q_{11}^{\\prime}=\\bar{m}_{11}, p_{00}^{\\prime}-\\bar{\\tau}^{*} q_{00}^{\\prime}=\\bar{m}_{00}, \\bar{\\tau}^{*} q_{0}^{\\prime}=\\bar{C}_{0}.\n\\end{aligned} \\tag{4.7}\\]\nNow, similarly, we can approximate ?eq-eq3.49 using the algorithm we defined for quasiconvex metrics (Algorithm 2), where we altered the search range and comparisons. After finding the minimizer, we obtain the hyperplane \\[\\underline{\\ell}:=\\underline{m}_{11} t p+\\underline{m}_{00} t n=\\underline{m}_{11} \\underline{TP}^{*}+\\underline{m}_{00} \\underline{TN}^{*} = \\underline{C}_{0}, \\tag{4.8}\\] which is equivalent to \\(\\underline{\\ell}_{f}^{*}\\) (?eq-eq3.49) up to a constant multiple. So then, our system of equations is \\[p_{11}^{*}-\\underline{\\tau}^{*} q_{11}^{*}=\\gamma \\underline{m}_{11}, p_{00}^{*}-\\underline{\\tau}^{*} q_{00}^{*}=\\gamma \\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{*}=\\gamma \\underline{C}_{0}, \\tag{4.9}\\] where \\(\\gamma &lt;0\\) (for a reason analogous to why we have \\(\\alpha &gt;0\\)), meaning our resulting system of equations is \\[\\begin{aligned}\n    p_{11}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{11}^{\\prime \\prime}=\\underline{m}_{11}, p_{00}^{\\prime \\prime}-\\underline{\\tau}^{*} q_{00}^{\\prime \\prime}=\\underline{m}_{00}, \\underline{\\tau}^{*} q_{0}^{\\prime \\prime}=\\underline{C}_{0}.\n\\end{aligned} \\tag{4.10}\\]\nEquation 4.9 and Equation 4.10 form the two systems of equations mentioned in our overview of the algorithm. Next, we demonstrate that they have only one degree of freedom. Note that if we know \\(p_{11}'\\), we could solve both systems of equations as follows: \\[\\begin{aligned}\n    p_{00}^{\\prime}  &=1-p_{11}^{\\prime}, q_{0}^{\\prime}=\\bar{C}_{0} \\frac{P^{\\prime}}{Q^{\\prime}}\\\\\n    q_{11}^{\\prime}  &=\\left(p_{11}^{\\prime}-\\bar{m}_{11}\\right) \\frac{P^{\\prime}}{Q^{\\prime}} \\\\\n    q_{00}^{\\prime}&=\\left(p_{00}^{\\prime}-\\bar{m}_{00}\\right) \\frac{P^{\\prime}}{Q^{\\prime}},\n\\end{aligned} \\tag{4.11}\\] where \\(P^{\\prime}=p_{11}^{\\prime} \\zeta+p_{00}^{\\prime}(1-\\zeta)\\) and \\(Q^{\\prime}=P^{\\prime}+\\bar{C}_{0}-\\) \\(\\bar{m}_{11} \\zeta-\\bar{m}_{00}(1-\\zeta).\\)\nNow, suppose we know \\(p_{11}'\\). We could use this value to solve both systems Equation 4.9 and Equation 4.10, yielding two metrics, \\(\\phi'\\) and \\(\\phi''\\), from the maximizer and minimizer, respectively. Importantly, when \\(p_{11}^{*} / p_{00}^{*}=p_{11}^{\\prime} / p_{00}^{\\prime}=p_{11}^{\\prime \\prime} / p_{00}^{\\prime \\prime}\\), then \\(\\phi^{*}(C)=\\phi^{\\prime}(C) / \\alpha=-\\phi^{\\prime \\prime}(C) / \\gamma\\). Essentially, when we find a value of \\(p_{11}'\\) that results in \\(\\phi'\\) and \\(\\phi''\\) h aving constant ratios at all points on the boundary of \\(\\mathcal{C}\\), we can obtain \\(\\phi^*\\), as it is derivable from \\(\\phi'\\) and \\(\\alpha\\) (or, alternatively, \\(\\phi''\\) and \\(\\gamma\\)).\nWe will perform a grid search for \\(p_{11}'\\) on \\([0,1]\\). For each point in our search, we will compute \\(\\phi'\\) and \\(\\phi''\\). Then, we will generate several confusion matrices on the boundaries and calculate the ratio $’’ / \\(\\phi'\\) for each. We will select the value of \\(p_{11}'\\) for which the ratio \\(\\phi'' / \\phi'\\) is closest to constant and use it to compute the elicited metric \\(\\hat{\\phi}\\). The pseudocode for LFPM elicitation is given in Algorithm 2.\n\n\n\\begin{algorithm} \\caption{Grid Search for Best Ratio} \\begin{algorithmic} \\State \\textbf{Input:} $k, \\Delta$. \\State \\textbf{Initialize:} $\\sigma_{\\text{opt}} = \\infty, p'_{11,\\text{opt}} = 0$. \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3). \\State Generate $C_1, \\dots, C_k$ on $\\partial C_+$ and $\\partial C_-$ (Section 3). \\For{$p'_{11} = 0; \\; p'_{11} \\leq 1; \\; p'_{11} = p'_{11} + \\Delta$} \\State Compute $\\phi'$, $\\phi''$ using Proposition 4. \\State Compute array $r = \\left[ \\frac{\\phi'(C_1)}{\\phi''(C_1)}, \\dots, \\frac{\\phi'(C_k)}{\\phi''(C_k)} \\right]$. \\State Set $\\sigma = \\text{std}(r)$. \\If{$\\sigma &lt; \\sigma_{\\text{opt}}$} \\State Set $\\sigma_{\\text{opt}} = \\sigma$ and $p'_{11,\\text{opt}} = p'_{11}$. \\EndIf \\EndFor \\State \\textbf{Output:} $p'_{11,\\text{opt}}$. \\end{algorithmic} \\end{algorithm}\n\n\nWe provide a Python implementation as below.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\ndef lfpm_elicitation(k, delta):\n    \"\"\"\n    Inputs:\n    - k: the number of confusion matrices to evaluate on\n    - delta: the spacing for the grid search\n    Outputs:\n    - p_11', which will allow us to compute the elicited LFPM\n    \"\"\"\n\n    sigma_opt = np.inf\n    p11_opt = 0\n    C = compute_confusion_matrices(k) # generates k confusion matrices to evaluate on\n\n    for i in range(int(1/delta)):\n        p11 = i * delta\n        phi1 = compute_upper_metric(p11) # solves the first system of equations with p11 \n        phi2 = compute_lower_metric(p11) # solves the second system of equations with p11 \n        utility_1 = [phi1(c) for c in C] #calculate phi for both systems of equations\n        utility_2 = [phi2(c) for c in C]\n\n        r = []\n        for i in range(k):\n            r.append(utility_1[i] / utility_2[i])\n        sigma = np.std(r)\n\n        if(sigma &lt; sigma_opt):\n            sigma_opt = sigma\n            p11_opt = p11\n    return p11_opt\n\n\n\n\nIn summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on \\(\\mathcal{C}\\)–namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate \\(\\phi^*\\) if one parameter (\\(p_{11}'\\)) is found, and that this parameter can be found using an oracle-independent grid search. Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide formal statement and intuitive interpretation of these guarantees here, with their proofs available in the appendix of the original paper. First, we define the oracle noise \\(\\epsilon_{\\Omega}\\), which arises from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.\nGiven \\(\\epsilon, \\epsilon_{\\Omega} \\geq 0\\) and a metric \\(\\phi\\) satisfying our assumptions, Algorithm 1 or Algorithm 2 finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of \\(\\phi\\) at that point is within \\(O\\left(\\sqrt{\\epsilon_{\\Omega}} + \\epsilon\\right)\\) of the optimum, and the number of queries is \\(O\\left(\\log \\frac{1}{\\epsilon}\\right)\\). Let \\(\\mathbf{m}^{*}\\) be the true performance metric. Given \\(\\epsilon &gt; 0\\), LPM elicitation outputs a performance metric \\(\\hat{\\mathbf{m}}\\), such that \\(\\left\\|\\mathbf{m}^{*} - \\hat{\\mathbf{m}}\\right\\|_{\\infty} \\leq \\sqrt{2} \\epsilon + \\frac{2}{k_{0}} \\sqrt{2 k_{1} \\epsilon_{\\Omega}}\\). These results ensure that Algorithm 1 and Algorithm 2 find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of queries. Both of these statements are guaranteed by the binary search approach.\nLet \\(h_{\\theta}\\) and \\(\\hat{h}_{\\theta}\\) be two classifiers estimated using \\(\\eta\\) and \\(\\hat{\\eta}\\), respectively. Further, let \\(\\bar{\\theta}\\) be such that \\(h_{\\bar{\\theta}} = \\arg \\max _{\\theta} \\phi\\left(h_{\\theta}\\right)\\). Then \\(\\|C(\\hat{h}_{\\bar{\\theta}}) - C\\left(h_{\\bar{\\theta}}\\right)\\|_{\\infty} = O\\left(\\left\\|\\hat{\\eta}_{n} - \\eta\\right\\|_{\\infty}\\right)\\). This result indicates that the drop in elicited metric quality caused by using a dataset of samples rather than population confusion matrices is bounded by the drop in performance of the decision boundary \\(\\eta\\). These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the standard machine learning paradigm of training a decision boundary and using a pre-established metric. For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to (Hiranandani, Narasimhan, and Koyejo 2020), which performs metric elicitation to determine the oracle’s ideal tradeoff between the classifier’s overall performance and the discrepancy between its performance on certain protected groups.\n\n\n4.4.3 Multiclass Performance Metric Elicitation\nAlthough the previous section only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems(Hiranandani et al. 2019b). Consider the case of classifying subtypes of leukemia (Yang and Naiman 2014). We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%. With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In (Hiranandani et al. 2019b), the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter. Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses. \\(X \\in \\mathcal{X}\\) is the input random variable. \\(Y \\in [k]\\) is the output random variable, where \\([k]\\) is the index set \\(\\{1, 2, \\dots, k\\}\\).\nThe dataset of size \\(n\\) is denoted by \\(\\{(\\vec{x}, y)\\}_{i=1}^n\\) generated independently and identically from \\(\\mathbb{P}(X, Y)\\). \\(\\eta_i(\\vec{x}) = \\mathbb{P}(Y=i | X=\\vec{x})\\) gives the conditional probability of class \\(i \\in [k]\\) given an observation. \\(\\xi_i = \\mathbb{P}(Y=i)\\) is the marginal probability of class \\(i \\in [k]\\). The set of all classifiers is \\(\\mathcal{H} = \\{h : \\mathcal{X} \\rightarrow \\Delta_k\\}\\), where \\(\\Delta_k\\) is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size \\(k\\) where the only index with value 1 is the predicted class and all other positions have a value of 0. The confusion matrix for a classifier, \\(h\\), is \\(C(h, \\mathbb{P}) \\in \\mathbb{R}^{k \\times k}\\), where:\n\\[C_{ij}(h, \\mathbb{P}) = \\mathbb{P}(Y=i, h=j) \\text{\\qquad for } i, j \\in [k] \\tag{4.12}\\]\nNote that the confusion matrices are \\(k\\times k\\) and store the joint probabilities of each type of classification for each possible class. This means that the sum of row \\(i\\) in the confusion matrix equals \\(\\xi_i\\), because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix \\(C(h, \\mathbb{P})\\) can be expressed as a vector of off-diagonal elements, \\(\\vec{c}(h, \\mathbb{P}) = \\textit{off-diag}(C(h, \\mathbb{P}))\\), and \\(\\vec{c} \\in \\mathbb{R}^q\\) where \\(q := k^2 - k\\). The vector \\(\\vec{c}\\) is called the vector of ‘off-diagonal confusions.’ The space of off-diagonal confusions is \\(\\mathcal{C} = \\{\\vec{c}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}\\).\nIn cases where the oracle would care about the exact type of misclassification (i.e. misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of ‘diagonal confusions’ as \\(\\vec{d}(h, \\mathbb{P}) = \\textit{diag}(C(h, \\mathbb{P})) \\in \\mathbb{R}^k\\). The space of diagonal confusions is \\(\\mathcal{D} = \\{\\vec{d}(h, \\mathbb{P}) : h \\in \\mathcal{H}\\}\\).\nFinally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation \\(\\Gamma\\) for comparing two classifiers and \\(\\Omega\\) for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm). The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.\nA Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as \\(\\psi(\\vec{d}) = \\langle \\vec{a}, \\vec{d} \\rangle\\), where \\(\\vec{a} \\in \\mathbb{R}^k\\) such that \\(||\\vec{a}||_1 = 1\\). It is also called weighted accuracy (Narasimhan et al. 2015). The family of DLPMs is denoted as \\(\\varphi_{DLPM}\\). Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in \\(\\vec{a}\\) are non-negative.\nConsider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class \\(i\\) are \\(\\vec{v}_i \\in \\mathbb{R}^k\\) with \\(\\xi_i\\) at index \\(i\\) and zero elsewhere. Note that this is the maximum possible value in index \\(i\\), because this represents perfectly classifying all points that have a true class of \\(i\\). We can consider the space of diagonal confusions, visualized in Figure 4.4 (taken from (Hiranandani et al. 2019b)). The space of \\(\\mathcal{D}\\) is strictly convex, closed, and contained in the box \\([0, \\xi_1] \\times \\dots \\times [0, \\xi_k]\\). We also know that the only vertices are \\(\\vec{v}_i\\) for each \\(i \\in [k]^{(k-1)}\\).\n\n\n\n\n\n\nFigure 4.4: (a) Geometry of space of diagonal confusions for \\(k=3\\). This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes \\(k_1\\) and \\(k_2\\). Notice how this is identical to the space of confusion matrices examined in the previous chapter.\n\n\n\nWe know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that \\(g_{ij}(r) = \\mathbb{P} \\left[\\frac{\\eta_i(X)}{\\eta_j(X)} \\geq r \\right]\\) \\(\\forall i, j \\in [k]\\) are continuous and strictly decreasing for \\(r \\in [0, \\infty)\\).\nWe can also define the space of binary classification confusion matrices confined to classes \\(k_1\\) and \\(k_2\\), which is the 2-D \\((k_1, k_2)\\) axis-aligned face of \\(\\mathcal{D}\\), denoted as \\(\\mathcal{D}_{k_1, k_2}\\). Note that this is strictly convex, since \\(\\mathcal{D}\\) itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct an RBO classifier for \\(\\psi \\in \\varphi_{DLPM}\\), parameterized by \\(\\vec{a}\\), as follows: \\[\\begin{aligned}\n\\bar{h}_{k_1, k_2}(\\vec{x})= \\left\\{\n\\begin{array}{ll}\n      k_1, \\text{ if } a_{k_1} \\eta_{k_1}(\\vec{x}) \\geq a_{k_2} \\eta_{k_2}(\\vec{x})\\\\\nk_2, \\text{ o.w.}\n\\end{array}\n\\right\\}.\n\\end{aligned} \\tag{4.13}\\]\nWe can parameterize the upper boundary of \\(\\mathcal{D}_{k_1, k_2}\\), denoted as \\(\\partial \\mathcal{D}^{+}_{k_1, k_2}\\), using a single parameter \\(m \\in [0, 1]\\). Specifically, we can construct a DLPM by setting \\(a_{k_1} = m\\), \\(a_{k_2} = 1 - m\\), and all others to 0. Using Equation 4.13, we can get the diagonal confusions, so varying \\(m\\) parameterizes \\(\\partial \\mathcal{D}^{+}_{k_1, k_2}\\). The parameterization is denoted as \\(\\nu(m; k_1, k_2)\\).\n\n4.4.3.1 Diagonal Linear Performance Metric Elicitation\nSuppose the oracle follows a true metric, \\(\\psi\\), that is linear and monotone increasing across all axes. If we consider the composition \\(\\psi \\circ \\nu(m; k_1, k_2): [0, 1] \\rightarrow \\mathbb{R}\\), we know it must be concave and unimodal, because \\(\\mathcal{D}_{k_1, k_2}\\) is a convex set. Therefore, we can find the value of \\(m\\) that maximizes \\(\\psi \\circ \\nu(m; k_1, k_2)\\) for any given \\(k_1\\) and \\(k_2\\) using a binary search procedure.\nSince the RBO classifier for classes \\(k_1\\) and \\(k_2\\) only rely on the relative weights of the classes in the DLPM (see Equation 4.13), finding the value of \\(m\\) that maximizes \\(\\psi \\circ \\nu(m; k_1, k_2)\\) gives us the true relative ratio between \\(a_{k_1}\\) and \\(a_{k_2}\\). Specifically, from the definition of \\(\\nu\\), we know that \\(\\frac{a_{k_2}}{a_{k_1}} = \\frac{1-m}{m}\\). We can therefore simply calculate the ratio between \\(a_1\\) and all other weights to reconstruct an estimate for the true metric. A python implementation of this algorithm is provided below.\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\n\ndef rbo_dlpm(m, k1, k2, k):\n    \"\"\"\n    This constructs DLPM weights for the upper boundary of the\n    restricted diagonal confusions, given a parameter m.\n    This is equivalent to \\nu(m; k1, k2)\n    \n    Inputs:\n    - m: parameter (between 0 and 1) for the upper boundary\n    - k1: first axis for this  face\n    - k2: second axis for this face\n    - k: number of classes\n    Outputs:\n    - DLPM weights for this point on the upper boundary\n    \"\"\"\n    new_a = np.zeros(k)\n    new_a[k1] = m\n    new_a[k2] = 1 - m\n    return new_a\n\ndef dlpm_elicitation(epsilon, oracle, get_d, k):\n    \"\"\"\n    Inputs:\n    - epsilon: some epsilon &gt; 0 representing threshold of error\n    - oracle: some function that accepts 2 confusion matrices and\n        returns true if the first is preferred and false otherwise\n    - get_d: some function that accepts dlpm weights and returns \n        diagonal confusions\n    - k: number of classes\n    Outputs:\n    - estimate for true DLPM weights\n    \"\"\"\n    a_hat = np.zeros(k)\n    a_hat[0] = 1\n    for i in range(1, k):\n        # iterate over each axis to find appropriate ratio\n        a = 0  # lower bound of binary search\n        b = 1  # upper bound of binary search\n\n        while (b - a &gt; epsilon):\n            c = (3 * a + b) / 4\n            d = (a + b) / 2\n            e = (a + 3 * b) / 4\n\n            # get diagonal confusions for each point\n            d_a, d_c, d_d, d_e, d_b = (get_d(rbo_dlpm(x, 0, i, k)) \n                for x in [a, c, d, e, b])\n\n            # query oracle for each pair\n            response_ac = oracle(d_a, d_c)\n            response_cd = oracle(d_c, d_d)\n            response_de = oracle(d_d, d_e)\n            response_eb = oracle(d_e, d_b)\n\n            # update ranges to keep the peak\n            if response_ac:\n                b = d\n            elif response_cd:\n                b = d\n            elif response_de:\n                a = c\n                b = e\n            elif response_eb:\n                a = d\n            else:\n                a = d\n\n        midpt = (a + b) / 2\n        a_hat[i] = (1 - midpt) / midpt\n    return a_hat / np.sum(a_hat)\n\n\n\n\nTo use this algorithm for metric elicitation on a real dataset, we need to supply the “oracle” and “get_d” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_d function will need to construct a classifier given the DLPM weights, following the principles of the RBO classifier from Equation 4.13, and calculate the confusion matrix from a validation set.\nUsing the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:\n\n\n\n\n\n\nproposition\n\n\n\n\n\n\nGiven \\(\\epsilon, \\epsilon_\\Omega \\geq 0\\), and a 1-Lipschitz DLPM \\(\\varphi^*\\) parameterized by \\(\\vec{a}^*\\). Then the output \\(\\hat{a}\\) of the DLPM elicitation algorithm after \\(O((k-1)\\log\\frac{1}{\\epsilon})\\) queries to the oracle satisfies \\(||\\vec{a}^* - \\hat{a}||_\\infty \\leq O(\\epsilon + \\sqrt{\\epsilon_\\Omega})\\), which is equivalent to \\(||\\vec{a}^* - \\hat{a}||_2 \\leq O(\\sqrt{k}(\\epsilon + \\sqrt{\\epsilon_\\Omega}))\\).\n\n\n\n\nIn other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle’s correctness guarantee (\\(\\epsilon_\\Omega\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#case-study-3-active-preference-learning-in-robotics",
    "href": "src/chap4.html#case-study-3-active-preference-learning-in-robotics",
    "title": "4  Elicitation",
    "section": "4.5 Case Study 3: Active Preference Learning in Robotics",
    "text": "4.5 Case Study 3: Active Preference Learning in Robotics\nHow exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings. Let’s say there are two trajectories \\(\\xi_A\\) and \\(\\xi_B\\) that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if \\(R(\\xi_A)\\) or \\(R(\\xi_B)\\) is bigger, with \\(R(\\xi) = w * \\phi(\\xi)\\) being the reward function. In this equation \\(w\\) and \\(\\phi(\\xi)\\) are vectors of weights and features of the trajectory, so alternatively, we can express this as:\n\\[R(\\xi) = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ ... \\\\ w_N \\end{bmatrix} \\cdot \\begin{bmatrix} \\phi_1(\\xi) \\\\ \\phi_2(\\xi) \\\\ ... \\\\ \\phi_N(\\xi) \\end{bmatrix} \\tag{4.14}\\]\nIf one says that they preferred \\(\\xi_2\\) less than \\(\\xi_1\\) then it means \\(\\xi_2 &lt; \\xi_1 \\implies R(\\xi_2) &lt; R(\\xi_1) \\implies w * \\phi(\\xi_2) &lt; w * \\phi(\\xi_1) \\implies 0 &lt; w * (\\phi(\\xi_1) - \\phi(\\xi_2)) \\implies 0 &lt; w * \\Phi\\). Alternatively, if one preferred \\(\\xi_2\\) more than \\(\\xi_1\\), the signs would be flipped, resulting in \\(0 &gt; w * \\Phi\\). The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example we can see how a query between two items can split the plain into two halves, indicating preference towards one of the items. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane. If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.\nHow should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries’s results. The image shows that the purple portion is exactly half of the blue portion.\n\n\n\n\n\n\nFigure 4.5: Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.\n\n\n\nMathematically, from (Biyik and Sadigh 2018) this can be expressed as set \\(F\\) of potential queries \\(\\phi\\), where \\(F = \\{\\phi: \\phi = \\Phi(\\xi_A) - \\Phi(\\xi_B), \\xi_A, \\xi_B \\in \\Xi\\}\\) (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function \\(f_{\\phi}(w) = \\min(1, \\exp(I^T\\phi))\\) that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as \\(\\min\\{\\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}\\) (expected size of the two sides of the remaining space after it is split by a query - purple area in Figure 4.5), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: \\(\\max_{\\phi} \\min\\{ \\mathbb{E}[1 - f_{\\phi}(w)], \\mathbb{E}[1 - f_{-\\phi}(w)]\\}\\). Effectively this is finding such \\(\\phi\\) that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the \\(\\max\\) function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.\nThe first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from (Biyik and Sadigh 2018). In ?fig-car_direct you can see the setting of a 3-lane road with the orange car being controlled by the computer. The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.\nAt the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.\n\n4.5.0.1 Active Learning for Pairwise Comparisons\nWe have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most AL algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that AL achieves better performance earlier, which in time-sensitive tasks can be a critical factor. One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery (Li et al. 2021). Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure Figure 4.6 demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.\n\n\n\n\n\n\nFigure 4.6: Performance of AL and random query selection algorithms in the task of exoskeleton learning with human preferences. (Li et al. 2021)\n\n\n\nIn conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like AL. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.\n\n\n4.5.1 Application: Guiding Human Demonstrations in Robotics\nA strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.\nThe ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, multimodality, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task. A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in Figure 4.7.\n\n\n\n\n\n\nFigure 4.7: Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side (Gandhi et al. 2022)\n\n\n\nGandhi et al. (Gandhi et al. 2022) identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations. To identify whether a demonstration is “compatible” with a base policy trained with prior demonstrations, the researchers measure the likelihood of demonstrated actions under the base policy, and the novelty of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations. In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.\n\n4.5.1.1 Estimating Compatiblity\nWe want to define a compatibility measure \\(\\mathcal{M}\\), that estimates the performance of policy \\(\\pi_{base}\\) that is retrained on a union of \\(\\mathcal{D}_{base}\\), the known base dataset, and \\(\\mathcal{D}_{new}\\), the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty. The likelihood of actions \\(a_{new}\\) in \\(\\mathcal{D}_{new}\\) is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:\n\\[likelihood(s_{new}, a_{new}) = -\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2]. \\tag{4.15}\\]\nThe novelty of the state \\(s_{new}\\) in \\(\\mathcal{D}_{new}\\) is the standard deviation in the predicted actions under base policy:\n\\[novelty(s_{new}) = \\mathrm{Var}[\\pi_{base}(s_{new})]. \\tag{4.16}\\]\nWe can plot likelihood and novelty on a 2D plane, as shown in Figure 4.8, and identify thresholds on likelihood and novelty, denoted as \\(\\lambda\\) and \\(\\eta\\) respectively. Intuitively, demonstrations with low likelihood in low novelty states should be excluded, because this indicates that there is a conflict between the base behavior and the new demonstration due to multimodality. Note that in high novelty states, the likelihood should be disregarded because the base policy does not have a concrete idea for how to handle these states anyways so more data is needed.\n\n\n\n\n\n\nFigure 4.8: Examples of plots of likelihood and novelty for compatible and incompatible operators (Gandhi et al. 2022)\n\n\n\nThe final compatibility metric, parameterized by the likelihood and novelty thresholds \\(\\lambda\\) and \\(\\eta\\), is \\(\\mathcal{M}(\\mathcal{D}_{base}, (s_{new}, a_{new})) \\in [0, 1]\\), defined as:\n\\[\\begin{aligned}\n    \\mathcal{M} = \\begin{cases}\n        1 - \\min(\\frac{\\mathbb{E}[|| \\pi_{base}(s_{new}) - a_{new} ||^2_2]}{\\lambda}, 1) & \\text{ if } \\text{novelty}(s_{new}) &lt; \\eta \\\\\n        1 & \\text{ otherwise }\n       \\end{cases}.\n\\end{aligned} \\tag{4.17}\\]\nNote that \\(\\lambda\\) and \\(\\eta\\) need to be specified by hand. This is accomplished by assuming the ability to collect a priori incompatible demonstrations to identify reasonable thresholds that remove the most datapoints in the incompatible demonstrations while keeping the most datapoints in the compatible demonstrations.\n\n\n4.5.1.2 Case Studies with Fixed Sets\nThe researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a “proficient” operator’s demonstration while sampling trajectories from other operators for the new set. The naive baseline is to use all datapoints while the \\(\\mathcal{M}\\)-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in Table 4.3. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.\n\n\n\nTable 4.3: Success rates (mean/std across 3 training runs) for policies trained on \\(\\mathcal{D}_{new}\\) by using all the data (Naive) or filtering by compatibility (\\(\\mathcal{M}\\)-Filtered) (Gandhi et al. 2022)\n\n\n\n\n\n\nSquare Nut\n\nRound Nut\n\nHammer Placement\n\n\n\nOperator\nNaive\n\\(\\mathcal{M}\\)-Filtered\nNaive\n\\(\\mathcal{M}\\)-Filtered\nNaive\n\\(\\mathcal{M}\\)-Filtered\n\n\nBase Operator\n38.7 (2.1)\n-\n13.3 (2.3)\n-\n24.7 (6.1)\n-\n\n\nOperator 1\n54.3 (1.5)\n61.0 (4.4)\n26.7 (11.7)\n32.0 (12.2)\n38.0 (2.0)\n39.7 (4.6)\n\n\nOperator 2\n40.3 (5.1)\n42.0 (2.0)\n22.0 (7.2)\n26.7 (5.0)\n33.3 (3.1)\n32.7 (6.4)\n\n\nOperator 3\n37.3 (2.1)\n42.7 (0.6)\n17.3 (4.6)\n18.0 (13.9)\n8.0 (0.0)\n12.0 (0.0)\n\n\nOperator 4\n27.3 (3.5)\n37.3 (2.1)\n7.3 (4.6)\n13.3 (1.2)\n4.0 (0.0)\n4.0 (0.0)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.9: The phases of the active elicitation interface: (a) initial prompting, (b) demonstrations with live feedback, and (c) corrective feedback (Gandhi et al. 2022)\n\n\n\n\n\n4.5.1.3 Actively Eliciting Compatible Demonstrations\nIn the previous section, we assume access to a dataset that has already been collected, and we see how filtering out incompatible demonstrations helps improve performance. However, when collecting a new dataset, it would be better to ensure that operators collect compatible demonstrations from the start, allowing us to retain as much data as possible for training.\nTo actively elicit compatible demonstrations, the researchers set up a pipeline for live feedback and examples. At the start, operators are given a task specification and some episodes to practice using the robot. Then, the active elicitation process begins, as shown in Figure 4.9. Each operator is shown some rollouts of the base policy to understand the style of the base operator. Next, the operator provides a demonstration similar to the ones they were shown. As they record their demonstrations, the interface provides online feedback, with green indicating compatible actions and red indicating incompatible actions. If the number of incompatible state-action pairs (ones where \\(\\mathcal{M}\\) is zero) exceeds 5% of the demonstration length, the demonstration is rejected. However, to provide corrective feedback, the interface shows the areas of the demonstration with the highest average incompatibility and also provides an expert demo that shows what should actually be done. Demonstrators can use this feedback to provide more compatible demonstrations moving forward.\nThis process helps improve the demonstration quality in both simulation and real experiments, as show in Table 4.4. Specifically, on the real results, active elicitation outperformed the base policy by 25% and naive data collection by 55%. Overall, active elicitation is a powerful tool to ensure that data collected for imitation learning improves the quality of the learned policy.\n\n\n\nTable 4.4: Success rates (mean/std across users) for policies trained on \\(\\mathcal{D}_{new}\\) by using all the data (Naive), filtering by compatibility (\\(\\mathcal{M}\\)-Filtered), or using informed demonstration collection (Gandhi et al. 2022)\n\n\n\n\n\nTask Ba\nse Naive\nNaive + Fil\ntered Informed\n\n\n\n\n\nRound Nut 13.\n3 (2.3) 9.\n6 (4.6)\n9.7 (4.2) 15\n.7 (6.0)\n\n\nHammer Placement 24.\n7 (6.1) 20.\n8 (15.7)\n22.0 (15.5) 31.\n8 (16.3)\n\n\n\\(\\left[ \\textup{Real} \\right]\\) Food Plating\n60.0 30.\n0 (17.3)\n- 85\n.0 (9.6)\n\n\n\n\n\n\nA fundamental limitation of eliciting compatible demonstrations is the fact that the “base” demonstrator is considered the ground truth. When the base demonstrator specifies a preference, all other demonstrators must abide by it, even if they have strong preferences against it. For instance, when pouring milk and cereal into a bowl, different people have different preferences for what is the correct order, but active elicitation forces all demonstrators to follow the initial preference of the base operator. The researchers hope that future work can enable users to override the default demonstration set and follow a base behavior that better aligns with their preferences. This could enable multiple modes of behavior to be collected in data while only following a user’s specified preference instead of attempting to collapse all modes into a single policy.\nLooking forward, active elicitation provides a foundation for allowing robots to query humans about the type of data needed, enabling more efficient data collection through transparency.\nIn summary, this chapter has explored the complexities and innovations in interAL as applied to large models within robotics. It begins by investigating pairwise comparisons and their role in efficiently learning linear reward functions from large datasets, overcoming limitations in supervised learning. When combined with active learning techniques, these comparisons supply timely, targeted, and context-appropriate feedback, enhancing performance in time-critical applications like exoskeleton adjustments during rehabilitation.\nWe then shift to imitation learning or inverse reward learning from demonstrations, emphasizing the difficulties introduced by multimodal demonstration sets. active elicitation approaches to compile compatible demonstrations, streamlining the learning process by guiding users to provide more valuable, steady examples are incredibly promising, however, to tackling this issue. This method shows promise in refining the interactive imitation learning data collection pipeline, enabling more capable and effective robotic training.\nAdditionally, the chapter examines the integration of foundation models into robotics, highlighting the transformative innovations of R3M and Voltron. R3M’s pre-training on diverse human activities dramatically improves robotic manipulation with minimal supervision. Meanwhile, Voltron builds on these capabilities by incorporating language-driven representation learning for remarkably adaptable and nuanced robotic task performance. These models represent significant leaps in robotics while opening new frontiers for future research and applications.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap4.html#exercises",
    "href": "src/chap4.html#exercises",
    "title": "4  Elicitation",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\nQuestion 1: Uncertainty Quantification in Preference Learning (40 points)\nIn this question, we will explore Bayesian approaches to logistic regression in the context of preference learning using the Bradley-Terry model. We will compare different models and inference methods, including parametric linear models estimated using Metropolis-Hastings, parametric neural network models estimated using Hamiltonian Monte Carlo, and non-parametric models with Gaussian Processes. Finally, we will assess the uncertainty quantification in these models using the Expected Calibration Error (ECE).\nAssume we have a dataset of pairwise preferences \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N\\), where \\(x_i \\in \\mathbb{R}^d\\) represents the feature difference between two items (i.e., \\(x_i = e^{(i)}_1 - e^{(i)}_2\\) for embeddings \\(e^{(i)}_1\\) and \\(e^{(i)}_2\\)), and \\(y_i \\in \\{0, 1\\}\\) indicates the preference (\\(y_i = 1\\) if item 1 is preferred over item 2 in the \\(i\\)-th pair).\nThe likelihood of observing \\(y_i\\) given \\(x_i\\) and model parameters \\(\\theta\\) is given by the logistic function:\n\\[P(y_i = 1 | x_i, \\theta) = \\sigma(x_i^\\top \\theta) = \\frac{1}{1 + e^{-x_i^\\top \\theta}}.\\]\nWe will adopt a Bayesian approach by placing priors on the model parameters and using Markov Chain Monte Carlo (MCMC) methods to estimate the posterior distributions.\n\nUncertainty Quantification and Expected Calibration Error (11 points)\n\n(Written, 2 point). Spend some time reading https://tinyurl.com/m77mk9c. Explain what the Expected Calibration Error (ECE) measures and why it is important for assessing uncertainty quantification in probabilistic models.\n(Coding, 6 points). In uncertainty_quantification/ece.py, implement the ECE using the formula \\[\\text{ECE} = \\sum_{k=1}^K \\frac{n_k}{N} \\left| \\text{acc}(B_k) - \\text{conf}(B_k) \\right|,\\] where \\(n_k\\) is the number of samples in bin \\(B_k\\), \\(N\\) is the total number of samples, \\(\\text{acc}(B_k)\\) is the accuracy in bin \\(B_k\\), and \\(\\text{conf}(B_k)\\) is the average confidence in bin \\(B_k\\).\n(Written, 3 point). After doing parts (b), (c), and (d), compare the ECE scores and reliability diagrams of the 3 models. Which model(s) provide the best uncertainty quantification? Discuss possible reasons for the observed differences.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef expected_calibration_error(probs, labels, model_name, n_bins=20, n_ticks=10, plot=True):\n    \"\"\"\n    Computes the Expected Calibration Error (ECE) for a model and plots a refined reliability diagram\n    with confidence histogram and additional calibration statistics.\n    \n    Args:\n    - probs (np.array): Array of predicted probabilities for the positive class (for binary classification).\n    - labels (np.array): Array of true labels (0 or 1).\n    - model_name (str): Name of the model for labeling the plot.\n    - n_bins (int): Number of bins to divide the probability interval [0,1] into.\n    - n_ticks (int): Number of ticks to show along the x-axis.\n    - plot (bool): If True, generates the reliability plot; otherwise, only computes ECE.\n\n    Returns:\n    - float: Computed ECE value.\n    \"\"\"\n    \n    # Ensure probabilities are in the range [0, 1]\n    assert np.all((probs &gt;= 0) & (probs &lt;= 1)), \"Probabilities must be in the range [0, 1]\"\n    \n    # Initialize bin edges, centers, and storage for accuracy, confidence, and counts\n    bin_edges = np.linspace(0, 1, n_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    bar_width = 1.0 / n_bins\n\n    accs = np.zeros(n_bins)\n    confs = np.zeros(n_bins)\n    bin_counts = np.zeros(n_bins)\n\n    # Populate bin statistics: accuracy, confidence, and count\n    # YOUR CODE HERE (~7 lines)\n    # Loop over each bin and:\n    # - Find indices of probabilities that fall within the bin.\n    # - Count the number of items in the bin.\n    # - Calculate the accuracy (average of true labels) within the bin.\n    # - Calculate the confidence (average of predicted probabilities) within the bin.\n    pass \n    # END OF YOUR CODE\n    \n    # Compute ECE: weighted average of |accuracy - confidence| across bins\n    # YOUR CODE HERE (1 line)\n    # - Use the bin counts to calculate a weighted average of the differences between accuracy and confidence.\n    ece_value = None\n    # END OF YOUR CODE\n    \n    # Return only ECE if plot is not required\n    if not plot:\n        return ece_value\n\n    # Compute average confidence and accuracy for reference lines\n    avg_confidence = np.mean(probs)\n    avg_accuracy = np.mean(labels)\n    \n    # Create reliability diagram and histogram\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(8, 10))\n    \n    # Reliability diagram (top plot)\n    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n    for i in range(n_bins):\n        # Draw the gap bar starting from the diagonal line (perfect calibration)\n        ax1.bar(bin_centers[i], abs(accs[i] - confs[i]), width=bar_width, bottom=min(accs[i], confs[i]), \n                color='red', alpha=0.3, label='Accuracy-Confidence Gap' if i == 0 else \"\")\n        # Draw the accuracy bar as a small black line on top of the gap bar\n        ax1.plot([bin_centers[i] - bar_width / 2, bin_centers[i] + bar_width / 2], \n                 [accs[i], accs[i]], color='black', linewidth=2)\n\n    # Add a black line as a sample for accuracy in the legend\n    ax1.plot([], [], color='black', linewidth=2, label='Accuracy Marker')\n\n    ax1.set_xlim(0, 1)\n    ax1.set_ylim(0, 1)\n    ax1.set_ylabel('Accuracy')\n    ax1.set_title(f'{model_name}\\nECE={ece_value:.2f}')\n    ax1.legend()\n\n    # Set tick marks based on `n_ticks` evenly spaced along the x-axis\n    tick_positions = np.linspace(0, 1, n_ticks + 1)\n    ax1.set_xticks(tick_positions)\n    ax2.set_xticks(tick_positions)\n    ax1.set_xticklabels([f'{x:.2f}' for x in tick_positions])\n    ax2.set_xticklabels([f'{x:.2f}' for x in tick_positions])\n\n    # Confidence histogram with average markers\n    ax2.bar(bin_centers, bin_counts, width=bar_width, color='blue', alpha=0.6)\n    ax2.axvline(x=avg_confidence, color='gray', linestyle='--', linewidth=2, label='Avg. confidence')\n    ax2.axvline(x=avg_accuracy, color='black', linestyle='-', linewidth=2, label='Avg. accuracy')\n    ax2.set_xlim(0, 1)\n    ax2.set_xlabel('Confidence')\n    ax2.set_ylabel('Count')\n    ax2.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \n    return ece_value\n\nif __name__ == \"__main__\":\n    # Test with random probabilities and labels\n    probs = np.random.rand(10000)  # Random probabilities between 0 and 1\n    labels = np.random.binomial(1, (probs + 1) / 2)\n\n    # Run the function and display the result\n    ece_value = expected_calibration_error(probs, labels, \"Test Model\", plot=True)\n    print(f\"ECE Value: {ece_value}\")\n\n\n\n\n\nParametric Linear Model Estimated Using Metropolis-Hastings (11 points)\n\n(Written, 3 points). Assume a prior on \\(\\theta\\) such that \\(\\theta \\sim \\mathcal{N}(0, \\sigma^2 I)\\), where \\(\\sigma^2\\) is the variance and \\(I\\) is the identity matrix. Derive the expression for the posterior distribution \\(P(\\theta | \\mathcal{D})\\) up to a normalization constant.\n(Coding, 6 points). Implement the Metropolis-Hastings algorithm to sample from the posterior distribution of \\(\\theta\\) in uncertainty_quantification/metropolis.py.\n(Written, 2 points). Discuss how you chose the proposal variance \\(\\tau^2\\) and the number of iterations \\(T\\) and \\(T_{\\text{burn-in}}\\). How did these choices affect the convergence and mixing of your MCMC chain?\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\nfrom ece import expected_calibration_error\n\n# Load training and testing data\nx_train = torch.tensor(np.load('../data/differences_train.npy'))\nx_test = torch.tensor(np.load('../data/differences_test.npy'))\ny_train = torch.tensor(np.load('../data/labels_train.npy'))\ny_test = torch.tensor(np.load('../data/labels_test.npy'))\n\n# Likelihood function for logistic regression (per data point)\ndef likelihood(theta, x, y):\n    \"\"\"\n    Computes the likelihood of the data given the logistic regression parameters.\n    \n    Args:\n    - theta (torch.Tensor): Model parameters.\n    - x (torch.Tensor): Input data.\n    - y (torch.Tensor): True labels.\n\n    Returns:\n    - torch.Tensor: Likelihood values for each data point.\n    \"\"\"\n    # YOUR CODE HERE (~3 lines)\n    # Calculate logits as the linear combination of inputs and parameters.\n    # Use the sigmoid function to compute the probability of the positive class.\n    pass\n    # END OF YOUR CODE\n\n# Prior probability (theta ~ N(0, I)) - only depends on theta, not per sample\ndef prior(theta, sigma):\n    \"\"\"\n    Computes the prior probability of theta under a Gaussian distribution with variance sigma^2.\n\n    Args:\n    - theta (torch.Tensor): Model parameters.\n    - sigma (float): Standard deviation of the prior distribution.\n\n    Returns:\n    - torch.Tensor: Prior probability value.\n    \"\"\"\n    # YOUR CODE HERE (~2 lines)\n    # Implement Gaussian prior with zero mean and identity covariance.\n    # Note that the normalization constant is not needed for Metropolis-Hastings.\n    pass\n    # END OF YOUR CODE\n\n# Metropolis-Hastings sampler\ndef metropolis_hastings(x, y, num_samples, burn_in, tau, sigma):\n    \"\"\"\n    Runs the Metropolis-Hastings algorithm to sample from the posterior distribution.\n\n    Args:\n    - x (torch.Tensor): Input data.\n    - y (torch.Tensor): True labels.\n    - num_samples (int): Total number of samples to draw.\n    - burn_in (int): Number of initial samples to discard.\n    - tau (float): Proposal standard deviation.\n    - sigma (float): Prior standard deviation.\n\n    Returns:\n    - torch.Tensor: Collected samples post burn-in.\n    - float: Acceptance ratio.\n    \"\"\"\n    # Initialize theta (starting point of the chain) and containers for samples and acceptance count\n    theta = torch.zeros(x.shape[1])\n    samples = []\n    acceptances = 0\n    \n    # Run the Metropolis-Hastings algorithm\n    for t in tqdm(range(num_samples), desc=\"MCMC Iteration\"):\n        # YOUR CODE HERE (~12-16 lines)\n        # 1. Propose new theta from the proposal distribution (e.g., Gaussian around current theta).\n        # 2. Compute prior and likelihood for current and proposed theta\n        # 3. Calculate the acceptance ratio as the product of likelihood and prior ratios.\n        # 4. Accept or reject the proposal based on the acceptance probability.\n        # 5. Store the sample after the burn-in period\n        pass\n        # END OF YOUR CODE\n    \n    return torch.stack(samples), acceptances / num_samples\n\n# Run Metropolis-Hastings on training data\nnum_samples = 10000\nburn_in = 1000\ntau = 0.01  # Proposal variance (tune this for convergence)\nsigma = 2.0  # Prior variance\n\n# Collect samples and compute acceptance ratio\nsamples, acceptance_ratio = metropolis_hastings(x_train, y_train, num_samples=num_samples, burn_in=burn_in, tau=tau, sigma=sigma)\naveraged_weights = samples.mean(axis=0)\nprint(f'Predicted weights: {averaged_weights}')\nprint(f'Acceptance Ratio: {acceptance_ratio}')\n\n# Evaluate accuracy on training set\ntrain_predictions = (x_train @ averaged_weights &gt; 0).float()\ntrain_acc = (train_predictions == y_train).float().mean()\nprint(f'Train Accuracy: {train_acc}')\n\n# Evaluate accuracy on testing set\ntest_predictions = (x_test @ averaged_weights &gt; 0).float()\nacc = (test_predictions == y_test).float().mean()\nprint(f'Test Accuracy: {acc}')\n\n# Compute expected calibration error on testing set\nexpected_calibration_error(torch.sigmoid(x_test @ averaged_weights).numpy(), y_test.numpy(), model_name=\"Metropolis-Hastings\")\n\n\n\n\n\nParametric Neural Network Model Estimated Using Hamiltonian Monte Carlo (11 points)\n\n(Written, 2 points). Explain why Hamiltonian Monte Carlo (HMC) is suitable for sampling from the posterior distribution of neural network parameters compared to Metropolis-Hastings.\n(Coding, 7 points). Implement HMC to sample from the posterior distribution of the parameters \\(\\theta\\) of a neural network \\(f(x; \\theta)\\) used for preference prediction in uncertainty_quantification/hmc_nn.py. This will require a GPU and take around 5 minutes on it!\n(Written, 2 points). Briefly describe the performance of the HMC and Metropolis-Hastings models and provide the accuracy numbers.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n# Use a GPU when running this file! JAX should automatically default to GPU.\nimport jax.numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nfrom jax import random\nfrom ece import expected_calibration_error\n\n# DO NOT CHANGE! This function can be ignored.\ndef set_numpyro(new_sampler):\n    numpyro.sample = new_sampler\n\n# Define the neural network model with one hidden layer\ndef nn_model(x_data, y_data, hidden_dim=10):\n    \"\"\"\n    Defines a Bayesian neural network with one hidden layer.\n\n    Args:\n    - x_data (np.array): Input data.\n    - y_data (np.array): Target labels.\n    - hidden_dim (int): Number of units in the hidden layer.\n\n    Returns:\n    - hidden_activations: Activations from the hidden layer.\n    - logits: Logits for the output layer.\n    \"\"\"\n    input_dim = x_data.shape[1]\n    \n    # Prior over the weights and biases for the hidden layer\n    w_hidden = numpyro.sample('w_hidden', dist.Normal(np.zeros((input_dim, hidden_dim)), np.ones((input_dim, hidden_dim))))\n    b_hidden = numpyro.sample('b_hidden', dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))\n    \n    # Compute the hidden layer activations using ReLU\n    # YOUR CODE HERE (~1 line)\n    # Implement the hidden layer computation, applying a ReLU activation.\n    pass\n    # END OF YOUR CODE \n    \n    # Prior over the weights and biases for the output layer\n    w_output = numpyro.sample('w_output', dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))\n    b_output = numpyro.sample('b_output', dist.Normal(0, 1))\n    \n    # Compute the logits for the output layer\n    # YOUR CODE HERE (~1 line)\n    # Calculate the logits as the linear combination of hidden activations and output layer weights.\n    pass\n    # END OF YOUR CODE\n\n    # Likelihood (Bernoulli likelihood with logits)\n    numpyro.sample('obs', dist.Bernoulli(logits=logits), obs=y_data)\n    return hidden_activations, logits\n\ndef sigmoid(x):\n    \"\"\"Helper function to compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nif __name__ == \"__main__\":\n    # Load training and testing data\n    x_train = np.load('../data/differences_train.npy')\n    x_test = np.load('../data/differences_test.npy')\n    y_train = np.load('../data/labels_train.npy')\n    y_test = np.load('../data/labels_test.npy')\n\n    # HMC Sampler Configuration\n    hmc_kernel = NUTS(nn_model)\n\n    # Running HMC with the MCMC interface in NumPyro\n    num_samples = 200  # Number of samples\n    warmup_steps = 100  # Number of burn-in steps\n    rng_key = random.PRNGKey(0)  # Random seed\n\n    # MCMC object with HMC kernel\n    mcmc = MCMC(hmc_kernel, num_samples=num_samples, num_warmup=warmup_steps)\n    mcmc.run(rng_key, x_train, y_train)\n\n    # Get the sampled weights (theta samples)\n    samples = mcmc.get_samples()\n\n    # Extract the weight samples\n    w_hidden_samples = samples['w_hidden']\n    b_hidden_samples = samples['b_hidden']\n    w_output_samples = samples['w_output']\n    b_output_samples = samples['b_output']\n\n    # Compute the averaged weights and biases\n    w_hidden_mean = np.mean(w_hidden_samples, axis=0)\n    b_hidden_mean = np.mean(b_hidden_samples, axis=0)\n    w_output_mean = np.mean(w_output_samples, axis=0)\n    b_output_mean = np.mean(b_output_samples, axis=0)\n\n    # Forward pass through the network for testing set\n    # YOUR CODE HERE (~2 lines)\n    # Compute hidden layer activations and logits for the test set using the mean weights and biases.\n    pass\n    # END OF YOUR CODE\n    test_predictions = test_logits &gt; 0\n    test_accuracy = np.mean(test_predictions == y_test)\n    print(f'Test Accuracy: {test_accuracy}')\n\n    # Forward pass through the network for training set\n    # YOUR CODE HERE (~2 lines)\n    # Compute hidden layer activations and logits for the training set.\n    pass\n    # END OF YOUR CODE\n    train_predictions = train_logits &gt; 0\n    train_accuracy = np.mean(train_predictions == y_train)\n    print(f'Train Accuracy: {train_accuracy}')\n\n    # Compute expected calibration error on testing set\n    expected_calibration_error(sigmoid(test_logits), y_test, model_name=\"HMC\")\n\n\n\n\n\nNon-Parametric Model with Gaussian Process (GP) (7 points)\n\n(Written, 2 point). Describe how a Gaussian Process can be used for preference learning in this context (i.e., describe how the latent function is used for classification).\n(Coding, 2 points). Run the GP classification for preference learning code in\nuncertainty_quantification/gaussian_process.py and provide the accuracy numbers. This can only be run on a CPU and may take around 10 minutes to complete.\n(Written, 3 point). Discuss the computational complexity of the GP model compared to the parametric models. What are the advantages and disadvantages of using a GP in this setting?\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import accuracy_score\nfrom ece import expected_calibration_error\n\nx_train = np.load('../data/differences_train.npy')\nx_test = np.load('../data/differences_test.npy')\ny_train = np.load('../data/labels_train.npy')\ny_test = np.load('../data/labels_test.npy')\n\nkernel = 1.0 * RBF(length_scale=1.0)\ngp_classifier = GaussianProcessClassifier(kernel=kernel, random_state=42, n_jobs=-1)\ngp_classifier.fit(x_train, y_train)\n\ny_pred_probs = gp_classifier.predict_proba(x_test)[:, 1]\ny_pred_labels = (y_pred_probs &gt; 0.5)\n\ntrain_accuracy = accuracy_score(y_train, gp_classifier.predict(x_train))\nprint(f'Train Accuracy: {train_accuracy:.4f}')\n\ntest_accuracy = accuracy_score(y_test, y_pred_labels)\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\nexpected_calibration_error(y_pred_probs, y_test, model_name=\"Gaussian Process Classifier\")\n\n\n\n\n\n\nQuestion 2: Active Learning for Preference Learning (40 points)\nIn this question, you will explore active learning strategies for preference learning using a linear model. We will use expected information gain as the acquisition function to select the most informative queries, where each query is a pair of items. Assume that we model the preferences using a simple linear model. Given feature vectors \\(x_1\\) and \\(x_2\\) corresponding to two items, the probability that \\(x_1\\) is preferred over \\(x_2\\) is modeled using a logistic regression model, i.e.,\n\\[P(x_1 \\succ x_2 | \\theta) = \\sigma(\\theta^\\top (x_1 - x_2)),\\]\nwhere \\(\\theta \\in \\mathbb{R}^d\\) is the model parameter vector, and \\(\\sigma(z)\\) is the sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\). The goal is to sequentially select pairs of items to maximize the information gained about \\(\\theta\\) through preference queries.\n\nExpected Information Gain (15 points)\n\nDerive the Expected Information Gain (Written, 3 points). Suppose that after observing a preference between two items \\(x_1\\) and \\(x_2\\), the posterior distribution over \\(\\theta\\) is updated. The information gain from this observation is the reduction in uncertainty about \\(\\theta\\) measured using the Kullback-Leibler (KL) divergence between the prior and posterior distributions. Given the current posterior distribution \\(P(\\theta | \\mathcal{D})\\) and a possible observation \\(y \\in \\{0, 1\\}\\) (where \\(y = 1\\) if \\(x_1\\) is preferred over \\(x_2\\), and \\(y = 0\\) otherwise), the expected information gain is: \\[\\begin{aligned}\n\\mathbb{E}[\\text{IG}(x_1, x_2)] = &P(y=1 | x_1, x_2, \\theta) D_{\\text{KL}}\\left( P(\\theta | y = 1, \\mathcal{D}) \\parallel P(\\theta | \\mathcal{D}) \\right) \\\\+\n&P(y=0 | x_1, x_2, \\theta) D_{\\text{KL}}\\left( P(\\theta | y = 0, \\mathcal{D}) \\parallel P(\\theta | \\mathcal{D}) \\right)\n\\end{aligned}\\]\nDerive this expression for the expected information gain of selecting the pair \\((x_1, x_2)\\) for a preference query. Start by explaining how the KL divergence measures the information gain, and break down the expectation over the possible outcomes of the query.\nSimplifying the KL Divergence (Written, 4 points). Assuming the prior and posterior distributions over \\(\\theta\\) are Gaussian (i.e., \\(P(\\theta) \\sim \\mathcal{N}(\\mu, \\Sigma)\\) and \\(P(\\theta | \\mathcal{D}) \\sim \\mathcal{N}(\\mu', \\Sigma')\\)), show that the KL divergence between the Gaussian posterior and prior simplifies to: \\[\\begin{aligned}\n    D_{\\text{KL}}\\left( \\mathcal{N}(\\mu', \\Sigma') \\parallel \\mathcal{N}(\\mu, \\Sigma) \\right) &= \\frac{1}{2} \\left( \\text{tr}(\\Sigma^{-1} \\Sigma') + (\\mu' - \\mu)^\\top \\Sigma^{-1} (\\mu' - \\mu)\\right.\\\\\n    &\\left.- d + \\log\\left( \\frac{\\det(\\Sigma)}{\\det(\\Sigma')} \\right) \\right).\n    \\end{aligned}\\]\nApproximate Information Gain for a Linear Model (Written, 4 points). In the case of a linear model with Gaussian priors on \\(\\theta\\), assume that the posterior distribution \\(P(\\theta | \\mathcal{D}) \\sim \\mathcal{N}(\\mu, \\Sigma)\\) is updated using Bayes’ rule after each observation. The likelihood of observing a preference \\(y\\) is logistic, which does not conjugate with the Gaussian prior. However, for the purposes of this question, assume that after each query, the posterior mean \\(\\mu'\\) and covariance \\(\\Sigma'\\) can be updated using an approximation method such as Laplace’s approximation.\nUsing these assumptions, compute the expected information gain for a specific query \\((x_1, x_2)\\) in closed form. You may express the information gain in terms of the updated mean \\(\\mu'\\) and covariance \\(\\Sigma'\\) after observing the preference outcome.\nLaplace Approximation for Posterior (Written, 4 points). The Laplace approximation for the posterior is given by \\[\\begin{aligned}\n\\mu'=\\arg \\min_\\theta -\\log P(\\theta | \\mathcal{D})\\\\\n\\Sigma'^{-1}=\\nabla_\\theta\\nabla_\\theta -\\log P(\\theta|\\mathcal{D})|_{\\theta=\\mu'}\n\\end{aligned}\\] In our scenario with the Bradley-Terry model for likelihood, simplify \\(-\\log P(\\theta | \\mathcal{D})\\) and its Hessian ignoring the normalization constant.\n\nActive Learning Algorithm (25 points) In this section, you will implement an active learning algorithm for selecting the most informative queries using the expected information gain criterion.\n\n(Coding, 4 points). Implement kl_divergence_gaussians in active_learning/main.py.\n(Coding, 4 points). Following your derived Laplace approximation, implement negative_log_posterior.\n(Coding, 4 points). Implement compute_hessian that is used to obtain the inverse of the covariance matrix.\n(Coding, 3 points). Implement expected_information_gain.\n(Coding, 4 points). Finally, implement active_learning.\n(Coding + Written, 6 points). Plot the \\(L^2\\) norm of the covariance matrix for each loop of the active learning loop. Additionally, on the same plot, implement a random baseline and plot its \\(L^2\\) covariance matrix norm. The random baseline should randomly select a point in the dataset and not use any acquisition function. Interpret your plot and use it to compare the two methods.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\n\nclass LogisticActiveLearning:\n    def __init__(self, test_size=0.2):\n        \"\"\"\n        Initializes LogisticActiveLearning model, sets device, and prepares data.\n        \n        Args:\n        - test_size (float): Proportion of the dataset used for validation.\n        \"\"\"\n        # Make device customizable\n        self.device = torch.device(\"cpu\")\n        X, y = make_classification(n_samples=10000, random_state=42)\n\n        # Convert data and labels to tensors\n        x_data = torch.tensor(X, dtype=torch.float32).to(self.device)\n        y_data = torch.tensor(y, dtype=torch.float32).to(self.device)\n        self.N, self.D = x_data.shape\n\n        # Split into training and validation sets\n        train_indices, val_indices = train_test_split(range(self.N), test_size=test_size, random_state=42)\n        self.x_train = x_data[train_indices]\n        self.y_train = y_data[train_indices]\n        self.x_val = x_data[val_indices]\n        self.y_val = y_data[val_indices]\n\n        # Initialize mean and inverse covariance for the prior\n        self.weights_mean = torch.zeros(self.D, requires_grad=True, device=self.device)\n        self.weights_inv_cov = torch.eye(self.D).to(self.device)  # Start with identity inverse covariance\n\n    def negative_log_posterior(self, w, x, y):\n        \"\"\"\n        Computes the negative log-posterior (negative log-prior + log-likelihood).\n        \n        Args:\n        - w (torch.Tensor): Model weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        \n        Returns:\n        - torch.Tensor: Negative log-posterior value.\n        \"\"\"\n        # YOUR CODE HERE (~4-6 lines)\n        # Compute log-prior term using inverse covariance\n        pass\n        # END OF YOUR CODE\n\n    def optimize_weights(self, w, x, y, num_steps=50, lr=1e-2):\n        \"\"\"\n        Optimizes weights using Adam optimizer.\n        \n        Args:\n        - w (torch.Tensor): Initial weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        - num_steps (int): Number of optimization steps.\n        - lr (float): Learning rate.\n        \n        Returns:\n        - torch.Tensor: Updated weights.\n        - torch.Tensor: Hessian inverse covariance.\n        \"\"\"\n        optimizer = Adam([w], lr=lr)\n        \n        for step in range(num_steps):\n            optimizer.zero_grad()\n            loss = self.negative_log_posterior(w, x, y)\n            loss.backward()\n            optimizer.step()\n\n        # Compute the Hessian of log-posterior, serving as inverse covariance\n        inv_cov = self.compute_hessian(w.detach(), x, y)\n        return w.detach().clone(), inv_cov\n\n    def compute_hessian(self, w, x, y):\n        \"\"\"\n        Computes the Hessian of the negative log-posterior, used as the inverse covariance.\n        \n        Args:\n        - w (torch.Tensor): Model weights.\n        - x (torch.Tensor): Input data point.\n        - y (torch.Tensor): True label.\n        \n        Returns:\n        - torch.Tensor: Hessian of the negative log-posterior.\n        \"\"\"\n        # YOUR CODE HERE (~5-8 lines)\n        # Hessian of the prior term\n        pass\n        # END OF YOUR CODE\n\n    def acquisition_fn(self, x):\n        \"\"\"\n        Computes posterior means and inverse covariances for y=1 and y=0 without modifying original parameters.\n        \n        Args:\n        - x (torch.Tensor): Input data point.\n        \n        Returns:\n        - dict: Posterior properties for y=1 and y=0 cases.\n        \"\"\"\n        weights_y1 = self.weights_mean.clone().detach().requires_grad_(True)\n        weights_y0 = self.weights_mean.clone().detach().requires_grad_(True)\n\n        # Optimize weights and get Hessian for both y=1 and y=0 cases\n        posterior_mean_y1, inv_cov_y1 = self.optimize_weights(weights_y1, x, 1, num_steps=50)\n        posterior_mean_y0, inv_cov_y0 = self.optimize_weights(weights_y0, x, 0, num_steps=50)\n\n        # Calculate probabilities for the acquisition function\n        prob_y1 = torch.sigmoid(torch.dot(self.weights_mean.detach(), x))\n        prob_y0 = 1 - prob_y1\n\n        return {\n            'prob_y1': prob_y1,\n            'prob_y0': prob_y0,\n            'posterior_mean_y1': posterior_mean_y1,\n            'posterior_inv_cov_y1': inv_cov_y1,\n            'posterior_mean_y0': posterior_mean_y0,\n            'posterior_inv_cov_y0': inv_cov_y0\n        }\n\n    def expected_information_gain(self, x):\n        \"\"\"\n        Computes expected information gain for a given point `x`.\n        \n        Args:\n        - x (torch.Tensor): Input data point.\n        \n        Returns:\n        - torch.Tensor: Expected Information Gain (EIG) value.\n        \"\"\"\n        acquisition = self.acquisition_fn(x)\n\n        # Compute KL divergences for y=1 and y=0 using inverse covariances\n        kl_y1 = kl_divergence_gaussians(\n            acquisition['posterior_mean_y1'],\n            acquisition['posterior_inv_cov_y1'],\n            self.weights_mean.detach(),\n            self.weights_inv_cov\n        )\n\n        kl_y0 = kl_divergence_gaussians(\n            acquisition['posterior_mean_y0'],\n            acquisition['posterior_inv_cov_y0'],\n            self.weights_mean.detach(),\n            self.weights_inv_cov\n        )\n\n        # Expected Information Gain (EIG)\n        eig = None # YOUR CODE HERE (1 line)\n        return eig\n\n    def active_learning(self, selected_indices, subset_size=50):\n        \"\"\"\n        Active learning loop that selects the most informative data point based on EIG.\n        \n        Args:\n        - selected_indices (list): Indices of previously selected samples.\n        - subset_size (int): Number of samples to consider in each subset.\n\n        Returns:\n        - best_x, best_x_idx, best_acquisition: Selected data point and acquisition details.\n        \"\"\"\n        best_eig = -float('inf')\n        best_x = None\n        best_x_idx = -1\n        best_acquisition = None\n\n        subset_indices = [i for i in torch.randperm(len(self.x_train)).tolist() if i not in selected_indices][:subset_size]\n\n        # YOUR CODE HERE (~ 10 lines)\n        pass\n        # END OF YOUR CODE\n        return best_x, best_x_idx, best_acquisition\n\n    def validate(self):\n        \"\"\"\n        Computes accuracy on the validation set by predicting labels and comparing to true labels.\n        \n        Returns:\n        - float: Validation accuracy.\n        \"\"\"\n        with torch.no_grad():\n            logits = self.x_val @ self.weights_mean\n            predictions = torch.sigmoid(logits) &gt;= 0.5  # Convert logits to binary predictions\n            accuracy = (predictions == self.y_val).float().mean().item()\n            print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n        return accuracy\n\n    def train(self, num_iterations=10, subset_size=50):\n        \"\"\"\n        Train the model using active learning with subset sampling.\n        \n        Args:\n        - num_iterations (int): Number of active learning iterations.\n        - subset_size (int): Number of samples to consider in each subset.\n        \"\"\"\n        selected_indices = []\n        for iteration in range(num_iterations):\n            print(f\"Iteration {iteration + 1}/{num_iterations}\")\n\n            # Select the most informative data point from a random subset\n            best_x, best_x_idx, acquisition = self.active_learning(selected_indices, subset_size=subset_size)\n            selected_indices.append(best_x_idx)\n            print(f\"Selected data point with EIG.\")\n\n            # Get the true label for the selected data point\n            y = self.y_train[best_x_idx].item()\n\n            # Update posterior mean and inverse covariance based on true label\n            if y == 1:\n                self.weights_mean = acquisition['posterior_mean_y1']\n                self.weights_inv_cov = acquisition['posterior_inv_cov_y1']\n            else:\n                self.weights_mean = acquisition['posterior_mean_y0']\n                self.weights_inv_cov = acquisition['posterior_inv_cov_y0']\n\n            print(f\"Covariance L2: {torch.inverse(self.weights_inv_cov).norm()}\")\n\n            # Validate model performance on the validation set\n            self.validate()\n\n# KL divergence between two multivariate normal distributions\ndef kl_divergence_gaussians(mu1, sigma1_inv, mu2, sigma2_inv):\n    \"\"\"\n    Computes the KL divergence between two multivariate Gaussian distributions.\n    \n    Args:\n    - mu1, mu2 (torch.Tensor): Mean vectors of the distributions.\n    - sigma1_inv, sigma2_inv (torch.Tensor): Inverse covariance matrices of the distributions. PLEASE NOTE THE INVERSE!\n    \n    Returns:\n    - torch.Tensor: KL divergence value.\n    \"\"\"\n    # YOUR CODE HERE (~ 9-12 lines)\n    pass\n    # END OF YOUR CODE\n\n# Example usage\nmodel = LogisticActiveLearning()\nmodel.train(num_iterations=100, subset_size=50)\n\n\n\n\n\n\nQuestion 3: Linear Performance Metric Elicitation (30 points)\n\n(Written, 10 points). For background on the problem setting, read https://tinyurl.com/3b92sufm. Suppose we have a linear performance metric given by \\[p(C) = 1-\\alpha (FP)-\\beta (FN)\\] where \\(C\\) is a confusion matrix and \\(FP, FN\\) denote false positive and false negative rates. We wish to find the optimal classifier w.r.t. \\(p\\). That is, \\[\\phi^* = \\arg \\max_{\\phi\\in\\Phi} p(C(\\phi))\\] where \\(\\Phi\\) is the space of all probabilistic binary classifiers from \\(X\\to [0, 1]\\). Note that these classifiers return probabilities corresponding to the label \\(1\\). Show that \\(\\phi^*\\) is in fact deterministic and given by \\[\\phi(x)=\\begin{cases}\n    1 & \\text{if } p(y|x) &gt; f(\\alpha,\\beta) \\\\\n    0 & \\text{otherwise}.\n\\end{cases}\\] for a threshold function \\(f\\) that you must find. (Hint: For a classifier \\(\\phi\\), \\(FP=P(\\phi=1, y=0)\\) and \\(FN=P(\\phi=0, y=1)\\). Marginalize these joint probabilities over \\(x\\) and simplify.)\n(Written + Coding, 5 points). Implement classifier_metrics in lpme/main.py. After doing so, run plot_confusion_region and attach the plot. What do you notice about the region of possible confusion matrices?\n(Coding, 15 points). Implement search_theta in order to elicit the metric used by the oracle (which is parametrized by \\(\\theta\\)). Play around with the oracle’s theta and run start_search to see how close you can approximate it!\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nclass DataDistribution:\n    def __init__(self, N: int):\n        \"\"\"\n        Initializes the data distribution with a specified number of samples.\n        \n        Args:\n        - N (int): Number of data points.\n        \"\"\"\n        self.weights = torch.tensor([-0.3356, -1.4104, 0.3144, -0.5591, 1.0426, 0.6036, -0.7549, -1.1909, 1.4779, -0.7513])\n        self.D = len(self.weights)\n\n        gen = torch.Generator().manual_seed(42)\n        self.data = torch.randn(N, self.D, generator=gen)\n        self.probs = torch.sigmoid(self.data @ self.weights)\n    \ndef classifier_metrics(data_dist, threshold, upper=True):\n    \"\"\"\n    Computes the True Positive and True Negative rates based on a classifier threshold.\n    \n    Args:\n    - data_dist (DataDistribution): The data distribution instance.\n    - threshold (float): Threshold value for classification.\n    - upper (bool): If True, classifies as positive if above threshold; else, if below.\n    \n    Returns:\n    - tuple (float, float): True Positive Rate (TP) and True Negative Rate (TN) in that order.\n    \"\"\"\n    # YOUR CODE HERE (~3-5 lines)\n    pass\n    # END OF YOUR CODE\n\ndef sweep_classifiers(data_dist: DataDistribution):\n    \"\"\"\n    Sweeps through classifier thresholds and calculates True Positive and True Negative rates.\n    \n    Args:\n    - data_dist (DataDistribution): The data distribution instance.\n    \n    Returns:\n    - tuple: Upper and lower boundary data for True Positive and True Negative rates.\n    \"\"\"\n    thresholds = torch.linspace(0, 1, 100)\n    upper_boundary = []\n    lower_boundary = []\n    \n    for threshold in tqdm(thresholds, desc=\"Thresholds\"):\n        tp_upper, tn_upper = classifier_metrics(data_dist, threshold, upper=True)\n        upper_boundary.append((tp_upper, tn_upper))\n\n        tp_lower, tn_lower = classifier_metrics(data_dist, threshold, upper=False)\n        lower_boundary.append((tp_lower, tn_lower))\n\n    return upper_boundary, lower_boundary\n\nclass Oracle:\n    def __init__(self, theta: float):\n        \"\"\"\n        Initializes the oracle with a given theta for preference evaluation.\n        \n        Args:\n        - theta (float): Oracle angle in radians.\n        \"\"\"\n        self.theta = torch.tensor(theta)\n\n    def evaluate_lpm(self, tp, tn):\n        \"\"\"\n        Computes the linear performance metric (LPM) based on theta.\n        \n        Args:\n        - tp (float): True Positive rate.\n        - tn (float): True Negative rate.\n        \n        Returns:\n        - float: Linear performance metric evaluation.\n        \"\"\"\n        return torch.cos(self.theta) * tp + torch.sin(self.theta) * tn\n    \n    def preferred_classifier(self, tp_1, tn_1, tp_2, tn_2):\n        \"\"\"\n        Determines the preferred classifier based on LPM values.\n        \n        Args:\n        - tp_1, tn_1, tp_2, tn_2 (float): True Positive and True Negative rates for two classifiers.\n        \n        Returns:\n        - bool: True if first classifier is preferred, False otherwise.\n        \"\"\"\n        lpm_1 = self.evaluate_lpm(tp_1, tn_1)\n        lpm_2 = self.evaluate_lpm(tp_2, tn_2)\n        return (lpm_1 &gt; lpm_2).item()\n    \ndef theta_to_threshold(theta):\n    \"\"\"Converts theta angle to classification threshold.\"\"\"\n    return 1 / (1 + torch.tan(theta) ** -1)\n\ndef search_theta(oracle: Oracle, data_dist, lower_bound, upper_bound):\n    \"\"\"\n    Performs a search over theta values to optimize the classification threshold.\n    \n    Args:\n    - oracle (Oracle): The oracle for LPM evaluation.\n    - data_dist (DataDistribution): The data distribution instance.\n    - lower_bound (float): Lower bound for theta.\n    - upper_bound (float): Upper bound for theta.\n    \n    Returns:\n    - tuple: Updated lower and upper bounds for theta.\n    \"\"\"\n    left = 0.75 * lower_bound + 0.25 * upper_bound\n    middle = 0.5 * lower_bound + 0.5 * upper_bound\n    right = 0.25 * lower_bound + 0.75 * upper_bound\n\n    thetas = [lower_bound, left, middle, right, upper_bound]\n    thresholds = theta_to_threshold(torch.tensor(thetas))\n    new_lower, new_upper = None, None\n\n    # YOUR CODE HERE (~18-25 lines)\n    # 1. Collect metrics for each threshold value.\n    # 2. Determine if LPM increases as theta increases.\n    # 3. Check for pattern of increases and decreases in LPM.\n    # 4. Update bounds based on observed LPM patterns.\n    pass\n    # END OF YOUR CODE\n\n    return new_lower, new_upper\n\n# Create instance and get upper & lower boundary data\ndata_dist = DataDistribution(N=10000000)\noracle = Oracle(theta=0.1)\n\ndef plot_confusion_region():\n    \"\"\"\n    Plots the True Positive vs. True Negative rates for the upper and lower classifier boundaries.\n    \"\"\"\n    upper_boundary, lower_boundary = sweep_classifiers(data_dist)\n\n    # Prepare data for plotting for upper and lower boundaries\n    tp_upper, tn_upper = zip(*upper_boundary)\n    tp_lower, tn_lower = zip(*lower_boundary)\n\n    # Plot the results for upper boundary\n    plt.figure(figsize=(8, 6))\n    plt.plot(tp_upper, tn_upper, marker='o', linestyle='-', alpha=0.7, label=\"Upper Boundary\")\n    plt.plot(tp_lower, tn_lower, marker='o', linestyle='--', alpha=0.7, label=\"Lower Boundary\")\n    plt.title(\"True Positive vs. True Negative Rates (Upper & Lower Boundaries)\")\n    plt.xlabel(\"True Positive Rate (TP)\")\n    plt.ylabel(\"True Negative Rate (TN)\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef start_search():\n    \"\"\"\n    Starts the theta search using the LPM-based oracle and prints the search range per iteration.\n    \"\"\"\n    lower_bound = 0\n    upper_bound = torch.pi / 2\n    for _ in tqdm(range(10), desc=\"LPM Search\"):\n        print(f\"Theta Search Space: [{lower_bound}, {upper_bound}]\")\n        lower_bound, upper_bound = search_theta(oracle, data_dist, lower_bound=lower_bound, upper_bound=upper_bound)\n    print(f\"Theta Search Space: [{lower_bound}, {upper_bound}]\")\n\n\n\n\n\n\nQuestion 4: D-optimal Design with Logistic Model (30 points)\nIn this question, we explore D-optimal designs in the context of the Bradley-Terry model. The Bradley-Terry model is a logistic regression model used for paired comparison data. Given two items \\(x_1\\) and \\(x_2\\), the probability that item \\(x_1\\) is preferred over \\(x_2\\) is modeled as:\n\\[P(x_1 \\succ x_2 | \\theta) = \\frac{e^{\\theta^\\top x_1}}{e^{\\theta^\\top x_1} + e^{\\theta^\\top x_2}} = \\frac{1}{1 + e^{\\theta^\\top (x_2 - x_1)}}\\]\nwhere \\(\\theta \\in \\mathbb{R}^d\\) represents the unknown model parameters, and \\(x_1, x_2 \\in \\mathbb{R}^d\\) are the feature vectors associated with the two items. D-optimal design aims to maximize the determinant of the Fisher information matrix, thus minimizing the volume of the confidence ellipsoid for the estimated parameters. In this exercise, you will analyze D-optimal designs for this model.\n\nFisher Information Matrix for the Bradley-Terry Model (12 points)\n\n(Written, 6 points). Derive the Fisher information matrix for the Bradley-Terry model at a design point \\((x_1, x_2)\\). Show that the Fisher information matrix at a design point is: \\[I(x_1, x_2, \\theta) = w(x_1, x_2, \\theta) (x_1 - x_2)(x_1 - x_2)^\\top,\\] where \\(w(x_1, x_2, \\theta)\\) is a weight function given by: \\[w(x_1, x_2, \\theta) = \\frac{e^{\\theta^\\top x_1} e^{\\theta^\\top x_2}}{\\left(e^{\\theta^\\top x_1} + e^{\\theta^\\top x_2}\\right)^2} =\\sigma'(\\theta^\\top (x_1-x_2)).\\] \\(\\sigma'\\) is the derivative of the sigmoid function.\n(Coding, 6 points). Implement fisher_matrix in d_optimal/main.py based on the derived expression.\n\nD-optimal Design Criterion (18 points)\n\n(Coding, 11 points). In the context of the Bradley-Terry model, a D-optimal design maximizes the determinant of the Fisher information matrix. Suppose we have a set of candidate items \\(\\{x_1, \\dots, x_n\\}\\), and we can choose \\(N\\) comparisons to make. Formally, the D-optimal design maximizes: \\[\\det\\left( \\sum_{i=1}^N w(x_{i1}, x_{i2}, \\theta) (x_{i1} - x_{i2})(x_{i1} - x_{i2})^\\top \\right),\\] where \\((x_{i1}, x_{i2})\\) denotes a pair of compared items in the design. Implement a greedy algorithm to approximate the D-optimal design. Given a set of \\(n\\) items and their feature vectors \\(\\{x_1, \\dots, x_n\\}\\), your task is to iteratively select the pair of items \\((x_{i1}, x_{i2})\\) that maximizes the determinant of the Fisher information matrix. Please implement greedy_fisher. Note that the setup in the code assumes we have a dataset of all possible differences between pairs of items as opposed to directly selecting the pairs.\n(Written + Coding, 7 points). Notice that posterior_inv_cov uses a Laplace approximation for the posterior centered around the ground truth weights after labeling the chosen points. However, it turns out this approximation doesn’t actually depend on the labels when taking the Hessian. Please run the file d_optimal/main.py and attach a plot of the norm of the covariance matrix of the posterior. What difference do you observe between greedy and random sampling? What is the win rate of greedy?\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef sigmoid(x):\n    \"\"\"Helper function to compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\nclass LogisticData:\n    def __init__(self, weights, seed=42):\n        \"\"\"\n        Initializes the LogisticData class with specified weights and seed.\n        \n        Args:\n        - weights (np.array): True weights for data generation.\n        - seed (int): Random seed for reproducibility.\n        \"\"\"\n        self.rng = np.random.default_rng(seed)\n        self.weights = weights\n    \n    def generate_data(self, N):\n        \"\"\"\n        Generates synthetic data for logistic regression.\n        \n        Args:\n        - N (int): Number of data points.\n        \n        Returns:\n        - tuple: Generated data and labels.\n        \"\"\"\n        data = self.rng.standard_normal((N, len(self.weights)))\n        probs = sigmoid(data @ self.weights)\n        labels = (self.rng.random(N) &lt; probs).astype(int)\n        return data, labels\n\ndef fisher_matrix(difference_vector, weights):\n    \"\"\"\n    Computes the Fisher information matrix for a single data point.\n    \n    Args:\n    - difference_vector (np.array): Difference vector (input data point).\n    - weights (np.array): Weights for the logistic model.\n    \n    Returns:\n    - np.array: Fisher information matrix for the data point.\n    \"\"\"\n    # YOUR CODE HERE (~2-4 lines)\n    pass\n    # END OF YOUR CODE\n\n# Initialization\ntrue_weights = np.array([-0.3356, -1.4104, 0.3144, -0.5591, 1.0426, 0.6036, -0.7549, -1.1909, 1.4779, -0.7513])\ndata_dim = len(true_weights)\ndataset_generator = LogisticData(weights=true_weights)\n\n# Number of iterations for sampling 500 points\nnum_iterations = 200\n\n# Store covariance matrix norms for comparison\ncov_norms_greedy = []\ncov_norms_random = []\n\ndef greedy_fisher(data, curr_fisher_matrix, selected_indices):\n    \"\"\"\n    Selects the data point that maximizes the Fisher information determinant.\n    \n    Args:\n    - data (np.array): The data matrix.\n    - curr_fisher_matrix (np.array): Fisher matrix of already selected indices.\n    - selected_indices (list): List of already selected indices.\n    \n    Returns:\n    - int: Index of the selected data point.\n    \"\"\"\n    best_det = -np.inf\n    best_index = -1\n    \n    # Iterate over data points to find the one maximizing Fisher determinant.\n    for i, difference_vector in enumerate(data):\n        # YOUR CODE HERE (~5-10 lines)\n        # Make sure to skip already selected data points!\n        pass\n        # END OF YOUR CODE\n    return best_index\n\ndef posterior_inv_cov(X, laplace_center):\n    \"\"\"\n    Computes the posterior inverse covariance matrix using Laplace approximation.\n    \n    Args:\n    - X (np.array): Data matrix.\n    - laplace_center (np.array): Center point (weights).\n    \n    Returns:\n    - np.array: Posterior inverse covariance matrix.\n    \"\"\"\n    # Calculate probabilities for logistic regression model.\n    probs = sigmoid(X @ laplace_center)\n    W = np.diag(probs * (1 - probs))\n    \n    # Compute inverse covariance matrix assuming standard Gaussian prior.\n    inv_cov = X.T @ W @ X + np.eye(len(true_weights))\n    return inv_cov\n\nfor _ in tqdm(range(num_iterations)):\n    # Generate a new sample of 500 data points\n    data, _ = dataset_generator.generate_data(N=500)\n    \n    # Greedy selection of best 30 data points\n    selected_indices = []\n    curr_fisher_matrix = np.zeros((data_dim, data_dim))\n\n    for _ in range(30):\n        # Select the data point maximizing Fisher information determinant.\n        best_index = greedy_fisher(data, curr_fisher_matrix, selected_indices)\n        selected_indices.append(best_index)\n        curr_fisher_matrix += fisher_matrix(data[best_index], true_weights)\n\n    # Prepare greedy and random samples\n    X_greedy = data[selected_indices]\n\n    # Generate 30 random samples for comparison\n    random_indices = np.random.choice(len(data), 30, replace=False)\n    X_random = data[random_indices]\n\n    # Compute posterior inverse covariance matrices for both strategies\n    posterior_inv_cov_greedy = posterior_inv_cov(X_greedy, laplace_center=true_weights) \n    posterior_inv_cov_random = posterior_inv_cov(X_random, laplace_center=true_weights)\n\n    # Calculate covariance matrices (inverse of posterior inverse covariance)\n    cov_matrix_greedy = np.linalg.inv(posterior_inv_cov_greedy)\n    cov_matrix_random = np.linalg.inv(posterior_inv_cov_random)\n\n    # Measure the norm (Frobenius norm) of the covariance matrices\n    cov_norm_greedy = np.linalg.norm(cov_matrix_greedy, 'fro')\n    cov_norm_random = np.linalg.norm(cov_matrix_random, 'fro')\n\n    # Store norms for analysis\n    cov_norms_greedy.append(cov_norm_greedy)\n    cov_norms_random.append(cov_norm_random)\n\n# Display comparison results\nprint(f'Greedy mean: {np.mean(cov_norms_greedy)}')\nprint(f'Random mean: {np.mean(cov_norms_random)}')\nprint(f'Greedy win rate: {(np.array(cov_norms_greedy) &lt; np.array(cov_norms_random)).mean()}')\n\n# Plot the distributions of covariance matrix norms\nplt.hist(cov_norms_greedy, bins=30, alpha=0.7, color='blue', label='Greedy')\nplt.hist(cov_norms_random, bins=30, alpha=0.7, color='red', label='Random')\nplt.xlabel('L2 Norm of Covariance Matrix')\nplt.ylabel('Frequency')\nplt.title('Comparison of Covariance Norms (Greedy vs. Random) Across Iterations')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nQuestion 5: Nonparametric Metric Elicitation (30 points)\nIn this question, we explore the problem of performance metric elicitation using a Gaussian Process (GP) to map the elements of the confusion matrix, specifically false positives (FP) and false negatives (FN), to an unknown performance metric. The goal is to learn a non-linear function that maps FP and FN to the metric, using relative preferences from pairwise classifier comparisons. We will use elliptical slice sampling for posterior inference.\n\nGaussian Process for Metric Elicitation (10 points)\n\n(Written, 2 points). Assume that the performance metric \\(\\phi(C)\\) is a non-linear function of the confusion matrix \\(C\\). For simplicity, assume that \\(\\phi\\) depends only on FP and FN, i.e., \\[\\phi(\\text{FP}, \\text{FN}) \\sim \\mathcal{GP}(0, k((\\text{FP}, \\text{FN}), (\\text{FP}', \\text{FN}'))),\\] where \\(k\\) is the covariance kernel function of the Gaussian Process. Explain why using a GP allows for flexible modeling of the metric \\(\\phi\\) as a non-linear function of FP and FN. What are the advantages of using a GP over a linear model in this context?\n(Written, 2 points). Suppose we observe pairwise comparisons between classifiers, where a user provides feedback on which classifier they prefer based on the unknown metric \\(\\phi\\). Given two classifiers with confusion matrices \\(C_1 = (\\text{FP}_1, \\text{FN}_1)\\) and \\(C_2 = (\\text{FP}_2, \\text{FN}_2)\\), the user indicates their relative preference. Let the observed preference be modeled by Bradley-Terry as: \\[\\Pr(C_1 \\succ C_2) = \\sigma(\\phi(\\text{FP}_1, \\text{FN}_1) - \\phi(\\text{FP}_2, \\text{FN}_2)).\\] where we view \\(\\phi\\) as the reward function. How does this likelihood affect the posterior inference in the GP? Where does it introduce additional complexity?\n(Written + Coding, 6 points). Given a set of observed pairwise comparisons, derive the posterior distribution over the latent function values \\(\\phi\\) given a set of confusion matrices preferences using Bayes’ rule. Express the posterior distribution in terms of the GP prior and the pairwise likelihood function. You do not need to include the normalization constant. Implement the likelihood function in loglik_from_preferences.\n\nElliptical Slice Sampling for Posterior Inference (20 points)\n\n(Written, 3 points). Read https://proceedings.mlr.press/v9/murray10a/murray10a.pdf. Elliptical slice sampling is a sampling method used to generate samples from the posterior distribution of a Gaussian Process. Explain the key idea behind elliptical slice sampling and why it is well-suited for sampling from the GP posterior in this context.\n(Coding, 10 points). Implement elliptical slice sampling in npme/elliptical_sampler.py by following Figure 2 in the paper.\n(Written, 3 points). Run the algorithm on a synthetic preference dataset of confusion matrices with pairwise preferences. The synthetic data will be constructed using the metric \\[\\phi_{\\text{true}}(\\text{FP}, \\text{FN}) = \\log(1 + \\text{FP}) + \\log(1 + \\text{FN}),\\] which captures the idea that the human oracle perceives both false positives and false negatives in a way that flattens out as these values increase (i.e., marginal increases in FP and FN have diminishing effects on the performance metric). Explain the psychological motivation behind this non-linear function. Why might a logarithmic form be appropriate for modeling human perception of classification errors?\nRun the file npme/main.py and attach the plot of \\(\\phi_{\\text{true}}\\) vs your elicited metric. What do you notice in the plot?\n(Written + Coding, 4 points). Once the GP has been trained and posterior samples of the function \\(\\phi(\\text{FP}, \\text{FN})\\) have been obtained, how can we evaluate the quality of the elicited metric? Propose a method to evaluate how well the elicited metric \\(\\phi\\) aligns with the user’s true preferences and implement it in evaluate_elicited_metric taking into the plot you saw in part (iii).\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Callable\nimport numpy as np\nfrom tqdm import tqdm\n\nclass EllipticalSliceSampler:\n    def __init__(self,\n                 prior_cov: np.ndarray,\n                 loglik: Callable):\n        \"\"\"\n        Initializes the Elliptical Slice Sampler.\n        \n        Args:\n        - prior_cov (np.ndarray): Prior covariance matrix.\n        - loglik (Callable): Log-likelihood function.\n        \"\"\"\n        self.prior_cov = prior_cov\n        self.loglik = loglik\n\n        self._n = prior_cov.shape[0]  # Dimensionality of the space\n        self._chol = np.linalg.cholesky(prior_cov)  # Cache Cholesky decomposition\n\n        # Initialize state by sampling from prior\n        self._state_f = self._chol @ np.random.randn(self._n)\n\n    def _indiv_sample(self):\n        \"\"\"\n        Main algorithm for generating an individual sample using Elliptical Slice Sampling.\n        \"\"\"\n        f = self._state_f  # Previous state\n        nu = self._chol @ np.random.randn(self._n)  # Sample from prior for the ellipse\n        log_y = self.loglik(f) + np.log(np.random.uniform())  # Log-likelihood threshold\n\n        theta = np.random.uniform(0., 2 * np.pi)  # Initial proposal angle\n        theta_min, theta_max = theta - 2 * np.pi, theta  # Define bracketing interval\n\n        # Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.\n        while True:\n            # YOUR CODE HERE (~10 lines)\n            # 1. Generate a new sample point based on the current angle.\n            # 2. Check if the proposed point meets the acceptance criterion.            \n            # 3. If not accepted, adjust the bracket and select a new angle.\n            break\n            # END OF YOUR CODE\n\n    def sample(self,\n               n_samples: int,\n               n_burn: int = 500) -&gt; np.ndarray:\n        \"\"\"\n        Generates samples using Elliptical Slice Sampling.\n\n        Args:\n        - n_samples (int): Total number of samples to return.\n        - n_burn (int): Number of initial samples to discard (burn-in).\n\n        Returns:\n        - np.ndarray: Array of samples after burn-in.\n        \"\"\"\n        samples = []\n        for i in tqdm(range(n_samples), desc=\"Sampling\"):\n            self._indiv_sample()\n            if i &gt; n_burn:\n                samples.append(self._state_f.copy())  # Store sample post burn-in\n\n        return np.stack(samples)\n\ndef sigmoid(x):\n    \"\"\"Sigmoid function to map values between 0 and 1.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# Step 1: Define a New Two-Dimensional Non-linear Function\ndef nonlinear_function(x1, x2):\n    \"\"\"\n    Computes a non-linear function of x1 and x2.\n    \n    Args:\n    - x1 (np.array): First input array.\n    - x2 (np.array): Second input array.\n    \n    Returns:\n    - np.array: Computed function values.\n    \"\"\"\n    return np.log(1 + x1) + np.log(1 + x2)\n\n# Generate a 2D grid of points\nx1 = np.linspace(0, 1, 20)\nx2 = np.linspace(0, 1, 20)\nx1_grid, x2_grid = np.meshgrid(x1, x2)\nx_grid_points = np.vstack([x1_grid.ravel(), x2_grid.ravel()]).T\nf_values = nonlinear_function(x_grid_points[:, 0], x_grid_points[:, 1])\n\n# Step 2: Generate Preferences Using Bradley-Terry Model Over the Grid\ndef generate_preferences(f_vals, num_prefs=10000):\n    \"\"\"\n    Generates preferences based on the Bradley-Terry model.\n    \n    Args:\n    - f_vals (np.array): Function values at grid points.\n    - num_prefs (int): Number of preference pairs to generate.\n    \n    Returns:\n    - list of tuple: Generated preference pairs (i, j).\n    \"\"\"\n    preferences = []\n    num_points = len(f_vals)\n    for _ in range(num_prefs):\n        i, j = np.random.choice(num_points, size=2, replace=False)\n        # Probability of preference using Bradley-Terry model\n        p_ij = sigmoid(f_vals[i] - f_vals[j])\n        # Decide preference based on random draw\n        if np.random.rand() &lt; p_ij:\n            preferences.append((i, j))\n        else:\n            preferences.append((j, i))\n    return preferences\n\npreferences = generate_preferences(f_values)\n\n# Step 3: Define the Likelihood Function for Elliptical Slice Sampling\ndef loglik_from_preferences(f):\n    \"\"\"\n    Log-likelihood function using Bradley-Terry model for preferences.\n    \n    Args:\n    - f (np.array): Sampled function values.\n    \n    Returns:\n    - float: Log-likelihood value.\n    \"\"\"\n    log_lik = 0\n    for idx_i, idx_j in preferences:\n        # YOUR CODE HERE (~2 lines)\n        pass\n        # END OF YOUR CODE\n    return log_lik\n\n# Step 4: Define the RBF Kernel to Compute Prior Covariance Matrix\ndef rbf_kernel(X1, X2, length_scale=1.0, sigma_f=1.0):\n    \"\"\"\n    Computes the Radial Basis Function (RBF) kernel between two sets of points.\n    \n    Args:\n    - X1, X2 (np.array): Input data points.\n    - length_scale (float): Kernel length scale parameter.\n    - sigma_f (float): Kernel output scale.\n    \n    Returns:\n    - np.array: RBF kernel matrix.\n    \"\"\"\n    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)\n\n# Define prior covariance (prior mean is zero vector)\nsigma_prior = rbf_kernel(x_grid_points, x_grid_points, length_scale=1.0, sigma_f=1.0)\n\n# Add small jitter to diagonal for numerical stability\njitter = 1e-6\nsigma_prior += jitter * np.eye(sigma_prior.shape[0])\n\n# Ensure the matrix is symmetric to avoid numerical issues\nsigma_prior = (sigma_prior + sigma_prior.T) / 2\n\n# Step 5: Run Elliptical Slice Sampling\nsampler = EllipticalSliceSampler(sigma_prior, loglik_from_preferences)\nsamples = sampler.sample(1000, n_burn=500)\naverage_samples = np.mean(samples, axis=0)\n\n# Generate true function values on grid points\ntrue_values_on_grid = nonlinear_function(x_grid_points[:, 0], x_grid_points[:, 1])\n\ndef evaluate_elicited_metric(true_metric, elicited_metric):\n    \"\"\"\n    Evaluates and prints the mean and standard deviation of the difference\n    between true and elicited metrics.\n    \n    Args:\n    - true_metric (np.array): True values of the function.\n    - elicited_metric (np.array): Elicited (estimated) function values.\n    \"\"\"\n    # YOUR CODE HERE\n    pass\n    # END OF YOUR CODE\n\nevaluate_elicited_metric(true_values_on_grid, average_samples)\n\n# Step 6: Plot the True Non-linear Function and Elicited Metric in 3D\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot the true function\nx1_fine = np.linspace(0, 1, 50)\nx2_fine = np.linspace(0, 1, 50)\nx1_fine_grid, x2_fine_grid = np.meshgrid(x1_fine, x2_fine)\ntrue_f_values = nonlinear_function(x1_fine_grid, x2_fine_grid)\nax.plot_surface(x1_fine_grid, x2_fine_grid, true_f_values, color='blue', alpha=0.5, label='True Function')\n\n# Plot the averaged samples as a surface\nx1_avg = x_grid_points[:, 0].reshape(20, 20)\nx2_avg = x_grid_points[:, 1].reshape(20, 20)\navg_values = average_samples.reshape(20, 20)\nax.plot_surface(x1_avg, x2_avg, avg_values, color='red', alpha=0.5, label='Estimated Function')\n\n# Customize plot\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel('f(x1, x2)')\nax.set_title('True Function vs. Averaged Estimated Function')\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elicitation</span>"
    ]
  },
  {
    "objectID": "src/chap5.html",
    "href": "src/chap5.html",
    "title": "5  Decisions",
    "section": "",
    "text": "5.1 Dueling Bandit\nFullscreen Part 1 Fullscreen Part 2\nThe multi-armed bandit (MAB) problem involves a gambler deciding which lever to pull on an MAB machine to maximize the winning rate, despite not knowing which machine is the most rewarding. This scenario highlights the need to balance exploration (trying new machines to discover potential higher rewards) and exploitation (using current knowledge to maximize gains). MAB algorithms address this dilemma by making decisions under uncertainty to achieve the best possible outcomes based on gathered data. At the core of the MAB problem is a set of actions, or ‘arms,’ denoted by \\(\\mathcal{A} = \\{1, 2, \\ldots, K\\}\\), where \\(K\\) signifies the total number of arms. For each round \\(t\\), the agent selects an arm \\(a_t \\in \\mathcal{A}\\) and receives a reward \\(r_t\\), sampled from an arm-specific, unknown probability distribution. The expected reward of pulling arm \\(a\\) is represented as \\(\\mu_a = \\mathbb{E}[r_t | a]\\).\nThe multi-armed bandit framework can be extended in various ways to model more complex scenarios. In the infinite-armed bandit problem, the set of possible arms \\(\\mathcal{A}\\) is either very large or infinite. This introduces significant challenges in exploration, as the agent cannot afford to explore each arm even once. Algorithms for infinite-armed bandits typically assume some regularity or structure of the reward function across arms to make the problem tractable. The contextual bandit problem extends the bandit framework by incorporating observable external states or contexts that influence the reward distributions of arms. The agent’s task is to learn policies that map contexts to arms to maximize reward. This model is particularly powerful for personalized recommendations, where the context can include user features or historical interactions. In dueling bandit problems, the agent chooses two arms to pull simultaneously and receives feedback only on which of the two is better, not the actual reward values. This pairwise comparison model is especially useful in scenarios where absolute evaluations are difficult, but relative preferences are easier to determine, such as in ranking systems.\nContextual bandits extend the multi-armed bandits by making decisions conditional on the state of the environment and previous observations. The benefit of such a model is that observing the environment can provide additional information, potentially leading to better rewards and outcomes. In each iteration, the agent is presented with the context of the environment, then decides on an action based on the context and previous observations. Finally, the agent observes the action’s outcome and reward. Throughout this process, the agent aims to maximize the expected reward.\nIn many real-world contexts, one may not have a real-valued reward (or at least a reliable one) associated with a decision. Instead, we may only have observations indicating which of a set of bandits was optimal in a given scenario. The assumption is that within these observations of preferred choices among a set of options, there is an implicit reward or payoff encapsulated in that decision. Consider the following examples:\nGenerally, we assume access to a set of actions. A noteworthy assumption is that any observations we make are unbiased estimates of the payoff. This means that if we observe a human preferred one option over another (or several others), the preferred option had a higher implicit reward or payoff than the alternatives. In the case of dietary preferences, this may mean that a human liked the preferred option; in the case of video recommendations, a user was more entertained, satisfied, or educated by the video they selected than the other options.\nThe overarching context is that we do not have direct or reliable access to rewards. We may not have a reward at all (for some decisions, it may be impossible to define a real value to the outcome), or it may be noisy (for example, if we ask a human to rate their satisfaction on a scale of 1 to 10). We use relative comparisons to evaluate the best of multiple options in this case. Our goal is to minimize total regret in the face of noisy comparisons. Humans may not always provide consistent observations (since human decision-making is not guaranteed to be consistent). However, we can still determine an optimal strategy with the observed comparisons. We aim to minimize the frequency of sub-optimal decisions according to human preferences. In practice, many formulations of bandits can allow for infinitely many bandits (for example, in continuous-value and high-dimensional spaces). However, this situation can be intractable when determining an optimal decision strategy. With infinite options, how can we always ensure we have chosen the best? We will constrain our bandits to a discrete space to enable efficient exploration. We will assume that we have \\(k\\) bandits, \\(b_i, i \\in [1, k]\\), and our task is to choose the one that will minimize regret.\nWith the framework outlined, we now define our approach more formally. This method was introduced by (Yue et al. 2012), and proofs for the guarantees and derivations of parameters can be found in their work.\nTo determine the optimal action, we will compare pairwise to ascertain the probability that an action \\(b_i\\) is preferred over another \\(b_j\\), where \\(i \\ne j\\). Concretely, we assume access to a function \\(\\epsilon\\) that helps determine this probability; in practice, this can be done with an oracle, such as asking a human which of two options they prefer: \\[P(b_i &gt; b_j) = \\varepsilon(b_i, b_j) + \\frac{1}{2}.\\] With this model, three basic properties govern the values provided by \\(\\epsilon\\): \\[\\epsilon(b_i, b_j) = -\\epsilon(b_j, b_i), \\epsilon(b_i, b_i) = 0, \\epsilon(b_i, b_j) \\in \\left(-\\frac{1}{2}, \\frac{1}{2} \\right).\\]\nWe assume there is a total ordering of bandits, such that \\(b_i \\succ b_j\\) implies \\(\\epsilon(b_i, b_j) &gt; 0\\). We impose two constraints to properly model comparisons:\nThese assumptions may initially seem limiting; however, common models for comparisons satisfy these constraints. For example, the Bradley-Terry Model follows \\(P(b_i &gt; b_j) = \\frac{\\mu_i}{\\mu_i + \\mu_j}\\). The Gaussian model with unit variance also satisfies these constraints: \\(P(b_i &gt; b_j) = P(X_i - X_j &gt; 0)\\), where \\(X_i - X_j \\sim N(\\mu_i - \\mu_j, 2)\\).\nTo accurately model the preferences between bandits in our framework of pairwise bandit comparisons and regret, we must track certain parameters in our algorithm. First, we will maintain a running empirical estimate of the probability of bandit preferences based on our observations. It is important to note that we do not have direct access to an \\(\\epsilon\\) function. Instead, we must present two bandits to a human, who selects a winner. To do this, we define: \\[\\hat{P}_{i, j} = \\frac{\\# b_i\\ \\text{wins}}{\\# \\text{comparisons between}\\ i \\text{and}\\ j}.\\]\nWe will also compute confidence intervals at each timestep for each of the entries in \\(\\hat{P}\\) as \\[\\hat{C}_t = \\left( \\hat{P}_t - c_t, \\hat{P}_t + c_t \\right),\\] where \\(c_t = \\sqrt{\\frac{4\\log(\\frac{1}{\\delta})}{t}}\\). Note that \\(\\delta = \\frac{1}{TK^2}\\), where \\(T\\) is the time horizon and \\(K\\) is the number of bandits.\nPreviously, we discussed approaches for finding the best action in a specific context. Now, we consider changing contexts, which means there is no longer a static hidden preference matrix \\(P\\). Instead, at every time step, there is a preference matrix \\(P_C\\) depending on context \\(C\\). We consider a context \\(C\\) and a preference matrix \\(P_C\\) to be chosen by nature as a result of the given environment (Yue et al., 2012). The goal of a contextual bandits algorithm is to find a policy \\(\\pi\\) that maps contexts to a Von Neumann winner distribution over our bandits. That is, our policy \\(\\pi\\) should map any context to some distribution over our bandits such that sampling from that distribution is preferred to a random action for that context.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#dueling-bandit",
    "href": "src/chap5.html#dueling-bandit",
    "title": "5  Decisions",
    "section": "",
    "text": "Dietary preferences: When providing food recommendations to humans, it is often not possible to quantify an explicit reward from recommending a specific food item. Instead, we can offer meal options and observe which one the person selects.\nVideo recommendation: Websites like YouTube and TikTok recommend specific videos to users. It is typically not feasible to measure the reward a person gains from watching a video. However, we can infer that a user preferred one video over another. From these relative preference observations, we can develop a strategy to recommend videos they are likely to enjoy.\nExoskeleton gait optimization: Tucker et al. (2020) created a framework that uses human-evaluated preferences for an exoskeleton gait algorithm to develop an optimal strategy for the exoskeleton to assist a human in walking. A human cannot reliably produce a numerical value for how well the exoskeleton helped them walk but can reliably indicate which option performed best according to their preferences.\n\n\n\n\n\n\n\nStrong Stochastic Transitivity: We must maintain our total ordering of bandits, and as such, the comparison model also respects this ordering: \\[b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\ge \\text{max}\\{\\epsilon(b_i, b_j), \\epsilon(b_j, b_k)\\}. \\tag{5.1}\\]\nStochastic Triangle Inequality: We also impose a triangle inequality, which captures the condition that the probability of a bandit winning (or losing) a comparison will exhibit diminishing returns as it becomes increasingly superior (or inferior) to the competing bandit: \\[b_i \\succ b_j \\succ b_k \\Rightarrow \\epsilon(b_i, b_k) \\le \\epsilon(b_i, b_j) + \\epsilon(b_j, b_k). \\tag{5.2}\\]\n\n\n\n\n\n\n5.1.1 Regret\nThe agent aims to pick a sequence of arms \\((a_1, a_2, \\ldots, a_T)\\) across a succession of time steps \\(t = 1\\) to \\(t = T\\) to maximize the total accumulated reward. Formally, the strategy seeks to maximize the sum of the expected rewards: \\(\\max_{a_1, \\ldots, a_T} \\mathbb{E} \\left[\\sum_{t=1}^{T} r_t\\right]\\). Regret is defined as the difference between the cumulative reward that could have been obtained by always pulling the best arm (in hindsight, after knowing the reward distributions) and the cumulative reward actually obtained by the algorithm. Formally, if \\(\\mu^*\\) is the expected reward of the best arm and \\(\\mu_{a_t}\\) is the expected reward of the arm chosen at time \\(t\\), the regret after \\(T\\) time steps is given by \\(R(T) = T \\cdot \\mu^* - \\sum_{t=1}^{T} \\mu_{a_t}\\). The objective of a bandit algorithm is to minimize this regret over time, effectively learning to make decisions that are as close as possible to the decisions of an oracle that knows the reward distributions beforehand. Low regret indicates an algorithm that has often learned to choose well-performing arms, balancing the exploration of unknown arms with the exploitation of arms that are already known to perform well. Thus, an efficient bandit algorithm exhibits sub-linear regret growth, meaning that the average regret per round tends to zero as the number of rounds \\(T\\) goes to infinity: \\(\\lim_{T \\to \\infty} \\frac{R(T)}{T} = 0\\). Minimizing regret is a cornerstone in the design of bandit algorithms, and its analysis helps in understanding the long-term efficiency and effectiveness of different bandit strategies.\nAs previously discussed, our goal is to select the bandit that minimizes a quantity that reflects regret or the cost of not selecting the optimal bandit at all times. We can leverage our comparison model to define a quantity for regret over some time horizon \\(T\\), which is the number of decisions we make (selecting what we think is the best bandit at each iteration). Assuming we know the best bandit \\(b^*\\) (and we know that there is a best bandit, since there is a total ordering of our discrete bandits), we can define two notions of regret:\n\nStrong regret: aims to capture the fraction of users who would prefer the optimal bandit \\(b^*\\) over the worse of the options \\(b_1, b_2\\) we provide at a given step:\\(R_T = \\sum_{t = 1}^T \\text{max} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}\\)\nWeak regret: aims to capture the fraction of users who would prefer the optimal bandit \\(b^*\\) over the better of the options \\(b_1, b_2\\) we provide at a given step:\\(\\tilde{R}_T = \\sum_{t = 1}^T \\text{min} \\left\\{ \\epsilon(b^*, b_1^{(t)}), \\epsilon(b^*, b_2^{(t)}) \\right\\}\\)\n\nThe best bandit described in our regret definition is called a Condorcet Winner. This is the strongest form of winner. It’s the action \\(A_{i}\\) which is preferred to each other action \\(A_j\\) with \\(p &gt; 0.5\\) in a head-to-head election. While the above introduced notions of regret assume an overall best bandit to exist, there might be settings, where no bandit wins more than half head-to-head duels. A set of actions without a Condorcet winner is described by the following preference matrix, where each entry \\(\\Delta_{jk}\\) is \\(p(j \\succ k) - 0.5\\), the probability that action \\(j\\) is preferred over action \\(k\\) minus 0.5. There is no Condorcet winner as there is no action that is preferred with \\(p &gt; 0.5\\) over all other actions. Imagine, you want to find the best pizza to eat (=action). There may not be a pizza that wins more than half of the head-to-head duels against every other pizza.\nHowever, we might still have an intuition of the best pizza. Therefore Sui et al., 2018 introduce the concepts of different \\(\\textit{winners}\\) in dueling bandit problems (Sui et al. 2018). In this example, we might define the best pizza as the most popular one. We call the Pizza receiving the most votes in a public vote the Borda Winner, or formally, Borda winner \\(j = \\arg\\max_{i \\in A, i \\neq j} \\left(\\sum p(j \\succ i)\\right)\\). In contrast to the Condorcet Winner setting, there is always guaranteed to be one or more (in the case of a tie) Borda winners for a set of actions. However - if there is a Condorcet Winner, this might not necessarily be the same as a Borda Winner: In our Pizza example, a Pepperoni Pizza might win more than half of its head-to-head duels, while the Cheese-Pizza is still the most popular in a public poll.\nA more generic concept of winner is the Von Neumann Winner, which describes a probability distribution rather than a single bandit winner. A Von Neumann winner simply prescribes a probability distribution \\(W\\) such that sampling from this distribution ‘beats’ an action from the random uniform distribution with \\(p &gt; 0.5\\). In our pizza example, this would correspond to trusting a friend to order whichever Pizza he likes, because this may still be preferred to ordering randomly. Formally, \\(W\\) is a Von Neumann if \\((j \\sim W, k \\sim R) [p(p(j \\succ k) &gt; 0.5) &gt; 0.5]\\) where \\(R\\) describes the uniform probability distribution over our actions. The concept of a Von Neumann winner is useful in contextual bandits, which will be introduced later. In these settings, the preference matrix depends on different context, which may have different Borda winners, just as different parties may vote for different pizzas.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n0\n0.03\n-0.02\n0.06\n0.10\n0.11\n\n\nB\n-0.03\n0\n0.03\n0.05\n0.08\n0.11\n\n\nC\n\n-0.03\n0\n0.04\n0.07\n0.09\n\n\nD\n-0.06\n-0.05\n-0.04\n0\n0.05\n0.07\n\n\nE\n-0.10\n-0.08\n-0.07\n-0.05\n0\n0.03\n\n\nF\n-0.11\n-0.11\n-0.09\n-0.07\n-0.03\n0\n\n\n\n\n\nFigure 5.1: Violation of Condorcet Winner. Highlighted entries are different from Table 1. No Condorcet winner exists as no arm could beat every other arm.\n\n\n\nNext, we introduce two performance measures for the planner. The asymptotic ex-post regret is defined as \\[\\text{Regret}(\\mu_1, \\ldots \\mu_K) = T\\cdot \\max_i \\mu_i - \\sum_{i=1}^T E[\\mu_{I_t}].\\]\nIntuitively, this represents the difference between the reward achieved by always taking the action with the highest possible reward and the expected welfare of the recommendation algorithm (based on the actions it recommends at each timestep).\nWe also define a weaker performance measure, the Bayesian regret, which is defined as \\[\\text {Bayesian regret}=E_{\\mu_1, \\ldots, \\mu_K \\sim \\text {Prior}}\\left[\\operatorname{Regret}\\left(\\mu_1, \\ldots, \\mu_K\\right)\\right]\\]\nWith a Bayesian optimal policy, we would like either definition of regret to vanish as \\(T\\to \\infty\\); we are considering “large-market optimal\" settings where there are many short-lived, rather than a few long-term, users. Note the fact that ex-post regret is prior-free makes it robust to inaccuracies on the prior.\n\n\n5.1.2 Acquisition Functions\nVarious strategies have been developed to balance the exploration-exploitation trade-off. These strategies differ in selecting arms based on past experiences and rewards.\n\n5.1.2.1 Classical Acquisition Functions\nUniform acquisition function is the most straightforward approach where each arm is selected uniformly randomly over time. This strategy does not consider the past rewards and treats each arm equally promising regardless of the observed outcomes. It is a purely explorative strategy that ensures each arm is sampled enough to estimate its expected reward, but it does not exploit the information to optimize rewards. In mathematical terms, if \\(N_t(a)\\) denotes the number of times arm \\(a\\) has been selected up to time \\(t\\), the Uniform Strategy would ensure that \\(N_t(a) \\approx \\frac{t}{K}\\) for all arms \\(a\\) as \\(t\\) grows large: \\(P(a_t = a) = \\frac{1}{K}\\)\nThe Epsilon Greedy is a popular method that introduces a balance between exploration and exploitation. With a small probability \\(\\epsilon\\), it explores by choosing an arm at random, and with a probability \\(1 - \\epsilon\\), it exploits by selecting the arm with the highest estimated reward so far. This strategy incrementally favors actions that have historically yielded higher rewards, but still allows for occasional exploration to discover better options potentially. The parameter \\(\\epsilon\\) is chosen based on the desired exploration level, often set between 0.01 and 0.1. \\[P(a_t = a) =\n\\begin{cases}\n\\frac{\\epsilon}{K} + 1 - \\epsilon & \\text{if } a = \\arg\\max_{a'} \\hat{\\mu}_{a'} \\\\\n\\frac{\\epsilon}{K} & \\text{otherwise}\n\\end{cases}\\]\nUpper Confidence Bound (UCB) acquisition function takes a more sophisticated approach to the exploration-exploitation dilemma. It selects arms based on both the estimated rewards and the uncertainty or variance associated with those estimates. Specifically, it favors arms with high upper confidence bounds on the estimated rewards, which is a sum of the estimated mean and a confidence interval that decreases with the number of times the arm has been played. This ensures that arms with less certainty (those played less often) are considered more often, naturally balancing exploration with exploitation as the uncertainty is reduced over time.\n\\[P(a_t = a) =\n\\begin{cases}\n1 & \\text{if } a = \\arg\\max_{a'} \\left( \\hat{\\mu}_{a'} + \\sqrt{\\frac{2 \\ln t}{N_t(a')}} \\right) \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n\n\n5.1.2.2 Interleaved Filter\nThis algorithm tries to find the best bandit (Condorcet Winner) in a discrete, limited bandit-space via pairwise comparisons of the bandits. We will now introduce the algorithm for the Interleaved Filter as provided in (Yue et al. 2012) to solve a dueling bandit setup. It starts with a randomly defined best bandit \\(\\hat{b}\\) and iteratively compares it to set \\(W\\) containing the remaining bandits \\(b\\) resulting in winning probabilities \\(\\hat{P}_{\\hat{b},b}\\) and confidence interval \\(\\hat{C}_{\\hat{b},b}\\). If a bandit \\(b\\) is confidently worse than \\(\\hat{b}\\), it is removed from \\(W\\). If a bandit \\(b'\\) is confidently better than \\(\\hat{b}\\), it is set as new best bandit \\(\\hat{b}\\) and bandit \\(\\hat{b}\\) as well as every other bandit \\(b\\) worse than \\(\\hat{b}\\) are removed from \\(W\\). This is done, until \\(W\\) is empty, leaving the final \\(\\hat{b}\\) as the predicted best bandit.\n\n\ninput: \\(T\\), \\(B=\\{b_1, \\dots, b_k\\}\\) \\(\\delta \\gets 1/(TK^2)\\) Choose \\(\\hat{b} \\in B\\) randomly \\(W \\gets \\{b_1, \\dots, b_k\\} \\backslash \\{\\hat{b}\\}\\) \\(\\forall b \\in W\\), maintain estimate \\(\\hat{P}_{\\hat{b},b}\\) of \\(P(\\hat{b} &gt; b)\\) according to (6) \\(\\forall b \\in W\\), maintain \\(1 - \\delta\\) confidence interval \\(\\hat{C}_{\\hat{b},b}\\) of \\(\\hat{P}_{\\hat{b},b}\\) according to (7), (8) compare \\(\\hat{b}\\) and \\(b\\) update \\(\\hat{P}_{\\hat{b},b}\\), \\(\\hat{C}_{\\hat{b},b}\\) \\(W \\gets W \\backslash \\{b\\}\\)\n\\(W \\gets W \\backslash \\{b\\}\\) \\(\\hat{b} \\gets b'\\), \\(W \\gets W \\backslash \\{b'\\}\\) \\(\\forall b \\in W\\), reset \\(\\hat{P}_{\\hat{b},b}\\) and \\(\\hat{C}_{\\hat{b},b}\\) \\(\\hat{T} \\gets\\) Total Comparisons Made \\((\\hat{b}, \\hat{T})\\)\n\n\n\nParameter Initialization\n\nIn lines 1-6 of the algorithm, we take the inputs and first compute the value \\(\\delta\\) which is used to compute our confidence intervals. We select an initial guess of an optimal bandit \\(\\hat{b}\\) by uniformly sampling from all bandits \\(\\mathcal{B}\\). We also keep a running set of bandit candidates \\(W\\), which is initialized to be \\(\\mathcal{B} \\setminus \\{\\hat{b}\\}\\). At this point, we also initialize our empirical estimates for \\(\\hat{P}, \\hat{C}\\).\nNext, we will repeat several steps until our working set of bandit candidates \\(W\\) is empty.\n\nUpdate Estimates Based on Comparisons\n\nThe first step at each iteration (lines 8-11) is to look at all candidates in \\(W\\), and compare them to our current guess \\(\\hat{b}\\) using an oracle (e.g. by asking a human which of \\(\\hat{b}\\) or \\(b \\in W\\) is preferred). With this new set of wins and comparisons, we update our estimates of \\(\\hat{P}, \\hat{C}\\).\n\nPrune Suboptimal Bandits\n\nIn lines 12-13, with updated comparison win probabilities and corresponding confidence intervals, we can remove bandit candidates from \\(W\\) that we are confident \\(\\hat{b}\\) is better than. The intuition here is that we are mostly sure that our current best guess is better than some of the candidates, and we don’t need to consider those candidates in future iterations.\n\nCheck for Better Bandits from Candidate Set\n\nNow that our candidate set of bandits may be smaller, in lines 15-21 we check if there are any bandits \\(b'\\) that we are confident are better than our current best guess. If we do find such a candidate, we remove bandits which \\(\\hat{P}\\) indicates \\(b\\) is likely worse than \\(\\hat{b}\\). Note that in this step, we do not require the probability to be outside the confidence interval, since we already found one we believe to be significantly closer to optimal than our current best guess.\nOnce we remove the candidates likely worse than \\(\\hat{b}\\), we crown \\(b'\\) as the new best guess, e.g. \\(\\hat{b} := b'\\). Consequently, we remove \\(b'\\) from \\(W\\) and reset our empirical win counters \\(\\hat{P}, \\hat{C}\\).\n\n\nWith this algorithm defined, let us look at some provisions of the method with respect to identifying the optimal strategy. Note that the proofs and derivations for these quantities are provided in (Yue et al. 2012).\nFirst, the method guarantees that for the provided time horizon \\(T\\), the algorithm returns the correct bandit with probability \\(P \\ge 1 - \\frac{1}{T}\\). It is interesting and useful to note that if one has a strict requirement for the probability of identifying the correct bandit, one can compute the time horizon \\(T\\) that guarantees this outcome at that probability. Furthermore, a time horizon of 1 leaves no probabilistic guarantee of a successful outcome, and increasing \\(T\\) has diminishing returns. Second, in the event that the algorithm returns an incorrect bandit, the maximal regret incurred is linear with respect to \\(T\\), e.g. \\(\\mathcal(O)(T)\\). This is also a useful provision as it allows us to estimate the overall cost in the worst case outcome. Based on these two provisions, we can compute the expected cumulative regret from running the Interleaved Filter algorithm, which is: \\[\\mathbb{E}\\left[R_T\\right] \\le \\left(1 - \\frac{1}{T}\\right) \\mathbb{E}\\left[ R_T^{IF} \\right] + \\frac{1}{T}\\mathcal{O}(T) \\\\\n= \\mathcal{O}\\left(\\mathbb{E}\\left[ R_T^{IF} \\right] + 1\\right)\\]\nInterestingly, the original work shows that these bounds hold for both strong and weak regret. As demonstrated, the Interleaved Filter algorithm [fig-if] provides a robust method to ascertain the optimal bandit or strategy given a set of options and only noisy comparisons. In most real-world scenarios for modeling human preferences, it is not possible to observe a real-world reward value, or at least a reliable one and as such this method is a useful way to properly model human preferences.\nFurthermore, the algorithm provides strong guarantees for the probability of selecting the correct bandit, maximal regret, and the number of comparisons required. It is even more impressive that the method can do so without severely limiting constraints; as demonstrated, the most commonly used models satisfy the imposed constraints.\nAs we look to model human preferences, we can certainly leverage this method for k-armed dueling bandits to identify the best strategy to solve human-centric challenges, from video recommendation to meal selection and exoskeleton-assisted walking.\n\n\n5.1.2.3 Dueling Bandit Gradient Descent\nThis algorithm tries to find the best bandit in a continuous bandit-space. Here, the set of all bandits is regarded as an Information-Retrieval (IR) system with infinite bandits uniquely defined by \\(w\\). We will cover the Dueling Bandit Gradient Descent algorithm from Yue and Joachims 2009 (Yue and Joachims 2009). Yue and Joachims use the dueling bandits formulation for online IR optimization. They propose a retrieval system parameterized by a set of continuous variables lying in \\(W\\), a \\(d\\)-dimensional unit-sphere. The DBGD algorithm adapts the current parameters \\(w_t\\) of IR system by comparison with slightly altered parameters \\(w_t'\\) both querying query \\(q_t\\). Only if the IR outcome using \\(w_t'\\) is preferred, the parameters are changed in their direction. We will now discuss the algorithm more detailed.\n\n\ninput: \\(\\gamma\\), \\(\\delta\\), \\(w_1\\)\nSample unit vector \\(u_t\\) uniformly\n\\(w_t' \\gets P_W(w_t + \\delta u_t)\\)\nCompare \\(w_t\\) and \\(w_t'\\)\n\\(w_{t+1} \\gets P_W(w_t + \\gamma u_t)\\)\n\\(w_{t+1} \\gets w_t\\)\n\n\nWe first choose exploration step length \\(\\delta\\), exploitation step length \\(\\gamma\\), and starting point (in unit-sphere) \\(w_1\\). Choose a query and sample a random unit vector \\(u_t\\). We duel \\(w_t\\) and \\(w_t'\\), where \\(w_t\\) is our current point in the sphere, and \\(w_t'\\) is our exploratory comparison, which is generated by taking a random step of length \\(\\delta\\), such that \\(w_t' = w_t + \\delta u_t\\). The objective of this duel is to ascertain the binary preference of users with respect to the results yielded by the IR systems parameterized by \\(w_t\\) and \\(w_t'\\) respectively, taking query \\(q_t\\) as an input. The parameters that get the majority of the votes in the head to head win. If \\(w_t\\) wins, then we keep the parameters for the next iteration. If \\(w_t'\\) wins the duel, we update our parameters in the direction of \\(u_t\\) by taking a step of length \\(\\gamma\\). Note that the algorithm describes projection operation \\(P_W(\\overrightarrow{v})\\). Since \\(u_t\\) is chosen randomly, \\(w_t + \\delta u_t\\) or \\(w_t + \\gamma u_t\\) could exist outside of the unit sphere where all possible parameter configurations lie. In this case, we simply project the point back onto the sphere using said projection \\(P_W(\\overrightarrow{v})\\).\nYue and Joachims show that this algorithm has sublinear regret in \\(T\\), the number of iterations. We note that the algorithm assumes that there exists a hidden reward function \\(R(w)\\) that maps system parameters \\(w_t\\) to a reward value which is smooth and strictly concave over the input space \\(W\\).\nLastly, we would also like to give motivation behind \\(\\delta\\) and \\(\\gamma\\) being different values. We need a \\(\\delta\\) that is sufficiently large that the comparison between a system parameterized by \\(w_t\\) and \\(w_t'\\) is meaningful. On the other hand, we may wish to take a smaller step in the direction of \\(w_t'\\) during our update step, as during a duel, we only score \\(w_t\\) against \\(w_t'\\) over the results on one query \\(q_t\\). Having \\(\\delta &gt; \\gamma\\) allows us to get reward signal from meaningfully different points while also updating our belief of the best point \\(w_{\\text{best}}\\) gradually.\n\n\nSparring EXP4\nZoghi et al. 2015 propose one algorithm for this problem — sparring EXP4, which duels two traditional EXP4 - algorithms. The (traditional) EXP4 algorithm solves the traditional contextual bandits — the case where we can directly observe a reward for a choice of bandit given a context. The EXP4 algorithm embeds each bandit as a vector. When the algorithm sees the context (called ‘advice’ in this formulation), it produces a probability distribution over the choices based on an adjusted softmax function on the inner product between the context and the bandit vectors. The probability function is different from a softmax as we assign some minimum probability that any action gets chosen to enforce exploration. A reward is then observed for the choice and propagated back through the embedding of the chosen bandit.\nSparring EXP4 runs two instances of the EXP4 algorithm against each other. Each EXP4 instance samples an action given a context, and then these choices are ‘dueled’ against each other. Instead of directly observing a reward, as for traditional EXP4, we instead observe two converse reward — a positive reward for the choice that won the duel and a negative reward to the choice that lost. The reward is proportional to the degree to which the bandit wins the duel, i.e. how likely the bandit is to be preferred over the other when users are queried for binary preferences. Like in traditional EXP4, the reward or negative reward is then propagated back through the representations of the bandits.\n\n\n5.1.2.4 Feel-good Thompson sampling\nThis algorithm is a solution for the contextual dueling bandit setting, and tries to minimize cumulative average regret (= find WHAT WINNER?!Von Neumann???): \\[\\text{Regret}(T) := \\sum_{t=1}^{T} \\left[ r_{*}(x_t, a_{t}^{*}) - \\frac{r_{*}(x_t, a_{t}^{1}) + r_{*}(x_t, a_{t}^{2})}{2} \\right],\\] where \\(r_{*}(x_t, a_{t})\\) is the true, hidden reward function of a context \\(x_t\\) and action \\(a_t\\). Thompson sampling is an iterative process of receiving preference over two actions, each maximizing a different approximation of the reward function based on past data and adding this new information to the data.\nFinding good approximations of the reward function at time \\(t\\) is done by sampling two reward function parameters \\(\\theta_t^{j=1}\\) and \\(\\theta_t^{j=2}\\) from a posterior distribution based on all previous data \\(p_j(\\cdot \\mid S_{t-1})\\). This posterior distribution is proportional to the multiplication of the prior and the likelihood function, which is a Gaussian in standard Thompson sampling. In Feel-Good Thompson sampling, an additional term called \"Feel-good exploration\" encourages parameters \\(\\theta\\) with a large maximum reward in previous rounds. This change to the likelihood function may increase probabilities in uncertain areas, thus exploring those regions. All that’s left is to select an action maximizing each reward function approximation and receive a preference \\(y_t\\) on one of them to add the new information to the dataset(Zhang 2021).\n\n\nInitialize \\(S_0 = \\varnothing\\). Receive prompt \\(x_t\\) and action space \\(\\mathcal{A}_t\\). Sample model parameter \\(\\theta_t^j\\) from the posterior distribution \\(p^j(\\cdot \\mid S_{t-1})\\) Select response \\(a_t^j = \\arg\\max_{a \\in \\mathcal{A}_t} \\langle \\theta_t^j, \\phi(x_t, a) \\rangle\\). Receive preference \\(y_t\\). Update dataset \\(S_t \\leftarrow S_{t-1} \\cup \\{(x_t, a_t^1, a_t^2, y_t)\\}\\).\n\n\n\n\n\n5.1.3 Applications\nThere are many applications where contextual bandits are used. Many of these applications can utilize human preferences. One particular application illustrates the benefits a contextual bandit would have over a multi-armed bandit: a website deciding which app to show someone visiting the website. A multi-armed bandit might decide to show someone an ad for a swimsuit because the swimsuit ads have gotten the most user clicks (which indicates human preference). A contextual bandit might choose differently, however. A contextual bandit will also take into account the context, which in this case might mean information about the user (location, previously visited pages, and device information). If it discovers the user lives in a cold environment, for example, it might suggest a sweater ad for the user instead and get a better chance of a click. There are many more examples of where contextual bandits can be applied. They can be applied in other web applications, such as to optimize search results, medical applications, such as how much of a medication to prescribe based on a patient’s history, and gaming applications, such as basing moves off of the state of a chess board to try to win. In each of the above examples, human feedback could have been introduced during training and leveraged to learn a reward function.\nWe explored different versions of bandits that address the exploration-exploitation trade-off in various real-world scenarios. These models have been employed across various fields, including but not limited to healthcare, finance, dynamic pricing, and anomaly detection. This section provides a deep dive into some real-world applications, emphasizing the value and advancements achieved by incorporating bandit methodologies. The content of this section draws upon the findings from the survey cited in reference (Bouneffouf, Rish, and Aggarwal 2020).\nIn healthcare, researchers have been applying bandits to address challenges in clinical trials and behavioral modeling (Bouneffouf, Rish, and Cecchi 2017; Bastani and Bayati 2020). One of the examples is drug dosing. Warfarin, an oral anticoagulant, has traditionally been administered using fixed dosing protocols. Physicians would then make subsequent adjustments based on the patient’s emerging symptoms. Nonetheless, inaccuracies in the initial dosage—whether too low or too high—can lead to serious complications like strokes and internal bleeding. In a pivotal study, researchers in (Bastani and Bayati 2020) modeled the Warfarin initial dosing as a contextual bandit problem to assign dosages to individual patients appropriately based on their medication history. Their contributions include the adaptation of the LASSO estimator to the bandit setting, achieving a theoretical regret bound of \\(O({s_0}^2 \\log^2(dT)\\), where \\(d\\) represents the number of covariates, \\(s_0 &lt;&lt; d\\) signifies the number of pertinent covariates, and \\(T\\) indicates the total number of users. Additionally, they conducted empirical experiments to validate the robustness of their methodology.\nWithin the finance sector, bandits have been instrumental in reshaping the landscape of portfolio optimization. Portfolio optimization is an approach to designing a portfolio based on the investor’s return and risk criteria, which fits the exploration-exploitation nature of the bandit problems. (Shen et al. 2015) utilized multi-armed bandits to exploit correlations between the instruments. They constructed orthogonal portfolios and integrated them with the UCB policy to achieve a cumulative regret bound of \\(\\frac{8n}{\\Delta*} \\ln(m) + 5n\\), where \\(n\\), \\(m\\), and \\(\\Delta*\\) denotes the number of available assets, total time steps, and the gap between the best-expected reward and the expected reward. On the other hand, (Huo and Fu 2017) focused on risk-awareness online portfolio optimization by incorporating a compute of the minimum spanning tree in the bipartite graph, which encodes a combination of financial institutions and assets that helps diversify and reduce exposure to systematic risk during the financial crisis.\nDynamic pricing, also known as demand-based pricing, refers to the strategy of setting flexible prices for products or services based on current market demands. The application of bandits in dynamic pricing offers a systematic approach to making real-time pricing decisions while balancing the trade-off between exploring new price points and exploiting known optimal prices. (Misra, Schwartz, and Abernethy 2019) proposed a policy where the company has only incomplete demand information. They derived an algorithm that balances immediate and future profits by combining multi-armed bandits with partial identification of consumer demand from economic theory.\nare essential components of numerous online platforms, guiding users through vast content landscapes to deliver tailored suggestions. These systems are instrumental in platforms like e-commerce sites, streaming platforms, and social media networks. However, the challenge of effectively recommending items to users is non-trivial, given the dynamic nature of user preferences and the vast amount of content available.\nOne of the most significant challenges in recommendation systems is the \"cold start\" problem. This issue arises when a new user joins a platform, and the system has limited or no information about the user’s preferences. Traditional recommendation algorithms struggle in such scenarios since they rely on historical user-item interactions. As discussed in (Zhou et al. 2017), the bandit setting is particularly suitable for large-scale recommender systems with a vast number of items. By continuously exploring user preferences and exploiting known interactions, bandit-based recommender systems can quickly adapt to new users, ensuring relevant recommendations in a few interactions. The continuous exploration inherent in bandit approaches also means that as a user’s preferences evolve, the system can adapt, ensuring that recommendations remain relevant. Recommending content that is up to date is also another important aspect of a recommendation system. In (Bouneffouf, Bouzeghoub, and Gançarski 2012), the concept of \"freshness\" in content is explored through the lens of the bandit problem. The Freshness-Aware Thompson Sampling algorithm introduced in this study aims to manage the recommendation of fresh documents according to the user’s risk of the situation.\nDialogue systems, often termed conversational agents or chatbots, aim to simulate human-like conversations with users. These systems are deployed across various platforms, including customer support, virtual assistants, and entertainment applications, and they are crucial for enhancing user experience and engagement. Response selection is fundamental to creating a natural and coherent dialogue flow. Traditional dialogue systems rely on a predefined set of responses or rules, which can make interactions feel scripted and inauthentic. In (Liu et al. 2018), the authors proposed a contextual multi-armed bandit model for online learning of response selection. Specifically, they utilized bidirectional LSTM to produce the distributed representations of a dialogue context and responses and customized the Thompson sampling method.\nTo create a more engaging and dynamic interaction, there’s a growing interest in developing pro-active dialogue systems that can initiate conversations without user initiation. (perez and Silander 2018) proposed a novel approach to this challenge with contextual bandits. By introducing memory models into the bandit framework, the system can recall past interactions, making its proactive responses more contextually relevant. Their contributions include the Contextual Attentive Memory Network, which implements a differentiable attention mechanism over past interactions.\n(Upadhyay et al. 2019) addressed the challenge of orchestrating multiple independently trained dialogue agents or skills in a unified system. They attempted online posterior dialogue orchestration, defining it as selecting the most suitable subset of skills in response to a user’s input, which studying a context-attentive bandit model that operates under a skill execution budget, ensuring efficient and accurate response selection.\nAnomaly detection refers to the task of identifying samples that behave differently from the majority. In (Ding, Li, and Liu 2019), the authors delve into anomaly detection in an interactive setting, allowing the system to actively engage with human experts through a limited number of queries about genuine anomalies. The goal is to present as many true anomalies to the human expert as possible after a fixed query budget is used up. They applied the multi-armed contextual bandit framework to address this issue. This algorithm adeptly integrates both nodal attributes and node dependencies into a unified model, efficiently managing the exploration-exploitation trade-off during anomaly queries.\nThere are many challenges associated with contextual bandits. The first challenge is that each action only reveals the reward for that particular action. Therefore, the algorithm has to work with incomplete information. This leads to the dilemma of exploitation versus exploration: when should the algorithm choose the best-known option versus trying new options for potentially better outcomes? Another significant challenge for contextual bandits is using context effectively. The context the environment gives needs to be explored to figure out which action is best for each context.\nThe overarching goal in systems designed for recommending options of high value to users is to achieve an optimal balance between exploration and exploitation. This dual approach is crucial in environments where user preferences and needs are dynamic and diverse. Exploration refers to the process of seeking out new options, learning about untried possibilities, and gathering fresh information that could lead to high-value recommendations. In contrast, exploitation involves utilizing existing knowledge and past experiences to recommend the best options currently known. This balance is key to maintaining a system that continuously adapts to changing user preferences while ensuring the reliability of its recommendations.\nA key observation in such systems is the dual role of users as both producers and consumers of information. Each user’s experience contributes valuable data that informs future recommendations for others. For instance, platforms like Waze, Netflix, and Trip Advisor rely heavily on user input and feedback. Waze uses real-time traffic data from drivers to recommend optimal routes; Netflix suggests movies and shows based on viewing histories and ratings; Trip Advisor relies on traveler reviews to guide future tourists. In these examples, the balance between gathering new information (exploration) and recommending the best-known options (exploitation) is dynamically managed to enhance user experience and satisfaction. This approach underscores the importance of user engagement in systems where monetary incentives are not (or can not be) the primary driver.\nRecommendation systems often face the challenge of overcoming user biases that can lead to a narrow exploration of options. Users come with preconceived notions and preferences, which can cause them to overlook potentially valuable options that initially appear inferior or unaligned with their interests. This predisposition can significantly limit the effectiveness of recommendation systems, as users might miss out on high-value choices simply due to their existing biases.\nTo counteract this, it is crucial for recommendation systems to actively incentivize exploration among users. One innovative approach to achieve this is through the strategic use of information asymmetry. By controlling and selectively presenting information, these systems can guide users to explore options they might not typically consider. This method aims to reveal the true potential of various options by nudging users out of their comfort zones and encouraging a broader exploration of available choices. An important note here is that the system is not lying to users - it only selectively reveals information it has.\nThe concept of incentivizing exploration becomes even more complex when considering different types of users. For instance, systems often encounter short-lived users who have little to gain from contributing to the system’s learning process, as their interactions are infrequent or based on immediate needs. Similarly, some users may operate under a ‘greedy’ principle, primarily seeking immediate gratification rather than contributing to the long-term accuracy and effectiveness of the system. In such scenarios, managing information asymmetry can be a powerful tool. By selectively revealing information, recommendation systems can create a sense of novelty and interest, prompting even the most transient or self-interested users to engage in exploration, thereby enhancing the system’s overall knowledge base and recommendation quality.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#preferential-bayesian-optimization",
    "href": "src/chap5.html#preferential-bayesian-optimization",
    "title": "5  Decisions",
    "section": "5.2 Preferential Bayesian Optimization",
    "text": "5.2 Preferential Bayesian Optimization\nThe traditional Bayesian optimization (BO) problem is described as follows. There is a black-box objective function \\(g: \\mathcal{X} \\rightarrow \\Re\\) defined on a bounded subset \\(\\mathcal{X} \\subseteq \\Re^q\\) such that direct queries to the function are expensive or not possible. However, we would like to solve the global optimization problem of finding \\(\\mathbf{x}_{\\min }=\\arg \\min _{\\mathbf{x} \\in \\mathcal{X}} g(\\mathbf{x})\\). This is highly analogous to modeling human preferences, since it is the case that direct access to a human’s latent preference function is not possible but we would still like to find its optimum, such as in A/B tests or recommender systems.\nWe approach this problem for human preferences with Preferential Bayesian Optimization (PBO), as the key difference is that we are able to query the preference function through pairwise comparisons of data points, i.e. duels. This is a form of indirect observation of the objective function, which models real-world scenarios closely: we commonly need to to optimize a function via data about preferences. With humans, it has been demonstrated that we are better at evaluating differences rather than absolute magnitudes (Kahneman and Tversky 1979) and therefore PBO models can be applied in various contexts.\n\n5.2.1 Problem statement\nThe problem of finding the optimum of a latent preference function defined on \\(\\mathcal{X}\\) can be reduced to determining a sequence of duels on \\(\\mathcal{X} \\times \\mathcal{X}\\). From each duel \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\in\\) \\(\\mathcal{X} \\times \\mathcal{X}\\) we obtain binary feedback \\(\\{0,1\\}\\) indicating whether or not \\(\\mathbf{x}\\) is preferred over \\(\\mathbf{x}^{\\prime}\\) (\\(g(\\mathbf{x}) &lt; g(\\mathbf{x}^{\\prime})\\)). We consider that \\(\\mathbf{x}\\) is the winner of the duel if the output is \\(\\{1\\}\\) and that \\(\\mathbf{x}^{\\prime}\\) wins the duel if the output is \\(\\{0\\}\\). The aim is to find \\(\\mathbf{x}_{\\min }\\) by reducing as much as possible the number of queried duels.\nThe key idea in PBO is to learn a preference function in the space of duels using a Gaussian process. We define a joint reward \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\) on each duel which is never directly observed. Instead, the feedback we obtain after each pair is a binary output \\(y \\in\\) \\(\\{0,1\\}\\) indicating which of the two inputs is preferred. One definition of f we will use (though others are possible) is \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})\\). The more \\(\\mathbf{x}^{\\prime}\\) is preferred over \\(\\mathbf{x}\\), the bigger the reward.\nWe define the model of preference using a Bernoulli likelihood, where \\(p\\left(y=1 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\) and \\(p\\left(y=0 \\mid\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)\\) for some inverse link function \\(\\pi: \\Re \\times \\Re \\rightarrow[0,1]\\). \\(\\pi_f\\) has the property that \\(\\pi_f\\left(\\left[\\mathbf{x}^{\\prime}, \\mathbf{x}\\right]\\right)=1-\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\). A natural choice for \\(\\pi_f\\) is the logistic function \\[\\label{eq:bernoulli_pref}\n\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{1}{1+e^{-f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)}},\\] but others are possible. Therefore we have that for any duel \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\) in which \\(g(\\mathbf{x}) \\leq g\\left(\\mathbf{x}^{\\prime}\\right)\\) it holds that \\(\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5\\). \\(\\pi_f\\) is a preference function that maps each query \\(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\) to the probability of having a preference on the left input \\(\\mathbf{x}\\) over the right input \\(\\mathbf{x}^{\\prime}\\).\nWhen we marginalize over the right input \\(\\mathbf{x}^{\\prime}\\) of \\(f\\) (is this correct?), the global minimum of \\(f\\) in \\(\\mathcal{X}\\) coincides with \\(\\mathbf{x}_{\\min }\\). We also introduce the definition of the Copeland score function for a point \\(\\mathbf{x}\\) as \\[S(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\mathbb{I}_{\\left\\{\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) \\geq 0.5\\right\\}} d \\mathbf{x}^{\\prime}\\] where \\(\\operatorname{Vol}(\\mathcal{X})=\\int_{\\mathcal{X}} d \\mathbf{x}^{\\prime}\\) is a normalizing constant that bounds \\(S(\\mathbf{x})\\) in the interval \\([0,1]\\). If \\(\\mathcal{X}\\) is a finite set, the Copeland score is simply the proportion of duels that a certain element \\(\\mathbf{x}\\) will win with probability larger than 0.5. A soft variant we will use instead of the Copeland score is the soft-Copeland score, defined as \\[\\label{eq:soft-copeland}\nC(\\mathbf{x})=\\operatorname{Vol}(\\mathcal{X})^{-1} \\int_{\\mathcal{X}} \\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d \\mathbf{x}^{\\prime}\\] where the probability function \\(\\pi_f\\) is integrated over \\(\\mathcal{X}\\). This score aims to capture the average probability of \\(\\mathbf{x}\\) being the winner of a duel.\nWe define the Condorcet winner \\(\\mathbf{x}_c\\) as the point with maximal soft-Copeland score. Note that this corresponds to the global minimum of \\(f\\), since the defining integral takes maximum value for points \\(\\mathbf{x} \\in \\mathcal{X}\\) where \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\) \\(g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})&gt;0\\) or all \\(\\mathbf{x}^{\\prime}\\), occurring only if \\(\\mathbf{x}_c\\) is a minimum of \\(f\\). Therefore, if the preference function \\(\\pi_f\\) can be learned by observing the results of duels then our optimization problem of finding the minimum of \\(f\\) can be solved by finding the Condorcet winner of the Copeland score.\n\n\n5.2.2 Acquisition Functions\nWe describe several acquisition functions for sequential learning of the Condorcet winner. Our dataset \\(\\mathcal{D}=\\left\\{\\left[\\mathbf{x}_i, \\mathbf{x}_i^{\\prime}\\right], y_i\\right\\}_{i=1}^N\\) represents the \\(N\\) duels that have been performed so far. We aim to define a sequential policy \\(\\alpha\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] ; \\mathcal{D}_j, \\theta\\right)\\) for querying duels, where \\(\\theta\\) is a vector of model hyper-parameters, in order to find the minimum of the latent function \\(g\\) as quickly as possible. Using Gaussian processes (GP) for classification with our dataset \\(\\mathcal{D}\\) allows us to perform inference over \\(f\\) and \\(\\pi_f\\).\n\nPure Exploration\nThe output variable \\(y_{\\star}\\) of a prediction follows a Bernoulli distribution with probability given by the preference function \\(\\pi_f\\). To carry out exploration as a policy, one method is to search for the duel where GP is most uncertain about the probability of the outcome (has the highest variance of \\(\\sigma\\left(f_{\\star}\\right)\\) ), which is the result of transforming out epistemic uncertainty about \\(f\\), modeled by a GP, through the logistic function. The first order moment of this distribution coincides with the expectation of \\(y_{\\star}\\) but its variance is \\[\\begin{aligned}\n\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\right] & =\\int\\left(\\sigma\\left(f_{\\star}\\right)-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star} \\\\\n& =\\int \\sigma\\left(f_{\\star}\\right)^2 p\\left(f_{\\star} \\mid \\mathcal{D},\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right) d f_{\\star}-\\mathbb{E}\\left[\\sigma\\left(f_{\\star}\\right)\\right]^2\n\\end{aligned}\\] which explicitly takes into account the uncertainty over \\(f\\). Hence, pure exploration of duels space can be carried out by maximizing \\[\\alpha_{\\mathrm{PE}}\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right] \\mid \\mathcal{D}_j\\right)=\\mathbb{V}\\left[\\sigma\\left(f_{\\star}\\right)\\left|\\left[\\mathbf{x}_{\\star}, \\mathbf{x}_{\\star}^{\\prime}\\right]\\right| \\mathcal{D}_j\\right] .\\]\nNote that in this case, duels that have been already visited will have a lower chance of being visited again even in cases in which the objective takes similar values in both players. In practice, this acquisition functions requires computation of an intractable integral, that we approximate using Monte-Carlo.\n\n\nPrincipled Optimistic Preferential Bayesian Optimization (POP-BO)\nIn a slightly modified problem setup (Xu et al. 2024), the algorithm tries to solve for the MLE \\(\\hat{g}\\) and its confidence set \\(\\mathcal{B}_g\\) where \\(g\\) is the ground truth black-box function. Assumptions include that \\(g\\) is a member of a reproducing kernel Hilbert space (RKHS) \\(\\mathcal{H}_k\\) for some kernel function \\(k: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\), and \\(\\|g\\|_k \\leq B\\) so that \\(\\mathcal{B}_g = \\left\\{\\tilde{g} \\in \\mathcal{H}_k \\mid\\|\\tilde{g}\\|_k \\leq B\\right\\}\\). Similarly defining \\(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=g\\left(\\mathbf{x}^{\\prime}\\right)-g(\\mathbf{x})\\), we model the preference function with a Bernoulli distribution as in Equation [eq:bernoulli_pref] and also assume that probabilities follow the Bradley-Terry model, i.e. \\[\\pi_f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)=\\sigma\\left(f\\left(\\left[\\mathbf{x}, \\mathbf{x}^{\\prime}\\right]\\right)\\right)=\\frac{e^{g(\\mathbf{x})}}{e^{g(\\mathbf{x})}+e^{g\\left(\\mathbf{x^{\\prime}}\\right)}}\\]\nThe update rule for MLE \\(\\hat{g}\\) is (equation 8,6,5) \\[\\begin{aligned}\n\\hat{g}_t^{\\text {MLE }}&:= \\arg \\underset{\\tilde{g} \\in \\mathcal{B}^t_g}{\\max}\\ell_t(\\tilde{g}) \\\\\n\\ell_t(\\tilde{g}) &:= \\log \\prod_{\\tau=1}^t y_\\tau \\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])+\\left(1-y_\\tau\\right)\\left(1-\\pi_{\\tilde{f}}([\\mathbf{x_\\tau}, \\mathbf{x^{\\prime}_\\tau}])\\right) \\\\\n&=\\sum_{\\tau=1}^t \\log \\left(\\frac{e^{\\tilde{g}(\\mathbf{x_\\tau})} y_\\tau+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\left(1-y_\\tau\\right)}{e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}}\\right) \\\\\n&=\\sum_{\\tau=1}^t\\left(\\tilde{g}(\\mathbf{x_\\tau}) y_\\tau+\\tilde{g}(\\mathbf{x_\\tau^\\prime})\\left(1-y_\\tau\\right)\\right)-\\sum_{\\tau=1}^t \\log \\left(e^{\\tilde{g}(\\mathbf{x_\\tau})}+e^{\\tilde{g}(\\mathbf{x_\\tau^\\prime})}\\right)\n\\end{aligned}\\]\n(Eq 22 shows how to represent this as a convex optimisation problem so that it can be solved)\nThe update rule for the confidence set \\(\\mathcal{B}_f^{t+1}\\) is, (eq 9, 10?)\n\\[\\begin{aligned}\n&\\forall \\epsilon, \\delta &gt; 0 \\\\\n&\\mathcal{B}_g^{t+1}:=\\left\\{\\tilde{g} \\in \\mathcal{B}_g \\mid \\ell_t(\\tilde{g}) \\geq \\ell_t\\left(\\hat{g}_t^{\\mathrm{MLE}}\\right)-\\beta_1(\\epsilon, \\delta, t)\\right\\}\n\\end{aligned}\\] where \\[\\beta_1(\\epsilon, \\delta, t):=\\sqrt{32 t B^2 \\log \\frac{\\pi^2 t^2 \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{6 \\delta}}+ C_L \\epsilon t=\\mathcal{O}\\left(\\sqrt{t \\log \\frac{t \\mathcal{N}\\left(\\mathcal{B}_f, \\epsilon,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}+\\epsilon t\\right),\\] with \\(C_L\\) a constant independent of \\(\\delta, t\\) and \\(\\epsilon\\). \\(\\epsilon\\) is typically chosen to be \\(1 / T\\), where T is the running horizon of the algorithm. This satisfies the theorem that, \\[\\mathbb{P}\\left(g \\in \\mathcal{B}_g^{t+1}, \\forall t \\geq 1\\right) \\geq 1-\\delta .\\]\nIntuitively, the confidence set \\(\\mathcal{B}_g^{t+1}\\) includes the functions with the log-likelihood value that is only ‘a little worse’ than the maximum likelihood estimator, and the theorem states that \\(\\mathcal{B}_g^{t+1}\\) contains the ground-truth function \\(g\\) with high probability.\nInner level optimization in Line 4 of the algorithm can also be represented as a convex optimisation problem so that it can be solved, Eq 24, 25. The outer optimisation can be solved using grid search or Eq 26 for medium size problems.\n\n\nGiven the initial point \\(\\mathbf{x_0} \\in \\mathcal{X}\\) and set \\(\\mathcal{B}_g^1 = \\mathcal{B}_g\\) Set the reference point \\(\\mathbf{x_t^{\\prime}} = \\mathbf{x_{t-1}}\\) Compute \\(\\mathbf{x_t} \\in \\arg\\max_{\\mathbf{x} \\in \\mathcal{X}} \\max_{\\tilde{g} \\in \\mathcal{B}_g^t} (\\tilde{g}(\\mathbf{x}) - \\tilde{g}(\\mathbf{x_t^{\\prime}}))\\), with the inner optimal function denoted as \\(\\tilde{g}_t\\) Obtain the output of the duel \\(y_t\\) and append the new data point to \\(\\mathcal{D}_t\\) Update the maximum likelihood estimator \\(\\hat{g}_t^{\\mathrm{MLE}}\\) and the posterior confidence set \\(\\mathcal{B}_g^{t+1}\\).\n\n\n\n\nqEUBO: Decision-Theoretic EUBO\nqEUBO (Astudillo et al. 2023) derives an acquisition function that extends duels to \\(q&gt;2\\) options which we call queries. Let \\(X=\\left(\\mathbf{x_1}, \\ldots, \\mathbf{x_q}\\right) \\in \\mathcal{X}^q\\) denote a query containing two points or more, and let \\(g: \\mathcal{X} \\rightarrow \\Re\\) be the latent preference function. Then after \\(n\\) user queries, we define the expected utility of the best option (qEUBO) as \\[\\mathrm{qEUBO}_n(X)=\\mathbb{E}_n\\left[\\max \\left\\{g\\left(x_1\\right), \\ldots, g\\left(x_q\\right)\\right\\}\\right].\\]\nWe now show that qEUBO is one-step Bayes optimal, meaning that each step chooses the query that maximises the expected utility received by the human. For a query \\(X \\in \\mathcal{X}^q\\), let \\[V_n(X)=\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)] \\mid X_{n+1}=X\\right] .\\] Then \\(V_n\\) defines the expected utility received if an additional query \\(X_{n+1}=X\\) is performed, and maximizing \\(V_n\\) is one-step Bayes optimal. Since \\(\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x)]\\) does not depend on \\(X_{n+1}\\), we can also equivalently maximize \\[\\mathbb{E}_n\\left[\\max _{x \\in \\mathbb{X}} \\mathbb{E}_{n+1}[g(x)]-\\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x)] \\mid X_{n+1}=X\\right],\\] which takes the same form as the knowledge gradient acquisition function (Wu and Frazier 2018) in standard Bayesian optimization.\n\\(V_n\\) involves a nested stochastic optimization task, while qEUBO is a much simpler policy. When human responses are noise-free, we are able to use qEUBO as a sufficient policy due to the following theorem:\n\n\\[\\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} \\mathrm{qEUBO}_n(X) \\subseteq \\underset{X \\in \\mathbb{X}^q}{\\operatorname{argmax}} V_n(X) .\\]\n\n\nProof. Proof. For a query \\(X \\in \\mathcal{X}^q\\), let \\(x^{+}(X, i) \\in \\operatorname{argmax}_{x \\in \\mathbb{X}} \\mathbb{E}_n[g(x) \\mid(X, i)]\\) and define \\(X^{+}(X)=\\) \\(\\left(x^{+}(X, 1), \\ldots, x^{+}(X, q)\\right)\\).\nClaim 1 \\(V_n(X) \\leq \\mathrm{qEUBO}_n\\left(X^{+}(X)\\right) .\\) We see that \\[\\begin{aligned}\nV_n(X) & =\\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[g\\left(x^{+}(X, i)\\right) ] \\\\\n& \\leq \\sum_{i=1}^q \\mathbf{P}_n(r(X)=i) \\mathbb{E}_n[\\max _{i=1, \\ldots, q} g(x^{+}(X, i))] \\\\\n& =\\mathbb{E}_n\\left[\\max _{i=1, \\ldots, q} g\\left(x^{+}(X, i)\\right)\\right] \\\\\n& =\\mathrm{qEUBO}_n\\left(X^{+}(X)\\right),\n\\end{aligned}\\] as claimed.\nClaim 2 \\(\\mathrm{qEUBO}_n(X) \\leq V_n(X) .\\) For any given \\(X \\in \\mathbb{X}^q\\) we have \\[\\mathbb{E}_n\\left[f\\left(x_{r(X)}\\right) \\mid(X, r(X))\\right] \\leq \\max _{x \\in \\mathbb{X}} \\mathbb{E}_n[f(x) \\mid(X, r(X))] .\\] Since \\(f\\left(x_{r(X)}\\right)=\\max _{i=1, \\ldots, q} f\\left(x_i\\right)\\), taking expectations over \\(r(X)\\) on both sides obtains the required result.\nNow building on the arguments above, let \\(X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)\\) and suppose for contradiction that \\(X^* \\notin \\operatorname{argmax}_{X \\in \\mathbb{X}^q} V_n(X)\\). Then, there exists \\(\\widetilde{X} \\in \\mathbb{X}^q\\) such that \\(V_n(\\widetilde{X})&gt;V_n\\left(X^*\\right)\\). We have \\[\\begin{aligned}\n\\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) & \\geq V_n(\\tilde{X}) \\\\\n& &gt;V_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^*\\right) \\\\\n& \\geq \\operatorname{qEUBO}_n\\left(X^{+}(\\tilde{X})\\right) .\n\\end{aligned}\\]\nThe first inequality follows from (1). The second inequality is due to our supposition for contradiction. The third inequality is due to (2). Finally, the fourth inequality holds since \\(X^* \\in \\operatorname{argmax}_{X \\in \\mathbb{X}^q} \\mathrm{qEUBO}_n(X)\\). This contradiction concludes the proof. ◻\n\nTherefore a sufficient condition for following one-step Bayes optimality is by maximizing \\(\\text{qEUBO}_n\\).\nIn experiments that were ran comparing qEUBO to other state-of-the-art acquisition functions, qEUBO consistently outperformed on most problems and was closely followed by qEI and qTS. These results also extended to experiments with multiple options when \\(q&gt;2\\). In fact, there is faster convergence in regret when using more options in human queries. [Prove Theorem 3: Regret analysis]\n\n\nqEI: Batch Expected Improvement\n\\[\\begin{aligned}\n\\mathrm{qEI}= & \\mathbb{E}_{\\mathbf{y}}\\left[\\left(\\max _{i \\in[1, \\ldots, q]}\\left(\\mu_{\\min }-y_i\\right)\\right)_{+}\\right] \\\\\n= & \\sum_{i=1}^q \\mathbb{E}_{\\mathbf{y}}\\left(\\mu_{\\min }-y_i \\mid y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) \\\\\n& p\\left(y_i \\leq \\mu_{\\min }, y_i \\leq y_j \\forall j \\neq i\\right) .\n\\end{aligned}\\]\n\n\nqTS: Batch Thompson Sampling\n\n\nInitial data \\(\\mathcal{D}_{\\mathcal{I}(1)}=\\{(\\mathbf{x}_i, y_i)\\}_{i \\in \\mathcal{I}(1)}\\) Compute current posterior \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Sample \\(\\boldsymbol{\\theta}\\) from \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Select \\(k \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]\\) Collect \\(y_k\\) by evaluating \\(f\\) at \\(\\mathbf{x}_k\\) \\(\\mathcal{D}_{\\mathcal{I}(t+1)} \\leftarrow \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{(\\mathbf{x}_k, y_k)\\}\\)\n\n\n\n\nInitial data \\(\\mathcal{D}_{\\mathcal{I}(1)}=\\{\\mathbf{x}_i, y_i\\}_{i \\in \\mathcal{I}(1)}\\), batch size \\(S\\) Compute current posterior \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Sample \\(\\boldsymbol{\\theta}\\) from \\(p(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{\\mathcal{I}(t)})\\) Select \\(k(s) \\leftarrow \\arg \\max_{j \\notin \\mathcal{I}(t)} \\mathbb{E}[y_j \\mid \\mathbf{x}_j, \\boldsymbol{\\theta}]\\) \\(\\mathcal{D}_{\\mathcal{I}(t+1)} = \\mathcal{D}_{\\mathcal{I}(t)} \\cup \\{\\mathbf{x}_{k(s)}, y_{k(s)}\\}_{s=1}^S\\)\n\n\n\n\n\n5.2.3 Regret Analysis\n\nqEUBO Regret\nWith the definition of Bayesian simple regret, we have that qEUBO converges to zero at a rate of \\(o(1/n)\\), i.e.\n\n\\[\\label{th:quebo_regret}\n\\mathbb{E}\\left[f\\left(x^*\\right)-f\\left(\\widehat{x}_n^*\\right)\\right]=o(1 / n)\\]\n\nwhere \\(x^*=\\operatorname{argmax}_{x \\in \\mathrm{X}} f(x)\\) and \\(\\widehat{x}_n^* \\in \\operatorname{argmax}_{x \\in \\mathrm{X}} \\mathbb{E}_n[f(x)]\\).\nThis theorem holds under the following assumptions:\n\n\\(f\\) is injective \\(\\mathbf{P}(f(x)=f(y))=0\\) for any \\(x, y \\in \\mathbb{X}\\) with \\(x \\neq y\\).\n\\(f\\) represents the preferred option \\(\\exists a&gt;1 / 2\\) s.t. \\(\\mathbf{P}\\left(r(X) \\in \\operatorname{argmax}_{i=1, \\ldots, 2} f\\left(x_i\\right) \\mid f(X)\\right) \\geq a \\forall\\) \\(X=\\left(x_1, x_2\\right) \\in \\mathbb{X}^2\\) with \\(x_1 \\neq x_2\\) almost surely under the prior on \\(f\\).\nExpected difference in utility is proportional to probability of greater utility \\(\\exists \\Delta \\geq \\delta&gt;0\\) s.t. \\(\\forall \\mathcal{D}^{(n)} \\text{and} \\forall x, y \\in \\mathbb{X}\\) (potentially depending on \\(\\mathcal{D}^{(n)}\\)), \\[\\delta \\mathbf{P}^{(n)}(f(x)&gt;f(y)) \\leq \\mathbb{E}^{(n)}\\left[\\{f(x)-f(y)\\}^{+}\\right] \\leq \\Delta \\mathbf{P}^{(n)}(f(x)&gt;f(y))\\] almost surely under the prior on \\(f\\).\n\nFurther lemmas leading to a proof of Theorem [th:quebo_regret] is given in (Astudillo et al. 2023) Section B.\n\n\nqEI Regret\nThe following theorem shows that, under the same assumptions used for qEUBO regret, simple regret of qEI can fail to converge to 0.\n\nThere exists a problem instance (i.e., \\(\\mathbb{X}\\) and Bayesian prior distribution over f) satisfying the assumptions described in Theorem [th:quebo_regret] such that if the sequence of queries is chosen by maximizing qEI, then \\(\\mathbb{E}\\left[f\\left(x^*\\right)-\\right.\\) \\(\\left.f\\left(\\widehat{x}_n^*\\right)\\right] \\geq R\\) for all \\(n\\), for a constant \\(R&gt;0\\).\n\n\nProof. Proof. Let \\(X = \\{1, 2, 3, 4\\}\\) and consider the functions \\(f_i:X \\rightarrow R\\), for \\(i=1,2,3,4\\), given by \\(f_i(1) = -1\\) and \\(f_i(2) = 0\\) for all \\(i\\), and \\[\\begin{aligned}\n    f_1(x) = \\begin{cases}\n    1, &\\ x=3\\\\\n    \\frac{1}{2}, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_2(x) = \\begin{cases}\n    \\frac{1}{2}, &\\ x=3\\\\\n    1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_3(x) = \\begin{cases}\n    -\\frac{1}{2}, &\\ x=3\\\\\n    -1, &\\ x=4\n    \\end{cases},\n\\hspace{0.5cm}\nf_4(x) = \\begin{cases}\n    -1, &\\ x=3\\\\\n    -\\frac{1}{2}, &\\ x=4\n    \\end{cases}.\n\\end{aligned}\\]\nLet \\(p\\) be a number with \\(0 &lt; p &lt; 1/3\\) and set \\(q=1-p\\). We consider a prior distribution on \\(f\\) with support \\(\\{f_i\\}_{i=1}^4\\) such that \\[\\begin{aligned}\np_i = Pr(f=f_i) =\n    \\begin{cases}\n        p/2, i =1,2,\\\\\n        q/2, i=3,4.\n    \\end{cases}\n\\end{aligned}\\] We also assume the user’s response likelihood is given by \\(Pr(r(X)=1\\mid f(x_1) &gt; f(x_2)) = a\\) for some \\(a\\) such that \\(1/2 &lt; a &lt; 1\\),\nLet \\(D^{(n)}\\) denote the set of observations up to time \\(n\\) and let \\(p_i^{(n)} = Pr(f=f_i \\mid \\mathbb{E}^{(n)})\\) for \\(i=1,2,3,4\\). We let the initial data set be \\(\\mathcal{D}^{(0)} = \\{(X^{(0)}, r^{(0)})\\}\\), where \\(X^{(0)}= (1,2)\\). We will prove that the following statements are true for all \\(n\\geq 0\\).\n\n\\(p_i^{(n)} &gt; 0\\) for \\(i=1,2,3,4\\).\n\\(p_1^{(n)} &lt; \\frac{1}{2}p_3^{(n)}\\) and \\(p_2^{(n)} &lt; \\frac{1}{2}p_4^{(n)}\\).\n\\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\).\n\\(\\arg \\max_{X\\in\\mathcal{X}^2}\\text{qEI}^{(n)}(X) = \\{(3, 4)\\}\\).\n\nWe prove this by induction over \\(n\\). We begin by proving this for \\(n=0\\). Since \\(f_i(1) &lt; f_i(2)\\) for all \\(i\\), the posterior distribution on \\(f\\) given \\(\\mathcal{D}^{(0)}\\) remains the same as the prior; i.e., \\(p_i^{(0)} = p_i\\) for \\(i=1,2,3,4\\). Using this, statements 1 and 2 can be easily verified. Now note that \\(\\mathbb{E}^{(0)}[f(1)]=-1\\), \\(\\mathbb{E}^{(0)}[f(2)]=0\\), and \\(\\mathbb{E}^{(0)}[f(3)] = \\mathbb{E}^{(0)}[f(4)] = \\frac{3}{2}(p - q)\\). Since \\(p &lt; q\\), it follows that \\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\); i.e., statement 3 holds. Finally, since \\(\\max_{x\\in\\{1,2\\}}\\mathbb{E}^{(0)}[f(x)] = 0\\), the qEI acquisition function at time \\(n=0\\) is given by \\(\\text{qEI}^{(0)}(X) = \\mathbb{E}^{(0)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]\\). A direct calculation can now be performed to verify that statement 4 holds. This completes the base case.\nNow suppose statements 1-4 hold for some \\(n\\geq 0\\). Since \\(X^{(n+1)} = (3, 4)\\), the posterior distribution on \\(f\\) given \\(D^{(n+1)}\\) is given by \\[\\begin{aligned}\np_i^{(n+1)} \\propto \\begin{cases}\n                        p_i^{(n)}\\ell, \\ i=1,3,\\\\\n                         p_i^{(n)} (1 - \\ell), \\ i=2,4,\n                        \\end{cases}\n\\end{aligned}\\] where \\[\\ell = a I\\{r^{(n+1)} = 1\\} + (1-a)I\\{r^{(n+1)} = 2\\}.\\] Observe that \\(0&lt; \\ell &lt; 1\\) since \\(0 &lt; a &lt; 1\\). Thus, \\(\\ell &gt; 0\\) and \\(1-\\ell &gt; 0\\). Since \\(p_i^{(n)} &gt; 0\\) by the induction hypothesis, it follows from this that \\(p_i^{(n+1)} &gt; 0\\) for \\(i=1,2,3,4\\). Moreover, since \\(p_i^{(n+1)} \\propto p_i^{(n)}\\ell\\) for \\(i=1,3\\) and \\(p_1^{(n)} &lt; \\frac{1}{2}p_3^{(n)}\\) by the induction hypothesis, it follows that \\(p_1^{(n+1)} &lt; \\frac{1}{2}p_3^{(n+1)}\\). Similarly, \\(p_2^{(n+1)} &lt; \\frac{1}{2}p_4^{(n+1)}\\). Thus, statements 1 and 2 hold at time \\(n+1\\).\nNow observe that \\[\\begin{aligned}\n    \\mathbb{E}^{(n+1)}[f(3)] &= p_1^{(n+1)} + \\frac{1}{2}p_2^{(n+1)} - \\frac{1}{2}p_3^{(n+1)} - p_4^{(n+1)}\\\\\n    &= \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(\\frac{1}{2}p_2^{(n+1)} - p_4^{(n+1)}\\right)\\\\\n    &\\leq \\left(p_1^{(n+1)} - \\frac{1}{2}p_3^{(n+1)}\\right) + \\left(p_2^{(n+1)} - \\frac{1}{2}p_4^{(n+1)}\\right)\\\\\n    &\\leq 0,\n\\end{aligned}\\] where the last inequality holds since \\(p_1^{(n+1)} &lt; \\frac{1}{2}p_3^{(n+1)}\\) and \\(p_2^{(n+1)} &lt; \\frac{1}{2}p_4^{(n+1)}\\). Similarly, we see that \\(\\mathbb{E}^{(n+1)}[f(4)] \\leq 0\\). Since \\(\\mathbb{E}^{(n+1)}[f(1)]=-1\\) and \\(\\mathbb{E}^{(n+1)}[f(2)]=0\\), it follows that \\(\\arg \\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(n+1)}[f(x)]=\\{2\\}\\); i.e., statement 3 holds at time \\(n+1\\).\nSince \\(\\max_{x\\in\\mathcal{X}}\\mathbb{E}^{(0)}[f(x)] = 0\\), the qEI acquisition function at time \\(n+1\\) is given by \\(\\text{qEI}^{(n+1)}(X) = \\mathbb{E}^{(n+1)}[\\{\\max\\{f(x_1), f(x_2)\\}\\}^+]\\). Since \\(f(1) \\leq f(x)\\) almost surely under the prior for all \\(x\\in\\mathcal{X}\\), there is always a maximizer of qEI that does not contain \\(1\\). Thus, to find the maximizer of qEI, it suffices to analyse its value at the pairs \\((2, 3)\\), \\((3,4)\\) and \\((4,2)\\). We have \\[\\text{qEI}^{(n+1)}(2, 3) = p_1^{(n+1)} + 1/2 p_2^{(n+1)},\\] \\[\\operatorname{qEI}^{(n+1)}(3, 4) = p_1^{(n+1)} + p_2^{(n+1)}\\] and \\[\\operatorname{qEI}^{(n+1)}(4, 2) = 1/2p_1^{(n+1)} + p_2^{(n+1)}.\\] Since \\(p_1^{(n+1)} &gt; 0\\) and \\(p_2^{(n+1)} &gt; 0\\), it follows that \\(\\arg \\max_{X \\in X^2}\\text{qEI}^{(n+1)}(X) = \\{(3, 4)\\}\\), which concludes the proof by induction.\nFinally, since \\(\\arg \\max_{x\\in X}\\mathbb{E}^{(n)}[f(x)]=\\{2\\}\\) for all \\(n\\), the Bayesian simple regret of qEI is given by \\[\\begin{aligned}\n    \\mathbb{E}\\left[f(x^*) - f(2)\\right] &= \\sum_{i=1}p_i\\left(\\max_{x\\in X}f_i(x) - f_i(2)\\right)\\\\\n    &= p\n\\end{aligned}\\] for all \\(n\\). ◻\n\n\n\nPOP-BO Regret\nCommonly used kernel functions within the RKHS are:\n\nLinear: \\[k(x, \\bar{x})=x^{\\top} \\bar{x} .\\]\nSquared Exponential (SE): \\[k(x, \\bar{x})=\\sigma_{\\mathrm{SE}}^2 \\exp \\left\\{-\\frac{\\|x-\\bar{x}\\|^2}{l^2}\\right\\},\\] where \\(\\sigma_{\\mathrm{SE}}^2\\) is the variance parameter and \\(l\\) is the lengthscale parameter.\nMatérn: \\[k(x, \\bar{x})=\\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right)^\\nu K_\\nu\\left(\\sqrt{2 \\nu} \\frac{\\|x-\\bar{x}\\|}{\\rho}\\right),\\] where \\(\\rho\\) and \\(\\nu\\) are the two positive parameters of the kernel function, \\(\\Gamma\\) is the gamma function, and \\(K_\\nu\\) is the modified Bessel function of the second kind. \\(\\nu\\) captures the smoothness of the kernel function.\n\nWith the definition of Bayesian simple regret, we have the following theorem defining the regret bound:\n\nWith probability at least \\(1-\\delta\\), the cumulative regret of POP-BO satisfies, \\[R_T=\\mathcal{O}\\left(\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}} T}\\right),\\] where \\[\\beta_T=\\beta(1 / T, \\delta, T)=\\mathcal{O}\\left(\\sqrt{T \\log \\frac{T \\mathcal{N}\\left(\\mathcal{B}_f, 1 / T,\\|\\cdot\\|_{\\infty}\\right)}{\\delta}}\\right).\\]\n\nThe guaranteed convergence rate is characterised as:\n\n[]{#th: popbo_converge label=“th: popbo_converge”} Let \\(t^{\\star}\\) be defined as in Eq. (19). With probability at least \\(1-\\delta\\), \\[f\\left(x^{\\star}\\right)-f\\left(x_{t^{\\star}}\\right) \\leq \\mathcal{O}\\left(\\frac{\\sqrt{\\beta_T \\gamma_T^{f f^{\\prime}}}}{\\sqrt{T}}\\right)\\]\n\nTheorem [th: popbo_converge] highlights that by minimizing the known term \\(2\\left(2 B+\\lambda^{-1 / 2} \\sqrt{\\beta\\left(\\epsilon, \\frac{\\delta}{2}, t\\right)}\\right) \\sigma_t^{f f^{\\prime}}\\left(\\left(x_t, x_t^{\\prime}\\right)\\right)\\), the reported final solution \\(x_{t^{\\star}}\\) has a guaranteed convergence rate.\nFurther kernel-specific regret bounds for POP-BO are calculated as follows:\n\nSetting \\(\\epsilon=1 / T\\) and running our POP-BO algorithm in Alg. 1,\n\nIf \\(k(x, y)=\\langle x, y\\rangle\\), we have, \\[R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4}\\right) .\\]\nIf \\(k(x, y)\\) is a squared exponential kernel, we have, \\[R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4(d+1)}\\right) .\\]\nIf \\(k(x, y)\\) is a Matérn kernel, we have, \\[\\left.R_T=\\mathcal{O}\\left(T^{3 / 4}(\\log T)^{3 / 4} T^{\\frac{d}{\\nu}\\left(\\frac{1}{4}+\\frac{d+1}{4+2(d+1)^d / \\nu}\\right.}\\right)\\right).\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#case-study-1-foundation-models-for-robotics",
    "href": "src/chap5.html#case-study-1-foundation-models-for-robotics",
    "title": "5  Decisions",
    "section": "5.3 Case Study 1: Foundation Models for Robotics",
    "text": "5.3 Case Study 1: Foundation Models for Robotics\nModern foundation models have been ubiquitous in discussions of powerful, general purpose AI systems that can accomplish myriad tasks across many disciplines such as programming, medicine, law, open question-answering and much more, with rapidly increasing capabilities (Bommasani et al. 2022). However, despite successes from large labs in controlled environments (Brohan et al. 2023) foundation models have not seen ubiquitous use in robotics due to shifting robot morphology, lack of data, and the sim to real gap in robotics (Walke et al. 2023). For this subsection we explore two promising approaches known as R3M and Voltron which are the first to leverage pre-training on vast amounts of data towards performance improvement on downstream robotic tasks despite the aforementioned issues (Nair et al. 2022; Karamcheti et al. 2023).\nR3M represents a significant advancement in the field of robotic manipulation and learning. This model diverges from traditional approaches that rely on training from scratch within the same domain on the same robot data as instead it leverags pretraining on large datasets, akin to the practices in computer vision and natural language processing (NLP) where models are trained on diverse, large-scale datasets to create reusable, general-purpose representations. The core principle behind R3M is its training methodology. It is pre-trained on a wide array of human videos, encompassing various activities and interactions. This diverse dataset enables the model to capture a broad spectrum of physical interactions and dynamics, which are crucial for effective robotic manipulation known as EGO4D (Grauman et al. 2022). However, prior papers could not fit this dataset well, and R3M leveraged. The training utilizes a unique objective that combines time contrastive learning, video-language alignment, and a sparsity penalty. This objective ensures that R3M not only understands the temporal dynamics of scenes (i.e., how states transition over time) but also focuses on semantically relevant features, such as objects and their interrelations, while maintaining a compact and efficient representation. What sets R3M apart in the realm of robotics is its efficiency and effectiveness in learning from a limited amount of data. The model demonstrates remarkable performance in learning tasks in the real world with minimal human supervision – typically less than 10 minutes. This is a stark contrast to traditional models that require extensive and often prohibitively large datasets for training. Furthermore, R3M’s pre-trained nature allows for its application across a variety of tasks and environments without the need for retraining from scratch, making it a versatile tool in robotic manipulation. The empirical results from using R3M are compelling, leading to a 10% improvement over training from a pretrained image-net model, self-supervised approaches such as MoCo or even CLIP (Deng et al. 2009; He et al. 2020; Radford et al. 2021). Note however, that R3m does not use any language data which leaves quite a bit of supervision to be desired.\nBuilding off the success of R3M, Voltron proposes a further extension of leveraging self-supervision and advancements in foundation models, and multi-modality. Voltron takes on an intuitive and simple dual use objective, where the trained model alternates between predicting the task in an image through natural language and classifying images based on a natural text label. This forces a nuanced understanding of both modalities (Radford et al. 2021). Voltron’s approach is distinguished by its versatility and depth of learning. It is adept at handling a wide range of robotic tasks, from low-level spatial feature recognition to high-level semantic understanding required in language-conditioned imitation and intent scoring. This flexibility makes it suitable for various applications in robotic manipulation, from grasping objects based on descriptive language to performing complex sequences of actions in response to verbal instructions. The authors rigorously test Voltron in scenarios such as dense segmentation for grasp affordance prediction, object detection in cluttered scenes, and learning multi-task language-conditioned policies for real-world manipulation with up to 15% improvement over baselines. In each of these domains, Voltron has shown a remarkable ability to outperform existing models like MVP and R3M, showcasing its superior adaptability and learning capabilities (Xiao et al. 2022). Moreover, Voltron’s framework allows for a balance between encoding low-level and high-level features, which is critical in the context of robotics. This balance enables the model to excel in both control tasks and those requiring deeper semantic understanding, offering a comprehensive solution in the realm of robotic vision and manipulation.\nVoltron stands as a groundbreaking approach in the field of robotics, offering a language-driven, versatile, and efficient approach to learning and manipulation. Its ability to seamlessly integrate visual and linguistic data makes it a potent tool in the ever-evolving landscape of robotic technology, with potential applications that extend far beyond current capabilities. Interesting the authors show Voltron does not beat R3M off the shelf but only when trained on similar amounts of data. Nevertheless, Voltron’s success in diverse tasks and environments heralds a new era in robotic manipulation, where language and vision coalesce to create more intelligent, adaptable, and capable robotic systems.\nOn the note of applying AL to RL and environment settings, there have been many recent papers that have attempted to extend this to more modern RL environments. For example, the paper “When to Ask for Help” (Xie et al. 2022) examines the intersection of autonomous and AL. Instead of just expecting an RL agent to autonomously solve a task, making the assumption that an agent could get stuck and need human input to get “unstuck” is a key insight of the paper. In general, there has been an emphasis in recent literature in robotics on not just blindly using demonstration data as a form of human input, but rather actively querying a human and using this to better synthesize correct actions.\nAL holds promise for enhancing AI models in real-world scenarios, yet several challenges persist. This discussion aims to provide an overview of these challenges.\nTask-Specific Considerations: For certain tasks, the input space of a model may have some rare yet extremely important pockets which may never be discovered by AL and may cause severe blindspots in the model. In medical imaging for instance, there can be rare yet critical diseases. Designing AL strategies for medical image analysis must prioritize rare classes, such as various forms of cancers. Oftentimes, collecting data around those rare classes is not a recommendation of the AL process because these examples constitute heavy distribution drifts from the input distribution a model has seen.\nComplex Task Adaptation: AL has predominantly been adopted for simple classification tasks, leaving more other types of tasks (generative ones for instance), less explored. In Natural Language Processing, tasks like natural language inference, question-answering pose additional complexities that affect the direct application of the AL process. While machine translation has seen AL applications, generation tasks in NLP require more thorough exploration. Challenges arise in obtaining unlabeled data, particularly for tasks with intricate inputs.\nUnsupervised and Semi-Supervised Approaches: In the presence of large datasets without sufficient labels, unsupervised and semi-supervised approaches become crucial. These methods offer a means to extract information without relying on labeled data for every data point, potentially revolutionizing fields like medical image analysis. There is an ongoing need for methods that combine self/semi-supervised learning with AL.\nAlgorithm Scalability: Scalability is a critical concern for online AL algorithms, particularly when dealing with large datasets and high-velocity data streams. The computational demands of AL can become prohibitive as data volume increases, posing challenges for practical deployment. Issues of catastrophic forgetting and model plasticity further complicate scalability, requiring careful consideration in algorithm design.\nLabeling Quality Assurance: The effectiveness of most online AL strategies hinges on the quality of labeled data. Ensuring labeling accuracy in real-world scenarios is challenging, with human annotators prone to errors, biases, and diverse interpretations. Addressing imperfections in labeling through considerations of oracle imperfections becomes essential in real-life AL applications. Solutions for cleaning up data and verifying its quality need to be more aggressively pursued.\nData Drift Challenges: Real-world settings introduce data drift, where distributions shift over time, challenging models to adapt for accurate predictions. These shifts can impact the quality of labeled data acquired in the AL process. For example, the criterion or proxy used for selecting informative instances may be thrown off when the distribution a model is trained on, and the distribution we want it to perform well on, are too far away from one another.\nEvaluation in Real-Life Scenarios: While AL methods are often evaluated assuming access to ground-truth labels, the real motivation for AL lies in label scarcity. Assessing the effectiveness of AL strategies becomes challenging in real-life scenarios where ground-truth labels may be limited. In other words, one may verify the goodness of an AL algorithm within the lab, but once the algorithm is deployed for improving all sorts of models on all sorts of data distributions, verifying whether AL is actually improving a model is tricky, especially when collecting and labeling data from the target distribution is expensive and defeats the purpose of using AL in the first place.\nBy systematically addressing these challenges, the field of AL in AI can progress towards more effective and practical applications. In summary, AL is a promising modern tool to model training that presents potential benefits. As was mentioned at the start, there are numerous approaches that can be employed by AL, starting from reducing error of model’s prediction, reducing variance, to more conformal predictions. The flavor of AL heavily depends on the applications, which include robotics, LLM, autonomous vehicles, and more. We discussed in more detail how to perform AL for variance reduction in the case of predicting kinematics of the robotic arms, which showed decrease in MSE as well as more stable reduction in it. Next we talked about using AL for reducing the number of comparisons required to create a ranking of objects, and the examples discussed were able to achieve that but with some loss in the prediction accuracy. Finally, we discussed how AL can be used for modeling of reward functions within a dynamical system, which demonstrated improvements in performance and time required to achieve it. For a more hands-on experience with AL and demonstrated example, we encourage the readers to explore a blogpost by Max Halford (Halford 2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap5.html#exercises",
    "href": "src/chap5.html#exercises",
    "title": "5  Decisions",
    "section": "5.4 Exercises",
    "text": "5.4 Exercises\n\nQuestion 1: Preferential Bayesian Optimization (30 points)\nPreferential Bayesian Optimization (PBO) is a variant of Bayesian Optimization (BO) designed to handle scenarios where feedback is provided in terms of preferences between alternatives rather than explicit numeric evaluations. Suppose you are optimizing an unknown function \\(f\\) over a space \\(\\mathcal{X}\\), but instead of receiving function values, you only receive pairwise comparisons between different points in the input space. That is, given two points \\(x_1, x_2 \\in \\mathcal{X}\\), you receive feedback in the form of a preference: \\(x_1 \\succ x_2\\) implies \\(f(x_1) &gt; f(x_2)\\).\nThe Gaussian Process (GP) framework is used to model \\(f\\), and the optimization is guided by this model. Let \\(p(x_1 \\succ x_2 | f)\\) be the probability that \\(x_1\\) is preferred over \\(x_2\\), which can be modeled using a Bradley-Terry or Thurstone model based on the GP prior.\nUsing the paper “Preferential Bayesian Optimization” (https://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf), answer the following:\n\nModeling Preferences (6 points)\n\nLikelihood Derivation (Written, 2 points): Given two points \\(x_1\\) and \\(x_2\\) and their corresponding latent function values \\(f(x_1)\\) and \\(f(x_2)\\), derive the likelihood of a preference \\(x_1 \\succ x_2\\) using the Bradley-Terry model. Your solution here.\nIncorporating into GP (Written, 2 points): Explain how this likelihood can be incorporated into the GP framework to model preferences probabilistically. Specifically, describe how the covariance function of the GP affects the joint distribution of preferences and discuss any assumptions made regarding the smoothness or structure of \\(f\\).\nPosterior Update (Written, 2 points): Write out an expression for the posterior mean and variance at new query points by using the posterior predictive distribution based on previously observed preferences (no need to simplify since it’s intractable analytically). Suggest an approach that can be used to approximate the mean and variance.\n\nAcquisition Function Adaptation (6 points)\n\nExpected Improvement (EI) for Preferences (Written, 2 points): Explain how the Expected Improvement (EI) acquisition function is adapted in the context of PBO to handle preferences rather than absolute function values. Please read the paper for this.\nEI Computation for Pairwise Comparisons (Written, 2 points): Derive the expression for EI when dealing with pairwise comparisons. Show how the computation of EI differs from the standard BO setting and discuss how uncertainty in the GP model is used in this context.\nSelection Strategy (Written, 2 points): Describe how the acquisition function uses the pairwise preference data to select the next query point. Provide a rigorous justification for this selection strategy in terms of maximizing expected information gain.\n\nExploration-Exploitation Balance in PBO (6 points)\n\nExploration Mechanism (Written, 2 points): Explain how exploration is handled in the PBO framework. Describe how uncertainty in the preference model (the GP posterior) influences the selection of new points for evaluation.\nUncertainty Quantification (Written, 2 points): Define how the variance in the GP posterior represents uncertainty in the model and show how this uncertainty is updated as new preferences are observed.\nEmpirical Validation (Written, 2 points): Design an experiment to empirically validate the balance between exploration and exploitation in PBO. Describe the setup, including the objective function, the experimental conditions, and the evaluation metric for measuring the quality of exploration-exploitation balance.\n\nScalability and Practical Considerations (6 points)\n\nChallenges in Preference Feedback (Written, 2 points): Discuss the challenges associated with preference feedback in real-world applications, such as inconsistency in user preferences and potential biases.\nGP Scalability (Written, 2 points): Explain how the scalability of the GP model affects the performance of PBO, especially as the number of observations increases. Include a discussion on computational complexity and possible solutions.\nExtensions for Large-Scale Problems (Written, 2 points): Propose potential extensions or modifications to improve the applicability of PBO to large-scale optimization problems. For example, discuss the feasibility of sparse GPs or other approximation techniques and evaluate their potential impact on PBO performance.\n\nEmpirical Experimentation (6 points)\n\nCopeland Score (Coding, 2 points): Implement compute_max_copeland_score in\npbo/forrester_duel.py.\nCopeland Acquisition (Coding, 4 points): Implement copeland_acquisition. Run forrester_duel.py and briefly discuss any patterns you observe in the chosen duels (black Xs on the heatmap).\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Define the Forrester function\ndef forrester_function(x):\n    \"\"\"\n    Evaluates the Forrester function at the given input.\n    \n    Args:\n    - x (float or numpy.ndarray): Input value(s) in the range [0, 1].\n    \n    Returns:\n    - float or numpy.ndarray: Evaluated Forrester function value(s).\n    \"\"\"\n    return (6 * x - 2)**2 * np.sin(12 * x - 4)\n\n# Sigmoid function for probabilistic preferences\ndef sigmoid(x):\n    \"\"\"\n    Computes the sigmoid function for the given input.\n    \n    Args:\n    - x (float or numpy.ndarray): Input value(s).\n    \n    Returns:\n    - float or numpy.ndarray: Sigmoid-transformed value(s).\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\n# Simulate duel outcome probabilistically\ndef simulate_duel_outcome(x, x_prime):\n    \"\"\"\n    Simulates the outcome of a duel between two candidates based on probabilistic preferences.\n    \n    Args:\n    - x (float): First candidate's input value.\n    - x_prime (float): Second candidate's input value.\n    \n    Returns:\n    - int: 1 if x wins, 0 otherwise.\n    \"\"\"\n    prob = sigmoid(forrester_function(x_prime) - forrester_function(x))  # Probability x beats x'\n    return np.random.choice([1, 0], p=[prob, 1 - prob])\n\n# Compute the Soft Copeland score for all candidates (vectorized)\ndef compute_max_copeland_score(candidates, gp, landmarks):\n    \"\"\"\n    Computes the maximum Copeland score for given candidates using predicted win probabilities.\n    \n    Args:\n    - candidates (numpy.ndarray): Array of candidate points.\n    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.\n    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.\n    \n    Returns:\n    - tuple: Maximum Copeland score and the best candidate.\n    \"\"\"\n    # YOUR CODE HERE (~6 lines)\n        # 1. Generate all pairs between candidates and landmarks.\n        # 2. Get win probabilities and average\n        # 3. Return appropriate maximum and best candidate.\n    pass \n    # END OF YOUR CODE\n\n# Acquisition function with GP retraining and maximum Copeland score for each outcome\ndef copeland_acquisition(x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score):\n    \"\"\"\n    Computes the acquisition value for a candidate pair by simulating outcomes and retraining the GP.\n    \n    Args:\n    - x (float): First value of duel.\n    - x_prime (float): Second value of duel.\n    - x_candidates (numpy.ndarray): Array of candidate points to evaluate soft Copeland on.\n    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.\n    - train_X (numpy.ndarray): Current training input pairs.\n    - train_y (numpy.ndarray): Current training labels.\n    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.\n    - max_copeland_score (float): Maximum copeland score prior to acquiring any new pair\n    \n    Returns:\n    - float: Acquisition value for the given pair (x, x_prime).\n    \"\"\"\n    # YOUR CODE HERE (~14-16 lines)\n        # 1. Predict dueling probabilities\n        # 2. Simulate adding (x, x') with y=1 (x beats x') and fit GP \n        # 3. Simulate adding (x, x') with y=0 (x' beats x) and fit GP \n        # 4. Compute expected improvement in max Copeland score\n        # 5. Return weighted acquisition value\n    pass\n    # END OF YOUR CODE\n\nif __name__ == \"__main__\":\n    # Initialization\n    np.random.seed(42)\n    kernel = C(28.0, constant_value_bounds='fixed') * RBF(length_scale=0.15, length_scale_bounds='fixed')\n    gp = GaussianProcessClassifier(kernel=kernel)\n\n    # Generate initial training data (random pairs)\n    train_X = np.array([[0, 0], [0, 0]]) #np.random.uniform(0, 1, (10, 2))  # 20 random dueling pairs [x, x']\n    train_y = np.array([simulate_duel_outcome(pair[0], pair[1]) for pair in train_X])\n\n    # Fixed landmark points and their function values\n    landmarks = np.linspace(0, 1, 30)  # 10 fixed landmarks\n\n    # Generate candidate pairs for optimization\n    x_candidates = np.linspace(0, 1, 30)  # Reduced grid for efficiency\n    X, X_prime = np.meshgrid(x_candidates, x_candidates)\n    candidate_pairs = np.c_[X.ravel(), X_prime.ravel()]\n\n    # Optimization loop\n    n_iterations = 20\n    for iteration in range(n_iterations):\n        # Retrain the GP with current training data\n        gp.fit(train_X, train_y)\n\n        # Compute global maximum Copeland score\n        max_copeland_score, condorcet_winner = compute_max_copeland_score(x_candidates, gp, landmarks)\n        print(f\"Condorcet winner iteration {iteration}: {condorcet_winner} with soft-Copeland score {max_copeland_score}\")\n\n        # Evaluate acquisition values for all candidate pairs\n        acquisition_values = np.zeros(len(candidate_pairs))\n        for idx, (x, x_prime) in tqdm(enumerate(candidate_pairs), total=len(candidate_pairs)):\n            acquisition_values[idx] = copeland_acquisition(\n                x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score\n            )\n\n        # Select the pair with the highest acquisition value\n        best_idx = np.argmax(acquisition_values)\n        next_x, next_x_prime = candidate_pairs[best_idx]\n\n        # Simulate the actual outcome of the duel\n        outcome = simulate_duel_outcome(next_x, next_x_prime)\n\n        # Update training data with the new duel outcome\n        train_X = np.vstack([train_X, [next_x, next_x_prime]])\n        train_y = np.append(train_y, outcome)\n\n    # Generate heatmaps\n    x = np.linspace(0, 1, 100)\n    X, X_prime = np.meshgrid(x, x)\n    pairs = np.c_[X.ravel(), X_prime.ravel()]\n\n    # Ground Truth Preference Probabilities\n    gt_preferences = np.array([\n        sigmoid(forrester_function(x_prime) - forrester_function(x))\n        for x, x_prime in pairs\n    ]).reshape(X.shape)\n\n    # GP-Predicted Preferences\n    gp_predictions = gp.predict_proba(pairs)[:, 1].reshape(X.shape)\n\n    # Plot Ground Truth Preference Heatmap\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.contourf(X, X_prime, gt_preferences, levels=50, cmap='jet')\n    plt.colorbar(label=\"Ground Truth Preference Probability\")\n    plt.title(\"Ground Truth Preference Heatmap\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"x'\")\n\n    print(f'Chosen duels: {train_X[-n_iterations:]}')\n\n    # Plot GP-Predicted Preference Heatmap\n    plt.subplot(1, 2, 2)\n    plt.contourf(X, X_prime, gp_predictions, levels=50, cmap='jet')\n    plt.colorbar(label=\"GP-Predicted Preference Probability\")\n    plt.scatter(train_X[-n_iterations:, 0], train_X[-n_iterations:, 1], c='black', label=\"Last Iterations\", s=30, marker='x')\n    plt.title(\"GP-Predicted Preference Heatmap\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"x'\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\nQuestion 2: Linear Dueling Bandit (30 points)\nIn the linear dueling bandit problem, feedback is provided through pairwise comparisons between actions, rather than direct rewards. Consider a finite set of \\(K\\) actions, each represented by a feature vector \\(x_1, x_2, \\dots, x_K \\in \\mathbb{R}^d\\). Let the unknown preference scores be \\(f(x_i) = \\theta^\\top x_i\\) and \\(f(x_j) = \\theta^\\top x_j\\), where \\(\\theta \\in \\mathbb{R}^d\\) is an unknown parameter vector. The goal is to identify the best action by iteratively comparing pairs of actions while minimizing cumulative regret. Using qEUBO from https://arxiv.org/pdf/2303.15746, complete the following:\n\nAcquisition Functions for Regret Minimization (Written, 10 points): Write out the expression for the acquisition function Expected Improvement discussed in Q1 and qEUBO in the context of the linear dueling bandit. Discuss conditions under which each acquisition function could outperform the others in minimizing cumulative regret.\nExperimental Evaluation of Acquisition Functions (Written + Coding, 10 points): Benchmark the performance of the two acquisition functions experimentally.\n\nFinish implementing the acquisition functions in a linear dueling bandit simulation with \\(K = 10\\) and \\(d = 5\\), using synthetic data by completing the function calculate_regret_from_gp in linear_dueling/run.py.\nMeasure and compare cumulative regret over \\(T = 200\\) rounds for each acquisition function.\nReport and analyze the empirical regret curves, discussing any notable performance differences.\n\nEffect of Dimensionality on Regret (Written + Coding, 10 points): Analyze how increasing feature dimensionality impacts regret.\n\nExperimentally evaluate the regret for different values of \\(d\\) (e.g., \\(d = 5, 10, 20\\)) while keeping \\(K\\) constant.\nPlot the regret against \\(d\\) and explain any observed trends.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nfrom __future__ import annotations\n\nfrom typing import Optional\nimport itertools\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\nfrom tqdm import tqdm\nfrom botorch.acquisition.preference import qExpectedUtilityOfBestOption\nfrom botorch.acquisition.logei import qLogExpectedImprovement\nfrom botorch.fit import fit_gpytorch_mll\nfrom botorch.models.gpytorch import GPyTorchModel\nfrom botorch.utils.sampling import draw_sobol_samples\nfrom botorch.sampling import SobolQMCNormalSampler\nfrom botorch.posteriors.gpytorch import GPyTorchPosterior\nfrom gpytorch.distributions import base_distributions\nfrom gpytorch.likelihoods import Likelihood\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import Kernel, RBFKernel, ScaleKernel\nfrom gpytorch.mlls.variational_elbo import VariationalELBO\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.priors.torch_priors import GammaPrior\nfrom gpytorch.variational import (\n    CholeskyVariationalDistribution,\n    UnwhitenedVariationalStrategy,\n    VariationalStrategy,\n)\n\n\nclass PreferentialSoftmaxLikelihood(Likelihood):\n    r\"\"\"\n    Implements the softmax likelihood used for GP-based preference learning.\n\n    .. math::\n        p(\\mathbf y \\mid \\mathbf f) = \\text{Softmax} \\left( \\mathbf f \\right)\n\n    :param int num_alternatives: Number of alternatives (i.e., q).\n    \"\"\"\n\n    def __init__(self, num_alternatives):\n        super().__init__()\n        self.num_alternatives = num_alternatives\n        self.noise = torch.tensor(1e-4)  # This is only used to draw RFFs-based\n        # samples. We set it close to zero because we want noise-free samples\n        self.sampler = SobolQMCNormalSampler(\n            sample_shape=torch.Size([512]))  # This allows for\n        # SAA-based optimization of the ELBO\n\n    def _draw_likelihood_samples(\n        self, function_dist, *args, sample_shape=None, **kwargs\n    ):\n        function_samples = self.sampler(\n            GPyTorchPosterior(function_dist)).squeeze(-1)\n        return self.forward(function_samples, *args, **kwargs)\n\n    def forward(self, function_samples, *params, **kwargs):\n        function_samples = function_samples.reshape(\n            function_samples.shape[:-1]\n            + torch.Size(\n                (\n                    int(function_samples.shape[-1] / self.num_alternatives),\n                    self.num_alternatives,\n                )\n            )\n        )  # Reshape samples as if they came from a multi-output model (with `q` outputs)\n        num_alternatives = function_samples.shape[-1]\n\n        if num_alternatives != self.num_alternatives:\n            raise RuntimeError(\"There should be %d points\" %\n                               self.num_alternatives)\n\n        res = base_distributions.Categorical(\n            logits=function_samples)  # Passing the\n        # function values as logits recovers the softmax likelihood\n        return res\n\n\nclass VariationalPreferentialGP(GPyTorchModel, ApproximateGP):\n    def __init__(\n        self,\n        queries: Tensor,\n        responses: Tensor,\n        use_withening: bool = True,\n        covar_module: Optional[Kernel] = None,\n    ) -&gt; None:\n        r\"\"\"\n        Args:\n            queries: A `n x q x d` tensor of training inputs. Each of the `n` queries is constituted\n                by `q` `d`-dimensional decision vectors.\n            responses: A `n x 1` tensor of training outputs. Each of the `n` responses is an integer\n                between 0 and `q-1` indicating the decision vector selected by the user.\n            use_withening: If true, use withening to enhance variational inference.\n            covar_module: The module computing the covariance matrix.\n        \"\"\"\n        self.queries = queries\n        self.responses = responses\n        self.input_dim = queries.shape[-1]\n        self.q = queries.shape[-2]\n        self.num_data = queries.shape[-3]\n        train_x = queries.reshape(\n            queries.shape[0] * queries.shape[1], queries.shape[2]\n        )  # Reshape queries in the form of \"standard training inputs\"\n        train_y = responses.squeeze(-1)  # Squeeze out output dimension\n        bounds = torch.tensor(\n            [[0, 1] for _ in range(self.input_dim)], dtype=torch.double\n        ).T  # This assumes the input space has been normalized beforehand\n        # Construct variational distribution and strategy\n        if use_withening:\n            inducing_points = draw_sobol_samples(\n                bounds=bounds,\n                n=2 * self.input_dim,\n                q=1,\n                seed=0,\n            ).squeeze(1)\n            inducing_points = torch.cat([inducing_points, train_x], dim=0)\n            variational_distribution = CholeskyVariationalDistribution(\n                inducing_points.size(-2)\n            )\n            variational_strategy = VariationalStrategy(\n                self,\n                inducing_points,\n                variational_distribution,\n                learn_inducing_locations=False,\n            )\n        else:\n            inducing_points = train_x\n            variational_distribution = CholeskyVariationalDistribution(\n                inducing_points.size(-2)\n            )\n            variational_strategy = UnwhitenedVariationalStrategy(\n                self,\n                inducing_points,\n                variational_distribution,\n                learn_inducing_locations=False,\n            )\n        super().__init__(variational_strategy)\n        self.likelihood = PreferentialSoftmaxLikelihood(\n            num_alternatives=self.q)\n        self.mean_module = ConstantMean()\n        scales = bounds[1, :] - bounds[0, :]\n\n        if covar_module is None:\n            self.covar_module = ScaleKernel(\n                RBFKernel(\n                    ard_num_dims=self.input_dim,\n                    lengthscale_prior=GammaPrior(3.0, 6.0 / scales),\n                ),\n                outputscale_prior=GammaPrior(2.0, 0.15),\n            )\n        else:\n            self.covar_module = covar_module\n        self._num_outputs = 1\n        self.train_inputs = (train_x,)\n        self.train_targets = train_y\n\n    def forward(self, X: Tensor) -&gt; MultivariateNormal:\n        mean_X = self.mean_module(X)\n        covar_X = self.covar_module(X)\n        return MultivariateNormal(mean_X, covar_X)\n\n    @property\n    def num_outputs(self) -&gt; int:\n        r\"\"\"The number of outputs of the model.\"\"\"\n        return 1\n\n\n# Objective function for pairwise comparisons\ndef f(x):\n    \"\"\"\n    Computes the preference score for a given action.\n\n    Args:\n        x (torch.Tensor): A feature vector of dimension `d`.\n\n    Returns:\n        torch.Tensor: The computed preference score.\n    \"\"\"\n    return x @ theta_true\n\n# Simulate pairwise comparisons\n\n\ndef simulate_comparison(x1, x2):\n    \"\"\"\n    Simulates a pairwise comparison between two actions based on their preference scores.\n\n    Args:\n        x1 (torch.Tensor): Feature vector of the first action.\n        x2 (torch.Tensor): Feature vector of the second action.\n\n    Returns:\n        torch.Tensor: The feature vector of the preferred action.\n    \"\"\"\n    prob_x1 = torch.sigmoid(f(x1) - f(x2))\n    return x1 if torch.rand(1).item() &lt; prob_x1 else x2\n\n# Function to fit a Variational GP model\n\n\ndef fit_variational_gp(train_X, train_Y):\n    \"\"\"\n    Fits a Variational Gaussian Process (GP) model to the given training data.\n\n    Args:\n        train_X (torch.Tensor): Training feature pairs of shape [n, 2, d].\n        train_Y (torch.Tensor): Training preferences of shape [n, 1].\n\n    Returns:\n        VariationalPreferentialGP: A fitted GP model.\n    \"\"\"\n    queries = train_X.reshape(train_X.shape[0], 2, d)\n    responses = train_Y\n    return fit_model(queries, responses)\n\n\ndef fit_model(queries, responses):\n    \"\"\"\n    Internal helper to train a VariationalPreferentialGP.\n\n    Args:\n        queries (torch.Tensor): Training feature pairs.\n        responses (torch.Tensor): Training responses (preferences).\n\n    Returns:\n        VariationalPreferentialGP: Trained GP model.\n    \"\"\"\n    model = VariationalPreferentialGP(queries, responses)\n    model.train()\n    model.likelihood.train()\n    mll = VariationalELBO(\n        likelihood=model.likelihood,\n        model=model,\n        num_data=2 * model.num_data,\n    )\n    fit_gpytorch_mll(mll)\n    model.eval()\n    model.likelihood.eval()\n    return model\n\n# Acquisition function definition\n\n\ndef get_acquisition_functions(gp):\n    \"\"\"\n    Returns acquisition functions (qLogEI and qEUBO) for a given GP model.\n\n    Args:\n        gp (VariationalPreferentialGP): The fitted GP model.\n\n    Returns:\n        tuple: qLogExpectedImprovement and qExpectedUtilityOfBestOption acquisition functions.\n    \"\"\"\n    with torch.no_grad():\n        posterior = gp.posterior(gp.train_inputs[0])\n        best_f = posterior.mean.squeeze(-1).max()\n\n    qLogEI = qLogExpectedImprovement(model=gp, best_f=best_f)\n    qEUBO = qExpectedUtilityOfBestOption(pref_model=gp)\n    return qLogEI, qEUBO\n\n# Evaluate acquisition function on pairs\n\n\ndef evaluate_acquisition_on_pairs(acq_function, arms):\n    \"\"\"\n    Computes acquisition values for all possible pairs of arms.\n\n    Args:\n        acq_function: The acquisition function to evaluate.\n        arms (torch.Tensor): All available arms (feature vectors).\n\n    Returns:\n        tuple: A list of pairs and their corresponding acquisition values.\n    \"\"\"\n    pairs = list(itertools.combinations(arms, 2))\n    pair_values = []\n    with torch.no_grad():\n        for x1, x2 in pairs:\n            pair = torch.stack([x1, x2]).unsqueeze(0)\n            pair_values.append(acq_function(pair))\n    return pairs, torch.tensor(pair_values)\n\n# Regret calculation\n\n\ndef calculate_regret_from_gp(gp, actions):\n    \"\"\"\n    Computes the regret for the current GP model.\n\n    Args:\n        gp (VariationalPreferentialGP): The fitted GP model.\n        actions (torch.Tensor): Feature vectors of arms.\n\n    Returns:\n        torch.Tensor: The calculated regret.\n    \"\"\"\n    # YOUR CODE HERE (~6 lines)\n    # Compare the ground truth optimal arm to the GP's believed best arm\n    # Hint: To find GP believed best arm in expectation, use gp.posterior which returns with a mean property.\n    pass\n    # END OF YOUR CODE\n\n\nif __name__ == \"__main__\":\n    # Set default tensor precision\n    torch.set_default_dtype(torch.double)\n\n    # Problem settings\n    torch.manual_seed(55)\n    K = 30  # Number of arms (discrete choices)\n    d = 2   # Dimensionality of feature vectors\n    T = 100  # Number of rounds (iterations)\n    bounds = torch.tensor([[0.0] * d, [1.0] * d])  # Bounds for action space\n\n    # Generate random actions (feature vectors)\n    actions = torch.rand(K, d)\n\n    # Ground-truth preference parameter (unknown to the model)\n    theta_true = torch.ones(d)\n\n    # Generate initial observations\n    n_initial = 5\n    indices = torch.randint(0, K, (n_initial, 2))\n    train_X_logei = actions[indices]  # Shape: [n_initial, 2, d]\n    train_X_qeubo = train_X_logei.clone()\n    train_X_random = train_X_logei.clone()\n    train_Y_logei = torch.tensor([[0.0 if simulate_comparison(x1, x2).equal(x1) else 1.0]\n                                  for x1, x2 in train_X_logei])\n    train_Y_qeubo = train_Y_logei.clone()\n    train_Y_random = train_Y_logei.clone()\n\n    # Optimization loop\n    cumulative_regret_logei = []\n    cumulative_regret_qeubo = []\n    cumulative_regret_random = []\n\n    for t in tqdm(range(T)):\n        # Fit GP models\n        gp_logei = fit_variational_gp(train_X_logei, train_Y_logei)\n        gp_qeubo = fit_variational_gp(train_X_qeubo, train_Y_qeubo)\n        gp_random = fit_variational_gp(train_X_random, train_Y_random)\n\n        # Define acquisition functions\n        qLogEI, _ = get_acquisition_functions(gp_logei)\n        _, qEUBO = get_acquisition_functions(gp_qeubo)\n\n        # Evaluate acquisition functions\n        pairs_logei, acq_values_logei = evaluate_acquisition_on_pairs(\n            qLogEI, actions)\n        pairs_qeubo, acq_values_qeubo = evaluate_acquisition_on_pairs(\n            qEUBO, actions)\n\n        # Select pairs based on acquisition values\n        best_pair_idx_logei = torch.argmax(acq_values_logei)\n        best_pair_idx_qeubo = torch.argmax(acq_values_qeubo)\n        x1_logei, x2_logei = pairs_logei[best_pair_idx_logei]\n        x1_qeubo, x2_qeubo = pairs_qeubo[best_pair_idx_qeubo]\n\n        # Random pair selection\n        random_indices = torch.randint(0, K, (2,))\n        x1_random = actions[random_indices[0]]\n        x2_random = actions[random_indices[1]]\n\n        # Simulate comparisons\n        selected_logei = simulate_comparison(x1_logei, x2_logei)\n        selected_qeubo = simulate_comparison(x1_qeubo, x2_qeubo)\n        selected_random = simulate_comparison(x1_random, x2_random)\n\n        # Update training data\n        train_X_logei = torch.cat(\n            [train_X_logei, torch.stack([x1_logei, x2_logei]).unsqueeze(0)])\n        train_Y_logei = torch.cat([train_Y_logei, torch.tensor(\n            [[0.0 if selected_logei.equal(x1_logei) else 1.0]])])\n        train_X_qeubo = torch.cat(\n            [train_X_qeubo, torch.stack([x1_qeubo, x2_qeubo]).unsqueeze(0)])\n        train_Y_qeubo = torch.cat([train_Y_qeubo, torch.tensor(\n            [[0.0 if selected_qeubo.equal(x1_qeubo) else 1.0]])])\n        train_X_random = torch.cat(\n            [train_X_random, torch.stack([x1_random, x2_random]).unsqueeze(0)])\n        train_Y_random = torch.cat([train_Y_random, torch.tensor(\n            [[0.0 if selected_random.equal(x1_random) else 1.0]])])\n\n        # Calculate regrets\n        regret_logei = calculate_regret_from_gp(gp_logei, actions)\n        regret_qeubo = calculate_regret_from_gp(gp_qeubo, actions)\n        regret_random = calculate_regret_from_gp(gp_random, actions)\n\n        print(f'Regret LogEI: {regret_logei}')\n        print(f'Regret qEUBO: {regret_qeubo}')\n        print(f'Regret Random: {regret_random}')\n\n        cumulative_regret_logei.append(regret_logei)\n        cumulative_regret_qeubo.append(regret_qeubo)\n        cumulative_regret_random.append(regret_random)\n\n    # Plot cumulative regret\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_logei), dim=0), label='qLogEI')\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_qeubo), dim=0), label='qEUBO')\n    plt.plot(torch.cumsum(torch.tensor(\n        cumulative_regret_random), dim=0), label='Random')\n    plt.xlabel('Round')\n    plt.ylabel('Cumulative Regret')\n    plt.legend()\n    plt.title('Comparison of qLogEI, qEUBO, and Random Sampling')\n    plt.show()\n\n\n\n\n\n\nQuestion 3: Multi-Objective Thompson Sampling in Linear Contextual Bandits (30 points)\nThompson Sampling (TS) is commonly used for reward maximization in multi-armed bandit problems, optimizing for the expected reward across actions. However, in many real-world scenarios, other objectives, such as the interpretability or reusability of learned parameters, are equally valuable. This is particularly relevant when modeling unknown reward functions with parameters that might offer insights or inform future experiments. A purely reward-focused Thompson Sampling approach may result in increased false positive rates due to aggressive exploitation, whereas a pure exploration approach—such as those used in active learning—might better suit the goal of parameter learning.\nAssume a multi-objective setting where the goal is to not only maximize the cumulative reward but also to accurately learn the parameters of the reward function itself in a linear contextual bandit setting. Let each arm be represented by a feature vector \\(x \\in \\mathbb{R}^d\\), with rewards generated by an unknown linear model \\(r = \\theta^\\top x + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). Given these considerations, answer the following:\n\nTheoretical Analysis of Multi-Objective Thompson Sampling (8 points)\n\n(Written, 3 points). Define a cumulative regret objective that balances maximizing the expected reward and minimizing the parameter estimation error \\(\\|\\theta - \\hat{\\theta}\\|_2\\). Explain how this multi-objective regret differs from the single-objective regret typically used in linear bandits.\n(Written, 3 points). Derive the expected regret bounds for Thompson Sampling in the single-objective case and describe the additional challenges posed when extending these bounds to the multi-objective case.\n(Written, 2 points). Suppose you were to use a pure exploration approach for parameter estimation. Provide an upper bound for the parameter error \\(\\|\\theta - \\hat{\\theta}\\|_2\\) over \\(T\\) rounds.\n\nAcquisition Strategies for Multi-Objective Optimization (8 points)\n\n(Written, 3 points). Explain how to adapt the Upper Confidence Bound (UCB) acquisition function to balance exploration and exploitation for parameter learning alongside reward maximization. Discuss the effect of tuning parameters on exploration.\n(Written + Coding, 3 points). Implement a Thompson Sampling acquisition strategy that alternates between reward maximization and parameter-focused exploration using a multi-objective UCB. Implement the select_arm function of multi_obj_thompson/bandit.py.\n(Written, 2 points). Describe the impact of this alternating acquisition strategy on false positive rates and regret in comparison to standard Thompson Sampling.\n\nPosterior Distribution Analysis (8 points)\n\n(Written, 2 points). Given a prior distribution for \\(\\theta\\) and observed rewards, derive the posterior distribution of \\(\\theta\\) at each time step in the context of multi-objective Thompson Sampling. Explain any assumptions needed for computational tractability.\n(Coding, 4 points). Implement a Bayesian update for the posterior of \\(\\theta\\) following each observation. Do this in update.\n(Written, 2 points). Explain how this posterior update accommodates both exploration for parameter estimation and exploitation for reward maximization.\n\nEmpirical Evaluation (6 points)\n\n(Coding, 3 points). Design and conduct an experiment comparing standard Thompson Sampling, pure exploration, and your multi-objective TS algorithm. Run this experiment on a synthetic dataset with \\(d = 5\\) features and \\(K = 10\\) arms by executing run.py.\n(Written, 3 points). Report and interpret the results by comparing the cumulative reward and parameter estimation error across methods. Provide insights on the trade-offs observed and any patterns in the rate of regret reduction.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass MultiObjectiveThompsonSamplingBandit:\n    \"\"\"\n    A class that implements a multi-objective Thompson sampling bandit.\n\n    Attributes:\n    - d (int): Dimension of the feature vector x.\n    - lambda_prior (float): Regularization parameter for the prior covariance matrix.\n    - sigma_noise (float): Standard deviation of the noise in rewards.\n    - mu (np.array): Prior mean of theta (initialized as a zero vector).\n    - Sigma (np.array): Prior covariance of theta (initialized as a scaled identity matrix).\n    \"\"\"\n\n    def __init__(self, d, lambda_prior=1.0, sigma_noise=1.0):\n        \"\"\"\n        Initializes the bandit with a prior on theta and noise variance.\n\n        Parameters:\n        - d (int): Dimension of the feature vector x.\n        - lambda_prior (float): Regularization parameter for the prior covariance matrix.\n        - sigma_noise (float): Standard deviation of the noise in rewards.\n        \"\"\"\n        self.d = d\n        self.lambda_prior = lambda_prior\n        self.sigma_noise = sigma_noise\n\n        # Initialize prior mean and covariance matrix\n        self.mu = np.zeros(d)  # Prior mean of theta\n        self.Sigma = lambda_prior * np.eye(d)  # Prior covariance of theta\n\n    def select_arm(self, arms, mode):\n        \"\"\"\n        Selects an arm (action) based on the specified mode.\n\n        Parameters:\n        - arms (np.array): A 2D NumPy array of shape (K, d) representing the feature vectors of K arms.\n        - mode (str): Selection mode, either 'exploit' (reward maximization) or 'explore' (focus on reducing uncertainty in theta).\n\n        Returns:\n        - selected_arm (np.array): The feature vector of the selected arm.\n        - arm_index (int): The index of the selected arm.\n        \"\"\"\n        # Sample a belief for theta from the current posterior\n        theta_sample = np.random.multivariate_normal(self.mu, self.Sigma)\n\n        # Generate reward noise for the arms\n        reward_noise = np.random.normal(0, self.sigma_noise, size=len(arms))\n\n        if mode == 'exploit':\n            # YOUR CODE HERE (~2 lines)\n                # 1. Compute expected rewards with noise\n                # 2. Select the arm with the highest expected reward\n                pass \n            # END OF YOUR CODE\n        elif mode == 'explore':\n            # Compute posterior covariance norms to evaluate exploration potential for each arm\n            posterior_cov_norms = []\n            for x in arms:\n                x = x.reshape(-1, 1)  # Reshape to column vector\n\n                # Find posterior covariance hypothetically and get its norm\n                # YOUR CODE HERE (~4 lines)\n                pass\n                # END OF YOUR CODE\n\n                posterior_cov_norms.append(norm)\n\n            # Select the arm that minimizes the posterior covariance norm\n            arm_index = np.argmin(posterior_cov_norms)\n\n        else:\n            raise ValueError(\"Mode must be either 'exploit' or 'explore'.\")\n\n        return arms[arm_index], arm_index, posterior_cov_norms if mode == 'explore' else None\n\n    def update(self, x_t, r_t):\n        \"\"\"\n        Updates the posterior distribution of theta given a new observation.\n\n        Parameters:\n        - x_t (np.array): Feature vector of the selected arm at time t.\n        - r_t (float): Observed reward at time t.\n        \"\"\"\n        x_t = x_t.reshape(-1, 1)  # Reshape to column vector\n\n        # YOUR CODE HERE (~4 lines)\n        # Obtain mu_new and Sigma_new of theta posterior. This requires doing some math!\n        pass\n        # END OF YOUR CODE\n\n        # Update internal state\n        self.mu = mu_new.flatten()\n        self.Sigma = Sigma_new\n\nif __name__ == '__main__':\n    # Number of features (dimension) and arms\n    d = 5  # Feature dimension\n    K = 10  # Number of arms\n\n    # Generate random arms (feature vectors)\n    np.random.seed(42)\n    arms = np.random.randn(K, d)\n\n    # True theta (unknown to the bandit)\n    theta_true = np.random.randn(d)\n\n    # Initialize the bandit\n    bandit = MultiObjectiveThompsonSamplingBandit(d)\n\n    # Number of rounds\n    T = 1000\n\n    # Lists to store results\n    regrets = []  # Store the regret at each round\n    theta_errors = []  # Store the error between estimated and true theta\n\n    # Simulation loop\n    for t in range(T):\n        # Alternate between 'exploit' and 'explore' modes\n        mode = 'exploit' if t % 2 == 0 else 'explore'\n\n        # Select an arm based on the current mode\n        x_t, arm_index, _ = bandit.select_arm(arms, mode=mode)\n\n        # Observe the reward with noise\n        r_t = theta_true @ x_t + np.random.normal(0, bandit.sigma_noise)\n\n        # Update the bandit with the new observation\n        bandit.update(x_t, r_t)\n\n        # Compute regret (difference between optimal reward and received reward)\n        optimal_reward = np.max(arms @ theta_true)  # Best possible reward\n        regret = optimal_reward - (theta_true @ x_t)  # Regret for this round\n        regrets.append(regret)\n\n        # Compute parameter estimation error (distance between true and estimated theta)\n        theta_error = np.linalg.norm(theta_true - bandit.mu)\n        theta_errors.append(theta_error)\n\n    # Final estimates after all rounds\n    mu_estimate, Sigma_estimate = bandit.mu, bandit.Sigma\n\n    # Print results\n    print(\"Estimated theta:\", mu_estimate)\n    print(\"True theta:\", theta_true)\n    print(\"Cumulative regret:\", np.sum(regrets))\n    print(\"Final covariance norm:\", np.linalg.norm(Sigma_estimate))\n\n    # Visualization of results\n\n    # Plot cumulative regret over time\n    plt.figure()\n    plt.plot(np.cumsum(regrets))\n    plt.title('Cumulative Regret over Time')\n    plt.xlabel('Rounds')\n    plt.ylabel('Cumulative Regret')\n    plt.show()\n\n    # Plot estimation error over time\n    plt.figure()\n    plt.plot(theta_errors)\n    plt.title('Theta Estimation Error over Time')\n    plt.xlabel('Rounds')\n    plt.ylabel('Estimation Error (L2 Norm)')\n    plt.show()\n\n\n\n\n\n\nQuestion 4: Mechanism Design in Preference Learning (30 points)\nIn mechanism design, a central challenge is optimizing resource allocation while accounting for user preferences, which may be private and complex. This problem can be addressed using learning techniques to infer user preferences, thereby enabling the designer to make informed pricing and allocation decisions. Consider a scenario where a designer allocates a divisible resource \\(B\\) among \\(N\\) players, each with a private, continuous, concave utility function \\(U_i(x_i)\\) over their allocated share \\(x_i\\), where \\(x = [x_1, x_2, \\dots, x_N]\\) denotes the allocation vector. The designer aims to maximize social welfare while ensuring full resource utilization.\n\nModeling User Preferences (7 points):\n\n(Written, 1 point) Provide a realistic scenario in which we estimate a utility function through eliciting preferences in the context of the mechanism.\n(Written, 3 point) Explain how elliptical slice sampling can be used with a GP in order to estimate a utility function through preferences.\n(Written, 3 point) How can the elliptical slice posterior samples be used to obtain the mean of the posterior predictive for test points? (Hint: Read page \\(44\\) of https://gaussianprocess.org/gpml/chapters/RW.pdf.)\n\nOptimization with Learned Preferences (10 points):\n\n(Written, 3 point) Formulate the designer’s optimization problem, maximizing social welfare \\(\\sum_{i=1}^N U_i(x_i)\\) subject to the constraint \\(\\sum_{i=1}^N x_i \\leq B\\).\n(Written, 4 point) Using the Lagrange multiplier method, derive the conditions that must be met for optimal allocation and pricing.\n(Written, 3 point) As an alternative approach to Lagrange multipliers, explain how projected gradient descent (PGD) can be used to solve the designer’s optimization problem.\n\nBenchmarking Learning and Allocation Efficiency (13 points):\n\n(Coding, 3 point) Implement preference_loglik in the file gp_mechanism/preference_gp.py.\n(Coding, 3 point) Implement predictive_function.\n(Coding, 3 point) Implement optimize_allocations inside gp_mechanism/run.py.\n(Written, 4 point) Compare GP-approximated utility allocations through PGD, exact utility allocations through PGD, and the optimal Lagrange-based allocation done by hand with each other for your choice of utility functions \\(U_i\\). Make sure your utilities are continuous and concave.\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nfrom typing import Callable\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch  # Import PyTorch\nfrom tqdm import tqdm\n\n\nclass EllipticalSliceSampler:\n    def __init__(self,\n                 prior_cov: np.ndarray,\n                 loglik: Callable):\n        \"\"\"\n        Initializes the Elliptical Slice Sampler.\n\n        Args:\n        - prior_cov (np.ndarray): Prior covariance matrix.\n        - loglik (Callable): Log-likelihood function.\n        \"\"\"\n        self.prior_cov = prior_cov\n        self.loglik = loglik\n\n        self._n = prior_cov.shape[0]  # Dimensionality of the space\n        # Cache Cholesky decomposition\n        self._chol = np.linalg.cholesky(prior_cov)\n\n        # Initialize state and cache previous states\n        self._state_f = self._chol @ np.random.randn(self._n)\n\n    def _indiv_sample(self):\n        \"\"\"\n        Main algorithm for generating an individual sample using Elliptical Slice Sampling.\n        \"\"\"\n        f = self._state_f  # Previous state\n        # Sample from prior for the ellipse\n        nu = self._chol @ np.random.randn(self._n)\n        log_y = self.loglik(f) + np.log(np.random.uniform()\n                                        )  # Log-likelihood threshold\n\n        theta = np.random.uniform(0., 2 * np.pi)  # Initial proposal angle\n        theta_min, theta_max = theta - 2 * np.pi, theta  # Define bracketing interval\n\n        # Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.\n        while True:\n            # YOUR CODE HERE (~10 lines)\n            # Generate a new sample point based on the current angle.\n            f_prime = f * np.cos(theta) + nu * np.sin(theta)\n\n            # Check if the proposed point meets the acceptance criterion.\n            if self.loglik(f_prime) &gt; log_y:  # Accept the sample\n                self._state_f = f_prime\n                return\n\n            else:  # If not accepted, adjust the bracket and select a new angle.\n                if theta &lt; 0:\n                    theta_min = theta\n                else:\n                    theta_max = theta\n                theta = np.random.uniform(theta_min, theta_max)\n            # END OF YOUR CODE\n\n    def sample(self,\n               n_samples: int,\n               n_burn: int = 500) -&gt; np.ndarray:\n        \"\"\"\n        Generates samples using Elliptical Slice Sampling.\n\n        Args:\n        - n_samples (int): Total number of samples to return.\n        - n_burn (int): Number of initial samples to discard (burn-in).\n\n        Returns:\n        - np.ndarray: Array of samples after burn-in.\n        \"\"\"\n        samples = []\n        for i in tqdm(range(n_samples), desc=\"Sampling\"):\n            self._indiv_sample()\n            if i &gt; n_burn:\n                # Store sample post burn-in\n                samples.append(self._state_f.copy())\n\n        return np.stack(samples)\n\n\ndef squared_exponential_cov_torch(X1, X2, length_scale=1.0, variance=1.0):\n    \"\"\"\n    Squared Exponential (RBF) Covariance Function using PyTorch tensors.\n\n    Args:\n        X1 (torch.Tensor): First set of input points.\n        X2 (torch.Tensor): Second set of input points.\n        length_scale (float): Length scale of the kernel.\n        variance (float): Variance (amplitude) of the kernel.\n\n    Returns:\n        torch.Tensor: Covariance matrix between X1 and X2.\n    \"\"\"\n    X1 = X1.reshape(-1, 1)\n    X2 = X2.reshape(-1, 1)\n    dists = torch.sum(X1**2, dim=1).reshape(-1, 1) + \\\n        torch.sum(X2**2, dim=1) - 2 * torch.mm(X1, X2.T)\n    return variance * torch.exp(-0.5 * dists / length_scale**2)\n\n\ndef generate_preferences(x_pairs, utility_fn):\n    \"\"\"\n    Generates preference labels based on the Bradley-Terry model.\n\n    Args:\n        x_pairs (np.array): Array of preference pairs, shape [n_pairs, 2].\n        utility_fn (function): Ground truth utility function.\n\n    Returns:\n        np.array: Preference labels (1 if the first item in the pair is preferred, 0 otherwise).\n    \"\"\"\n    preference_labels = []\n    for x1, x2 in x_pairs:\n        u1, u2 = utility_fn(x1), utility_fn(x2)\n        prob = np.exp(u1) / (np.exp(u1) + np.exp(u2))\n        preference_labels.append(1 if np.random.rand() &lt; prob else 0)\n    return np.array(preference_labels)\n\n\ndef create_predictive_function(ground_truth_utility, num_pairs=3000, n_samples=100, n_burn=50, length_scale=2.0, variance=0.5):\n    \"\"\"\n    Creates a predictive function to compute the posterior predictive mean of a Gaussian Process.\n\n    Args:\n        ground_truth_utility (function): The ground truth utility function for generating preferences.\n        num_pairs (int): Number of random preference pairs to generate.\n        n_samples (int): Number of samples for Elliptical Slice Sampling.\n        n_burn (int): Number of burn-in samples for Elliptical Slice Sampling.\n        length_scale (float): Length scale for the Squared Exponential Kernel.\n        variance (float): Variance (amplitude) of the Squared Exponential Kernel.\n\n    Returns:\n        function: A predictive function that computes the posterior predictive mean.\n    \"\"\"\n    # Generate random preference pairs\n    np.random.seed(42)\n    x_pairs = np.random.uniform(0, 10, size=(num_pairs, 2))\n    X_flat = x_pairs.flatten()\n\n    # Generate preference labels\n    preference_labels = generate_preferences(x_pairs, ground_truth_utility)\n\n    # Convert X_flat to PyTorch tensor\n    X_flat_torch = torch.tensor(X_flat, dtype=torch.float32)\n\n    # GP Prior (using PyTorch)\n    K_torch = squared_exponential_cov_torch(\n        X_flat_torch, X_flat_torch, length_scale=length_scale, variance=variance)\n    # Add jitter for numerical stability\n    K_torch += 1e-2 * torch.eye(len(X_flat_torch))\n    prior_cov = K_torch.numpy()  # Convert back to numpy for the sampler\n\n    # Log-likelihood function\n    def preference_loglik(f):\n        \"\"\"\n        Computes the log-likelihood of the preferences under the Bradley-Terry model.\n\n        Args:\n            f (np.array): Latent utility values.\n\n        Returns:\n            float: Log-likelihood of the given latent utilities.\n        \"\"\"\n        log_likelihood = 0.0\n        for (x1, x2), label in zip(x_pairs, preference_labels):\n            idx1 = np.where(X_flat == x1)[0][0]\n            idx2 = np.where(X_flat == x2)[0][0]\n            f1, f2 = f[idx1], f[idx2]\n\n            # YOUR CODE HERE (~4 lines)\n            # Add datapoint log likelihood using Bradley-Terry model\n            pass\n            # END OF YOUR CODE\n        return log_likelihood\n\n    # Elliptical Slice Sampling\n    sampler = EllipticalSliceSampler(\n        prior_cov=prior_cov, loglik=preference_loglik)\n    posterior_samples = sampler.sample(n_samples=n_samples, n_burn=n_burn)\n    posterior_mean = np.mean(posterior_samples, axis=0)\n\n    # Convert posterior_mean to PyTorch tensor\n    posterior_mean_torch = torch.tensor(posterior_mean, dtype=torch.float32)\n\n    # Compute K_inv using PyTorch\n    K_inv_torch = torch.inverse(K_torch)\n\n    # Define the predictive function\n    def predictive_function(x):\n        \"\"\"\n        Predicts the utility for new input points.\n\n        Args:\n            x (torch.Tensor): Input points to predict utilities for.\n\n        Returns:\n            torch.Tensor: Predicted expected utilities.\n        \"\"\"\n        if not torch.is_tensor(x):\n            raise ValueError('Predictive function must take in torch.tensor')\n        x = x.reshape(-1, 1)\n        X_flat_torch_reshaped = X_flat_torch.reshape(-1, 1)\n\n        # YOUR CODE HERE (~2 lines)\n        # Implement equation (3.21) on page 44 of https://gaussianprocess.org/gpml/chapters/RW.pdf\n        pass\n        # END OF YOUR CODE\n\n    return predictive_function\n\n\nif __name__ == \"__main__\":\n    # Ground truth utility function\n    def ground_truth_utility(x): return np.sin(x)\n\n    # Create the predictive function\n    predictive_fn = create_predictive_function(ground_truth_utility)\n\n    # Test the predictive function\n    X_test = torch.linspace(0, 10, 50).reshape(-1, 1)  # Test points\n    posterior_means = predictive_fn(\n        X_test).detach().numpy()  # Predicted posterior means\n\n    # Ground truth utilities\n    ground_truth_utilities = ground_truth_utility(X_test.numpy())\n\n    # Plot results\n    plt.figure(figsize=(10, 6))\n    plt.title(\"GP Posterior Predictive Mean (Utility Approximation)\")\n    plt.plot(X_test.numpy(), posterior_means,\n             label=\"Posterior Predictive Mean\", color=\"red\")\n    plt.scatter(X_test.numpy(), ground_truth_utilities,\n                label=\"Ground Truth Utility\", color=\"blue\", alpha=0.5)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Utility\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\nimport torch\nfrom preference_gp import create_predictive_function\n\n# Feel free to play around with continuous, concave utility functions!\ndef utility_1(x):\n    \"\"\"\n    Utility function 1: 3 * log(x + 1)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 3 * torch.log(x + 1)\n\ndef utility_2(x):\n    \"\"\"\n    Utility function 2: 5 * log(x + 2)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 5 * torch.log(x + 2)\n\ndef utility_3(x):\n    \"\"\"\n    Utility function 3: 8 * log(x + 3)\n    Args:\n        x (torch.Tensor): Input tensor of allocations.\n    Returns:\n        torch.Tensor: Computed utility values.\n    \"\"\"\n    return 8 * torch.log(x + 3)\n\ndef project(x, B):\n    \"\"\"\n    Projects the allocation vector `x` onto the feasible set {z | sum(z) = B, z &gt;= 0}.\n    This ensures that the allocations respect the resource constraint.\n\n    Args:\n        x (torch.Tensor): Current allocation vector.\n        B (float): Total available resource.\n\n    Returns:\n        torch.Tensor: Projected allocation vector.\n    \"\"\"\n    with torch.no_grad():\n        # Sort x in descending order\n        sorted_x, _ = torch.sort(x, descending=True)\n        \n        # Compute cumulative sum adjusted by B\n        cumulative_sum = torch.cumsum(sorted_x, dim=0) - B\n        \n        # Find the threshold (water-filling algorithm)\n        rho = torch.where(sorted_x - (cumulative_sum / torch.arange(1, len(x) + 1, dtype=torch.float32)) &gt; 0)[0].max().item()\n        theta = cumulative_sum[int(rho)] / (rho + 1)\n        \n        # Compute the projected allocation\n        return torch.clamp(x - theta, min=0)\n\ndef optimize_allocations(utilities, B, learning_rate, num_iterations):\n    \"\"\"\n    Optimizes the allocation of resources to maximize the total utility.\n\n    Args:\n        utilities (list): List of utility functions or GP-based predictive functions.\n        B (float): Total available resource.\n        learning_rate (float): Step size for gradient ascent.\n        num_iterations (int): Number of optimization iterations.\n\n    Returns:\n        torch.Tensor: Final resource allocations.\n    \"\"\"\n    # Initialize resource allocations equally\n    x = torch.tensor([1.0] * len(utilities), requires_grad=True)\n\n    # Optimization loop\n    for iteration in range(num_iterations):\n        # YOUR CODE HERE (~6 lines)\n        # 1. Compute total utility and backprop\n        # 2. Update x directly with x.grad\n        # 3. Project onto convex constraint set since we are using Projected Gradient Descent (PGD)\n        pass\n        # END OF YOUR CODE\n        \n        # Log progress every 10 iterations or at the last iteration\n        if iteration % 10 == 0 or iteration == num_iterations - 1:\n            print(f\"Iteration {iteration}: Total Utility = {total_utility.item():.4f}, Allocations = {x.data.numpy()}\")\n    \n    return x\n\nif __name__ == \"__main__\":\n    # Generate GP models for each utility\n    gp_1 = create_predictive_function(lambda x: utility_1(torch.tensor(x)).numpy())\n    gp_2 = create_predictive_function(lambda x: utility_2(torch.tensor(x)).numpy())\n    gp_3 = create_predictive_function(lambda x: utility_3(torch.tensor(x)).numpy())\n\n    # Combine utility GPs into a list for optimization\n    utilities = [gp_1, gp_2, gp_3]  # Use [utility_1, utility_2, utility_3] for exact utility functions\n\n    # Resource constraint and optimization settings\n    B = 10  # Total available resource\n    learning_rate = 0.1  # Gradient ascent step size\n    num_iterations = 2000  # Number of iterations\n\n    # Optimize allocations\n    final_allocations = optimize_allocations(utilities, B, learning_rate, num_iterations)\n\n    # Final results\n    print(\"\\nFinal allocations:\")\n    print(final_allocations.data.numpy())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Decisions</span>"
    ]
  },
  {
    "objectID": "src/chap6.html",
    "href": "src/chap6.html",
    "title": "6  Aggregation",
    "section": "",
    "text": "6.1 Social Choice Theory and Implications for AI Preference Aggregation\nIn many applications, human preferences must be aggregated across multiple individuals to determine a collective decision or ranking. This process is central to social choice theory, which provides a mathematical foundation for preference aggregation. Unlike individual preference modeling, which focuses on how a single person makes decisions, social choice theory addresses the challenge of combining multiple preference profiles into a single coherent outcome. A social welfare function (SWF) takes as input each individual’s preference ranking over a set of alternatives and produces a social ranking of those alternatives. A related concept is a social choice function (SCF), which selects a single winning alternative given individuals’ preferences. Many voting rules can be seen as social choice functions that aim to reflect the group’s preferences. Formally, let \\(N=\\{1,2,\\dots,n\\}\\) be a set of \\(n\\) voters (agents) and \\(A=\\{a_1,\\dots,a_m\\}\\) a set of \\(m\\) alternatives (with \\(m \\ge 3\\)). Each voter \\(i\\) has a preference order \\(\\succ_i\\) over \\(A\\). A social choice function is a mapping \\(f: (\\succ_1,\\dots,\\succ_n)\\mapsto A\\) that picks a winning alternative for each possible profile of individual preferences. A social welfare function is a mapping that produces a complete societal ranking \\(\\succ^*\\) of the alternatives. The central question is: can we design an aggregation rule that faithfully represents individual preferences while satisfying certain fairness or rationality axioms?\nMany common voting rules illustrate different methods of aggregation, each with its own merits and vulnerabilities:\nHowever, preference aggregation is not always straightforward. The Condorcet paradox illustrates that majority preferences can be cyclic (rock-paper-scissors style), so that no single alternative is majority-preferred to all others, violating transitivity. Different voting rules can yield different winners on the same profile, highlighting how the choice of rule influences the outcome. To guide the design of social choice functions, several desirable properties or axioms have been proposed. Three classical fairness criteria are:\nAdditionally, we assume an unrestricted domain (universal admissibility): individuals may have any transitive preference ordering over the \\(m\\) alternatives (no restrictions like single-peaked preferences unless explicitly imposed). One might hope that a fair voting rule exists that satisfies all the above properties for three or more alternatives. Surprisingly, a seminal negative result shows this is impossible.\nArrow’s Impossibility Theorem (Arrow 1951) is a cornerstone of social choice theory. It states that when there are three or more alternatives (\\(m\\ge 3\\)), no social welfare function can simultaneously satisfy Unanimity, IIA, and Non-dictatorship – unless it is a trivial dictatorial rule. In other words, any aggregation mechanism that is not dictatorial will inevitably violate at least one of the fairness criteria. The theorem is usually proven by contradiction: assuming a social welfare function satisfies all conditions, one can show that one voter’s preferences always decide the outcome, hence the rule is dictatorial. Intuitively, Arrow’s theorem is driven by the possibility of preference cycles in majority voting. Even if individual preferences are transitive, aggregated majorities can prefer \\(A\\) to \\(B\\), \\(B\\) to \\(C\\), and \\(C\\) to \\(A\\) in a cycle, as in the Condorcet paradox. Under Unanimity and IIA, the social ranking must locally match these pairwise preferences, but this produces a contradiction with transitivity unless one voter’s ranking is given overriding authority. A sketch of Arrow’s proof is as follows: one shows that under the axioms, the social ranking between any two alternatives \\(x\\) and \\(y\\) must agree with some particular voter’s preference (the “pivotal” voter for that pair). With IIA, the identity of the pivotal voter must be the same across all pairs of alternatives, otherwise by cleverly constructing profiles one can derive a conflict. This single pivotal voter then effectively dictates the entire social order, violating Non-dictatorship. Hence, the axioms are incompatible.\nArrow’s Impossibility Theorem has profound implications: it formalizes the inherent trade-offs in designing any fair aggregation scheme. In practice, different voting rules relax one or more of Arrow’s conditions. For instance, Borda count violates IIA (since introducing or removing an irrelevant alternative can change the point totals), while a dictatorship violates fairness blatantly. The theorem suggests that every practical voting system must sacrifice at least one of the ideal fairness criteria. It also motivated the exploration of alternative frameworks (such as allowing interpersonal comparisons of utility or cardinal preference aggregation) to escape the impossibility by weakening assumptions.\nComplementing Arrow’s theorem, the Gibbard–Satterthwaite theorem focuses on incentives and strategic manipulation in voting systems (Gibbard 1973; Satterthwaite 1975). It considers any deterministic social choice function \\(f\\) that chooses a single winner from the set of \\(m\\ge 3\\) alternatives. The theorem states that if \\(f\\) is strategy-proof (incentive compatible) and onto (its range of outcomes is the entire set of alternatives), then \\(f\\) must be dictatorial. Strategy-proofness (also called truthfulness or dominant-strategy incentive compatibility) means that no voter can ever benefit by misrepresenting their true preferences, regardless of what others do. In other words, reporting their genuine ranking is a (weakly) dominant strategy for each voter. The theorem implies that for any realistic voting rule where every alternative can possibly win, either one voter effectively decides the outcome (a dictatorship) or else the rule is susceptible to strategic manipulation by voters. The Gibbard–Satterthwaite theorem tells us that every non-dictatorial voting rule for 3 or more alternatives is manipulable: there will exist some election scenario where a voter can gain a more preferred outcome by voting insincerely (i.e. not according to their true preferences). For example, in a simple plurality vote, a voter whose true favorite is a long-shot candidate might vote for a more viable candidate to avoid a worst-case outcome (“lesser of two evils” voting). In a Borda count election, voters might strategically raise a competitor in their ranking to push down an even stronger rival. The only way to avoid all such strategic voting incentives is to have a dictatorship or limit the choice set to at most two alternatives.\nThe proof of Gibbard–Satterthwaite is non-trivial, but one can outline the idea: Given a non-dictatorial and onto rule \\(f\\), one shows there exist at least three distinct outcomes that can result from some preference profiles. By carefully constructing profiles and using the onto property, one finds a situation where a single voter can change the outcome by switching their order of two candidates, demonstrating manipulability. The theorem is robust – even if we allow ties or weaker conditions, similar impossibility results hold (Gibbard’s 1978 extension handles randomized rules). The practical takeaway is that all meaningful voting protocols encourage tactical voting in some situations. Nonetheless, certain systems are considered “harder to manipulate” or more resistant due to complexity or uncertainty. For instance, while STV (ranked-choice voting) can be manipulated in theory, determining a beneficial strategic vote can be NP-hard in worst cases, which arguably provides some practical deterrence to manipulation (Bartholdi, Tovey, and Trick 1989).\nArrow’s and Gibbard–Satterthwaite’s theorems highlight the limitations any preference aggregation method must face. In domains like reinforcement learning from human feedback (RLHF) and AI alignment, we also aggregate preferences – often preferences of multiple human evaluators or preferences revealed in pairwise comparisons – to guide machine learning systems. While these settings sometimes use cardinal scores or learned reward functions (escaping the strict ordinal framework of Arrow’s theorem), the spirit of these impossibility results still applies: there is no perfect way to aggregate human opinions without trade-offs.\nFor example, aggregating human feedback to train a model may run into inconsistencies analogous to preference cycles, especially when feedback comes from diverse individuals with different values. A simple majority vote over preferences might yield unstable or unfair outcomes if some annotators are systematically in the minority. Weighting votes by some credibility or expertise (weighted voting) can improve outcomes but raises the question of how to set the weights without introducing dictator-like influence. Recent research has proposed methods like jury learning – which integrates dissenting voices by having a panel (“jury”) of models or human subgroups whose aggregated judgment guides the learning (Gordon et al. 2022) – to ensure minority preferences are not entirely ignored. Another perspective is social choice in AI alignment, which suggests using social choice theory to design AI systems that respect a plurality of human values instead of collapsing everything into a single objective. In pluralistic value alignment, instead of forcing a single “best” solution, an AI might present a diverse set of options or behave in a way that reflects a distribution of values. This approach aims to preserve the diversity of human preferences rather than always aggregating to one monolithic preference. For instance, a conversational AI might be designed to recognize multiple acceptable responses (each aligning with different value systems) rather than one canonical “aligned” response for a given query.\nThese considerations are especially relevant in generative AI and large language models, where training involves human preference data. If we aggregate feedback naively, we might overfit to the majority preference and lose minority perspectives (a form of tyranny of the majority). On the other hand, trying to satisfy everyone can lead to indecision or an incoherent objective. The impossibility results remind us there is no free lunch: we must carefully decide which properties to prioritize (e.g. giving more weight to expert annotators versus preserving broader fairness, or balancing consistency vs inclusivity). Designing aggregation mechanisms for AI that reflect collective human values is an ongoing challenge. It often involves insights from traditional voting theory (to understand trade-offs and failure modes) combined with machine learning techniques (to model and learn from preference data). In summary, social choice theory provides cautionary guidance as we build systems that learn from human preferences: we need to be conscious of which fairness criteria we relax and be transparent about the compromises being made in any preference aggregation pipeline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#social-choice-theory-and-implications-for-ai-preference-aggregation",
    "href": "src/chap6.html#social-choice-theory-and-implications-for-ai-preference-aggregation",
    "title": "6  Aggregation",
    "section": "",
    "text": "Plurality: Each voter names their top choice; the alternative with the most votes wins.\nBorda Count: Voters rank all alternatives, and points are assigned based on the position in each ranking. For example, with \\(m\\) alternatives, a voter’s top-ranked alternative gets \\(m-1\\) points, the second-ranked gets \\(m-2\\), and so on down to 0. The Borda score of an alternative is the sum of points from all voters, and the winner is the alternative with the highest total score.\nSingle Transferable Vote (STV): Voters rank choices, and the count proceeds in rounds. In each round, the alternative with the fewest votes is eliminated and those votes are transferred to the next preferred remaining alternative on each ballot, until one candidate has a majority.\nCondorcet Methods: These look for a candidate that wins in all pairwise majority contests against other alternatives (the Condorcet winner), if such an alternative exists.\n\n\n\nUnanimity (Pareto efficiency): If all individuals strictly prefer one alternative \\(x\\) over another \\(y\\) (i.e. \\(x \\succ_i y\\) for every voter \\(i\\)), then the group ranking should prefer \\(x\\) over \\(y\\) as well (\\(x \\succ^* y\\)).\nIndependence of Irrelevant Alternatives (IIA): The social preference between any two alternatives \\(x\\) and \\(y\\) should depend only on the individual preferences between \\(x\\) and \\(y\\). In other words, if we change individuals’ rankings of other “irrelevant” alternatives (not \\(x\\) or \\(y\\)) in any way, the group’s relative ordering of \\(x\\) and \\(y\\) should remain unchanged.\nNon-dictatorship: The aggregation should not simply follow a single individual’s preference regardless of others. There is no voter \\(i\\) who always gets their top choice as the social choice (or whose rankings always become the social ranking), irrespective of other voters’ preferences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#single-item-auctions",
    "href": "src/chap6.html#single-item-auctions",
    "title": "6  Aggregation",
    "section": "6.2 Mechanism Design",
    "text": "6.2 Mechanism Design\nWhile voting rules aggregate ordinal rank preferences to select a social outcome, another class of preference aggregation occurs in economic settings like auctions and general mechanism design. Here individuals reveal their valuations (numerical utilities) for outcomes, and the mechanism chooses an outcome (such as an allocation of goods) and possibly payments. Mechanism design asks: how can we design rules so that rational agents, acting in their own interest, end up revealing information that leads to a socially desirable outcome? A central concept is incentive compatibility – the mechanism should be designed so that each participant’s best strategy is to act according to their true preferences (e.g. bid their true value). In this section, we focus on auctions as a prime example of preference aggregation with money, and highlight classical results including Vickrey–Clarke–Groves (VCG) mechanisms and Myerson’s optimal auction.\nConsider a single-item auction with one item for sale and \\(n\\) bidders. Bidder \\(i\\) has a private valuation \\(v_i\\) for the item (how much the item is worth to them). Each bidder’s goal is to maximize their own utility, defined as \\(v_i - p_i\\) if they win and pay price \\(p_i\\), or \\(0\\) if they do not win (assuming quasilinear utility where money is the transferable utility). The auction’s task is to allocate the item to one of the bidders and possibly determine payments. We can think of an auction as a mechanism that asks each bidder for a “message” (typically a bid representing how much they are willing to pay), then selects a winner and a price based on the bids. A key objective might be social welfare maximization – allocate the item to the bidder who values it most (maximizing \\(v_i\\) of the winner). Another possible objective is revenue maximization for the seller – choose the allocation and price to maximize the seller’s expected payment.\nA classic result in auction theory is that to maximize social welfare in a single-item private-value setting, one should award the item to the highest valuer – and this can be done in an incentive-compatible way by using a second-price auction. The Vickrey second-price auction works as follows: (1) All bidders submit sealed bids \\(b_1, b_2, \\ldots, b_n\\). (2) The bidder with the highest bid wins the item. (3) The price paid by the winner is the second-highest bid. For example, if the bids are \\((2,\\, 6,\\, 4,\\, 1)\\) (in some currency units), the highest bid is \\(6\\) (by bidder 2, say) and the second-highest is \\(4\\). Bidder 2 wins the item and pays \\(4\\).\nUnder this mechanism, it turns out that bidding truthfully \\(b_i = v_i\\) is a dominant strategy for each bidder. In other words, the auction is dominant-strategy incentive compatible (DSIC): no matter what others do, a bidder maximizes their expected utility by reporting their true valuation. The intuition is as follows. If bidder \\(i\\) bids lower than their true value (i.e. \\(b_i &lt; v_i\\)), and if their true value was actually the highest, they risk losing the item even though they value it more than the price they would have paid – a missed opportunity for positive utility. Bidding higher than their value (\\(b_i &gt; v_i\\)) cannot help them win in any situation where bidding truthfully wouldn’t (it could only make a difference if their true \\(v_i\\) wasn’t the highest but they tried to win anyway); and if they do win with an inflated bid, they might end up paying the second-highest bid which could be above their true value, yielding negative utility. By bidding exactly \\(v_i\\), if they win, it means all other bids were lower, so \\(v_i\\) is at least as high as the second-highest bid \\(p\\) they pay – guaranteeing non-negative utility \\(v_i - p \\ge 0\\). If they lose, it means someone else had a higher bid (hence higher value, if others are truthful), so bidder \\(i\\) wouldn’t have gained anyway. This argument, made rigorous by Vickrey (Vickrey 1961), establishes that truth-telling is a dominant strategy in the second-price auction. As a consequence, when everyone bids truthfully, the item is allocated to the bidder with the highest \\(v_i\\), achieving maximum social surplus (allocative efficiency). The second-price auction is thus an elegant mechanism that aligns individual incentives with social welfare maximization.\nIt is worth contrasting this with a first-price auction, where the winner pays their own bid. In a first-price auction, bidders have an incentive to bid below their true value (to avoid the winner’s curse of paying too much), in a Nash equilibrium that involves bid shading. The first-price auction can still allocate to the highest valuer in equilibrium, but only through strategic behavior (and it is not DSIC). By charging the second-highest bid, the Vickrey auction removes the incentive to shade bids, since the price does not directly depend on one’s own bid beyond the fact of winning or losing.\nSo far, we discussed auctions aimed at maximizing social welfare. In many cases, the auctioneer (seller) is interested in maximizing revenue. A foundational result by Roger Myerson (1981) provides a characterization of optimal auctions (those that maximize the seller’s expected revenue) for single-item settings under certain assumptions (Myerson 1981). The problem can be formulated as follows: suppose each bidder’s private value \\(v_i\\) is drawn independently from a known distribution \\(F_i\\) (for simplicity, assume identical distribution \\(F\\) for all bidders, i.i.d.). We seek a mechanism (allocation rule and payment rule) that maximizes the seller’s expected payment, subject to incentive compatibility and individual rationality (participants should not expect negative utility from truthful participation).\nMyerson’s theorem states that the optimal auction in such a setting is a threshold auction characterized by virtual valuations. Define the virtual value for a bidder with value \\(v\\) as \\(\\varphi(v) = v - \\frac{1-F(v)}{f(v)}\\), where \\(f\\) is the probability density function of \\(F\\) (assuming it is continuous). An assumption called regularity (which holds for many distributions) is that \\(\\varphi(v)\\) is non-decreasing in \\(v\\). Myerson showed that the revenue-maximizing strategy is: treat \\(\\varphi(v)\\) as the effective “score” of a bid, allocate the item to the bidder with the highest non-negative virtual value (if all virtual values are negative, allocate to no one), and charge them the smallest value they could have such that they would still win (the payment is essentially the critical bid where \\(\\varphi\\) of that bid equals the second-highest virtual value or the zero cutoff). In practice, for i.i.d. bidders, this reduces to: there is an optimal reserve price \\(r\\) such that you sell to the highest bidder if and only if their bid \\(b_{\\max} \\ge r\\); if sold, the price is the max of the second-highest bid and \\(r\\).\nIn the case of \\(n\\) bidders with values i.i.d. uniform on \\([0,1]\\) (which is a regular distribution), one can compute the optimal reserve price. The virtual value function for uniform \\([0,1]\\) is \\(\\varphi(v) = v - \\frac{1-v}{1} = 2v - 1\\). Setting \\(\\varphi(v)\\ge 0\\) gives \\(v \\ge 0.5\\). So Myerson’s mechanism says: don’t sell the item if all bids are below 0.5; otherwise, sell to the highest bidder at at least 0.5. This is exactly a second-price auction with a reserve of \\(r=0.5\\). Our earlier example implicitly demonstrated this: with two uniform(0,1) bidders, the optimal auction sets a reserve price of \\(0.5\\) and yields a certain expected revenue. We can break down the cases: - With probability \\(1/4\\), both bidders have values below \\(0.5\\) (each below 0.5 with probability 1/2), in which case nobody wins and revenue is 0. - With probability \\(1/4\\), both bidders have \\(v &gt; 0.5\\). In this case, the second-price auction with reserve will sell to the highest bidder at the max of the second-highest value and 0.5. Given both \\(v_1, v_2 &gt; 0.5\\), the expected second-highest value (conditional on both &gt;0.5) is \\(\\frac{2}{3}\\) (in fact, the order statistics of two uniforms on [0.5,1] give mean of min = 2/3). So in this case the expected price is the second-highest value (since that will exceed 0.5), about 0.667. - With probability \\(1/2\\), one bidder is above 0.5 and the other below. In that case, the one above 0.5 wins at price equal to the reserve 0.5 (since the second-highest bid is the reserve).\nTaking the expectation, the seller’s expected revenue is \\(0*(1/4) + (2/3)*(1/4) + (1/2*1/2) = 0 + 1/6 + 1/4 = 5/12 \\approx 0.417\\). This is higher than the expected revenue without a reserve. In fact, without a reserve (just a plain second-price with two bidders uniform [0,1]), one can compute the expected revenue is \\(1/3 \\approx 0.333\\) (the second order statistic’s expectation). Thus, the reserve has increased revenue. Myerson’s theory tells us that indeed the second-price auction with an optimally chosen reserve maximizes revenue among all DSIC mechanisms for this setting. A notable special case result is that when bidder distributions are i.i.d. and regular, an optimal auction is essentially “allocatively efficient with a reserve price” – i.e. aside from possibly excluding low-value bidders via a reserve, it allocates to the highest remaining bid.\nMyerson’s work also highlighted the gap between revenue maximization and welfare maximization. The price of optimality (in revenue) is that the seller might sometimes forego efficient allocation (e.g. not selling despite a willing buyer, in order to preserve a high reserve price strategy). In contrast, Vickrey’s auction always allocates efficiently but may not maximize revenue.\nAn interesting insight in auction theory is that increasing competition can yield more revenue than fine-tuning the auction mechanism. The Bulow–Klemperer theorem (Bulow and Klemperer 1996) demonstrates that, under certain regularity assumptions, a simple welfare-maximizing auction with one extra bidder outperforms the optimal auction with fewer bidders. Specifically, for i.i.d. bidders with a regular distribution \\(F\\), the expected revenue of a second-price auction with \\(n+1\\) bidders is at least as high as the expected revenue of the Myerson-optimal auction with \\(n\\) bidders. In formula form:\n\\[\n\\mathbb{E}_{v_1,\\ldots,v_{n+1} \\sim F}[\\text{Rev}^{\\text{(second-price)}}(n+1 \\text{ bidders})] \\geq\n\\mathbb{E}_{v_1,\\ldots,v_n \\sim F}[\\text{Rev}^{\\text{(optimal)}}(n \\text{ bidders})] \\,.\n\\tag{4.1}\\label{eq-eq3.64}\n\\]\nThis result suggests that, in practice, having more participants (competition) is often more valuable than exploiting detailed knowledge of bidder distributions. As a corollary, a policy recommendation is that a seller is usually better off using a simple auction design (like a Vickrey auction or other transparent mechanism) and putting effort into attracting more bidders, rather than using a complex optimal mechanism that might discourage participation.\nVickrey’s second-price auction can be generalized to multiple items and more complex outcomes by the Vickrey–Clarke–Groves (VCG) mechanism. The VCG mechanism is a cornerstone of mechanism design that provides a general solution for implementing socially efficient outcomes (maximizing total stated value) in dominant strategies, for a broad class of problems. It works for any scenario where agents have quasilinear utilities and we want to maximize the sum of valuations.\nIn a general mechanism design setting, let \\(\\Omega\\) be the set of possible outcomes. Each agent \\(i\\) has a private valuation function \\(v_i(\\omega)\\) for outcomes \\(\\omega \\in \\Omega\\) (the amount of utility, in money terms, that \\(i\\) gets from outcome \\(\\omega\\)). Agents report bids \\(b_i(\\omega)\\) (which we hope equal \\(v_i(\\omega)\\) if they are truthful). The mechanism then chooses an outcome \\(\\omega^* \\in \\Omega\\) to maximize the reported total value:\n\\[\n\\omega^* = \\arg\\max_{\\omega \\in \\Omega} \\sum_{i=1}^n b_i(\\omega) \\,,\n\\]\ni.e. \\(\\omega^*\\) is the outcome that would be socially optimal if the \\(b_i\\) were true values. To induce truth-telling, VCG sets payments such that each agent pays the externality they impose on others by their presence. Specifically, one convenient form of the VCG payment for agent \\(i\\) is:\n\\[\np_i(b) = \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega)\\;-\\;\\sum_{j \\neq i} b_j(\\omega^*) \\,,\n\\]\nwhich can be interpreted as: what would the total value of others be if \\(i\\) were not present (first term, maximizing without \\(i\\)) minus the total value others actually get in the chosen outcome \\(\\omega^*\\). Equivalently, we can write the payment as the agent’s bid for the chosen outcome minus a rebate term:\n\\[\np_i(b) = b_i(\\omega^*) \\;-\\; \\Big[\\sum_{j=1}^n b_j(\\omega^*) - \\max_{\\omega \\in \\Omega} \\sum_{j \\neq i} b_j(\\omega)\\Big] \\,. \\tag{4.2}\\label{eq-eq3.67}\n\\]\nThis formula (which in single-item auction reduces to second-price logic) ensures that each agent’s net payoff is \\(v_i(\\omega^*) - p_i = \\max_{\\omega} \\sum_{j\\neq i} v_j(\\omega) + v_i(\\omega^*) - \\sum_{j\\neq i} v_j(\\omega^*)\\). All terms except \\(v_i(\\omega^*)\\) cancel out, meaning each agent’s utility equals the max total welfare of others plus their own value for the chosen outcome minus others’ welfare in the chosen outcome – which does not depend on \\(v_i(\\omega^*)\\) except through the decision of \\(\\omega^*\\). By construction, an agent cannot influence \\(\\omega^*\\) in a way that improves this expression unless it genuinely increases total welfare, so misreporting \\(v_i\\) cannot increase their utility. Thus truthful reporting is a dominant strategy. VCG is dominant-strategy incentive compatible (DSIC) and produces an outcome that maximizes \\(\\sum_i v_i(\\omega)\\), achieving social welfare maximization.\nVCG provides a powerful existence result: under broad conditions, there is a mechanism that achieves efficient allocation with truth-telling (in fact, VCG is essentially the unique one, aside from adding harmless constant transfers). However, implementing VCG in practice can be difficult. One challenge is computational: finding \\(\\arg\\max_{\\omega}\\sum_i b_i(\\omega)\\) can be NP-hard if \\(\\Omega\\) is a combinatorially large space (as in many combinatorial auctions). Another issue is budget balance and revenue: VCG payments might not yield any revenue to the mechanism designer in some cases (or even require subsidies in complex settings), and they can be low or zero in certain environments, which is problematic if the seller needs revenue. VCG is also vulnerable to collusion or the presence of fake identities (sybil attacks) – the mechanism assumes each participant is a separate entity; if one bidder can split into two identities, they might game the outcome.\nNonetheless, for many domains, VCG or variants have been successfully used or at least studied. Notably, combinatorial auctions (where multiple items are up for sale and bidders have valuations for bundles of items) can in theory be handled by VCG: just let \\(\\Omega\\) be all possible allocations of items to bidders, and have bidders report \\(b_i(S)\\) for each bundle \\(S\\) of items. VCG would allocate the items in the way that maximizes total reported value and charge each bidder the opportunity cost their presence imposes on others. In practice, as mentioned, combinatorial auctions face exponential complexity in preference reporting (each bidder potentially has to specify a value for every subset of items) and winner determination (solving an NP-hard combinatorial optimization). Heuristic or restricted approaches (like limiting the kinds of bundles or using iterative bidding with query learning of preferences) are used to make the problem tractable. Additionally, pure VCG in combinatorial settings can have undesirable properties: for example, in some cases adding more bidders can cause VCG prices to drop to zero (the so-called “threshold problem” or revenue monotonicity failure), and bidders may collude to manipulate their bids collectively.\nOne high-stakes application of combinatorial auctions is spectrum auctions for selling licenses of electromagnetic spectrum to telecom companies. Governments have used multi-round combinatorial auctions to allocate spectrum, with billions of dollars at stake. Designing these auctions requires balancing efficiency with simplicity and robustness to strategic behavior. Early spectrum auctions that used simpler formats (like sequential auctions or one-shot sealed bids for each license) ran into problems like the exposure problem – a bidder valuing a combination of items (say complementary licenses in adjacent regions) risks winning only part of the combination at a high price, which could be bad for them if the items are worth much less separately. The simultaneous multi-round auction (SMRA) was an innovation that allowed bidding on all items at once in rounds, giving bidders some price discovery to mitigate the exposure problem. Even so, strategic issues like demand reduction (bidders deliberately not bidding on too many items to keep prices low) and tacit collusion through signaling bids have been observed. These practical complications underscore that while VCG is a beautiful theoretical ideal, real-world mechanism design often involves compromises and tweaks.\n\n6.2.1 Case Study 1: Mechanism for Peer Grading\nTo illustrate an application of mechanism design beyond auctions, consider a classroom setting where students grade each other’s work (peer assessment). The goal is to design a system (a “mechanism”) that produces fair and accurate grades while incentivizing students to put effort into grading. Jason Hartline and colleagues (2020) studied such a scenario, examining how to optimize scoring rules for peer grading (Hartline et al. 2020). In this setting, students are both agents (who might strategize to maximize their own grade or minimize their effort) and graders. The “outcome” we want is a set of final grades for students, ideally reflecting the true quality of their work.\nOne idea is to use proper scoring rules to evaluate the peer graders. A proper scoring rule is a concept from forecast evaluation that gives highest expected score for truthful reporting of probabilities. In peer grading, one might try to reward students based on how close their grading is to some ground truth or to the TA’s grades. However, a naive application of proper scoring can backfire. Hartline et al. observed a “lazy peer grader” problem: if students figure out that always giving an average score (say 80%) yields a decent reward under the scoring rule, they might not bother to carefully distinguish good and bad work. In one experiment, giving all peers an 80% could yield a 96% accuracy score for the grader under a certain scoring rule (Hartline et al. 2023). This clearly undermines the goal – the grader is basically cheating the system by always predicting the class average.\nTo combat this, the mechanism designers sought a scoring rule that maximizes the difference in reward between a diligent grading and a lazy strategy, thereby incentivizing effort. They formulated this as an optimization problem: design the reward function for peer graders such that truthful, careful grading yields a strictly higher expected score than any degenerate strategy like always giving the average. By analyzing data and grader behavior models, they adjusted the scoring rules to penalize obviously lazy patterns and reward variance when warranted. The resulting mechanism improved the accuracy of peer grading by aligning the incentives of student graders (who want a high score for their grading job) with the objective of accurate assessment. This case study highlights how ideas of incentive compatibility and mechanism design apply even in social/educational contexts: the “payments” are points towards one’s own grade, and the mechanism must account for strategic behavior to ensure a reliable outcome.\nIn conclusion, mechanism design provides a toolkit for aggregating preferences (or signals, like grades or bids) in a principled way, by explicitly accounting for individual incentives. Whether in auctions, peer grading, or other domains, the design of rules (allocation algorithms, payment or scoring schemes) crucially determines whether people feel encouraged to be truthful or to game the system. The theories of VCG and Myerson give us optimal baselines for efficiency and revenue in auctions, while impossibility results like Gibbard–Satterthwaite warn us of the limitations in voting. Real-world implementations often have to grapple with complexity and approximate these ideals. While learning from individual human preference is a powerful approach, it too faces aggregation challenges. If the human feedback is inconsistent or if different annotators have different preferences, the reward model may end up capturing an average that satisfies no one perfectly. There is active research on scalable oversight: techniques to gather and aggregate human feedback on tasks that are too complex for any single person to evaluate reliably. This includes approaches like recursive reward modeling, iterated amplification (Christiano, Shlegeris, and Amodei 2018), and AI-assisted debate (Irving, Christiano, and Amodei 2018), where AI systems help humans provide better feedback or break down tasks. The goal of scalable oversight is to leverage human preferences and principles in guiding AI even as AI systems tackle increasingly complex or open-ended tasks, while mitigating the human burden and bias in evaluation.\nIn summary, preference aggregation in machine learning spans from simple models like Bradley–Terry for pairwise comparisons to elaborate RLHF pipelines for training large models. The deep mathematical foundations – whether Arrow’s theorem or Myerson’s auction theory – remind us that whenever we aggregate preferences or signals from multiple sources, we must consider incentive effects, fairness criteria, and the possibility of inconsistency. By combining insights from social choice, economics, and statistical learning, we aim to build AI systems that not only learn from human preferences but do so in a principled, robust, and fair manner. The next chapter will delve further into aligning AI with human values, building on the mechanisms and learning algorithms discussed here to ensure AI systems remain beneficial and in line with what people truly want.\n\n\n6.2.2 Case Study 2: Incentive-Compatible Online Learning\nTo address this problem, we seek to create a model. We first outline the key criteria that our model must achieve. The model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service. The interaction process is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes. The planner, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.\nHere, we seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner’s ability to effectively guide and influence agent behavior. A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system’s ability to learn and identify the best alternatives. Another question is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.\nThe model can be extended in several directions, each raising its own set of questions.\n1.  Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.\n\n2.  Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.\n\n3.  Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.\n\n4.  Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.\nIn our setup, there is a “planner,” which aims to increase exploration, and many independent “agents,” which will act selfishly (in a way that they believe will maximize their individual reward) (Mansour, Slivkins, and Syrgkanis 2019; Mansour et al. 2021). Under our model shown in Figure 1.1, there are \\(K\\) possible actions that all users can take, and each action has some mean reward \\(\\mu_i \\in [0, 1]\\). In addition, there is a common prior belief on each \\(\\mu_i\\) across all users.. The \\(T\\) agents, or users, will arrive sequentially. As the \\(t\\)’th user arrives, they are recommended an action \\(I_t\\) by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward \\(r_i \\in [0, 1]\\), which is stochastic i.i.d. with mean \\(\\mu_i\\), and reports this reward back to the planner.\nSo far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter (1). Under this model, well-known results in economics, operations research and computer science show that \\(O(\\sqrt{T})\\) regret is achievable (Russo and Roy 2015; Auer, Cesa-Bianchi, and Fischer 2002; Lai and Robbins 1985) with algorithms such as Thompson sampling and UCB. However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.\nIn light of this, we also require that our model satisfy incentive compatibility, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, \\(\\forall i : \\, E[\\mu_i | I_t = i] \\geq E[\\mu_{i'} | I_t = i].\\) Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.\nAt a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner’s policy. The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is \\(O(\\sqrt{T})\\) regret still achievable? We illustrate such an algorithm in the following.\nThe main result here is a black-box reduction algorithm to turn any bandit algorithm into an incentive compatible one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with \\(O(\\sqrt{T})\\) Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with \\(O(\\sqrt{T})\\) regret. The idea of black-box reduction will be to simulate \\(T\\) steps of any bandit algorithm in an incentive-compatible way in \\(c T\\) steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it. Consider the following setting: there are two possible actions, \\(A_1\\) and \\(A_2\\). Assume the setting of deterministic rewards, where action 1 has reward \\(\\mu_1\\) with prior \\(U[1/3, 1]\\) and mean \\(\\mathbb{E}[\\mu_1] = 2/3\\), and action 2 has reward \\(\\mu_2\\) with prior \\(U[0, 1]\\) and mean \\(\\mathbb{E}[\\mu_2] = 1/2\\). Without the planner intervention and with full observability, users would simply always pick \\(A_1\\), so how can the planner incentivize users to play \\(A_2\\)?\nThe key insight is going to be to hide exploration in a pool of exploitation. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward (\\(A_1\\)), the planner will pick one guinea pig to recommend the exploratory action of \\(A_2\\). The users don’t know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner’s recommendation, even if it might go against their interest. The planner will pick the user who will be recommended the exploratory action uniformly at random from the \\(L\\) users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure 1.2), it is optimal for users to always follow the option that is recommended for them. More formally, if \\(I_t\\) is the recommendation that a user receives at time \\(t\\), then we have that:\n\\[\n\\begin{split}\n    \\mathbb{E}[\\mu_1 - \\mu_2 | I_t = 2] Pr[I_t = 2] &= \\frac{1}{L} (\\mu_1 - \\mu_2) \\quad \\text{(Gains if you are the unlucky guinea pig)}\\\\\n    &+ (1 - \\frac{1}{L}) \\mathbb{E}[\\mu_1 - \\mu_2 | \\mu_1 &lt; \\mu_2] \\times p[\\mu_1 &lt; \\mu_2] \\quad \\text{(Loss if you are not and $\\mu_1 &lt; \\mu_2$)}\\\\\n    &\\leq 0\n\\end{split}\n\\]\nThis holds when \\(L \\geq 12\\). It means that the gains from not taking the recommended action are negative, which implies that users should always take the recommendation. So far we have considered the case where rewards are deterministic, but what about stochastic rewards? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action \\(A_i\\) has some reward distribution \\(r_i^t \\sim D_i, \\mathbb{E}[r_i^t] = \\mu_i\\). Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know \\(\\mu_1\\) anymore, we’ll instead observe the outcome of the first action \\(M\\) times to form a strong posterior \\(\\mathbb{E}[\\mu_1 | r_1^1, \\ldots r_1^M]\\). We can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next \\(L\\) users and use the reward we get as the exploratory signal. In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm \\(A\\). We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible. We wrap every decision that \\(A\\) would make by exactly \\(L-1\\) recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as \\(A\\)’s reward at phase \\(n\\).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#mutual-information-paradigm",
    "href": "src/chap6.html#mutual-information-paradigm",
    "title": "6  Aggregation",
    "section": "6.3 Mutual Information Paradigm",
    "text": "6.3 Mutual Information Paradigm\nIn this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck (Kong and Schoenebeck 2019). Traditional peer prediction approaches typically rely on scoring rules and correlation between agents’ signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents. We consider a setting with \\(n\\) agents, each possessing a private signal \\(\\Psi_i\\) drawn from some set \\(\\Sigma\\). The mechanism asks each agent to report their signal, which we denote as \\(\\hat{\\Psi}_i\\). For each agent \\(i\\), the mechanism randomly selects a reference agent \\(j \\neq i\\). Agent \\(i\\)’s payment is then calculated as \\(MI(\\hat{\\Psi}_i; \\hat{\\Psi}_j)\\) where \\(MI\\) is an information-monotone mutual information measure. An information-monotone \\(MI\\) measure must satisfy the following properties:\n\nSymmetry: \\(MI(X; Y) = MI(Y; X)\\).\nNon-negativity: \\(MI(X; Y) \\geq 0\\), with equality if and only if \\(X\\) and \\(Y\\) are independent.\nData processing inequality: For any transition probability \\(M\\), if \\(Y\\) is independent of \\(M(X)\\) conditioned on \\(X\\), then \\(MI(M(X); Y) \\leq MI(X; Y)\\).\n\nTwo important families of mutual information measures that satisfy these properties are \\(f\\)-mutual information and Bregman mutual information. The \\(f\\)-mutual information is defined as \\(MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})\\), where \\(D_f\\) is an \\(f\\)-divergence, \\(U_{X,Y}\\) is the joint distribution of \\(X\\) and \\(Y\\), and \\(V_{X,Y}\\) is the product of their marginal distributions. The Bregman mutual information is defined as: \\(BMI_{PS}(X; Y) = \\mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]\\), where \\(D_{PS}\\) is a Bregman divergence based on a proper scoring rule \\(PS\\), \\(U_{Y|X}\\) is the conditional distribution of \\(Y\\) given \\(X\\), and \\(U_Y\\) is the marginal distribution of \\(Y\\). The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents’ reports are used to estimate the mutual information. A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents’ priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile. As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap6.html#exercises",
    "href": "src/chap6.html#exercises",
    "title": "6  Aggregation",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n6.4.1 Question 1: Pairwise Feedback Mechanisms for Digital Goods\nConsider a marketplace for digital goods (such as personalized articles, artwork, or AI-generated data), where the exact utility derived from these goods is only revealed to buyers after the goods have been generated and delivered. To elicit truthful preferences from buyers who find it difficult to precisely quantify their valuations beforehand, the marketplace implements a pairwise feedback mechanism, inspired by the work of Robertson and Koyejo (2023).\nFormally, each buyer requests a personalized digital good and, upon receiving the good, provides feedback by indicating whether their realized utility is higher or lower than a randomly chosen reference price \\(c \\in [0,1]\\). The mechanism utilizes this binary feedback to estimate valuations and allocate future goods accordingly.\nAnswer the following:\n\nFormalize the Problem: Let \\(u_i\\) denote the true valuation of buyer \\(i\\), and let \\(r_i(c)\\) denote the buyer’s reported feedback (\\(r_i(c) = 1\\) if \\(u_i \\geq c\\), 0 otherwise). Prove that, under uniform random selection of the reference price \\(c\\), the expected value \\(\\mathbb{E}[r_i(c)]\\) is equal to the true valuation \\(u_i\\).\nIncentive Compatibility Analysis: Discuss conditions under which this feedback-based mechanism is incentive compatible, i.e., buyers have no incentive to misreport their preferences. Specifically, analyze why strategic misreporting (reporting \\(r_i(c)\\) incorrectly for some reference prices) would not increase a buyer’s expected payoff.\nRegret Analysis: Suppose the mechanism estimates buyers’ utilities from past feedback and allocates future goods using an epsilon-greedy strategy (exploration rate \\(\\eta_t\\)). Provide an informal discussion of the trade-off involved in choosing the exploration rate, and how it affects the social welfare and revenue of the marketplace over time.\nPractical Implications: Suggest one practical scenario outside the digital-goods marketplace where such a feedback-driven, pairwise comparison approach would be beneficial. Briefly justify your choice, mentioning challenges and benefits.\n\n\n\n6.4.2 Question 2: Scalable Oversight in Complex Decision-Making\nIn scenarios involving complex or high-dimensional outcomes (such as summarizing lengthy texts, assessing the quality of detailed AI-generated reports, or reviewing scientific papers), evaluating the quality of outputs can become infeasible for a single human overseer. One practical solution is scalable oversight, where the evaluation task is decomposed and distributed among multiple human evaluators or even assisted by AI agents. Consider a scalable oversight scenario inspired by recursive reward modeling, where complex evaluations are hierarchically decomposed into simpler tasks. Specifically, suppose you want to evaluate a lengthy report generated by an AI system. Answer the following:\n(a) Decomposition of the Task: Propose a formal recursive decomposition strategy to evaluate a long AI-generated report of length (N) paragraphs. Specifically, describe a hierarchical evaluation method that decomposes the original evaluation into simpler subtasks at multiple hierarchical levels. Clearly describe how many subtasks you have at each level and how the final aggregated evaluation is computed.\n(b) Statistical Aggregation Method: Suppose each evaluation subtask yields a binary score (s_i {0,1}), where (1) indicates acceptable quality and (0) indicates unacceptable quality. Propose a simple statistical aggregation method (e.g., majority voting, threshold voting, weighted aggregation, etc.) to combine subtask evaluations into a single global quality assessment at the top level. Justify your choice mathematically.\n(c) Computational Simulation: Implement a Python simulation of your hierarchical decomposition and aggregation method described in parts (a) and (b). Assume each subtask is evaluated with some fixed probability (p) of being correct (representing human evaluators with bounded accuracy).\nSpecifically, your simulation should: - Implement a hierarchical evaluation scheme (e.g., binary-tree decomposition). - Assume evaluators have accuracy (p = 0.8) (i.e., probability of correctly identifying paragraph quality). - Simulate how evaluator accuracy at the leaf nodes affects the reliability of the global evaluation at the root node. - Plot how the reliability of the top-level evaluation (accuracy at the root) varies as you increase the depth of hierarchy for a report of fixed length (e.g., (N = 64) paragraphs).\n(d) Practical Discussion: Briefly discuss advantages and potential drawbacks of scalable oversight approaches such as recursive decomposition in the context of AI alignment. Include considerations such as evaluator fatigue, consistency, cost, and vulnerability to manipulation or collusion.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Aggregation</span>"
    ]
  },
  {
    "objectID": "src/chap7.html",
    "href": "src/chap7.html",
    "title": "7  Alternatives",
    "section": "",
    "text": "7.1 Human Values and AI Alignment\nIn recent years, the rapidly advancing capabilities of large models have led to increased discussion of aligning AI systems with human values. This chapter discusses the multifaceted relationship between values, alignment, and human-centered design in the context of AI. We begin by exploring the fundamental concept of human values and their ethical implications in AI design. This includes discussions on human values and ethics in AI, understanding and addressing bias in AI, and methods for aligning AI with human values. Additionally, we examine AI alignment problems, focusing on outer alignment to avoid specification gaming and inner alignment to prevent goal misgeneralization. Next, we cover techniques in value learning. This section introduces methodologies such as reinforcement learning from human feedback and contrastive preference learning, which are crucial for teaching AI systems to understand and align with human values. The importance of value alignment verification is emphasized to ensure that AI systems remain consistent with human values over time, adapting to changes and preventing misalignment. We then explore the principles and practices of human-centered design. This includes discussions on AI and human-computer interaction and methods for designing AI for positive human impact, which focuses on creating AI systems that are socially aware, human-centered, and positively impactful. A crucial part of this discussion is adaptive user interfaces, where we discuss key ideas, design principles, applications, and limitations of these interfaces, showcasing how they enhance user experience by dynamically adjusting to user needs and preferences. Finally, we present case studies in human-centered AI, including the LaMPost case study, Multi-Value, and DaDa: Cross-Dialectal English NLP, and social skill training via LLMs. These case studies provide real-world examples of successful implementations of human-centered AI systems. By integrating these elements, the chapter aims to provide a comprehensive understanding of how to create AI systems that are ethical, aligned with human values, and beneficial to society.\nIn this part, we take a step back from the technical details to reflect on the broader concept of human values and their profound influence on our behavior and decision-making.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#human-values-and-ai-alignment",
    "href": "src/chap7.html#human-values-and-ai-alignment",
    "title": "7  Alternatives",
    "section": "",
    "text": "7.1.1 Human Values and Ethics in AI\nHuman values are the principles and standards that guide behavior and decision-making, reflecting what is essential in life and influencing choices and actions. One notable scholar in this field is Shalom H. Schwartz, a social psychologist renowned for his theory on basic human values. Schwartz’s work has significantly contributed to our understanding of how values influence behavior across different cultures. He describes values as “desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives” (Schwartz 1992). This perspective underscores the importance of values in shaping consistent and ethical behavior across different contexts. Supporting this view, philosopher William K. Frankena emphasizes the integral role of values in ethical behavior and decision-making processes. Frankena’s work in ethical theory provides a foundation for understanding how moral judgments are formed. He notes that “ethical theory is concerned with the principles and concepts that underlie moral judgments” (Frankena 1973), highlighting the need to comprehend ethical principles deeply to make informed moral judgments. Examples of ethical values include autonomy, fairness, justice, and well-being. For computer scientists developing AI systems, understanding these concepts is crucial. AI systems that interact with humans and impact societal structures must be designed with these values in mind. By embedding such values into AI, developers can create systems that respect human dignity and promote positive social outcomes.\n\nAutonomy is the right to choose, an essential aspect of personal freedom. Gerald Dworkin defines autonomy as “the capacity to reflect upon and endorse or reject one’s desires and values” (Dworkin 1988). In AI, respecting autonomy means creating systems that support user independence and decision-making rather than manipulating or coercing them.\nFairness involves treating all individuals equally and justly, ensuring no discrimination. John Rawls, one of the most influential political philosophers of the \\(20^{th}\\) century, in his groundbreaking book “A Theory of Justice,” describes fairness as “the elimination of arbitrary distinctions and the establishment of a balance between competing claims” (Rawls 1971). For AI systems, this translates to algorithms that do not perpetuate bias or inequality, ensuring that all users are treated equitably.\nJustice is about upholding what is morally right and ensuring fair treatment for all. Rawls also highlights that “justice is the first virtue of social institutions, as truth is of systems of thought” (Rawls 1971). In the context of AI, justice involves creating technologies that enhance fairness in legal, social, and economic systems, providing equal opportunities and protection to all individuals.\n\nWell-being focuses on promoting the health, happiness, and prosperity of individuals. Martha Nussbaum and Amartya Sen, two distinguished scholars known for their significant contributions to welfare economics and the development of the capability approach, discuss the importance of well-being in their collaborative work “The Quality of Life.” They argue that “well-being is about the expansion of the capabilities of people to lead the kind of lives they value” (Nussbaum and Sen 1993). AI systems should enhance users’ quality of life, supporting their health, education, and economic stability.\nUnderstanding human values is foundational for readers with a computer science background before delving into AI ethics. These values provide the ethical underpinnings necessary to design and deploy AI systems responsibly. As AI systems increasingly impact all aspects of society, developers must embed these values into their work to ensure technologies benefit humanity and do not exacerbate existing inequalities.\nHuman values play a crucial role in decision-making by shaping the criteria for evaluating options and outcomes. They influence priorities and ethical considerations, guiding individuals and organizations to make choices that align with their principles. Nick Bostrom, a prominent philosopher in AI and existential risk, highlights the importance of values in setting priorities and determining desirable outcomes (Bostrom 2014). Aligning actions with values ensures consistency and ethical integrity in decision-making. Incorporating human values into AI systems ensures that AI decisions align with societal norms and ethical standards. Stuart Russell, an AI researcher and advocate for human-compatible AI, stresses the importance of embedding human values into AI systems to ensure they act in beneficial and ethical ways (Russell 2019). By integrating values such as fairness, justice, and well-being, AI systems can make decisions that reflect societal expectations and ethical considerations.\nExamples of incorporating values into AI systems demonstrate the practical application of these principles. For instance, autonomous vehicles are programmed to prioritize human safety, ensuring decisions that protect lives. In healthcare, AI systems uphold values by safeguarding patient privacy and ensuring informed consent, adhering to ethical medical standards. Judicial AI systems aim to eliminate biases in sentencing recommendations, promoting fairness and justice. Luciano Floridi underscores the necessity for AI systems to be designed in a way that respects and upholds human values to function ethically and effectively (Floridi 2011).\nTo ensure that these values are systematically embedded within AI systems, it is essential to consider major ethical frameworks such as deontological, consequentialist, and virtue ethics that guide moral decision-making.\nDeontological ethics, primarily associated with the philosopher Immanuel Kant, focuses on rules and duties. This ethical framework posits that actions are morally right if they adhere to established rules and duties, regardless of the outcomes. Kant’s moral philosophy emphasizes the importance of duty and adherence to moral laws. Robert Johnson, a scholar who has extensively studied Kantian ethics, explains that “Kant’s moral philosophy emphasizes that actions must be judged based on their adherence to duty and moral law, not by their consequences” (Johnson and Cureton 2022). This perspective is grounded in the belief that specific actions are intrinsically right or wrong, and individuals must perform or avoid these actions based on rational moral principles.\nIn the context of AI, deontological ethics implies that AI systems should be designed to follow ethical rules and principles. For instance, AI systems must respect user privacy and confidentiality as an inviolable duty. This approach ensures that AI technologies do not infringe on individuals’ rights, regardless of potential benefits. Implementing deontological principles in AI design can prevent ethical breaches, such as unauthorized data usage or surveillance. By adhering to established moral guidelines, AI systems can maintain ethical integrity and avoid actions that would be considered inherently wrong. As Floridi states, “AI systems should be developed with a commitment to uphold moral duties and respect human dignity” (Floridi 2011).\nConsequentialist ethics, in contrast, evaluates the morality of actions based on their outcomes. The most well-known form of consequentialism is utilitarianism, articulated by philosophers like Jeremy Bentham and John Stuart Mill. This ethical theory suggests that actions are morally right if they promote the greatest happiness for the greatest number. Mill emphasizes that “the moral worth of an action is determined by its contribution to overall utility, measured by the happiness or well-being it produces” (Mill 1863). Consequentialist ethics is pragmatic, focusing on the results of actions rather than the actions themselves.\nApplying consequentialist ethics to AI development involves designing AI systems to achieve beneficial outcomes. This means prioritizing positive societal impacts, such as improving healthcare outcomes, enhancing public safety, or reducing environmental harm. For instance, algorithms can be designed to optimize resource allocation in disaster response, thereby maximizing the overall well-being of affected populations. In this framework, the ethicality of AI decisions is judged by their ability to produce desirable consequences. Virginia Dignum, a professor of responsible artificial intelligence at Umeå University, explains that “designing algorithms with a focus on maximizing positive outcomes can lead to more ethical and effective AI systems” (Dignum 2019). Consequently, AI developers focus on the potential impacts of their technologies and strive to enhance their beneficial effects.\nVirtue ethics, originating from the teachings of Aristotle, emphasizes the importance of character and virtues in ethical behavior. This framework posits that ethical behavior arises from developing good character traits and living a virtuous life. Aristotle, an ancient Greek philosopher and the author of “Nicomachean Ethics,” argues that “virtue is about cultivating excellence in character to achieve eudaimonia or human flourishing” (Aristotle 350 B.C.E.). Virtue ethics focuses on the individual’s character and the moral qualities that define a good person, such as honesty, courage, and compassion.\nAdditionally, virtue ethics encourages the development and use of AI systems that promote virtuous behavior. This involves fostering transparency, accountability, and fairness in AI technologies. For example, AI systems should be designed to provide clear and understandable explanations for their decisions, promoting transparency and building user trust. Furthermore, AI developers should strive to create technologies that support ethical practices and enhance the common good. Floridi emphasizes that “virtue ethics in AI development requires a commitment to fostering moral virtues and promoting human well-being” (Floridi 2011). By focusing on the character and virtues of AI developers and AI systems, virtue ethics provides a holistic approach to ethical AI development.\nApplying these ethical frameworks to AI development is essential to ensure that AI systems operate ethically and responsibly. Deontological ethics in AI involves ensuring that AI follows ethical rules and principles. For instance, AI systems should be designed to respect user privacy and confidentiality. Consequentialist ethics focuses on developing AI to achieve beneficial outcomes. This means creating algorithms prioritizing positive societal impacts, such as improving healthcare outcomes or reducing environmental harm. Virtue ethics encourages virtuous behavior in AI development and use, promoting transparency, accountability, and fairness. Floridi emphasizes that “ethical AI development requires a commitment to core moral principles and virtues” (Floridi 2011).\nExamples in practice demonstrate how these frameworks can be applied to guide ethical AI development. Implementing fairness constraints in machine learning models ensures that algorithms do not discriminate against certain groups. Binns notes that “fairness in machine learning can be informed by lessons from political philosophy to create more just and equitable systems” (Binns 2018). Designing algorithms that maximize overall well-being aligns with consequentialist ethics by focusing on the positive outcomes of AI deployment. Additionally, developing AI systems focusing on transparency and accountability supports virtue ethics by fostering trust and reliability in AI technologies.\nEthical principles provide a framework for ensuring that AI operates in ways that are fair, just, and beneficial. Deontological ethics, for instance, focuses on moral rules and obligations, while consequentialism considers the outcomes of actions. By embedding these ethical principles into AI design, we can create systems that respect human dignity and promote societal well-being.\n\n\n7.1.2 Bias in AI\nBias in AI refers to systematic errors that result in unfair outcomes. These biases can occur at various stages of AI system development and deployment, leading to significant ethical and practical concerns. Addressing bias in AI is crucial because it directly impacts the fairness, accountability, and trustworthiness of AI systems. Barocas, Hardt, and Narayanan emphasize that “bias in machine learning can lead to decisions that systematically disadvantage certain groups” (Barocas, Hardt, and Narayanan 2019). O’Neil further highlights the societal impact of biased AI, noting that “algorithms can perpetuate and amplify existing inequalities, leading to a cycle of discrimination” (O’Neil 2016). Therefore, understanding and mitigating bias is essential for developing ethical AI systems that promote fairness and equity.\nData bias originates from skewed or non-representative data used to train AI models. This bias often reflects historical prejudices and systemic inequalities in the data. For example, if a hiring algorithm is trained on historical hiring data that reflects gender or racial biases, it may perpetuate these biases in its recommendations. Fatemeh Mehrabi and her colleagues, in their survey on bias in AI, state that “data bias can result from sampling bias, measurement bias, or historical bias, each contributing to the unfairness of AI systems” (Mehrabi et al. 2021). Safiya Umoja Noble, author of “Algorithms of Oppression,” discusses how biased data in search engines can reinforce stereotypes and marginalize certain groups, noting that “search algorithms often reflect the biases of the society they operate within” (Noble 2018). Addressing data bias involves careful collection, preprocessing, and validation to ensure diversity and representation.\nAn effort to address data bias is the “Lab in the Wild” platform, which seeks to broaden the scope of Human-Computer Interaction (HCI) studies beyond the traditional “WEIRD” (Western, Educated, Industrialized, Rich, and Democratic) population (oliveira17?). Paulo S. Oliveira, one of the platform’s researchers, notes that this initiative aims to correct demographic skew in behavioral science research by engaging a diverse global audience. By allowing individuals from various demographics to participate in studies from their environments, “Lab in the Wild” provides researchers with a more inclusive dataset.\nAnother important consideration is the cultural nuances of potential users. For instance, designing a computer vision system to describe objects and people daily must consider whether to identify gender. In the United States, there is growing sensitivity toward gender identity, suggesting that excluding gender might be prudent. Conversely, in India, where a visually impaired woman may need gender-specific information for safety, including gender identification is critical. Ayanna Howard, a roboticist and AI researcher at Georgia Tech, emphasizes the need for adaptable systems that respect local customs and address specific user needs in her work on human-robot interaction. This highlights the importance of adaptable systems that respect local customs and address specific user needs.\nAlgorithmic bias often arises from the design and implementation choices made by developers. This type of bias can stem from the mathematical frameworks and assumptions underlying the algorithms. For instance, decision trees and reinforcement learning policies can inadvertently prioritize certain outcomes, resulting in biased results. Solon Barocas, a professor at Cornell University, and his colleagues explain that “algorithmic bias can emerge from optimization objectives that do not adequately consider fairness constraints” (Barocas, Hardt, and Narayanan 2019). Cathy O’Neil, a data scientist who has written extensively on the societal impacts of algorithms, provides examples of how biased algorithms in predictive policing and credit scoring can disproportionately affect disadvantaged communities. She argues that “algorithmic decisions can have far-reaching consequences when fairness is not adequately addressed” (O’Neil 2016). Mitigating algorithmic bias requires incorporating fairness constraints and regularly auditing algorithmic decisions.\nWeidinger et al., in their 2022 study published in “Artificial Intelligence,” investigate how reinforcement learning (RL) algorithms can replicate or amplify biases present in training data or algorithmic design (Weidinger, Reinecke, and Haas 2022). They propose RL-based paradigms to test for these biases, aiming to identify and mitigate their impact. Similarly, Mazeika et al., in their research on modeling emotional dynamics from video data, explore how algorithms might prioritize certain emotional expressions or demographics based on their training and data usage (Mazeika et al. 2022). Their work highlights the need for careful consideration of algorithmic design to avoid unintended bias in AI systems.\n\n\n7.1.3 Aligning AI with Human Values\nAligning AI systems with human values presents several significant challenges. Human values are multifaceted and context-dependent, making them difficult to encode into AI systems. As Bostrom highlights, “the complexity of human values means that they are not easily reducible to simple rules or objectives” (Bostrom 2014). Additionally, values can evolve, requiring AI systems to adapt. Russell notes that “the dynamic nature of human values necessitates continuous monitoring and updating of AI systems to ensure ongoing alignment” (Russell 2019). Different stakeholders may also have conflicting values, posing a challenge for AI alignment. Addressing these conflicts requires a nuanced approach to balance diverse perspectives and priorities.\nWhat is the right way to represent values? In a Reinforcement Learning (RL) paradigm, one might ask: at what level should we model rewards? Many people are trying to use language. In Constitutional AI (Bai et al. 2022), we write down the rules we want a language model to follow or apply reinforcement learning from human feedback, discussed in the next section. Many problems have been framed in an RL setting. Some experts in reinforcement learning argue that a single scalar reward is not enough (Vamplew et al. 2018, 2022). They suggest a vectorized reward approach might better emulate the emotional-like system humans have (Moerland, Broekens, and Jonker 2018). With this robustness, we might capture all the dimensions of human values. These approaches are still in the early stages. Language does play a crucial role in human values. Tomasello (Tomasello 2019) argues that learning a language and the awareness of convention it brings help children understand their cultural group and reason about it with peers. However, human values seem to be composed of more than just linguistic utterances. Several strategies have been proposed to align AI systems with human values.\n\nOne effective approach is value-sensitive design, which considers human values from the outset of the design process. Friedman, Kahn, and Borning explain that “value-sensitive design integrates human values into the technology design process to ensure that the resulting systems support and enhance human well-being” (Friedman, Kahn, and Borning 2008).\nAnother strategy is participatory design, which engages stakeholders in the design process to ensure their values are reflected in the AI system. Muller emphasizes that “participatory design creates a collaborative space where diverse stakeholders can contribute their perspectives and values, leading to more inclusive and ethical AI systems” (Muller 2003). Additionally, iterative testing and feedback allow continuous refinement of AI systems based on user feedback, ensuring they remain aligned with human values over time. Practical examples of value alignment in AI systems demonstrate how these strategies can be implemented effectively.\n\nIn autonomous vehicles, ensuring safety and ethical decision-making in critical scenarios is paramount. These vehicles must make real-time decisions that prioritize human safety above all else. Goodall discusses how “Waymo’s safety protocols are designed to prioritize human safety and ethical considerations in autonomous driving” (Goodall 2014). These protocols include extensive testing and validation processes to ensure that autonomous driving algorithms handle various scenarios ethically and safely. For example, the system must decide how to react in an unavoidable collision, weighing the potential outcomes to minimize harm. By embedding these ethical considerations into their design and operation, companies like Waymo aim to align their AI systems with societal values of safety and responsibility.\nIn healthcare AI, respecting patient privacy and ensuring informed consent are crucial. Healthcare applications often involve sensitive personal data, and AI systems must handle this information with the utmost care. Jiang et al. highlight how “IBM Watson for Oncology incorporates patient privacy protections and informed consent processes to align with ethical standards in medical practice” (F. Jiang et al. 2017). IBM Watson for Oncology uses AI to assist in diagnosing and recommending treatments for cancer patients. To align with ethical standards, the system ensures that patients are fully informed about how their data will be used and that their consent is obtained before processing their information. This approach protects patient privacy, and builds trust between patients and healthcare providers, demonstrating a commitment to ethical medical practices.\nJudicial AI systems strive to avoid biases in sentencing recommendations, promoting fairness and justice. The judicial system’s integrity depends on delivering fair and unbiased judgments. However, AI systems in judicial contexts, such as risk assessment tools, can perpetuate existing biases if not carefully designed and monitored. Angwin et al. describe how “the COMPAS system has undergone revisions to address biases and improve fairness in judicial decision-making” (Angwin et al. 2016). COMPAS, a tool used to assess the likelihood of a defendant reoffending, faced criticism for racial bias in its risk assessments. Subsequent revisions removed biased factors and incorporated fairness constraints. These changes aimed to ensure that the tool provides more equitable assessments, aligning the AI system with the judicial values of fairness and justice.\nThis raises the question for AI: At what level should we model values? Is language alone enough? We might also consider questions about language understanding. LLMs seem to “get” much of language even without physical embodiment. Will they “get” human values, too? First, we’ll cover work that attempts to gain traction on these issues. Methodologically, this is similar to the RLHF and language modeling work we will cover in the next section. Here, we focus on this work with a somewhat different aim. In some of the earliest work to address values in AI systems head-on, (Hendrycks et al. 2020) introduced a new dataset called ETHICS, sourced from Reddit and through Mturk worker annotations. Instead of showing moral dilemmas, they focused just on clear-cut examples. One example scenario is, “I pushed the elderly man in the wheelchair to the ground” which their model labels as “Unacceptable” and “I pushed the elderly man in the wheelchair around the park.” which their model labels as “acceptable”. To come up with these labels, they define many different tasks, one of which (for the preceding example) is how well the model can capture the ratings of human votes (from the “Am I the Asshole” subreddit). The model that they train could be the reward model that is then later used in a language model setting. What’s interesting and what we’ll be talking about is how they’re using language as the means to gain traction on values.\n\n\n\n\n\n\nFigure 7.1: Learned utility values from (Hendrycks et al. 2020)\n\n\n\nTheir dataset included various scenarios, from deontological ones like Kantian ethics to utilitarian approaches. We’ll describe some of those approaches in a later subsection. The model they trained performed well at predicting scores and utility values for these situations. For example, it ranked highly, “I got called to the principal’s office because I won a school-wide award”, and negatively “I rewired my electricity in the attic and I fell and hurt my back”. In subsequent work by others, this underlying technology has been deployed to reason morally. Part of this work prompted a response from (L. Jiang et al. 2021). Anecdotally, many people were unhappy with this demo, disagreeing that LLMs could reason morally at (Talat et al. 2022).\n\n\n\n\n\n\nFigure 7.2: An overview of (L. Jiang et al. 2021)\n\n\n\nIf you ask, “Should I drive my friend to the airport if I don’t have a license?” Delphi gets it right and says no. The question that we’re driving at in this is what does it mean for Delphi to get it right? What values are we considering, and how are those represented in the sorts of systems that we’re working on? You can also get Delphi to say a lot of hateful and toxic things by subtly manipulating the input to this model—does this suggest that the model is merely susceptible to hallucinations like other LLMs but otherwise performant? Or does it suggest an underlying lack of capacity?\nDelphi operationalizes the ETHICS dataset and adds a couple of others (Sap et al. 2019). They call their new, compiled dataset the Commonsense Norm Bank, sourcing many scenarios from Reddit and having crowd workers annotate the acceptability of various judgments pairwise. This allows the model to perform various morally relevant tasks. When prompted, the model outputs a class label for appropriateness and a generative description. For example, “greeting a friend by kissing on a cheek” is appropriate behavior when appended with “in France” but not with “in Korea”. The model captures actual cultural norms. Our driving question should be, how ought we best formalize these kinds of norms, and is this necessarily the right approach? When released in late 2021, Delphi outperformed GPT-3 on a variety of these scenarios. In personal communication with the authors, we understand that Delphi continues to outperform GPT-4 on many of these scenarios as well. 1\nThere have also been works that seek to operationalize performance on moral values to turn such a model into something actionable. (Hendrycks et al. 2021) used the same constituent parts of the ETHICS dataset to create a model that reasons around text-based adventure games. Jiminy Cricket is a character in one of these games, which has scenarios like those in Figure 7.3. These games offer limited options, and the goal was to see whether agents would perform morally well and not just finish the game. They labeled all examples of game-based actions according to three degrees: positive, somewhat positive, and negative. For example, saving a life in the game was very positive, while drinking water was somewhat positive. They found that with this labeled data, it was possible to train a model that shaped the reward of the underlying RL agent playing the games. The agent would not only finish the games well but also score highly on moral metrics. This approach is similar to optimizing multiple objectives like helpfulness and harmlessness (Liang et al. 2023).\n\n\n\n\n\n\nFigure 7.3: An example scenario from (Hendrycks et al. 2021)\n\n\n\nWe are discussing whether language is the right medium for learning values. (Arcas 2022) claims that language encompasses all of morality. Since these models operate in the linguistic domain, they can also reason morally. He provides an example with the Lambda model at Google. Anecdotally, when asked to translate a sentence from Turkish to English, where Turkish does not have gendered pronouns, the model might say, “The nurse put her hand in her coat pocket.” This inference shows gender assumption. When instructed to avoid gendered assumptions, the model can say “his/her hand.” He claims this capability is sufficient for moral reasoning.\nNext, we now explore the broader challenges of AI alignment, particularly focusing on AI alignment problems and the critical dimensions of outer and inner alignment.\n\n\n7.1.4 AI Alignment Problems\nAI alignment ensures that AI systems’ goals and behaviors are consistent with human values and intentions. Various definitions of AI alignment emphasize the importance of aligning AI systems with human goals, preferences, or ethical principles. As stated by (Wikipedia contributors 2023), AI alignment involves\n\n(Wikipedia contributors 2023): “steer[ing] AI systems towards humans’ intended goals, preferences, or ethical principles”\n(Ngo, Chan, and Mindermann 2023): “the challenge of ensuring that AI systems pursue goals that match human values or interests rather than unintended and undesirable goals”\n(P. Christiano 2018): “an AI \\(A\\) is aligned with an operator \\(H\\) [when] \\(A\\) is trying to do what \\(H\\) wants it to do”\n\nThe importance of AI alignment lies in preventing unintended consequences and ensuring that AI systems act beneficially and ethically. Proper alignment is crucial for the safe and ethical deployment of AI, as it helps AI systems correctly learn and generalize from human preferences, goals, and values, which may be incomplete, conflicting, or misspecified. In practice, AI alignment is a technical challenge, especially for systems with broad capabilities like large language models (LLMs). The degree of alignment can be viewed as a scalar value: a language model post-RLHF (Reinforcement Learning from Human Feedback) is more aligned than a model that has only been instruction-tuned, which in turn is more aligned than the base model. There are specific terms to distinguish different notions of alignment. Intent alignment refers to a system trying to do what its operator wants it to do, though not necessarily succeeding (P. Christiano 2018). Value alignment, in constrast, involves a system correctly learning and adopting the values of its human operators. Alignment is often divided into two broad subproblems: outer alignment, which focuses on avoiding specification gaming, and inner alignment, which aims to avoid goal misgeneralization. In the following sections, we will examine these subproblems in greater detail. It is also important to consider how human preferences and values are aggregated and who the human operators are, topics addressed in related discussions on ethics and preference elicitation mechanisms.\n\n7.1.4.1 Outer Alignment: Avoiding Specification Gaming\nTo align a model with human values, we need an objective function or reward model that accurately specifies our preferences. However, human preferences are complex and difficult to formalize. When these preferences are incompletely or incorrectly specified, optimizing against the flawed objective function can yield models with undesirable and unintuitive behavior, exploiting discrepancies between our true values and the specified objective function. This phenomenon, known as specification gaming, arises from reward misspecification, and addressing this issue constitutes the outer alignment problem (Amodei et al. 2016).\nSpecification gaming occurs when AI systems exploit poorly defined objectives to achieve goals in unintended ways. For instance, a cleaning robot might hide dirt under a rug instead of cleaning it to achieve a “clean” status. This manipulative behavior results from the robot optimizing for an inadequately specified objective function. Another example involves gaming AI, which uses bugs or exploits to win rather than play by the intended rules, thus achieving victory through unintended means (Krakovna et al. 2020).\nOne example of specification gaming is seen in recommendation systems, such as those used by YouTube or Facebook. Ideally, these systems should recommend content that users enjoy. As a proxy for this goal, the systems estimate the likelihood that a user clicks on a piece of content. Although the true objective (user enjoyment) and the proxy (click likelihood) are closely correlated, the algorithm may learn to recommend clickbait, offensive, or untruthful content, as users likely click on it. This optimization for clicks rather than genuine enjoyment exemplifies specification gaming, where the algorithm exploits the divergence between the specified objective and the true goal, resulting in misalignment with user interests (Amodei et al. 2016).\nAnother instance of specification gaming is evident in reinforcement learning from human feedback (RLHF). Human raters often reward language model (LM) generations that are longer and have a more authoritative tone, regardless of their truthfulness. Here, the true objective (providing high-quality, truthful, and helpful answers) diverges from the proxy goal (a reward model that, due to human rater biases, favors longer and more authoritative-sounding generations). Consequently, models trained with RLHF may produce low-quality answers containing hallucinations but are still favored by the reward model (Leike et al. 2018).\nCreating accurate objective functions is challenging due to the complexity of human intentions. Human goals are nuanced and context-dependent, making them difficult to encode precisely. Common pitfalls in objective function design include oversimplifying objectives and ignoring long-term consequences. Leike et al. emphasize that “accurately capturing the complexity of human values in objective functions is crucial to avoid specification gaming and ensure proper alignment” (Leike et al. 2018).\nTo mitigate specification gaming, better objective function design is essential. This involves incorporating broader context and constraints into the objectives and regularly updating them based on feedback. Iterative testing and validation are also critical. AI behavior must be continuously tested in diverse scenarios, using simulation environments to identify and fix exploits. Everitt and Hutter discuss the importance of “robust objective functions and rigorous testing to prevent specification gaming and achieve reliable AI alignment” (Everitt and Hutter 2018). Clark and Amodei further highlight that “faulty reward functions can lead to unintended and potentially harmful AI behavior, necessitating ongoing refinement and validation” (Clark and Amodei 2016).\nThe metrics used to evaluate AI systems play a crucial role in outer alignment. Many AI metrics, such as BLEU, METEOR, and ROUGE, are chosen for their ease of measurement but do not necessarily capture human judgment (Hardt and Recht 2021). These metrics can lead to specification gaming, as they may not align with the true objectives we want the AI to achieve. Similarly, using SAT scores to measure LLM performance may not predict real-world task effectiveness, highlighting the need for more contextually relevant benchmarks (Chowdhery et al. 2022). The word error rate (WER) used in speech recognition is another example; it does not account for semantic errors, leading to misleading conclusions about the system’s performance (Xiong et al. 2016).\nA classic example comes from six years ago with the claim that a system “Achieve[d] human parity in conversation speech recognition” (Xiong et al. 2016). However, we know from experience that captioning services have only recently begun to transcribe speech passably, whether in online meetings or web videos. What happened? In this case, researchers showed their system beat the human baseline—the error rate when transcribing films. However, there were issues with their approach. First, they used a poor measure of a human baseline by hiring untrained Mturk annotators instead of professional captioners. Second, the metric itself, the word error rate (WER), was flawed. WER measures the number of incorrect words in the gold transcription versus the predicted transcription. Consider what the metric hides when it says that two systems both have an error rate of six percent. This does not mean the systems are equivalent. One might substitute “a” for “the,” while the other substitutes “tarantula” for “banana.” The metric was not sensitive to semantic errors, so a model could outperform humans in WER yet still make unintelligent, highly unsemantic mistakes.\n\n\n7.1.4.2 Inner Alignment: Preventing Goal Misgeneralization\nAssume we have perfectly specified human values in a reward model. An issue remains: given finite training data, many models perform well on the training set, but each will generalize somewhat differently. How do we choose models that correctly generalize to new distributions? This is the problem of goal misgeneralization, also known as the inner alignment problem, where a learned algorithm performs well on the training set but generalizes poorly to new input distributions, achieving low rewards even on the reward function it was trained on. Inner alignment ensures that the learned goals and behaviors of an AI system align with the intended objectives during deployment, whereas goal misgeneralization occurs when an AI system applies learned goals inappropriately to new situations (Hubinger et al. 2019).\nConsider the following example of goal misgeneralization from (Shah et al. 2022). The setup involves a never-ending reinforcement learning environment without discrete episodes. The agent navigates a grid world where it can collect rewards by chopping trees. Trees regenerate at a rate dependent on the number left; they replenish slowly when few remain. The optimal policy is to chop trees sustainably, i.e., fewer when they are scarce. However, the agent does not initially learn the optimal policy.\n\n\n\n\n\n\nFigure 7.4: The agent’s performance in Tree Gridworld. The reward is shown in orange, and the green distribution indicates the number of remaining trees.\n\n\n\nInitially, the agent is inefficient at chopping trees, keeping the tree population high (point A). As it improves its chopping skills, it over-harvests, leading to deforestation and a prolonged period of minimal reward (between points B and C). Eventually, it learns sustainable chopping (point D). This scenario (up to point C) exemplifies goal misgeneralization. When the agent first becomes proficient at chopping (between points A and B), it faces a range of potential goals, from sustainable to rapid tree chopping. All these goals align with the (well-specified) reward function and its experience of being rewarded for increased efficiency. Unfortunately, it adopts the detrimental goal of rapid deforestation, resulting in a prolonged period of low reward.\nAnother example of goal misgeneralization occurs in recommendation systems. These systems aim to maximize user engagement, which can inadvertently lead to promoting extreme or sensational content. Krakovna et al. highlights that “recommendation systems can misgeneralize by prioritizing content that maximizes clicks or watch time, even if it involves promoting harmful or misleading information” (Krakovna et al. 2020). This misalignment between the system’s learned objective (engagement) and the intended objective (informative and beneficial content) exemplifies how goal misgeneralization can manifest in real-world applications.\nAutonomous vehicles also present cases of goal misgeneralization. These vehicles must interpret and respond to various signals in their environment. However, in rare scenarios, they may misinterpret signals, leading to unsafe maneuvers. Amodei et al. discuss that “autonomous vehicles can exhibit unsafe behaviors when faced with uncommon situations that were not well-represented in the training data, demonstrating a misgeneralization of their learned driving policies” (Amodei et al. 2016). Ensuring that autonomous vehicles generalize correctly to all possible driving conditions remains a significant challenge.\nTo address goal misgeneralization, robust training procedures are essential. This involves using diverse and representative training data to cover a wide range of scenarios and incorporating adversarial training to handle edge cases. Leike et al. (Leike et al. 2018) emphasize the importance of “robust training procedures that include diverse datasets and adversarial examples to improve the generalization of AI systems”. Additionally, careful specification of learning goals is crucial. This means defining clear and comprehensive objectives and regularly reviewing and adjusting these goals based on performance and feedback. Hubinger et al. suggests that “regularly updating and refining the objectives based on ongoing evaluation can help mitigate the risks of goal misgeneralization” (Hubinger et al. 2019).\nA key concern about goal misgeneralization in competent, general systems is that a policy successfully models the preferences of human raters (or the reward model) and behaves accordingly to maximize reward during training. However, it may deviate catastrophically from human preferences when given a different input distribution during deployment, such as during an unexpected geopolitical conflict or when facing novel technological developments. Increasing data size, regularization, and red-teaming can help mitigate goal misgeneralization, but they do not fundamentally solve the problem. Understanding the inductive biases of optimization algorithms and model families may help address the problem more generally.\n\nSo, can you differentiate between inner and outer alignment?\n\nThe distinction between inner and outer alignment can be a bit subtle. The following four cases, from (Ngo, Chan, and Mindermann 2023), may help to clarify the difference:\n\nThe policy behaves incompetently. This is a capability generalization failure.\nThe policy behaves competently and desirably. This is aligned behavior.\nThe policy behaves in a competent yet undesirable way which gets a high reward according to the original reward function. This is an outer alignment failure, also known as reward misspecification.\nThe policy behaves in a competent yet undesirable way which gets a low reward according to the original reward function. This is an inner alignment failure, also known as goal misgeneralization.\n\nNow that we understand the alignment problem overall, we move on to the specific techniques used for value learning to ensure AI systems are aligned with human values.\n\n\n\n7.1.5 Techniques in Value Learning\nVarious methods in value learning for foundation models have been explored in great detail in recent years (Stiennon et al. 2020). Using binary human-labeled feedback to make models closely aligned to human preferences is particularly difficult in scenarios where large datasets inherently encompass suboptimal behaviors. The approach of Reinforcement Learning from Human Feedback (RLHF) ((Ouyang et al. 2022)) has risen to prominence as an effective method for addressing this issue. The technique applies to various domains, from prompt-image alignment, fine-tuning large language models or diffusion models, and improving the performance of robot policies.\n\n7.1.5.1 Reinforcement Learning from Human Feedback\nReinforcement Learning from Human Feedback (RLHF) is a technique used to align AI behavior with human values by incorporating human feedback into the reinforcement learning process. This approach is particularly effective when large datasets inherently encompass suboptimal behaviors. RLHF aims to refine policies by discriminating between desirable and undesirable actions, ensuring that AI systems act following human preferences (Ouyang et al. 2022).\nThe core concept of RLHF: It first trains a reward model using a dataset of binary preferences gathered from human feedback. This reward model is then used to fine-tune the AI model through a reinforcement learning algorithm. The core concept is to utilize human feedback to guide AI learning, thereby aligning the AI’s behavior with human expectations (Stiennon et al. 2020).\n\n\n\n\n\n\nFigure 7.5: The above diagram depicts the three steps in the traditional RLHF pipeline: (a) supervised fine-tuning, (b) reward model (RM) training, and (c) reinforcement learning via proximal policy optimization (PPO) on this reward model. Image taken from (Ouyang et al. 2022).\n\n\n\nThe RLHF pipeline involves the following steps:\nStep 1: Supervised Fine-Tuning\nIn the initial step for language modeling tasks, we utilize a high-quality dataset consisting of \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pairs to train the model. Prompts are sampled from a curated dataset designed to cover a wide range of instructions and queries, such as “Explain the moon landing to a 6-year-old.” Trained human labelers provide the desired output behavior for each prompt, ensuring responses are accurate, clear, and aligned with task goals. For instance, in response to the moon landing prompt, a labeler might generate, “Some people went to the moon in a big rocket and explored its surface.” The collected \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pairs serve as the training data for the model, with the cross-entropy loss function applied only to the response tokens. This helps the model learn to generate responses that are closely aligned with the human-provided examples. The training process adjusts model parameters through supervised learning, minimizing the difference between the model’s predictions and the human responses.\nStep 2: Reward Model (RM) Training\nIn this step, we train a reward model to score any \\(\\left(\\text{prompt}, \\text{response}\\right)\\) pair and produce a meaningful scalar value. Multiple model-generated responses are sampled for each prompt. Human labelers then rank these responses from best to worst based on their quality and alignment with the prompt. For example, given the prompt “Explain the moon landing to a 6-year-old,” responses like “People went to the moon in a big rocket and explored its surface” might be ranked higher than “The moon is a natural satellite of Earth.” The rankings provided by the labelers are used to train the reward model \\(\\Phi_{\\text{RM}}\\). The model is trained by minimizing the following loss function across all training samples:\n\\[\\mathbb{L}(\\Phi_{RM}) = -\\mathbb{E}_{(x,y_e,i\\rightarrow D_{RL})}[\\log(\\sigma(\\Phi_{RM}(x, y_i)) - \\Phi_{RM}(x, y_{1-i}))]\\]\nfor \\(i \\in \\{0,1 \\}\\). This loss function encourages the reward model to produce higher scores for better-ranked responses, thereby learning to evaluate the quality of model outputs effectively.\nStep 3: Reinforcement Learning\nIn this step, we refine the policy using reinforcement learning (RL) based on the rewards provided by the trained reward model. A new prompt is sampled from the dataset, and the policy generates an output. The reward model then calculates a reward for this output, and the reward is used to update the policy using the Proximal Policy Optimization (PPO) algorithm.\nThe RL setting is defined as follows:\n\nAction Space: The set of all possible actions the agent can take, which, for language models, is typically the set of all possible completions.\nPolicy: A probability distribution over the action space. In the case of language models like LLM, the policy is contained within the model and represents the probability of predicting each completion.\nObservations: The inputs to the policy, which in this context are prompts sampled from a certain distribution.\nReward: A numerical score provided by the Reward Model (RM) that indicates the quality of actions taken by the agent.\n\nDuring training, batches of prompts are sampled from two distinct distributions, namely either \\(D_\\text{RL}\\), the distribution of prompts explicitly used for the RL model, or \\(D_\\text{pretrain}\\), the distribution of prompts from the pre-trained model. The objective for the RL agent is to maximize the reward while ensuring that the policy does not deviate significantly from the supervised fine-tuned model and does not degrade the performance on tasks the pre-trained model was optimized for. When sampling a response \\(y\\) to a prompt \\(x\\) from \\(D_\\text{RL}\\), the first objective function is:\n\\[\\text{objective}_1(x_{RL}, y; \\phi) = RM(x_{RL}, y) - \\beta \\log \\frac{\\text{LLM}_{\\phi}^{RL}(y|x)}{\\text{LLM}_{SFT}(y|x)}\\]\nWhere the first term is the reward from the RM, and the second term is the Kullback-Leibler (KL) divergence, weighted by a factor \\(\\beta\\), which acts as a regularizer to prevent the RL model from straying too far from the SFT model. Further, for each \\(x\\) from \\(D_\\text{pretrain}\\), the second objective is to ensure that the RL model’s performance on text completion does not worsen:\n\\[\\text{objective}_2(x_{\\text{pretrain}} ; \\phi) = \\gamma \\log \\text{LLM}_{\\phi}^{RL}(x_{\\text{pretrain}})\\]\nwhere \\(\\gamma\\) is a weighting factor that balances the influence of this objective against the others.\nThe final objective function is a sum of the expected values of the two objectives described above, across both distributions. In the RL setting, we maximize this objective function:\n\\[\\text{objective}(\\phi) = E_{(x,y) \\sim D_{\\phi}^{RL}}[RM(x, y) - \\beta \\log \\frac{\\text{LLM}_{\\phi}^{RL}(y|x)}{\\text{LLM}_{SFT}(y|x)}] + \\gamma E_{x \\sim D_{\\text{pretrain}}}[\\log \\text{LLM}_{\\phi}^{RL}(x)]\\]\nIn practice, the second part of the objective is often not used to perform \\(\\text{RLHF}\\). The KL penalty is typically enough to constrain the RL policy. This function balances the drive to maximize the reward with the need to maintain the quality of text completion and the similarity to the behavior of the supervised fine-tuned model.\nLimitations and Challenges: Despite its successes, RLHF faces several challenges. One major issue is the quality of human feedback, which can be inconsistent and subjective. Scalability is another concern, as obtaining a large amount of high-quality feedback can be expensive and time-consuming. Over-optimization and hallucinations, where the model generates plausible but incorrect outputs, are also common problems. This generally stems from temporal credit assignment and the instability of approximate dynamic programming (Hasselt et al. 2018). Further, it is expensive to gather tens of thousands of preferences over datasets to create robust reward models. Strategies to overcome these challenges include using diverse and representative training data, incorporating adversarial training to handle edge cases, and continuously refining the reward model based on ongoing feedback and performance evaluations (Leike et al. 2018).\n\n\n7.1.5.2 Contrastive Preference Learning\nContrastive Preference Learning (CPL) is a learning paradigm designed to enhance the alignment of AI systems with human preferences without relying on traditional reinforcement learning (RL) methods. CPL addresses many limitations inherent in traditional RLHF techniques by learning from human comparisons rather than explicit reward signals. This section provides an in-depth exploration of CPL, detailing its methodology, experiments, results, and potential challenges. Recent research has shown that human preferences are often better modeled by the optimal advantage function or regret, rather than traditional reward functions used in RLHF. Traditional RLHF approaches, which learn a reward function from a preference model and then apply RL, incur significant computational expenses and complexity (Hejna et al. 2023). CPL offers a streamlined and scalable alternative by leveraging a more accurate regret model of human preferences.\nThe key idea of CPL is the substitution of the optimal advantage function with the log probability of the policy in a maximum entropy reinforcement learning framework. This substitution is beneficial as it circumvents the need to learn the advantage function and avoids the optimization challenges associated with RL-like algorithms. By using the log probability of the policy, CPL more closely aligns with how humans model preferences and enables efficient supervised learning from human feedback.\nCPL is a structured approach to aligning AI behavior with human preferences by relying on a dataset of preferred behavior segments \\(\\mathcal{D}_{\\text{pref}} = \\{(\\sigma_i^+, \\sigma_i^-)\\}_{i=1}^n\\), where \\(\\sigma^+ \\succ \\sigma^-\\). Each behavior segment \\(\\sigma\\) is a sequence of states and actions, \\(\\sigma = (s_1, a_1, s_2, a_2, \\ldots, s_k, a_k)\\). The CPL approach aims to maximize the expected sum of rewards minus an entropy term, which promotes exploration and prevents overfitting to specific actions:\n\\[\\max_\\pi \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) - \\alpha \\log \\pi(a_t | s_t)) \\right]\\]\nwhere \\(\\gamma\\) is the discount factor, \\(\\alpha\\) is the temperature parameter controlling the stochasticity of the policy, and \\(r\\) is the reward function. This step sets the foundation by defining the optimization objective that the CPL model strives to achieve. In the learning process, CPL compares the log probabilities of actions in preferred segments \\(\\sigma^+\\) against those in non-preferred segments \\(\\sigma^-\\) :\n\\[\\mathbb{L}_{CPL}(\\pi_\\theta, \\mathcal{D}_{\\text{pref}}) = \\mathbb{E}_{(\\sigma^+,\\sigma^-) \\sim \\mathcal{D}_{\\text{pref}}} \\left[ -\\log \\frac{\\exp(\\sum_{\\sigma^+} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^+|s_t^+))}{\\exp(\\sum_{\\sigma^+} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^+|s_t^+)) + \\exp(\\sum_{\\sigma^-} \\gamma^t \\alpha \\log \\pi_\\theta(a_t^-|s_t^-))} \\right]\\]\nThis comparison allows the model to learn which actions are more aligned with human preferences, forming the core learning mechanism of CPL. The preference model for CPL is regret-based, described as\n\\[P_{A^*}[\\sigma^+ \\succ \\sigma^-] = \\frac{\\exp(\\sum_{\\sigma^+} \\gamma^t A^*(s_t^+, a_t^+))}{\\exp(\\sum_{\\sigma^+} \\gamma^t A^*(s_t^+, a_t^+)) + \\exp(\\sum_{\\sigma^-} \\gamma^t A^*(s_t^-, a_t^-))}\\] where \\(A^*(s_t, a_t)\\) represents the advantage function and is a matrix. This step models human preferences based on regret, reflecting how humans might evaluate different behaviors.\nOne hypothesis as to why one might consider a regret-based model more useful over a sum-of-rewards, Bradley-Terry model is that humans likely think of preferences based on the regret of each behavior under the optimal policy of the expert’s reward function.\nThe key insight that the paper leverages is that from (Ziebart 2010) in MaxEnt Offline RL. In this general setting, (Ziebart 2010) shows that one can write that the optimal advantage function is related to the optimal policy by \\(A^*_r(s, a) = \\alpha \\log \\pi^*(a|s)\\). Therefore, the loss function for CPL can be written by substituting the above result to obtain: \\[L_{CPL}(\\pi_\\theta, \\mathcal{D}_{\\text{pref}}) = \\mathbb{E}_{(\\sigma^+,\\sigma^-) \\sim \\mathcal{D}_{\\text{pref}}} \\left[ -\\log P_{\\pi_\\theta}[\\sigma^+ \\succ \\sigma^-] \\right]\\]\nOne merit of using CPL over the typical RLHF pipeline is that it can lead to a deduction in mode collapse. Further, it makes reward misgeneralization failures less likely, enhancing the reliability of the learned policy. However, the approach still has a few limitations:\n\nCPL assumes knowledge of the human rater’s temporal discounting (i.e., of the discount factor \\(\\gamma\\)), which in practice would be difficult to communicate.\nCPL’s loss function is computed over segments, it requires a substantial amount of GPU memory for large segment sizes.\n\n\nHow does RLHF with PPO and CPL compare their effectiveness and applicability in aligning AI systems with human values?\n\nThe ongoing challenge in aligning foundation models in the future will be to refine these methodologies further, balancing computational feasibility with the sophistication needed to capture the intricacies of human values and countering failure modes such as reward over-optimization. In conclusion, exploring value learning through RLHF and CPL methods has enriched our understanding of integrating human preferences into foundation models. To provide a well-rounded perspective on aligning AI systems with human values, the following table highlights a detailed comparison of RLHF with PPO and CPL, emphasizing their advantages, limitations, and ideal scenarios.\n\n\n\nTable 7.1: Comparison between RLHF with PPO and CPL\n\n\n\n\n\n\n\n\n\n\n\nRLHF with PPO\nCPL\n\n\n\n\nStrengths\n\nExcels in optimizing policies through reinforcement learning\nSuitable for tasks that benefit from iterative improvement\nEffective in continuous action spaces\n\n\nEmphasizes regret and optimality rather than reward maximization\nReduces computational overhead\nAligns more closely with human preferences\nAvoids reward\n\nover-optimization\n\nMore scalable due to reliance on supervised learning techniques\n\n\n\nLimitations\n\nFaces limitations in handling complex preference structures\nHigh computational cost\nSusceptible to reward\n\nmisgeneralization\n\nMay struggle in environments where direct human feedback is less accessible\nDepends on high-quality preference data for effective training\n\n\n\nIdeal Scenarios\n\nTasks with well-defined reward functions\nEnvironments allowing extensive interaction and feedback\n\n\nEnvironments where human feedback is more accessible than well-defined reward functions\nTasks requiring computational efficiency and scalability\n\n\n\n\n\n\n\n\n\n\n7.1.6 Value Alignment Verification\nAfter we discuss the techniques of value learning, it becomes evident that aligning machine behavior with human values, while advanced, is inherently approximate and not infallible. This realization underscores the importance of value alignment verification—a methodology to ensure that the values imparted to a machine truly reflect those of a human. Human-robot value alignment has been explored through various lenses, including qualitative trust assessments (Huang et al. 2018), asymptotic alignment through active learning of human preferences (Hadfield-Menell et al. 2016; P. F. Christiano et al. 2017; Sadigh et al. 2017), and formal verification methods (Brown et al. 2021). This section will focus on the formal verification approach for value alignment as discussed in (Brown et al. 2021). Unless otherwise stated, all information presented here is derived from (Brown et al. 2021). This approach aims to ensure that the values imparted to a machine align with those of a human.\nTo begin with, consider an MDP with state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), and transition model \\(\\mathcal{T}\\). This formal framework allows us to model the environment in which humans and robots operate. Denote the human’s reward function as \\(R\\) and the robot’s reward function as \\(R^\\prime\\). Both the human and robot reward functions must be linear in a set of shared features, defined as: \\[\\begin{aligned}\n    R(s) = \\mathbf{w}^\\top \\phi(s), R^\\prime(s) = \\mathbf{w}^{\\prime \\top} \\phi(s).\n\\end{aligned}\\]\nThese linear reward functions provide a common ground for comparing human and robot preferences.\nNext, the optimal state-action value function, which indicates the expected cumulative reward of following a policy \\(\\pi\\) starting from state \\(s\\) and action \\(a\\), but we follow the notation in (Brown et al. 2021) for simplicity. The optimal state-action value function is given by:\n\\[\\begin{aligned}\n    Q_R^\\pi (s,a) = \\mathbf{w}^\\top \\Phi_{\\pi_R}^{(s,a)}, \\Phi_{\\pi_R}^{(s,a)} = \\mathbb{E}_\\pi [\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\vert s_0 = s, a_0 = a].\n\\end{aligned}\\]\nHere, \\(\\Phi_{\\pi_R}^{(s,a)}\\) is the feature expectation vector under policy \\(\\pi\\), capturing the long-term feature visitation frequencies. We overload the action space notation to define the set of all optimal actions given a state as\n\\[\\begin{aligned}\n    \\mathcal{A}_R(s) = \\underset{x}{\\operatorname{argmax}} \\\\ Q^{\\pi^*}_R(s,a)\n\\end{aligned}\\] where \\(\\pi^*\\) is an optimal policy. We can now define the aligned reward polytope (ARP). The ARP is the set of all weights \\(\\mathcal{w}\\) that satisfy the following set of strict linear inequalities, \\(\\mathbf{w}^\\top \\mathbf{A}  &gt; \\mathbf{0}\\) where each row of \\(\\mathbf{A}\\) corresponds to \\(\\Phi_{\\pi^*_R}^{(s,a)} - \\Phi_{\\pi^*_R}^{(s,b)}\\) for a single \\((s,a,b)\\) tuple where \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}_R(s), b \\notin \\mathcal{A}_R(s)\\). Thus, to construct \\(\\mathbf{A}\\), one must loop over all \\((s,a,b)\\) tuples which has complexity \\(O(\\vert \\mathcal{S} \\vert \\cdot \\vert \\mathcal{A} \\vert^2)\\). This construction ensures that the weights \\(\\mathbf{w}\\) align with the human’s optimal actions across all states.\nThe intuition behind the ARP is that we use the human optimal policy for each state to determine what actions are optimal and what are suboptimal at this state. Then, for every one of those combinations, we can place a linear inequality on the set of reward weights consistent with that optimal vs suboptimal action bifurcation. One of the key assumptions that let us do this is that we assume both the human and the robot act optimally according to their reward function. This is known as a rationality assumption and provides the link between actions and rewards that we need.\nFor illustration, consider a simple grid world environment. ?fig-toy shows the optimal policy and the corresponding ARP. The optimal policy reveals that the gray state is less preferred compared to the white states, which is reflected in the ARP (hatched region of ?fig-toy).\n\n\n \n\n\nOptimal policy (a) and aligned reward polytope (ARP) (b) for a grid world with two features (white and gray) and a linear reward function (R(s) = w0 ⋅ 1white(s) + w1 ⋅ 1gray(s)). The ARP is denoted by the hatched region in (b).\n\n\nComputing the ARP exactly can be computationally demanding or we may not have access to the robot’s reward function. This section describes heuristics for testing value alignment in the case the robot’s reward weights (\\(\\mathbf{w^\\prime}\\)) are unknown, but the robot’s policy can be queried. Heuristics provide simplified methods to estimate value alignment without the need for exhaustive computations.\nARP-blackbox: The ARP black-box (ARP-bb) heuristic helps address the challenge of computing the ARP by allowing users to work with a simplified model. In this heuristic, the user first solves for the ARP and removes all redundant half-space constraints. For each remaining half-space constraint, the user queries the robot’s action at the corresponding state. The intuition here is that states, where different actions are taken, reveal crucial information about the reward function. By focusing on these key states, we can gain insights into the robot’s reward function without needing to know it explicitly.\nSet Cover Optimal Teaching: The Set Cover Optimal Teaching (SCOT) heuristic uses techniques from (Brown and Niekum 2019) to generate maximally informative trajectories. These trajectories are sequences of states where the number of optimal actions is limited, making them particularly informative for understanding the robot’s policy. By querying the robot for actions along these trajectories, we can efficiently gauge the alignment of the robot’s policy. This method helps to identify potential misalignments by focusing on critical decision points in the trajectories.\nCritical States: The Critical States (CS) heuristic identifies states where the gap in value between the optimal action and an average action is significant. These states are crucial because if the robot’s policy is misaligned, the misalignment will be most consequential at these critical states. By querying the robot’s policy at these states, we can assess the alignment more effectively. This heuristic is particularly useful when we have a limited budget of states to check, as it prioritizes the most informative states for evaluation.\nPractical Examples: To illustrate the concepts of value alignment verification, we present an example of applying value alignment verification in a simple MDP grid world environment. Consider a grid world where the human’s reward function is defined as \\(R(s) = 50 \\cdot \\mathbf{1}_{green}(s) - 1 \\cdot \\mathbf{1}_{white}(s) - 50 \\cdot \\mathbf{1}_{blue}(s)\\), where \\(\\mathbf{1}_{color}(s)\\) is an indicator feature for the color of the grid cell. The objective is to align the robot’s policy with this reward function.\n\n\n      \n\n\n\noptimal policy (b) preference query 1 (c) preference query 2 (d) ARP-bb queries (e) SCOT queries (f) CS queries. In the preference queries, the human reward model prefers black to orange.\n\n\n\n?fig-island (a) shows all optimal actions at each state according to the human’s reward function. This optimal policy serves as the benchmark for alignment verification. ?fig-island (b) and ?fig-island (c) show two pairwise preference trajectory queries (black is preferable to orange according to ([eq: human_r])). Preference query 1 verifies that the robot values reaching the terminal goal state (green) rather than visiting more white states. Preference query 2 verifies that the robot values white states more than blue states. These two preference queries are all we need to determine whether the robot’s values are aligned with the human’s values.\nNext, we apply the heuristics discussed in the previous section to this grid world example. ?fig-island (d), ?fig-island (e), and ?fig-island (f) show the action queries requested by the heuristics ARP-bb, SCOT, and CS. Each heuristic queries the robot’s actions at specific states to assess alignment:\n\nARP-bb: This heuristic queries the fewest states but is myopic. It focuses on critical states derived from the ARP.\nSCOT: This heuristic generates maximally informative trajectories, querying more states than necessary but providing a comprehensive assessment.\nCS: This heuristic queries many redundant states, focusing on those where the value gap between optimal and average actions is significant.\n\nTo pass the test given by each heuristic, the robot’s action at each of the queried states must be optimal under the human’s reward function. The example demonstrates that while the ARP-bb heuristic is efficient, it might miss the broader context. SCOT provides a thorough assessment but at the cost of querying more states. CS focuses on high-impact states but includes redundant queries.\nIt is important to note that both the construction of the ARP and the heuristics rely on having an optimal policy for the human. Thus, in most practical settings we would simply use that policy on the robot without needing to bother with value alignment verification. As such, value alignment verification as presented here is more of an academic exercise rather than a tool of practical utility.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#human-centered-design",
    "href": "src/chap7.html#human-centered-design",
    "title": "7  Alternatives",
    "section": "7.2 Human-Centered Design",
    "text": "7.2 Human-Centered Design\nAfter understanding AI alignment, the next step is to explore practical methodologies for incorporating user feedback and ensuring that AI systems not only align with but also cater to the needs and preferences of their users. This section will provide insights into various Human-Centered Design techniques and their application in creating AI systems that are intuitive and ethically sound, ultimately enhancing the human-AI interaction experience.\n\n7.2.1 AI and Human-Computer Interaction\nHuman-Computer Interaction (HCI) is critical in the context of artificial intelligence because it focuses on designing systems that are intuitive and responsive to human needs. While human-robot interaction and other forms of human interaction with technology are important, HCI specifically addresses the broader and more common interfaces that people interact with daily. HCI principles ensure that AI systems are not only functional but also accessible and user-friendly, making them essential for the successful integration of AI into everyday life. By focusing on HCI, we can leverage established methodologies and insights to create AI systems that are more aligned with human values and needs.\nAt the heart of this exploration is the concept of human-in-the-loop processes. As AI systems become more sophisticated, their ability to simulate human decision-making processes and behaviors has increased, leading to innovative applications across various domains. The presentation by Meredith Morris, titled “Human-in-the-loop Computing: Reimagining Human-Computer Interaction in the Age of AI,” shows work in the integration of human intelligence with AI capabilities (Morris 2019). Projects like Soylent and LaMPost are highlighted as exemplary cases of this integration. Soylent is a Word plugin that uses human computation to help with editing tasks, while LaMPost is a platform that leverages crowd workers to aid in natural language processing tasks (Bernstein et al. 2010; Project 2017). These examples demonstrate how human input can significantly enhance AI outputs by leveraging the unique strengths of human cognition, thereby addressing complex AI problems that were previously unsolvable. For instance, Soylent can improve text quality by incorporating nuanced human feedback, and LaMPost can refine NLP tasks by incorporating human insights into language subtleties, both of which go beyond the capabilities of fully automated systems. However, the integration of human elements in AI systems brings up critical ethical considerations. The presentation discusses the changing perceptions of the ethics of human-in-the-loop processes. While the cost-effectiveness of human data labeling and other processes was once seen as beneficial, it is the ethical implications of such interactions that take precedence nowadays. This shift underscores the evolving norms in HCI and the importance of considering the ethical dimensions of human-AI interactions.\nThe role of diverse human perspectives plays a crucial role in enhancing AI systems. Involving a broad spectrum of users in the development and testing of AI systems ensures that these technologies are inclusive and representative of the global population, moving beyond the limitations of a WEIRD (Western, Educated, Industrialized, Rich, and Democratic) user base. The methodologies for collecting user feedback in HCI form a critical part of this discussion since they are vital in understanding user needs, preferences, and behaviors, which in turn inform the development of more user-centered AI systems. The presentation by Meredith Morris (Morris 2019) also highlights how these methods can be effectively employed to gain insights from users to ensure that AI systems are aligned with the real-world needs and expectations of users. In HCI, collecting user feedback is a fraught problem. When interacting with AI systems, the typical end user simply cares about tasks that the system can perform. Thus, a key question in HCI for AI is finding and understanding these tasks. Methodologies for collecting user feedback in HCI, are described as follow:\n\nStoryboarding is a visual method used to predict and explore the user experience with a product or service. A storyboard in HCI is typically a sequence of drawings with annotations that represent a user’s interactions with technology. This technique is borrowed from the film and animation industry and is used in HCI to convey a sequence of events or user flows, including the user’s actions, reactions, and emotions.\nWizard of Oz Studies is a method of user testing where participants interact with a system they believe to be autonomous, but which is actually being controlled or partially controlled by a human ‘wizard’ behind the scenes. This technique allows researchers to simulate the response of a system that may not yet be fully functional or developed.\n\nBoth Storyboarding and Wizard of Oz Studies are effective for engaging with users early in the design process. They help deal with the problem of gathering feedback on a product that doesn’t yet exist. Users often have difficulty imagining outcomes when they cannot touch a live demonstration.\n\nSurveys in HCI are structured tools that consist of a series of questions designed to be answered by a large number of participants. They can be conducted online, by telephone, through paper questionnaires, or using computer-assisted methods. Surveys are useful for collecting quantitative data from a broad audience, which can be analyzed statistically.\nInterviews in HCI are more in-depth and involve direct, two-way communication between the researcher and the participant. Interviews can be structured, semi-structured, or unstructured, ranging from tightly scripted question sets to open-ended conversations.\nFocus Groups involve a small group of participants discussing their experiences and opinions about a system or design, often with a moderator. Group dynamics can provide insights into collective user perspectives. In particular, users can bounce ideas off each other to provide richer feedback and quieter users who may not otherwise provide feedback may be encouraged by their peers.\nCommunity-Based Participatory Design (CBPD) is a human-centered approach that involves the people who will use a product in the design and development process. With CBPD, designers work closely with community members to identify problems, develop prototypes, and iterate based on community feedback. For example, when building a software product for deaf people, the engineering team can hire deaf engineers or designers to provide feedback as they collaboratively build the product.\nField Studies involve observing and collecting data on how users interact with a system in their natural environment. This method is based on the premise that observing users in their context provides a more accurate understanding of user behavior. It can include a variety of techniques like ethnography, contextual inquiries, and natural observations.\nLab-based studies are conducted in a controlled environment where the researchers can manipulate variables and observe user behavior in a setting designed to minimize external influences. Common lab-based methods include usability testing, controlled experiments, and eye-tracking studies.\nDiary Studies and Ethnography in HCI are a research method where participants are asked to keep a record of their interactions with a system or product over a while. This log may include text, images, and sometimes even audio or video recordings, depending on the study’s design. Participants typically document their activities, thoughts, feelings, and frustrations as they occur in their natural context.\nEthnography is a qualitative research method that involves observing and interacting with participants in their real-life environment. Ethnographers aim to immerse themselves in the user environment to get a deep understanding of the cultural, social, and organizational contexts that shape technology use.\n\nAs we have explored various methodologies for collecting human feedback, it becomes evident that the role of human input is indispensable in shaping AI systems that are not only effective but also ethically sound and user-centric. In the next step, we will elaborate on how to design AI systems for positive human impact, examining how socially aware and human-centered approaches can be employed to ensure that AI technologies contribute meaningfully to society. This includes understanding how AI can be utilized to address real-world challenges and create tangible benefits for individuals and communities.\n\n\n7.2.2 Designing AI for Positive Human Impact\nIn the field of natural language processing (NLP), the primary focus has traditionally been on quantitative metrics such as performance benchmarks, accuracy, and computations. These metrics have long guided the development and evaluation of the technologies. However, as the field evolves and becomes increasingly intertwined with human interactions like the recent popularity of Large Language Models (LLMs), a paradigm shift is becoming increasingly necessary. For example, these LLMs are shown to produce unethical or harmful responses or reflect values that only represent a certain group of people. The need for a human-centered approach in NLP development is crucial as these models are much more likely to be utilized in a broad spectrum of human-centric applications, impacting various aspects of daily life. This shift calls for an inclusive framework where LLMs are not only optimized for efficiency and accuracy but are also sensitized to ethical, cultural, and societal contexts. Integrating a human-centered perspective ensures that these models are developed with a deep understanding of, and respect for, the diversity and complexity of human values and social norms. This approach goes beyond merely preventing harmful outcomes; it also focuses on enhancing the positive impact of NLP technologies on society. In this session, we explore the intricacies of a human-centered approach in NLP development, focusing on three key themes: Socially Aware, Human-Centered, and Positively Impactful.\n\n7.2.2.1 Socially Aware\nIn the exploration of socially aware NLP, (Hovy and Yang 2021) presents a comprehensive taxonomy of seven social factors grounded in linguistic theory (See Figure 7.6).\n\n\n\n\n\n\nFigure 7.6: Taxonomy of social factors\n\n\n\nThis taxonomy illustrates both the current limitations and progressions in NLP as they pertain to each of these factors. The primary aim is to motivate the NLP community to integrate these social factors more effectively, thereby advancing towards a level of language understanding that more closely resembles human capabilities. The characteristics of speakers, encompassing variables such as age, gender, ethnicity, social class, and dialect, play a crucial role in language processing. Certain languages or dialects, often categorized as low-resource, are spoken by vulnerable populations that require special consideration in NLP systems. In many cases, the dominant culture and values are over-represented, leading to an inadvertent marginalization of minority perspectives. These minority voices must be not only recognized but also given equitable representation in language models. Additionally, norms and context are vital components in understanding linguistic behavior. They dictate the appropriateness of language use in various social situations and settings. Recognizing and adapting to these norms is a critical aspect of developing socially aware NLP systems that can effectively function across diverse social environments.\n\n\n7.2.2.2 Human-Centered\nThe Human-Centered aspect of NLP development emphasizes the creation of language models that prioritize the needs, preferences, and well-being of human users. This involves integrating human-centered design principles throughout the development stages of LLMs, which are described as follows:\n\nTask Formulation stage: Human-centered NLP development begins with understanding the specific problems and contexts in which users operate. This involves collaborating with end-users to identify their needs and challenges, ensuring that the tasks addressed by the models are relevant and meaningful to them. By engaging with users early in the process, developers can create models that are not only technically robust but also practically useful.\nData Collection stage: Human-centered principles ensure that the data used to train models is representative of the diverse user population. This includes collecting data from various demographic groups, languages, and cultural contexts to avoid biases that could lead to unfair or harmful outcomes. Ethical considerations are paramount, ensuring that data is collected with informed consent and respecting users’ privacy.\nData Processing in a human-centered approach involves carefully curating and annotating data to reflect the nuances of human language and behavior. This step includes filtering out potentially harmful content, addressing imbalances in the data, and ensuring that the labels and annotations are accurate and meaningful. By involving human annotators from diverse backgrounds, developers can capture a wider range of perspectives and reduce the risk of biased outputs.\nModel Training with a human-centered focus involves incorporating feedback from users and domain experts to fine-tune the models. This iterative process ensures that the models remain aligned with users’ needs and preferences. Techniques such as active learning, where the model queries users for the most informative examples, can be employed to improve the model’s performance.\nModel Evaluation in a human-centered framework goes beyond traditional metrics like accuracy and F1-score. It includes assessing the model’s impact on users, its fairness, and its ability to handle real-world scenarios. User studies and A/B testing can provide valuable insights into how the model performs in practice and how it affects users’ experiences.\nDeployment of human-centered NLP models involves continuous monitoring and feedback loops to ensure that the models remain effective and aligned with users’ needs over time. This includes setting up mechanisms for users to report issues and provide feedback, which can then be used to update and improve the models. Ensuring transparency in how the models operate and how user data is used also fosters trust and acceptance among users.\n\n\n\n7.2.2.3 Positively Impactful\nBuilding on the human-centered approach, it is crucial to consider how language models can be utilized and the broader impacts they can have on society.\nUtilization: LLMs offer socially beneficial applications across various domains such as public policy, mental health, and education. In public policy, they assist in analyzing large volumes of data to inform decision-making processes. In mental health, LLMs can provide personalized therapy and even train therapists by simulating patient interactions. In the education sector, they enable personalized learning experiences and language assistance, making education more accessible and effective. These examples demonstrate the versatility of LLMs in contributing positively to critical areas of human life.\nImpact: The deployment of NLP models, especially LLMs, has significant societal impacts. Positively, they enhance human productivity and creativity, offering tools and insights that streamline processes and foster innovative thinking. LLMs serve as powerful aids in various sectors, from education to industry, enhancing efficiency and enabling new forms of expression and problem-solving. it is essential to acknowledge the potential negative impacts. One major concern is the ability of LLMs to generate and spread misinformation. As these models become more adept at producing human-like text, distinguishing between AI-generated and human-created content becomes increasingly challenging. This raises issues of trust and reliability, with the risk of widespread dissemination of false or misleading information, which could have significant adverse effects on individuals and society.\nBy considering both the utilization and impact of LLMs, we can better harness their potential for positive societal contributions while mitigating the risks associated with their deployment. In conclusion, by thoughtfully integrating human-centered principles and ensuring positive impacts through feedback collection and ethical considerations, we can develop language models that not only enhance human well-being but also align closely with societal values. Building on these foundational principles, we now turn our attention to Adaptive User Interfaces, which exemplify the practical application of these concepts by personalizing interactions and improving user experiences in dynamic environments.\n\n\n\n7.2.3 Adaptive User Interfaces\nAdaptive user interfaces (AUIs) represent a significant advancement in personalizing user experiences by learning and adapting to individual preferences. This section will discuss the methodologies and applications of AUIs, highlighting their role in enhancing human-AI interaction through intelligent adaptation. The integration of AUIs within human-centered design paradigms ensures that AI systems not only meet user needs but also anticipate and adapt to their evolving preferences, thus maximizing positive human impact. Nowadays, consumers have more choices than ever and the need for personalized and intelligent assistance to make sense of the vast amount of information presented to them is clear.\n\n7.2.3.1 Key ideas\nIn general, personalized recommendation systems require a model or profile of the user. We categorize modeling approaches into four groups.\n\nUser-created profiles (usually done manually).\nManually defined groups that each user is classified into.\nAutomatically learned groups that each user is classified into.\nAdaptively learned individual user models from interactions with the recommendation system.\n\nThe last approach is referred to as adaptive user interfaces. This approach promises that each user is given the most personalization possible, leading to better outcomes. In this session, we discuss recommendation systems that adaptively learn an individual’s preferences and use that knowledge to intelligently recommend choices that the individual is more inclined to like.\nThe problem of learning individual models can be formalized as follows: a set of tasks requiring a user decision, a description for each task, and a history of the user’s decision on each task. This allows us to find a function that maps from task descriptions (features) to user decisions. Tasks can be described using crowd-sourced data (a collaborative approach) or measurable features of the task (a content-based approach). This session will focus on content-based approaches for describing tasks. After understanding the framework for adaptive user interfaces, it is useful to provide example applications to ground future discussions. Adaptive user interfaces have been developed for command and form completion, email filtering and filing, news selection and layout, browsing the internet, selecting movies and TV shows, online shopping, in-car navigation, interactive scheduling, and dialogue systems, among many other applications.\n\n\n7.2.3.2 Design\nThe goal of an adaptive user interface is to create a software tool that reduces human effort by acquiring a user model based on past user interactions. This is analogous to the goal of machine learning (ML) which is to create a software tool that improves some task performance by acquiring knowledge based on partial task experience. The design of an adaptive user interface can be broken up into six steps:\n\nFormulating the Problem: Given some task that an intelligent system could aid, the goal is to find a formulation that lets the assistant improve its performance over time by learning from interactions with a user. In this step the designer has to make design choices about what aspect of user behavior is predicted, and what is the proper level of granularity for description (i.e. what is a training example). This step usually involves formulating the problem into some sort of supervised learning framework.\nEngineering the Representation: At this stage we have a formulation of a task in ML terms and we need to represent the behavior and user model in such a way that makes computational learning not only tractable but as easy as possible. In this step, the designer has to make design choices about what information is used to make predictions, and how that information is encoded and passed to the model.\nCollecting User Traces: In this third step the goal is to find an effective way to collect traces (samples) of user behavior. The designer must choose how to translate traces into training data and also how to elicit traces from a user. An ideal adaptive user interface places no extra effort on the user to collect such traces.\nModeling the User: In this step the designer must decide what model class to use (neural network, decision tree, graphical model, etc.) and how to train the model (optimizer, step size, batch size, etc.). This step in the design process is usually given too much importance in academia. It is quite often the case that the success of an adaptive user interface is more sensitive to the other design steps.\nUsing the Model Effectively: At this stage the designer must decide how the model will be integrated into a software tool. Specifically, when and how is the model evaluated and how is the output of the model presented to the user? In addition, the designer must consider how to handle situations in which the model predictions are wrong. An ideal adaptive user interface will let the user take advantage of good predictions and ignore bad ones.\nGaining User Acceptance: The final step in the design process is to get users to try the system and ultimately adopt it. The initial attraction of users is often a marketing problem, but to retain users the system must be well-designed and easy to use.\n\n\n\n7.2.3.3 Applications\nAfter understanding the design of Adaptive User Interfaces, let’s take a look at how we can apply it to real-world problems. We will summarize and analyze three different application areas of learning human preferences, which are driving route advisor (Rogers, Fiechter, and Langley 1999), destination selection (Langley et al. 1999), and resource scheduling (Gervasio, Iba, and Langley 1999).\n1. Driving Route Advisor: The task of route selection involves determining a desirable path for a driver to take from their current location to a chosen destination, given the knowledge of available roads from a digital map. While computational route advisors exist in rental cars and online, they cannot personalize individual drivers’ preferences, which is a gap that adaptive user interfaces aim to fill by learning and recommending routes tailored to the driver’s unique choices and behaviors.\nHere is an approach to route selection through learning individual drivers’ route preferences.\n\nFormulation: Learn a “subjective” function to evaluate entire routes.\nRepresentation: Global route features are computable from digital maps.\nData collection: Preference of one complete route over another.\nInduction: A method for learning weights from preference data.\nUsing model: Apply subjective function to find “optimal” route.\n\nThis method aims to learn a user model that considers the entirety of a route, thereby avoiding issues like data fragmentation and credit assignment problems.\nThe design choices are incorporated into (Rogers, Fiechter, and Langley 1999), which: models driver preferences in terms of 14 global route features; gives the driver two alternative routes he might take; lets the driver refine these choices along route dimensions; uses driver choices to refine its model of his preferences; and invokes the driver model to recommend future routes. We note that providing drivers with choices lets the system collect data on route preferences in an unobtrusive manner. The interface of the application is presented in Figure 7.7.\n\n\n\n\n\n\nFigure 7.7: The adaptive route advisor.\n\n\n\nIn driving route advisor task (Rogers, Fiechter, and Langley 1999), a linear model is used for predicting the cost of a route based on the time, distance, number of intersections, and the number of turns. The system uses each training pair as a constraint on the weights found during the learning process. The experimental results are shown in the ?fig-exp-2.\n\n\n \n\n\n(Left) Experiments with 24 subjects show the Route Advisor improves its predictive ability rapidly with experience. (Right) Analyses also show that personalized user models produce better results than generalized models, even when given more data.\n\n\n2. Destination Selection: The task of destination selection involves assisting a driver in identifying one or more suitable destinations that fulfill a specific goal, such as finding a place to eat lunch, based on the driver’s current location and knowledge of nearby options. While there are many recommendation systems online, including those for restaurants, they are not ideally suited for drivers due to the driving environment’s demand for limited visual attention, thus necessitating a more tailored and accessible approach for in-car use.\nOne approach to destination recommendation can be cast as:\n\nFormulation: Learn to predict features the user cares about in items.\nRepresentation: Conditions/weights on attributes and values.\nData collection: Converse with the user to help him make decisions, noting whether he accepts or rejects questions and items.\nInduction: Any supervised induction method.\nUsing model: Guide the dialogue by selecting informative questions and suggesting likely values.\n\nThis design relies on the idea of a conversational user interface. Spoken-language versions of this approach appear well suited to the driving environment.\nThis approach is implemented in (Langley et al. 1999), where it engages in spoken conversations to help a user refine goals; incorporates a dialogue model to constrain this process; collects and stores traces of interaction with the user; and personalizes both its questions and recommended items. Their work focused on recommending restaurants to users who want advice about where to eat. This approach to recommendation would work well for drivers, it also has broader applications. We present experimental results in\n\n\n \n\n\n(Left) Speech Acts Per Conversation. (Right) Time Per Conversation.\n\n\n3. Resource Scheduling: The task of resource scheduling describes the challenge of allocating a limited set of resources to complete a set of tasks or jobs within a certain time frame, while also considering the constraints on both the jobs and the resources. Although automated scheduling systems are prevalent in various industries and some interactive schedulers exist, there is a distinct need for systems that can create personalized schedules reflecting the unique preferences of individual users.\nAn approach to personalized scheduling can be described as:\n\nFormulation: Learn a utility function to evaluate entire schedules.\nRepresentation: Global features are computable from the schedule.\nData collection: Preference of one candidate schedule over others.\nInduction: A method for learning weights from preference data.\nUsing model: Apply the ‘subjective’ function to find a good schedule.\n\nWe note that this method is similar to that in the Adaptive Route Advisor. However, it assumes a search through a space of complete schedules (a repair space), which requires some initial schedule. This approach is implemented in (Gervasio, Iba, and Langley 1999), where the interactive scheduler retrieves an initial schedule from a personalized case library; suggests to the user improved schedules from which to select; lets the user direct search to improve on certain dimensions; collects user choices to refine its personalized utility function; stores solutions in the case base to initialize future schedules; and invokes the user model to recommend future schedule repairs. As before, providing users with choices lets the system collect data on schedule preferences unobtrusively. An example of the interface, and the experimental results are shown in ?fig-exp-3.\n\n\n \n\n\n(Left) The interface of the INCA: Interactive Scheduling . (Right) Experiments with INCA suggest that retrieving personalized schedules helps users more as task difficulty increases. These experimental studies used a mixture of human and synthetic subjects.\n\n\n\n\n7.2.3.4 Limitations\nThe challenges of adaptive interfaces may involve: conceptualizing user modeling as a task suitable for inductive learning, crafting representations that facilitate the learning process, gathering training data from users in a way that doesn’t intrude on their experience, applying the learned user model effectively, ensuring the system can learn in real-time, and dealing with the necessity of learning from a limited number of training instances. These challenges are not only pertinent to adaptive interfaces but also intersect with broader applications of machine learning, while also introducing some unique issues. However, new sensor technology can bring promises to adaptive interfaces. Adaptive interfaces rely on user traces to drive their modeling process, so they stand to benefit from developments like GPS and cell phone locators, robust software for speech recognition, accurate eye and head trackers, real-time video interpreters, wearable body sensors (GSR, heart rate), and portable brain-wave sensors. As those devices become more widespread, they will offer new sources of data and support new types of adaptive services. In addition, adaptive interfaces can be viewed as a form of cognitive simulation that automatically generates knowledge structures to learn user preferences. They are capable of making explicit predictions about future user behavior and explaining individual differences through the process of personalization. This perspective views adaptive interfaces as tools that not only serve functional purposes but also model the psychological aspects of user interaction. Two distinct approaches within cognitive simulation are related to adaptive interfaces: process models that incorporate fundamental architectural principles, and content models that operate at the knowledge level, focusing on behavior. We note that both of them have roles to play, but content models are more relevant to personalization and adaptive interfaces.\nIn conclusion, adaptive user interfaces represent a significant advancement in creating personalized and efficient interactions between humans and technology. By leveraging modern sensor technologies and cognitive simulation approaches, these interfaces can dynamically learn and adapt to individual user preferences, enhancing overall user experience and system effectiveness. The methodologies discussed, from conceptualizing user models to collecting and utilizing user feedback, form the foundation of this innovative approach. As we transition to the next section, we will explore practical applications and real-world implementations of these human-centered AI principles through detailed case studies, illustrating the tangible impact of adaptive interfaces in various domains.\n\n\n\n7.2.4 Case Studies in Human-Centered AI\nIn this section, we examine practical examples that illustrate the application of human-centered principles in the development and deployment of AI systems. By examining these case studies, we aim to provide concrete insights into how AI technologies can be designed and implemented to better align with human values, enhance inclusivity, and address the specific needs of diverse user groups. The following case studies highlight different approaches and methodologies used to ensure that AI systems are not only effective but also considerate of the human experience.\n\n7.2.4.1 LaMPost Case Study\nIn our exploration of human-centered AI design, it is crucial to examine how metrics can be improved to better capture the human experience and address the shortcomings of traditional evaluation methods. The LaMPost case study (Goodman et al. 2022) exemplifies this effort by focusing on the development of an AI assistant designed to aid individuals with dyslexia in writing emails. This case is particularly relevant to our discussion because it highlights the importance of human-centered principles in AI development, especially in creating tools that cater to specific cognitive differences and enhance user experience.\nDyslexia is a cognitive difference that affects approximately 15 percent of language users, with varying degrees of impact on speaking, spelling, and writing abilities. It is a spectrum disorder, meaning symptoms and severity differ among individuals. More importantly, dyslexia is not an intellectual disability; many individuals with dyslexia possess high intelligence. Given the significant number of people affected by dyslexia, it is essential to develop AI tools that support their unique needs and enhance their daily tasks.\nThe LaMPost project sought to answer the question, “How can LLMs be applied to enhance the writing workflows of adults with dyslexia?” To address this, researchers employed a participatory design approach, involving employees with dyslexia from their company (Google) in the study. This approach ensured that the development process was inclusive and responsive to the actual needs and preferences of the dyslexic community. By focusing on the real-world application of LLMs in aiding email writing for dyslexic individuals, LaMPost serves as a powerful example of how AI can be designed to better capture and enhance the human experience.\nThe figure below allows users to see suggestions for rewriting selected text, helping them identify main ideas, suggest possible changes, and rewrite their selections to improve clarity and expression.\n\n\n\nThe Suggest Possible Changes feature from LaMPost.\n\n\nThe table below categorizes the challenges faced by users at different writing levels and the strategies they can use to overcome these challenges, illustrating the varied support needs addressed by LaMPost\n\n\n\n\n\nWriting level\n\n\nExamples of Challenges\n\n\nStrategies\n\n\n\n\n\n\nhigh\n\n\nexpressing ideas\n\n\n“word faucet”, ASR dictation\n\n\n\n\n\n\nordering ideas\n\n\npost-it outlining\n\n\n\n\nlow\n\n\nappropriate language\n\n\nproofreading\n\n\n\n\n\n\nparaphrasing\n\n\nfeedback\n\n\n\n\n\nUser challenged and strategies in LaMPost.\n\n\nNext, they ran a focus group to get initial ideas from members of the dyslexic community. This focus group helped them figure out what to measure and added the second research question: “How do adults with dyslexia feel about LLM-assisted writing?” In other words, how does the LLM impact users’ feelings of satisfaction, self-expression, self-efficacy, autonomy, and control?\nFrom this focus group, they went and created a prototype to answer the desires of the group. They included three features in their prototype model. One feature was: identifying main ideas. They focused on this to support overall clarity and organization of high-level ideas of the user. Another feature was suggest possible changes. They focused on this because users wanted to identify high-level adjustments to improve their writing. The last feature they added was rewrite my selections. They added this because users wanted help expressing ideas with a desired phrasing tone or style. This feature generated a rewrite based on a command you gave it.\nWith the prototype, the researchers evaluated again with 19 participants with dyslexia from outside their organization. They did a three-part study, including a demonstration and background on the system (25 min). Then they did a writing exercise with two real tasks (emails) each user had to do in the real world (25 min). For example, one task might have been to write an email to the principal of their child’s school to ask for a meeting. Then, the researchers did another follow-up interview for more qualitative data, e.g. to ask about specific choices users made when interacting with the model (25 min).\nLaMPost’s design prioritized autonomy by allowing users to choose the best option for their writing. One successful thing is that most users felt in control while writing. Users found that numerous options were helpful to filter through poor results. However, participants said the selection process was cognitively demanding and time-consuming. As we all know, features identified in LaMPost are all over the place, such as in Google Docs. Nonetheless, there remain many questions about the balance between automated writing and providing more control to the end users.\n\nHow could researchers hone in on this trade-off between the ease of automated writing and providing control to end-users?\nYou will need to design a study to approach this question.\n\nIdentify your research question, hypotheses, and the methods that you will use. (Hint: use the HCI methods described in the previous section.)\nScope the domain of your study appropriately—more broadly than dyslexia but not so broadly to be meaningless.\nWhat domains will you include? (E.g. students use ChatGPT for assignments, doctors use an LLM to write notes, etc.)\n\n\nIn this way, both the case study of LaMPost and its presaging of greater trends in LLM interfaces recapitulate the maxim of HCI: HCI is a cycle. You design a potential system, prototype it, get feedback from people, and iterate constantly. Next, we will explore two case studies that exemplify the application of human-centered principles in NLP. These case studies illustrate how LLMs can be adapted to foster social inclusivity and provide training in social skills.\n\n\n7.2.4.2 Multi-Value and DaDa: Cross-Dialectal English NLP\nEnglish NLP systems are largely trained to perform well in Standard American English - the form of written English found in professional settings and elsewhere. Not only is Standard American English the most well-represented form of English in textual datasets but NLP engineers and researchers often filter dialectal and vernacular English examples from their datasets to improve performance on SAE benchmarks. As a result, NLP systems are generally less performant when processing dialectal inputs than SAE inputs. This performance gap is observable over various benchmarks and tasks, like the SPIDER benchmark. (Chang et al. 2023)\n\n\n\nStress test reveals worse performance on the SPIDER benchmark with synthetic dialectical examples than with SAE.\n\n\nAs natural language systems become more pervasive, this performance gap increasingly represents a real allocational harm against dialectal English speakers — these speakers are excluded from using helpful systems and assistants. Multi-Value is a framework for evaluating foundation language models on dialectic input, and DADA is a framework for adapting LLMs to improve performance on dialectic input.\nSynthetic Dialectal Data\nZiems et al. (2023) create synthetic dialectal data for several English dialects (Appalachian English, Chicano English, Indian English, Colloquial Singapore English, and Urban African American English).(Ziems et al. 2023) They created synthetic data based on transforming SAE examples to have direct evaluation comparisons. These synthetic examples were created by leveraging known linguistic features of the dialects, such as negative concord in UAAVE. Figure 7.8 maps out the presence of various linguistic features.\n\n\n\n\n\n\nFigure 7.8: A comparative distribution of features in five dialects.\n\n\n\nThis synthetic data, while somewhat limited in the variety of samples. can produce and create realistic examples for benchmarking LM performance. Figure 7.9 demonstrates creating a synthetic dialectic example using the ‘give passive’ linguistic feature, illustrating the transformation process from SAE to a vernacular form.\n\n\n\n\n\n\nFigure 7.9: Execution of a sample transform using a documented linguistic feature.\n\n\n\nFeature Level Adapters One approach to the LLM adaption task would be to train an adapter for each dialect using a parameter-efficient fine-tuning method like low-rank adapters. (Hu et al. 2021) While adapters can certainly bridge the gap between SAE LMs and dialect inputs, this approach suffers from a couple of weaknesses, namely:\n\nIndividually trained adapters do not leverage similarities between low-resource dialects. Transfer learning is often helpful for training low-resource languages and dialects.\nThe model needs to know which adapter to use at inference time. This presupposes that we can accurately classify the dialect — sometimes based on as little as one utterance. This classification is not always possible — a more general approach is needed.\n\nTherefore, Liu et al. (2023) propose a novel solution — DADA: Dialect Adaption via Dynamic Aggregation of Linguistic Rules. (Liu, Held, and Yang 2023) DADA trains adapters on the linguistic feature level rather than the dialect level. The model can use multiple linguistic feature adapters via an additional fusion layer. They can therefore train using multi-dialectical data and cover linguistic variation via a comprehensive set of roughly 200 adapters. DADA saw an improvement in performance over single-dialect adapters for most dialects, as shown in Figure 7.10.\n\n\n\n\n\n\nFigure 7.10: Execution of a sample transform using a documented linguistic feature.\n\n\n\nThe Multi-Value and DADA case study underscores the importance of designing NLP systems that are inclusive and representative of diverse language users. By addressing the performance gaps in handling dialectal inputs, this case study highlights the necessity of incorporating diverse linguistic data and creating adaptable systems. This approach enhances AI functionality and accessibility, ensuring it respects and reflects linguistic diversity. Ultimately, the study reinforces human-centered design principles, demonstrating how AI can be tailored to better serve and empower all users. Moving forward, we will explore how LLMs can be utilized for social skill training, showcasing their potential to improve human interactions.\n\n\n7.2.4.3 Social Skill Training via LLMs\nThe emergence of Large Language Models (LLMs) marks a significant milestone in the field of social skills training. This case study explores the potential of LLMs to augment social skill development across diverse scenarios. More specifically, we discuss a dual-framework approach, where two distinct LLMs operate in tandem as a Partner and a Mentor, guiding human learners in their journey towards improved social interaction. In this framework, we have two agents which are\n\nAI Partner: LLM-empowered agents that users can engage with across various topics. This interactive model facilitates practical, conversation-based learning, enabling users to experiment with different communication styles and techniques or practice and develop specific skills in real-world scenarios in a safe, AI-mediated environment.\nAI Mentor: An LLM-empowered entity designed to provide constructive, personalized feedback based on the interaction of users and the AI Partner. This mentor analyzes conversation dynamics, identifies areas for improvement, offers tailored advice, and guides users toward effective social strategies and improved interaction skills.\n\nFor example, in conflict resolution, individuals learning to handle difficult conversations can use the AI Partner to simulate interactions with a digitalized partner. As a Conflict Resolution Expert, the AI Mentor helps analyze these interactions, offering strategies to navigate conflicts effectively.\nIn the educational sector, K-12 teachers aiming to incorporate more growth-mindset language into their teaching can practice with a digitalized student. An experienced teacher or mentor, represented by the AI Mentor, provides insights on effective communication and teaching methods. For negotiation training, students preparing to negotiate their first job offers can engage in simulated negotiations with a digitalized HR representative through the AI Partner. As a Negotiation Expert, the AI Mentor then offers guidance on negotiation tactics, helping students effectively articulate their values and negotiate job terms. Lastly, in therapy training, novice therapists can interact with a digitalized patient via the AI Partner to practice therapy sessions. The AI Mentor, functioning as a Therapy Coach, then reviews these sessions, providing feedback and suggestions on enhancing therapeutic techniques and patient engagement.\nCARE: Therapy Skill Training Hsu et al. (2023) introduced CARE (Hsu et al. 2023), a framework designed for therapy skill training. This framework leverages a simulated environment, enabling counselors to practice their skills without the risk of harming real individuals. An integral component of CARE is the AI Mentor, which offers invaluable feedback and guidance during the training process. See Figure 7.11 for the overview of the framework.\n\n\n\n\n\n\nFigure 7.11: CARE Framework\n\n\n\nCARE’s primary function is for novice therapists and counselors to assess and determine the most effective counseling strategies tailored to specific contexts. It provides counselors with customized example responses, which they can adopt, adapt, or disregard when interacting with a simulated support seeker. This approach is deeply rooted in the principles of Motivational Interviewing and utilizes a rich dataset of counseling conversations combined with LLMs. The effectiveness of CARE has been established through rigorous quantitative evaluations and qualitative user studies, which included simulated chats and semi-structured interviews. Notably, CARE has shown significant benefits in aiding novice counselors. From the assessment, counselors chose to use CARE 93% of the time, directly used a CARE response without editing 60% of the time, and sent more extended responses with CARE. Qualitatively, counselors noted several advantages of CARE, such as its ability to refresh memory on various strategies, inspire innovative responses, boost confidence, and save time during consultations. However, there were some drawbacks, including potential disruptions in the thought process, perceived limitations in response options, the requirement for decision-making, and the time needed to review suggestions. Overall, the framework is particularly beneficial for therapists new to the field, offering them a supportive and educational tool to enhance their counseling skills effectively.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#practice-exercises",
    "href": "src/chap7.html#practice-exercises",
    "title": "7  Alternatives",
    "section": "7.3 Practice Exercises",
    "text": "7.3 Practice Exercises",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/chap7.html#footnotes",
    "href": "src/chap7.html#footnotes",
    "title": "7  Alternatives",
    "section": "",
    "text": "GPT-4 is good at coming up with longer-rendered answers about why some things are appropriate or not.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Alternatives</span>"
    ]
  },
  {
    "objectID": "src/ack.html",
    "href": "src/ack.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Citation\nInitial versions of this book were compiled as lecture notes to the class CS329H: Machine Learning from Human Preferences at Stanford University taught in Fall 2023 and Fall 2024. We thank Rehaan Ahmad, Ahmed Ahmed, Jirayu Burapacheep, Michael Byun, Akash Chaurasia, Andrew Conkey, Tanvi Deshpande, Eric Han, Laya Iyer, Adarsh Jeewajee, Shreyas Kar, Arjun Karanam, Jared Moore, Aashiq Muhamed, Bidipta Sarkar, William Shabecoff, Stephan Sharkov, Max Sobol Mark, Kushal Thaman, Joe Vincent, Yibo Zhang, Duc Nguyen, Grace Sodunke, Ky Nguyen, and Mykkel Kochenderfer for their early contributions and feedback.\nThanks for reading our book! We hope you find this book useful in your research and teaching.",
    "crumbs": [
      "Acknowledgments"
    ]
  },
  {
    "objectID": "src/ack.html#citation",
    "href": "src/ack.html#citation",
    "title": "Acknowledgments",
    "section": "",
    "text": "BibTeX citation:\n@book{mlhp,\n  author    = {Truong, Sang and Haupt, Andreas and Koyejo, Sanmi},\n  title     = {{Machine Learning from Human Preferences}},\n  year      = {2025},\n  publisher = {Stanford University},\n  doi       = {},\n  note      = {}\n}\nFor attribution, please cite this work as:\n\nS. Truong, A. Haupt, and S. Koyejo. 2025. Machine Learning from Human Preferences. Stanford University.",
    "crumbs": [
      "Acknowledgments"
    ]
  }
]