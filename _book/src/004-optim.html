<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; Model-Free Preference Optimization – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/005-align.html" rel="next">
<link href="../src/003-measure.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a0aefded8822f1bee14b20ac4fd2b1d6.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-5c897cb370a42f0721f6bac59365aff2.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/004-optim.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#individual-preference-optimization-via-dueling-bandit" id="toc-individual-preference-optimization-via-dueling-bandit" class="nav-link active" data-scroll-target="#individual-preference-optimization-via-dueling-bandit"><span class="header-section-number">4.1</span> Individual Preference Optimization via Dueling Bandit</a>
  <ul class="collapse">
  <li><a href="#introduction-to-dueling-bandit-problem-and-its-extension" id="toc-introduction-to-dueling-bandit-problem-and-its-extension" class="nav-link" data-scroll-target="#introduction-to-dueling-bandit-problem-and-its-extension"><span class="header-section-number">4.1.1</span> Introduction to Dueling Bandit Problem and Its Extension</a></li>
  <li><a href="#regret" id="toc-regret" class="nav-link" data-scroll-target="#regret"><span class="header-section-number">4.1.2</span> Regret</a></li>
  <li><a href="#acquisition-functions" id="toc-acquisition-functions" class="nav-link" data-scroll-target="#acquisition-functions"><span class="header-section-number">4.1.3</span> Acquisition Functions</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">4.1.4</span> Applications</a></li>
  <li><a href="#incentive-compatible-online-learning" id="toc-incentive-compatible-online-learning" class="nav-link" data-scroll-target="#incentive-compatible-online-learning"><span class="header-section-number">4.1.5</span> Incentive-Compatible Online Learning</a></li>
  </ul></li>
  <li><a href="#preferential-bayesian-optimization" id="toc-preferential-bayesian-optimization" class="nav-link" data-scroll-target="#preferential-bayesian-optimization"><span class="header-section-number">4.2</span> Preferential Bayesian Optimization</a>
  <ul class="collapse">
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement"><span class="header-section-number">4.2.1</span> Problem statement</a></li>
  <li><a href="#acquisition-functions-1" id="toc-acquisition-functions-1" class="nav-link" data-scroll-target="#acquisition-functions-1"><span class="header-section-number">4.2.2</span> Acquisition Functions</a></li>
  <li><a href="#regret-analysis" id="toc-regret-analysis" class="nav-link" data-scroll-target="#regret-analysis"><span class="header-section-number">4.2.3</span> Regret Analysis</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">4.3</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#sec-question-1-preferential-bayesian-optimization-30-points" id="toc-sec-question-1-preferential-bayesian-optimization-30-points" class="nav-link" data-scroll-target="#sec-question-1-preferential-bayesian-optimization-30-points">Question 1: Preferential Bayesian Optimization (30 points)</a></li>
  <li><a href="#sec-question-2-linear-dueling-bandit-30-points" id="toc-sec-question-2-linear-dueling-bandit-30-points" class="nav-link" data-scroll-target="#sec-question-2-linear-dueling-bandit-30-points">Question 2: Linear Dueling Bandit (30 points)</a></li>
  <li><a href="#sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points" id="toc-sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points" class="nav-link" data-scroll-target="#sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points">Question 3: Multi-Objective Thompson Sampling in Linear Contextual Bandits (30 points)</a></li>
  <li><a href="#sec-question-4-mechanism-design-in-preference-learning-30-points" id="toc-sec-question-4-mechanism-design-in-preference-learning-30-points" class="nav-link" data-scroll-target="#sec-question-4-mechanism-design-in-preference-learning-30-points">Question 4: Mechanism Design in Preference Learning (30 points)</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/004-optim.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-model-free" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 1</a> <a href="https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 2</a></p>
<section id="individual-preference-optimization-via-dueling-bandit" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="individual-preference-optimization-via-dueling-bandit"><span class="header-section-number">4.1</span> Individual Preference Optimization via Dueling Bandit</h2>
<section id="introduction-to-dueling-bandit-problem-and-its-extension" class="level3" data-number="4.1.1">
<h3 data-number="4.1.1" class="anchored" data-anchor-id="introduction-to-dueling-bandit-problem-and-its-extension"><span class="header-section-number">4.1.1</span> Introduction to Dueling Bandit Problem and Its Extension</h3>
<p>The multi-armed bandit (MAB) problem involves a gambler deciding which lever to pull on an MAB machine to maximize the winning rate, despite not knowing which machine is the most rewarding. This scenario highlights the need to balance exploration (trying new machines to discover potential higher rewards) and exploitation (using current knowledge to maximize gains). MAB algorithms address this dilemma by making decisions under uncertainty to achieve the best possible outcomes based on gathered data. At the core of the MAB problem is a set of actions, or ‘arms,’ denoted by <span class="math inline">\(\mathcal{A} = \{1, 2, \ldots, K\}\)</span>, where <span class="math inline">\(K\)</span> signifies the total number of arms. For each round <span class="math inline">\(t\)</span>, the agent selects an arm <span class="math inline">\(a_t \in \mathcal{A}\)</span> and receives a reward <span class="math inline">\(r_t\)</span>, sampled from an arm-specific, unknown probability distribution. The expected reward of pulling arm <span class="math inline">\(a\)</span> is represented as <span class="math inline">\(\mu_a = \mathbb{E}[r_t | a]\)</span>.</p>
<p>The multi-armed bandit framework can be extended in various ways to model more complex scenarios. In the infinite-armed bandit problem, the set of possible arms <span class="math inline">\(\mathcal{A}\)</span> is either very large or infinite. This introduces significant challenges in exploration, as the agent cannot afford to explore each arm even once. Algorithms for infinite-armed bandits typically assume some regularity or structure of the reward function across arms to make the problem tractable. The contextual bandit problem extends the bandit framework by incorporating observable external states or contexts that influence the reward distributions of arms. The agent’s task is to learn policies that map contexts to arms to maximize reward. This model is particularly powerful for personalized recommendations, where the context can include user features or historical interactions. In dueling bandit problems, the agent chooses two arms to pull simultaneously and receives feedback only on which of the two is better, not the actual reward values. This pairwise comparison model is especially useful in scenarios where absolute evaluations are difficult, but relative preferences are easier to determine, such as in ranking systems.</p>
<p>Contextual bandits extend the multi-armed bandits by making decisions conditional on the state of the environment and previous observations. The benefit of such a model is that observing the environment can provide additional information, potentially leading to better rewards and outcomes. In each iteration, the agent is presented with the context of the environment, then decides on an action based on the context and previous observations. Finally, the agent observes the action’s outcome and reward. Throughout this process, the agent aims to maximize the expected reward.</p>
<p>In many real-world contexts, one may not have a real-valued reward (or at least a reliable one) associated with a decision. Instead, we may only have observations indicating which of a set of bandits was optimal in a given scenario. The assumption is that within these observations of preferred choices among a set of options, there is an implicit reward or payoff encapsulated in that decision. Consider the following examples:</p>
<ol type="1">
<li><p><strong>Dietary preferences</strong>: When providing food recommendations to humans, it is often not possible to quantify an explicit reward from recommending a specific food item. Instead, we can offer meal options and observe which one the person selects.</p></li>
<li><p><strong>Video recommendation</strong>: Websites like YouTube and TikTok recommend specific videos to users. It is typically not feasible to measure the reward a person gains from watching a video. However, we can infer that a user preferred one video over another. From these relative preference observations, we can develop a strategy to recommend videos they are likely to enjoy.</p></li>
<li><p><strong>Exoskeleton gait optimization</strong>: Tucker et al.&nbsp;(2020) created a framework that uses human-evaluated preferences for an exoskeleton gait algorithm to develop an optimal strategy for the exoskeleton to assist a human in walking. A human cannot reliably produce a numerical value for how well the exoskeleton helped them walk but can reliably indicate which option performed best according to their preferences.</p></li>
</ol>
<p>Generally, we assume access to a set of actions. A noteworthy assumption is that any observations we make are unbiased estimates of the payoff. This means that if we observe a human preferred one option over another (or several others), the preferred option had a higher implicit reward or payoff than the alternatives. In the case of dietary preferences, this may mean that a human liked the preferred option; in the case of video recommendations, a user was more entertained, satisfied, or educated by the video they selected than the other options.</p>
<p>The overarching context is that we do not have direct or reliable access to rewards. We may not have a reward at all (for some decisions, it may be impossible to define a real value to the outcome), or it may be noisy (for example, if we ask a human to rate their satisfaction on a scale of 1 to 10). We use relative comparisons to evaluate the best of multiple options in this case. Our goal is to minimize total regret in the face of noisy comparisons. Humans may not always provide consistent observations (since human decision-making is not guaranteed to be consistent). However, we can still determine an optimal strategy with the observed comparisons. We aim to minimize the frequency of sub-optimal decisions according to human preferences. In practice, many formulations of bandits can allow for infinitely many bandits (for example, in continuous-value and high-dimensional spaces). However, this situation can be intractable when determining an optimal decision strategy. With infinite options, how can we always ensure we have chosen the best? We will constrain our bandits to a discrete space to enable efficient exploration. We will assume that we have <span class="math inline">\(k\)</span> bandits, <span class="math inline">\(b_i, i \in [1, k]\)</span>, and our task is to choose the one that will minimize regret.</p>
<p>With the framework outlined, we now define our approach more formally. This method was introduced by <span class="citation" data-cites="YUE20121538">(<a href="#ref-YUE20121538" role="doc-biblioref">Yue et al. 2012</a>)</span>, and proofs for the guarantees and derivations of parameters can be found in their work.</p>
<p>To determine the optimal action, we will compare pairwise to ascertain the probability that an action <span class="math inline">\(b_i\)</span> is preferred over another <span class="math inline">\(b_j\)</span>, where <span class="math inline">\(i \ne j\)</span>. Concretely, we assume access to a function <span class="math inline">\(\epsilon\)</span> that helps determine this probability; in practice, this can be done with an oracle, such as asking a human which of two options they prefer: <span class="math display">\[P(b_i &gt; b_j) = \varepsilon(b_i, b_j) + \frac{1}{2}.\]</span> With this model, three basic properties govern the values provided by <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[\epsilon(b_i, b_j) = -\epsilon(b_j, b_i), \epsilon(b_i, b_i) = 0, \epsilon(b_i, b_j) \in \left(-\frac{1}{2}, \frac{1}{2} \right).\]</span></p>
<p>We assume there is a total ordering of bandits, such that <span class="math inline">\(b_i \succ b_j\)</span> implies <span class="math inline">\(\epsilon(b_i, b_j) &gt; 0\)</span>. We impose two constraints to properly model comparisons:</p>
<ul>
<li><p><strong>Strong Stochastic Transitivity</strong>: We must maintain our total ordering of bandits, and as such, the comparison model also respects this ordering: <span id="eq-stochastic-transitivity"><span class="math display">\[b_i \succ b_j \succ b_k \Rightarrow \epsilon(b_i, b_k) \ge \text{max}\{\epsilon(b_i, b_j), \epsilon(b_j, b_k)\}. \tag{4.1}\]</span></span></p></li>
<li><p><strong>Stochastic Triangle Inequality</strong>: We also impose a triangle inequality, which captures the condition that the probability of a bandit winning (or losing) a comparison will exhibit diminishing returns as it becomes increasingly superior (or inferior) to the competing bandit: <span id="eq-triangle-inequality"><span class="math display">\[b_i \succ b_j \succ b_k \Rightarrow \epsilon(b_i, b_k) \le \epsilon(b_i, b_j) + \epsilon(b_j, b_k). \tag{4.2}\]</span></span></p></li>
</ul>
<p>These assumptions may initially seem limiting; however, common models for comparisons satisfy these constraints. For example, the Bradley-Terry Model follows <span class="math inline">\(P(b_i &gt; b_j) = \frac{\mu_i}{\mu_i + \mu_j}\)</span>. The Gaussian model with unit variance also satisfies these constraints: <span class="math inline">\(P(b_i &gt; b_j) = P(X_i - X_j &gt; 0)\)</span>, where <span class="math inline">\(X_i - X_j \sim N(\mu_i - \mu_j, 2)\)</span>.</p>
<p>To accurately model the preferences between bandits in our framework of pairwise bandit comparisons and regret, we must track certain parameters in our algorithm. First, we will maintain a running empirical estimate of the probability of bandit preferences based on our observations. It is important to note that we do not have direct access to an <span class="math inline">\(\epsilon\)</span> function. Instead, we must present two bandits to a human, who selects a winner. To do this, we define: <span class="math display">\[\hat{P}_{i, j} = \frac{\# b_i\ \text{wins}}{\# \text{comparisons between}\ i \text{and}\ j}.\]</span></p>
<p>We will also compute confidence intervals at each timestep for each of the entries in <span class="math inline">\(\hat{P}\)</span> as <span class="math display">\[\hat{C}_t = \left( \hat{P}_t - c_t, \hat{P}_t + c_t \right),\]</span> where <span class="math inline">\(c_t = \sqrt{\frac{4\log(\frac{1}{\delta})}{t}}\)</span>. Note that <span class="math inline">\(\delta = \frac{1}{TK^2}\)</span>, where <span class="math inline">\(T\)</span> is the time horizon and <span class="math inline">\(K\)</span> is the number of bandits.</p>
<p>Previously, we discussed approaches for finding the best action in a specific context. Now, we consider changing contexts, which means there is no longer a static hidden preference matrix <span class="math inline">\(P\)</span>. Instead, at every time step, there is a preference matrix <span class="math inline">\(P_C\)</span> depending on context <span class="math inline">\(C\)</span>. We consider a context <span class="math inline">\(C\)</span> and a preference matrix <span class="math inline">\(P_C\)</span> to be chosen by nature as a result of the given environment (Yue et al., 2012). The goal of a contextual bandits algorithm is to find a policy <span class="math inline">\(\pi\)</span> that maps contexts to a Von Neumann winner distribution over our bandits. That is, our policy <span class="math inline">\(\pi\)</span> should map any context to some distribution over our bandits such that sampling from that distribution is preferred to a random action for that context.</p>
</section>
<section id="regret" class="level3" data-number="4.1.2">
<h3 data-number="4.1.2" class="anchored" data-anchor-id="regret"><span class="header-section-number">4.1.2</span> Regret</h3>
<p>The agent aims to pick a sequence of arms <span class="math inline">\((a_1, a_2, \ldots, a_T)\)</span> across a succession of time steps <span class="math inline">\(t = 1\)</span> to <span class="math inline">\(t = T\)</span> to maximize the total accumulated reward. Formally, the strategy seeks to maximize the sum of the expected rewards: <span class="math inline">\(\max_{a_1, \ldots, a_T} \mathbb{E} \left[\sum_{t=1}^{T} r_t\right]\)</span>. Regret is defined as the difference between the cumulative reward that could have been obtained by always pulling the best arm (in hindsight, after knowing the reward distributions) and the cumulative reward actually obtained by the algorithm. Formally, if <span class="math inline">\(\mu^*\)</span> is the expected reward of the best arm and <span class="math inline">\(\mu_{a_t}\)</span> is the expected reward of the arm chosen at time <span class="math inline">\(t\)</span>, the regret after <span class="math inline">\(T\)</span> time steps is given by <span class="math inline">\(R(T) = T \cdot \mu^* - \sum_{t=1}^{T} \mu_{a_t}\)</span>. The objective of a bandit algorithm is to minimize this regret over time, effectively learning to make decisions that are as close as possible to the decisions of an oracle that knows the reward distributions beforehand. Low regret indicates an algorithm that has often learned to choose well-performing arms, balancing the exploration of unknown arms with the exploitation of arms that are already known to perform well. Thus, an efficient bandit algorithm exhibits sub-linear regret growth, meaning that the average regret per round tends to zero as the number of rounds <span class="math inline">\(T\)</span> goes to infinity: <span class="math inline">\(\lim_{T \to \infty} \frac{R(T)}{T} = 0\)</span>. Minimizing regret is a cornerstone in the design of bandit algorithms, and its analysis helps in understanding the long-term efficiency and effectiveness of different bandit strategies.</p>
<p>As previously discussed, our goal is to select the bandit that minimizes a quantity that reflects regret or the cost of not selecting the optimal bandit at all times. We can leverage our comparison model to define a quantity for regret over some time horizon <span class="math inline">\(T\)</span>, which is the number of decisions we make (selecting what we think is the best bandit at each iteration). Assuming we know the best bandit <span class="math inline">\(b^*\)</span> (and we know that there <em>is</em> a best bandit, since there is a total ordering of our discrete bandits), we can define two notions of regret:</p>
<ul>
<li><p>Strong regret: aims to capture the fraction of users who would prefer the optimal bandit <span class="math inline">\(b^*\)</span> over the <em>worse</em> of the options <span class="math inline">\(b_1, b_2\)</span> we provide at a given step:<span class="math inline">\(R_T = \sum_{t = 1}^T \text{max} \left\{ \epsilon(b^*, b_1^{(t)}), \epsilon(b^*, b_2^{(t)}) \right\}\)</span></p></li>
<li><p>Weak regret: aims to capture the fraction of users who would prefer the optimal bandit <span class="math inline">\(b^*\)</span> over the <em>better</em> of the options <span class="math inline">\(b_1, b_2\)</span> we provide at a given step:<span class="math inline">\(\tilde{R}_T = \sum_{t = 1}^T \text{min} \left\{ \epsilon(b^*, b_1^{(t)}), \epsilon(b^*, b_2^{(t)}) \right\}\)</span></p></li>
</ul>
<p>The best bandit described in our regret definition is called a <strong>Condorcet Winner</strong>. This is the strongest form of winner. It’s the action <strong><span class="math inline">\(A_{i}\)</span></strong> which is preferred to each other action <strong><span class="math inline">\(A_j\)</span></strong> with <span class="math inline">\(p &gt; 0.5\)</span> in a head-to-head election. While the above introduced notions of regret assume an overall best bandit to exist, there might be settings, where no bandit wins more than half head-to-head duels. A set of actions without a Condorcet winner is described by the following preference matrix, where each entry <span class="math inline">\(\Delta_{jk}\)</span> is <span class="math inline">\(p(j \succ k) - 0.5\)</span>, the probability that action <span class="math inline">\(j\)</span> is preferred over action <span class="math inline">\(k\)</span> minus 0.5. There is no Condorcet winner as there is no action that is preferred with <span class="math inline">\(p &gt; 0.5\)</span> over all other actions. Imagine, you want to find the best pizza to eat (=action). There may not be a pizza that wins more than half of the head-to-head duels against every other pizza.</p>
<p>However, we might still have an intuition of the best pizza. Therefore Sui et al., 2018 introduce the concepts of different <span class="math inline">\(\textit{winners}\)</span> in dueling bandit problems <span class="citation" data-cites="advancements_dueling">(<a href="#ref-advancements_dueling" role="doc-biblioref">Sui et al. 2018</a>)</span>. In this example, we might define the best pizza as the most popular one. We call the Pizza receiving the most votes in a public vote the <strong>Borda Winner</strong>, or formally, Borda winner <span class="math inline">\(j = \arg\max_{i \in A, i \neq j} \left(\sum p(j \succ i)\right)\)</span>. In contrast to the Condorcet Winner setting, there is always guaranteed to be one or more (in the case of a tie) Borda winners for a set of actions. However - if there is a Condorcet Winner, this might not necessarily be the same as a Borda Winner: In our Pizza example, a Pepperoni Pizza might win more than half of its head-to-head duels, while the Cheese-Pizza is still the most popular in a public poll.</p>
<p>A more generic concept of winner is the <strong>Von Neumann Winner</strong>, which describes a probability distribution rather than a single bandit winner. A Von Neumann winner simply prescribes a probability distribution <span class="math inline">\(W\)</span> such that sampling from this distribution ‘beats’ an action from the random uniform distribution with <span class="math inline">\(p &gt; 0.5\)</span>. In our pizza example, this would correspond to trusting a friend to order whichever Pizza he likes, because this may still be preferred to ordering randomly. Formally, <span class="math inline">\(W\)</span> is a Von Neumann if <span class="math inline">\((j \sim W, k \sim R) [p(p(j \succ k) &gt; 0.5) &gt; 0.5]\)</span> where <span class="math inline">\(R\)</span> describes the uniform probability distribution over our actions. The concept of a Von Neumann winner is useful in contextual bandits, which will be introduced later. In these settings, the preference matrix depends on different context, which may have different Borda winners, just as different parties may vote for different pizzas.</p>
<div id="fig-condorcet_violation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-condorcet_violation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th style="text-align: center;">A</th>
<th style="text-align: center;">B</th>
<th style="text-align: center;">C</th>
<th style="text-align: center;">D</th>
<th style="text-align: center;">E</th>
<th style="text-align: center;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"><strong>0.03</strong></td>
<td style="text-align: center;"><strong>-0.02</strong></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr class="even">
<td>B</td>
<td style="text-align: center;">-0.03</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"><strong>0.03</strong></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.11</td>
</tr>
<tr class="odd">
<td>C</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-0.03</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.09</td>
</tr>
<tr class="even">
<td>D</td>
<td style="text-align: center;">-0.06</td>
<td style="text-align: center;">-0.05</td>
<td style="text-align: center;">-0.04</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr class="odd">
<td>E</td>
<td style="text-align: center;">-0.10</td>
<td style="text-align: center;">-0.08</td>
<td style="text-align: center;">-0.07</td>
<td style="text-align: center;">-0.05</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr class="even">
<td>F</td>
<td style="text-align: center;">-0.11</td>
<td style="text-align: center;">-0.11</td>
<td style="text-align: center;">-0.09</td>
<td style="text-align: center;">-0.07</td>
<td style="text-align: center;">-0.03</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-condorcet_violation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Violation of Condorcet Winner. Highlighted entries are different from Table 1. No Condorcet winner exists as no arm could beat every other arm.
</figcaption>
</figure>
</div>
<p>Next, we introduce two performance measures for the planner. The <strong>asymptotic ex-post regret</strong> is defined as <span class="math display">\[\text{Regret}(\mu_1, \ldots \mu_K) = T\cdot \max_i \mu_i - \sum_{i=1}^T E[\mu_{I_t}].\]</span></p>
<p>Intuitively, this represents the difference between the reward achieved by always taking the action with the highest possible reward and the expected welfare of the recommendation algorithm (based on the actions it recommends at each timestep).</p>
<p>We also define a weaker performance measure, the <strong>Bayesian regret</strong>, which is defined as <span class="math display">\[\text {Bayesian regret}=E_{\mu_1, \ldots, \mu_K \sim \text {Prior}}\left[\operatorname{Regret}\left(\mu_1, \ldots, \mu_K\right)\right]\]</span></p>
<p>With a Bayesian optimal policy, we would like either definition of regret to vanish as <span class="math inline">\(T\to \infty\)</span>; we are considering “large-market optimal" settings where there are many short-lived, rather than a few long-term, users. Note the fact that ex-post regret is prior-free makes it robust to inaccuracies on the prior.</p>
</section>
<section id="acquisition-functions" class="level3" data-number="4.1.3">
<h3 data-number="4.1.3" class="anchored" data-anchor-id="acquisition-functions"><span class="header-section-number">4.1.3</span> Acquisition Functions</h3>
<p>Various strategies have been developed to balance the exploration-exploitation trade-off. These strategies differ in selecting arms based on past experiences and rewards.</p>
<section id="classical-acquisition-functions" class="level4" data-number="4.1.3.1">
<h4 data-number="4.1.3.1" class="anchored" data-anchor-id="classical-acquisition-functions"><span class="header-section-number">4.1.3.1</span> Classical Acquisition Functions</h4>
<p><strong>Uniform</strong> acquisition function is the most straightforward approach where each arm is selected uniformly randomly over time. This strategy does not consider the past rewards and treats each arm equally promising regardless of the observed outcomes. It is a purely explorative strategy that ensures each arm is sampled enough to estimate its expected reward, but it does not exploit the information to optimize rewards. In mathematical terms, if <span class="math inline">\(N_t(a)\)</span> denotes the number of times arm <span class="math inline">\(a\)</span> has been selected up to time <span class="math inline">\(t\)</span>, the Uniform Strategy would ensure that <span class="math inline">\(N_t(a) \approx \frac{t}{K}\)</span> for all arms <span class="math inline">\(a\)</span> as <span class="math inline">\(t\)</span> grows large: <span class="math inline">\(P(a_t = a) = \frac{1}{K}\)</span></p>
<p>The <strong>Epsilon Greedy</strong> is a popular method that introduces a balance between exploration and exploitation. With a small probability <span class="math inline">\(\epsilon\)</span>, it explores by choosing an arm at random, and with a probability <span class="math inline">\(1 - \epsilon\)</span>, it exploits by selecting the arm with the highest estimated reward so far. This strategy incrementally favors actions that have historically yielded higher rewards, but still allows for occasional exploration to discover better options potentially. The parameter <span class="math inline">\(\epsilon\)</span> is chosen based on the desired exploration level, often set between 0.01 and 0.1. <span class="math display">\[P(a_t = a) =
\begin{cases}
\frac{\epsilon}{K} + 1 - \epsilon &amp; \text{if } a = \arg\max_{a'} \hat{\mu}_{a'} \\
\frac{\epsilon}{K} &amp; \text{otherwise}
\end{cases}\]</span></p>
<p><strong>Upper Confidence Bound</strong> (UCB) acquisition function takes a more sophisticated approach to the exploration-exploitation dilemma. It selects arms based on both the estimated rewards and the uncertainty or variance associated with those estimates. Specifically, it favors arms with high upper confidence bounds on the estimated rewards, which is a sum of the estimated mean and a confidence interval that decreases with the number of times the arm has been played. This ensures that arms with less certainty (those played less often) are considered more often, naturally balancing exploration with exploitation as the uncertainty is reduced over time.</p>
<p><span class="math display">\[P(a_t = a) =
\begin{cases}
1 &amp; \text{if } a = \arg\max_{a'} \left( \hat{\mu}_{a'} + \sqrt{\frac{2 \ln t}{N_t(a')}} \right) \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p>
</section>
<section id="interleaved-filter" class="level4" data-number="4.1.3.2">
<h4 data-number="4.1.3.2" class="anchored" data-anchor-id="interleaved-filter"><span class="header-section-number">4.1.3.2</span> Interleaved Filter</h4>
<p>This algorithm tries to find the best bandit (Condorcet Winner) in a discrete, limited bandit-space via pairwise comparisons of the bandits. We will now introduce the algorithm for the Interleaved Filter as provided in <span class="citation" data-cites="YUE20121538">(<a href="#ref-YUE20121538" role="doc-biblioref">Yue et al. 2012</a>)</span> to solve a dueling bandit setup. It starts with a randomly defined <em>best bandit</em> <span class="math inline">\(\hat{b}\)</span> and iteratively compares it to set <span class="math inline">\(W\)</span> containing the remaining bandits <span class="math inline">\(b\)</span> resulting in winning probabilities <span class="math inline">\(\hat{P}_{\hat{b},b}\)</span> and confidence interval <span class="math inline">\(\hat{C}_{\hat{b},b}\)</span>. If a bandit <span class="math inline">\(b\)</span> is <em>confidently worse</em> than <span class="math inline">\(\hat{b}\)</span>, it is removed from <span class="math inline">\(W\)</span>. If a bandit <span class="math inline">\(b'\)</span> is <em>confidently better</em> than <span class="math inline">\(\hat{b}\)</span>, it is set as new <em>best bandit</em> <span class="math inline">\(\hat{b}\)</span> and bandit <span class="math inline">\(\hat{b}\)</span> as well as every other bandit <span class="math inline">\(b\)</span> <em>worse</em> than <span class="math inline">\(\hat{b}\)</span> are removed from <span class="math inline">\(W\)</span>. This is done, until <span class="math inline">\(W\)</span> is empty, leaving the final <span class="math inline">\(\hat{b}\)</span> as the predicted best bandit.</p>
<div class="algorithm">
<div class="algorithmic">
<p><strong>input:</strong> <span class="math inline">\(T\)</span>, <span class="math inline">\(B=\{b_1, \dots, b_k\}\)</span> <span class="math inline">\(\delta \gets 1/(TK^2)\)</span> Choose <span class="math inline">\(\hat{b} \in B\)</span> randomly <span class="math inline">\(W \gets \{b_1, \dots, b_k\} \backslash \{\hat{b}\}\)</span> <span class="math inline">\(\forall b \in W\)</span>, maintain estimate <span class="math inline">\(\hat{P}_{\hat{b},b}\)</span> of <span class="math inline">\(P(\hat{b} &gt; b)\)</span> according to (6) <span class="math inline">\(\forall b \in W\)</span>, maintain <span class="math inline">\(1 - \delta\)</span> confidence interval <span class="math inline">\(\hat{C}_{\hat{b},b}\)</span> of <span class="math inline">\(\hat{P}_{\hat{b},b}\)</span> according to (7), (8) compare <span class="math inline">\(\hat{b}\)</span> and <span class="math inline">\(b\)</span> update <span class="math inline">\(\hat{P}_{\hat{b},b}\)</span>, <span class="math inline">\(\hat{C}_{\hat{b},b}\)</span> <span class="math inline">\(W \gets W \backslash \{b\}\)</span></p>
<p><span class="math inline">\(W \gets W \backslash \{b\}\)</span> <span class="math inline">\(\hat{b} \gets b'\)</span>, <span class="math inline">\(W \gets W \backslash \{b'\}\)</span> <span class="math inline">\(\forall b \in W\)</span>, reset <span class="math inline">\(\hat{P}_{\hat{b},b}\)</span> and <span class="math inline">\(\hat{C}_{\hat{b},b}\)</span> <span class="math inline">\(\hat{T} \gets\)</span> Total Comparisons Made <span class="math inline">\((\hat{b}, \hat{T})\)</span></p>
</div>
</div>
<dl>
<dt>Parameter Initialization</dt>
<dd>
<p>In lines 1-6 of the algorithm, we take the inputs and first compute the value <span class="math inline">\(\delta\)</span> which is used to compute our confidence intervals. We select an initial guess of an optimal bandit <span class="math inline">\(\hat{b}\)</span> by uniformly sampling from all bandits <span class="math inline">\(\mathcal{B}\)</span>. We also keep a running set of bandit candidates <span class="math inline">\(W\)</span>, which is initialized to be <span class="math inline">\(\mathcal{B} \setminus \{\hat{b}\}\)</span>. At this point, we also initialize our empirical estimates for <span class="math inline">\(\hat{P}, \hat{C}\)</span>.</p>
<p>Next, we will repeat several steps until our working set of bandit candidates <span class="math inline">\(W\)</span> is empty.</p>
</dd>
<dt>Update Estimates Based on Comparisons</dt>
<dd>
<p>The first step at each iteration (lines 8-11) is to look at all candidates in <span class="math inline">\(W\)</span>, and compare them to our current guess <span class="math inline">\(\hat{b}\)</span> using an oracle (e.g.&nbsp;by asking a human which of <span class="math inline">\(\hat{b}\)</span> or <span class="math inline">\(b \in W\)</span> is preferred). With this new set of wins and comparisons, we update our estimates of <span class="math inline">\(\hat{P}, \hat{C}\)</span>.</p>
</dd>
<dt>Prune Suboptimal Bandits</dt>
<dd>
<p>In lines 12-13, with updated comparison win probabilities and corresponding confidence intervals, we can remove bandit candidates from <span class="math inline">\(W\)</span> that we are <em>confident</em> <span class="math inline">\(\hat{b}\)</span> is better than. The intuition here is that we are mostly sure that our current best guess is better than some of the candidates, and we don’t need to consider those candidates in future iterations.</p>
</dd>
<dt>Check for Better Bandits from Candidate Set</dt>
<dd>
<p>Now that our candidate set of bandits may be smaller, in lines 15-21 we check if there are any bandits <span class="math inline">\(b'\)</span> that we are <em>confident</em> are better than our current best guess. If we do find such a candidate, we remove bandits which <span class="math inline">\(\hat{P}\)</span> indicates <span class="math inline">\(b\)</span> is <em>likely</em> worse than <span class="math inline">\(\hat{b}\)</span>. Note that in this step, we do not require the probability to be outside the confidence interval, since we already found one we believe to be significantly closer to optimal than our current best guess.</p>
<p>Once we remove the candidates <em>likely</em> worse than <span class="math inline">\(\hat{b}\)</span>, we crown <span class="math inline">\(b'\)</span> as the new best guess, e.g.&nbsp;<span class="math inline">\(\hat{b} := b'\)</span>. Consequently, we remove <span class="math inline">\(b'\)</span> from <span class="math inline">\(W\)</span> and reset our empirical win counters <span class="math inline">\(\hat{P}, \hat{C}\)</span>.</p>
</dd>
</dl>
<p>With this algorithm defined, let us look at some provisions of the method with respect to identifying the optimal strategy. Note that the proofs and derivations for these quantities are provided in <span class="citation" data-cites="YUE20121538">(<a href="#ref-YUE20121538" role="doc-biblioref">Yue et al. 2012</a>)</span>.</p>
<p>First, the method guarantees that for the provided time horizon <span class="math inline">\(T\)</span>, the algorithm returns the correct bandit with probability <span class="math inline">\(P \ge 1 - \frac{1}{T}\)</span>. It is interesting and useful to note that if one has a strict requirement for the probability of identifying the correct bandit, one can compute the time horizon <span class="math inline">\(T\)</span> that guarantees this outcome at that probability. Furthermore, a time horizon of 1 leaves no probabilistic guarantee of a successful outcome, and increasing <span class="math inline">\(T\)</span> has diminishing returns. Second, in the event that the algorithm returns an incorrect bandit, the maximal regret incurred is linear with respect to <span class="math inline">\(T\)</span>, e.g.&nbsp;<span class="math inline">\(\mathcal(O)(T)\)</span>. This is also a useful provision as it allows us to estimate the overall cost in the worst case outcome. Based on these two provisions, we can compute the expected cumulative regret from running the Interleaved Filter algorithm, which is: <span class="math display">\[\mathbb{E}\left[R_T\right] \le \left(1 - \frac{1}{T}\right) \mathbb{E}\left[ R_T^{IF} \right] + \frac{1}{T}\mathcal{O}(T) \\
= \mathcal{O}\left(\mathbb{E}\left[ R_T^{IF} \right] + 1\right)\]</span></p>
<p>Interestingly, the original work shows that these bounds hold for both strong and weak regret. As demonstrated, the Interleaved Filter algorithm <a href="#fig-if" data-reference-type="ref" data-reference="fig-if">[fig-if]</a> provides a robust method to ascertain the optimal bandit or strategy given a set of options and only noisy comparisons. In most real-world scenarios for modeling human preferences, it is not possible to observe a real-world reward value, or at least a reliable one and as such this method is a useful way to properly model human preferences.</p>
<p>Furthermore, the algorithm provides strong guarantees for the probability of selecting the correct bandit, maximal regret, and the number of comparisons required. It is even more impressive that the method can do so without severely limiting constraints; as demonstrated, the most commonly used models satisfy the imposed constraints.</p>
<p>As we look to model human preferences, we can certainly leverage this method for k-armed dueling bandits to identify the best strategy to solve human-centric challenges, from video recommendation to meal selection and exoskeleton-assisted walking.</p>
</section>
<section id="dueling-bandit-gradient-descent" class="level4" data-number="4.1.3.3">
<h4 data-number="4.1.3.3" class="anchored" data-anchor-id="dueling-bandit-gradient-descent"><span class="header-section-number">4.1.3.3</span> Dueling Bandit Gradient Descent</h4>
<p>This algorithm tries to find the best bandit in a continuous bandit-space. Here, the set of all bandits is regarded as an Information-Retrieval (IR) system with infinite bandits uniquely defined by <span class="math inline">\(w\)</span>. We will cover the <em>Dueling Bandit Gradient Descent</em> algorithm from Yue and Joachims 2009 <span class="citation" data-cites="IR">(<a href="#ref-IR" role="doc-biblioref">Yue and Joachims 2009</a>)</span>. Yue and Joachims use the dueling bandits formulation for online IR optimization. They propose a retrieval system parameterized by a set of continuous variables lying in <span class="math inline">\(W\)</span>, a <span class="math inline">\(d\)</span>-dimensional unit-sphere. The DBGD algorithm adapts the current parameters <span class="math inline">\(w_t\)</span> of IR system by comparison with slightly altered parameters <span class="math inline">\(w_t'\)</span> both querying query <span class="math inline">\(q_t\)</span>. Only if the IR outcome using <span class="math inline">\(w_t'\)</span> is preferred, the parameters are changed in their direction. We will now discuss the algorithm more detailed.</p>
<div class="algorithm">
<div class="algorithmic">
<p><strong>input:</strong> <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta\)</span>, <span class="math inline">\(w_1\)</span></p>
<p>Sample unit vector <span class="math inline">\(u_t\)</span> uniformly</p>
<p><span class="math inline">\(w_t' \gets P_W(w_t + \delta u_t)\)</span></p>
<p>Compare <span class="math inline">\(w_t\)</span> and <span class="math inline">\(w_t'\)</span></p>
<p><span class="math inline">\(w_{t+1} \gets P_W(w_t + \gamma u_t)\)</span></p>
<p><span class="math inline">\(w_{t+1} \gets w_t\)</span></p>
</div>
</div>
<p>We first choose exploration step length <span class="math inline">\(\delta\)</span>, exploitation step length <span class="math inline">\(\gamma\)</span>, and starting point (in unit-sphere) <span class="math inline">\(w_1\)</span>. Choose a query and sample a random unit vector <span class="math inline">\(u_t\)</span>. We duel <span class="math inline">\(w_t\)</span> and <span class="math inline">\(w_t'\)</span>, where <span class="math inline">\(w_t\)</span> is our current point in the sphere, and <span class="math inline">\(w_t'\)</span> is our exploratory comparison, which is generated by taking a random step of length <span class="math inline">\(\delta\)</span>, such that <span class="math inline">\(w_t' = w_t + \delta u_t\)</span>. The objective of this duel is to ascertain the binary preference of users with respect to the results yielded by the IR systems parameterized by <span class="math inline">\(w_t\)</span> and <span class="math inline">\(w_t'\)</span> respectively, taking query <span class="math inline">\(q_t\)</span> as an input. The parameters that get the majority of the votes in the head to head win. If <span class="math inline">\(w_t\)</span> wins, then we keep the parameters for the next iteration. If <span class="math inline">\(w_t'\)</span> wins the duel, we update our parameters in the direction of <span class="math inline">\(u_t\)</span> by taking a step of length <span class="math inline">\(\gamma\)</span>. Note that the algorithm describes projection operation <span class="math inline">\(P_W(\overrightarrow{v})\)</span>. Since <span class="math inline">\(u_t\)</span> is chosen randomly, <span class="math inline">\(w_t + \delta u_t\)</span> or <span class="math inline">\(w_t + \gamma u_t\)</span> could exist outside of the unit sphere where all possible parameter configurations lie. In this case, we simply project the point back onto the sphere using said projection <span class="math inline">\(P_W(\overrightarrow{v})\)</span>.</p>
<p>Yue and Joachims show that this algorithm has sublinear regret in <span class="math inline">\(T\)</span>, the number of iterations. We note that the algorithm assumes that there exists a hidden reward function <span class="math inline">\(R(w)\)</span> that maps system parameters <span class="math inline">\(w_t\)</span> to a reward value which is smooth and strictly concave over the input space <span class="math inline">\(W\)</span>.</p>
<p>Lastly, we would also like to give motivation behind <span class="math inline">\(\delta\)</span> and <span class="math inline">\(\gamma\)</span> being different values. We need a <span class="math inline">\(\delta\)</span> that is sufficiently large that the comparison between a system parameterized by <span class="math inline">\(w_t\)</span> and <span class="math inline">\(w_t'\)</span> is meaningful. On the other hand, we may wish to take a smaller step in the direction of <span class="math inline">\(w_t'\)</span> during our update step, as during a duel, we only score <span class="math inline">\(w_t\)</span> against <span class="math inline">\(w_t'\)</span> over the results on one query <span class="math inline">\(q_t\)</span>. Having <span class="math inline">\(\delta &gt; \gamma\)</span> allows us to get reward signal from meaningfully different points while also updating our belief of the best point <span class="math inline">\(w_{\text{best}}\)</span> gradually.</p>
</section>
<section id="sparring-exp4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sparring-exp4">Sparring EXP4</h4>
<p>Zoghi et al.&nbsp;2015 propose one algorithm for this problem — sparring EXP4, which duels two traditional EXP4 - algorithms. The (traditional) EXP4 algorithm solves the traditional contextual bandits — the case where we can directly observe a reward for a choice of bandit given a context. The EXP4 algorithm embeds each bandit as a vector. When the algorithm sees the context (called ‘advice’ in this formulation), it produces a probability distribution over the choices based on an adjusted softmax function on the inner product between the context and the bandit vectors. The probability function is different from a softmax as we assign some minimum probability that any action gets chosen to enforce exploration. A reward is then observed for the choice and propagated back through the embedding of the chosen bandit.</p>
<p>Sparring EXP4 runs two instances of the EXP4 algorithm against each other. Each EXP4 instance samples an action given a context, and then these choices are ‘dueled’ against each other. Instead of directly observing a reward, as for traditional EXP4, we instead observe two converse reward — a positive reward for the choice that won the duel and a negative reward to the choice that lost. The reward is proportional to the degree to which the bandit wins the duel, i.e.&nbsp;how likely the bandit is to be preferred over the other when users are queried for binary preferences. Like in traditional EXP4, the reward or negative reward is then propagated back through the representations of the bandits.</p>
</section>
<section id="feel-good-thompson-sampling" class="level4" data-number="4.1.3.4">
<h4 data-number="4.1.3.4" class="anchored" data-anchor-id="feel-good-thompson-sampling"><span class="header-section-number">4.1.3.4</span> Feel-good Thompson sampling</h4>
<p>This algorithm is a solution for the contextual dueling bandit setting, and tries to minimize cumulative average regret (= find WHAT WINNER?!Von Neumann???): <span class="math display">\[\text{Regret}(T) := \sum_{t=1}^{T} \left[ r_{*}(x_t, a_{t}^{*}) - \frac{r_{*}(x_t, a_{t}^{1}) + r_{*}(x_t, a_{t}^{2})}{2} \right],\]</span> where <span class="math inline">\(r_{*}(x_t, a_{t})\)</span> is the true, hidden reward function of a context <span class="math inline">\(x_t\)</span> and action <span class="math inline">\(a_t\)</span>. Thompson sampling is an iterative process of receiving preference over two actions, each maximizing a different approximation of the reward function based on past data and adding this new information to the data.</p>
<p>Finding good approximations of the reward function at time <span class="math inline">\(t\)</span> is done by sampling two reward function parameters <span class="math inline">\(\theta_t^{j=1}\)</span> and <span class="math inline">\(\theta_t^{j=2}\)</span> from a posterior distribution based on all previous data <span class="math inline">\(p_j(\cdot \mid S_{t-1})\)</span>. This posterior distribution is proportional to the multiplication of the prior and the likelihood function, which is a Gaussian in standard Thompson sampling. In Feel-Good Thompson sampling, an additional term called "Feel-good exploration" encourages parameters <span class="math inline">\(\theta\)</span> with a large maximum reward in previous rounds. This change to the likelihood function may increase probabilities in uncertain areas, thus exploring those regions. All that’s left is to select an action maximizing each reward function approximation and receive a preference <span class="math inline">\(y_t\)</span> on one of them to add the new information to the dataset<span class="citation" data-cites="fgts_cdb">(<a href="#ref-fgts_cdb" role="doc-biblioref">Zhang 2021</a>)</span>.</p>
<div class="algorithm">
<div class="algorithmic">
<p>Initialize <span class="math inline">\(S_0 = \varnothing\)</span>. Receive prompt <span class="math inline">\(x_t\)</span> and action space <span class="math inline">\(\mathcal{A}_t\)</span>. Sample model parameter <span class="math inline">\(\theta_t^j\)</span> from the posterior distribution <span class="math inline">\(p^j(\cdot \mid S_{t-1})\)</span> Select response <span class="math inline">\(a_t^j = \arg\max_{a \in \mathcal{A}_t} \langle \theta_t^j, \phi(x_t, a) \rangle\)</span>. Receive preference <span class="math inline">\(y_t\)</span>. Update dataset <span class="math inline">\(S_t \leftarrow S_{t-1} \cup \{(x_t, a_t^1, a_t^2, y_t)\}\)</span>.</p>
</div>
</div>
</section>
</section>
<section id="applications" class="level3" data-number="4.1.4">
<h3 data-number="4.1.4" class="anchored" data-anchor-id="applications"><span class="header-section-number">4.1.4</span> Applications</h3>
<p>There are many applications where contextual bandits are used. Many of these applications can utilize human preferences. One particular application illustrates the benefits a contextual bandit would have over a multi-armed bandit: a website deciding which app to show someone visiting the website. A multi-armed bandit might decide to show someone an ad for a swimsuit because the swimsuit ads have gotten the most user clicks (which indicates human preference). A contextual bandit might choose differently, however. A contextual bandit will also take into account the context, which in this case might mean information about the user (location, previously visited pages, and device information). If it discovers the user lives in a cold environment, for example, it might suggest a sweater ad for the user instead and get a better chance of a click. There are many more examples of where contextual bandits can be applied. They can be applied in other web applications, such as to optimize search results, medical applications, such as how much of a medication to prescribe based on a patient’s history, and gaming applications, such as basing moves off of the state of a chess board to try to win. In each of the above examples, human feedback could have been introduced during training and leveraged to learn a reward function.</p>
<p>We explored different versions of bandits that address the exploration-exploitation trade-off in various real-world scenarios. These models have been employed across various fields, including but not limited to healthcare, finance, dynamic pricing, and anomaly detection. This section provides a deep dive into some real-world applications, emphasizing the value and advancements achieved by incorporating bandit methodologies. The content of this section draws upon the findings from the survey cited in reference <span class="citation" data-cites="bouneffouf2020survey">(<a href="#ref-bouneffouf2020survey" role="doc-biblioref">Bouneffouf, Rish, and Aggarwal 2020</a>)</span>.</p>
<p>In healthcare, researchers have been applying bandits to address challenges in clinical trials and behavioral modeling <span class="citation" data-cites="bouneffouf2017bandit bastani2020online">(<a href="#ref-bouneffouf2017bandit" role="doc-biblioref">Bouneffouf, Rish, and Cecchi 2017</a>; <a href="#ref-bastani2020online" role="doc-biblioref">Bastani and Bayati 2020</a>)</span>. One of the examples is drug dosing. Warfarin, an oral anticoagulant, has traditionally been administered using fixed dosing protocols. Physicians would then make subsequent adjustments based on the patient’s emerging symptoms. Nonetheless, inaccuracies in the initial dosage—whether too low or too high—can lead to serious complications like strokes and internal bleeding. In a pivotal study, researchers in <span class="citation" data-cites="bastani2020online">(<a href="#ref-bastani2020online" role="doc-biblioref">Bastani and Bayati 2020</a>)</span> modeled the Warfarin initial dosing as a contextual bandit problem to assign dosages to individual patients appropriately based on their medication history. Their contributions include the adaptation of the LASSO estimator to the bandit setting, achieving a theoretical regret bound of <span class="math inline">\(O({s_0}^2 \log^2(dT)\)</span>, where <span class="math inline">\(d\)</span> represents the number of covariates, <span class="math inline">\(s_0 &lt;&lt; d\)</span> signifies the number of pertinent covariates, and <span class="math inline">\(T\)</span> indicates the total number of users. Additionally, they conducted empirical experiments to validate the robustness of their methodology.</p>
<p>Within the finance sector, bandits have been instrumental in reshaping the landscape of portfolio optimization. Portfolio optimization is an approach to designing a portfolio based on the investor’s return and risk criteria, which fits the exploration-exploitation nature of the bandit problems. <span class="citation" data-cites="shen2015portfolio">(<a href="#ref-shen2015portfolio" role="doc-biblioref">Shen et al. 2015</a>)</span> utilized multi-armed bandits to exploit correlations between the instruments. They constructed orthogonal portfolios and integrated them with the UCB policy to achieve a cumulative regret bound of <span class="math inline">\(\frac{8n}{\Delta*} \ln(m) + 5n\)</span>, where <span class="math inline">\(n\)</span>, <span class="math inline">\(m\)</span>, and <span class="math inline">\(\Delta*\)</span> denotes the number of available assets, total time steps, and the gap between the best-expected reward and the expected reward. On the other hand, <span class="citation" data-cites="huo2017risk">(<a href="#ref-huo2017risk" role="doc-biblioref">Huo and Fu 2017</a>)</span> focused on risk-awareness online portfolio optimization by incorporating a compute of the minimum spanning tree in the bipartite graph, which encodes a combination of financial institutions and assets that helps diversify and reduce exposure to systematic risk during the financial crisis.</p>
<p>Dynamic pricing, also known as demand-based pricing, refers to the strategy of setting flexible prices for products or services based on current market demands. The application of bandits in dynamic pricing offers a systematic approach to making real-time pricing decisions while balancing the trade-off between exploring new price points and exploiting known optimal prices. <span class="citation" data-cites="misra2019dynamic">(<a href="#ref-misra2019dynamic" role="doc-biblioref">Misra, Schwartz, and Abernethy 2019</a>)</span> proposed a policy where the company has only incomplete demand information. They derived an algorithm that balances immediate and future profits by combining multi-armed bandits with partial identification of consumer demand from economic theory.</p>
<p>are essential components of numerous online platforms, guiding users through vast content landscapes to deliver tailored suggestions. These systems are instrumental in platforms like e-commerce sites, streaming platforms, and social media networks. However, the challenge of effectively recommending items to users is non-trivial, given the dynamic nature of user preferences and the vast amount of content available.</p>
<p>One of the most significant challenges in recommendation systems is the "cold start" problem. This issue arises when a new user joins a platform, and the system has limited or no information about the user’s preferences. Traditional recommendation algorithms struggle in such scenarios since they rely on historical user-item interactions. As discussed in <span class="citation" data-cites="zhou2017large">(<a href="#ref-zhou2017large" role="doc-biblioref">Zhou et al. 2017</a>)</span>, the bandit setting is particularly suitable for large-scale recommender systems with a vast number of items. By continuously exploring user preferences and exploiting known interactions, bandit-based recommender systems can quickly adapt to new users, ensuring relevant recommendations in a few interactions. The continuous exploration inherent in bandit approaches also means that as a user’s preferences evolve, the system can adapt, ensuring that recommendations remain relevant. Recommending content that is up to date is also another important aspect of a recommendation system. In <span class="citation" data-cites="bouneffouf2012a">(<a href="#ref-bouneffouf2012a" role="doc-biblioref">Bouneffouf, Bouzeghoub, and Gançarski 2012</a>)</span>, the concept of "freshness" in content is explored through the lens of the bandit problem. The Freshness-Aware Thompson Sampling algorithm introduced in this study aims to manage the recommendation of fresh documents according to the user’s risk of the situation.</p>
<p>Dialogue systems, often termed conversational agents or chatbots, aim to simulate human-like conversations with users. These systems are deployed across various platforms, including customer support, virtual assistants, and entertainment applications, and they are crucial for enhancing user experience and engagement. Response selection is fundamental to creating a natural and coherent dialogue flow. Traditional dialogue systems rely on a predefined set of responses or rules, which can make interactions feel scripted and inauthentic. In <span class="citation" data-cites="liu2018customized">(<a href="#ref-liu2018customized" role="doc-biblioref">Liu et al. 2018</a>)</span>, the authors proposed a contextual multi-armed bandit model for online learning of response selection. Specifically, they utilized bidirectional LSTM to produce the distributed representations of a dialogue context and responses and customized the Thompson sampling method.</p>
<p>To create a more engaging and dynamic interaction, there’s a growing interest in developing pro-active dialogue systems that can initiate conversations without user initiation. <span class="citation" data-cites="perez2018contextual">(<a href="#ref-perez2018contextual" role="doc-biblioref">perez and Silander 2018</a>)</span> proposed a novel approach to this challenge with contextual bandits. By introducing memory models into the bandit framework, the system can recall past interactions, making its proactive responses more contextually relevant. Their contributions include the Contextual Attentive Memory Network, which implements a differentiable attention mechanism over past interactions.</p>
<p><span class="citation" data-cites="upadhyay2019a">(<a href="#ref-upadhyay2019a" role="doc-biblioref">Upadhyay et al. 2019</a>)</span> addressed the challenge of orchestrating multiple independently trained dialogue agents or skills in a unified system. They attempted online posterior dialogue orchestration, defining it as selecting the most suitable subset of skills in response to a user’s input, which studying a context-attentive bandit model that operates under a skill execution budget, ensuring efficient and accurate response selection.</p>
<p>Anomaly detection refers to the task of identifying samples that behave differently from the majority. In <span class="citation" data-cites="ding2019interactive">(<a href="#ref-ding2019interactive" role="doc-biblioref">Ding, Li, and Liu 2019</a>)</span>, the authors delve into anomaly detection in an interactive setting, allowing the system to actively engage with human experts through a limited number of queries about genuine anomalies. The goal is to present as many true anomalies to the human expert as possible after a fixed query budget is used up. They applied the multi-armed contextual bandit framework to address this issue. This algorithm adeptly integrates both nodal attributes and node dependencies into a unified model, efficiently managing the exploration-exploitation trade-off during anomaly queries.</p>
<p>There are many challenges associated with contextual bandits. The first challenge is that each action only reveals the reward for that particular action. Therefore, the algorithm has to work with incomplete information. This leads to the dilemma of exploitation versus exploration: when should the algorithm choose the best-known option versus trying new options for potentially better outcomes? Another significant challenge for contextual bandits is using context effectively. The context the environment gives needs to be explored to figure out which action is best for each context.</p>
<p>The overarching goal in systems designed for recommending options of high value to users is to achieve an optimal balance between exploration and exploitation. This dual approach is crucial in environments where user preferences and needs are dynamic and diverse. Exploration refers to the process of seeking out new options, learning about untried possibilities, and gathering fresh information that could lead to high-value recommendations. In contrast, exploitation involves utilizing existing knowledge and past experiences to recommend the best options currently known. This balance is key to maintaining a system that continuously adapts to changing user preferences while ensuring the reliability of its recommendations.</p>
<p>A key observation in such systems is the dual role of users as both producers and consumers of information. Each user’s experience contributes valuable data that informs future recommendations for others. For instance, platforms like Waze, Netflix, and Trip Advisor rely heavily on user input and feedback. Waze uses real-time traffic data from drivers to recommend optimal routes; Netflix suggests movies and shows based on viewing histories and ratings; Trip Advisor relies on traveler reviews to guide future tourists. In these examples, the balance between gathering new information (exploration) and recommending the best-known options (exploitation) is dynamically managed to enhance user experience and satisfaction. This approach underscores the importance of user engagement in systems where monetary incentives are not (or can not be) the primary driver.</p>
<p>Recommendation systems often face the challenge of overcoming user biases that can lead to a narrow exploration of options. Users come with preconceived notions and preferences, which can cause them to overlook potentially valuable options that initially appear inferior or unaligned with their interests. This predisposition can significantly limit the effectiveness of recommendation systems, as users might miss out on high-value choices simply due to their existing biases.</p>
<p>To counteract this, it is crucial for recommendation systems to actively incentivize exploration among users. One innovative approach to achieve this is through the strategic use of <strong>information asymmetry</strong>. By controlling and selectively presenting information, these systems can guide users to explore options they might not typically consider. This method aims to reveal the true potential of various options by nudging users out of their comfort zones and encouraging a broader exploration of available choices. An important note here is that the system is not lying to users - it only selectively reveals information it has.</p>
<p>The concept of incentivizing exploration becomes even more complex when considering different types of users. For instance, systems often encounter short-lived users who have little to gain from contributing to the system’s learning process, as their interactions are infrequent or based on immediate needs. Similarly, some users may operate under a ‘greedy’ principle, primarily seeking immediate gratification rather than contributing to the long-term accuracy and effectiveness of the system. In such scenarios, managing information asymmetry can be a powerful tool. By selectively revealing information, recommendation systems can create a sense of novelty and interest, prompting even the most transient or self-interested users to engage in exploration, thereby enhancing the system’s overall knowledge base and recommendation quality.</p>
</section>
<section id="incentive-compatible-online-learning" class="level3" data-number="4.1.5">
<h3 data-number="4.1.5" class="anchored" data-anchor-id="incentive-compatible-online-learning"><span class="header-section-number">4.1.5</span> Incentive-Compatible Online Learning</h3>
<p>To address this problem, we seek to create a model. But first, it is useful to outline the key criteria that our model must achieve.</p>
<ul>
<li><p>The <em>core</em> of the model revolves around repeated interactions between a planner (the system) and multiple agents (the users). Each agent, upon arrival in the system, is presented with a set of available options to choose from. These options could vary widely depending on the application of the model, such as routes in a transportation network, a selection of hotels in a travel booking system, or even entertainment choices in a streaming service.</p></li>
<li><p>The <em>interaction process</em> is straightforward but crucial: agents arrive, select an action from the provided options, and then report feedback based on their experience. This feedback is vital as it forms the basis upon which the planner improves and evolves its recommendations. The agents in this model are considered strategic; they aim to maximize their reward based on the information available to them. This aspect of the model acknowledges the real-world scenario where users are typically self-interested and seek to optimize their own outcomes.</p></li>
<li><p>The <em>planner</em>, on the other hand, has a broader objective. It aims to learn which alternatives are best in a given context and works to maximize the overall welfare of all agents. This involves a complex balancing act: the planner must accurately interpret feedback from a diverse set of agents, each with their own preferences and biases, and use this information to refine and improve the set of options available. The ultimate goal of the planner is to create a dynamic, responsive system that not only caters to the immediate needs of individual agents but also enhances the collective experience over time, leading to a continually improving recommendation ecosystem.</p></li>
</ul>
<p>Let’s break this up into a set of tangible research questions that we seek to answer in the rest of this chapter.</p>
<ul>
<li><p><strong>Planner Limitations</strong>: We seek to address the inherent limitations faced by the planner, particularly in scenarios where monetary transfers are not an option, and the only tool at its disposal is the control over the flow of information between agents. This inquiry aims to understand the extent to which these limitations impact the planner’s ability to effectively guide and influence agent behavior.</p></li>
<li><p><strong>Inducing Exploration</strong>: A critical question is whether the planner can successfully induce exploration among agents, especially in the absence of financial incentives. This involves investigating strategies to encourage users to try less obvious or popular options, thus broadening the scope of feedback and enhancing the system’s ability to learn and identify the best alternatives.</p></li>
<li><p><strong>Rate of Learning</strong>: Another essential research area is understanding the rate at which the planner learns from agent interactions. This encompasses examining how different agent incentives, their willingness to explore, and their feedback impact the speed and efficiency with which the planner can identify optimal recommendations.</p></li>
<li><p><strong>Model Extensions</strong>: The model can be extended in several directions, each raising its own set of questions.</p>
<ol type="1">
<li><p>Multiple Agents with Interconnected Payoffs: When multiple agents arrive simultaneously, their choices and payoffs become interconnected, resembling a game. The research question here focuses on how these interdependencies affect individual and collective decision-making.</p></li>
<li><p>Planner with Arbitrary Objective Function: Investigating scenarios where the planner operates under an arbitrary objective function, which might not align with maximizing overall welfare or learning the best alternative.</p></li>
<li><p>Observed Heterogeneity Among Agents: This involves situations where differences among agents are observable and known, akin to contextual bandits in machine learning. The research question revolves around how these observable traits can be used to tailor recommendations more effectively.</p></li>
<li><p>Unobserved Heterogeneity Among Agents: This aspect delves into scenarios where differences among agents are not directly observable, necessitating the use of causal inference techniques to understand and cater to diverse user needs.</p></li>
</ol></li>
</ul>
<section id="bayesian-incentive-compatible-bandit-model" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bayesian-incentive-compatible-bandit-model">Bayesian Incentive-Compatible Bandit Model</h4>
<p>In this section, we introduce the main model of study in this chapter <span class="citation" data-cites="mansour2019bayesianincentivecompatiblebanditexploration mansour2021bayesianexplorationincentivizingexploration">(<a href="#ref-mansour2019bayesianincentivecompatiblebanditexploration" role="doc-biblioref">Mansour, Slivkins, and Syrgkanis 2019</a>; <a href="#ref-mansour2021bayesianexplorationincentivizingexploration" role="doc-biblioref">Mansour et al. 2021</a>)</span>. In our setup, there is a “planner," which aims to increase exploration, and many independent”agents," which will act selfishly (in a way that they believe will maximize their individual reward).</p>
<p>Under our model shown in Figure <a href="#fig-planner-agent" data-reference-type="ref" data-reference="fig-planner-agent">1.1</a>, there are <span class="math inline">\(K\)</span> possible actions that all users can take, and each action has some mean reward <span class="math inline">\(\mu_i \in [0, 1]\)</span>. In addition, there is a common prior belief on each <span class="math inline">\(\mu_i\)</span> across all users.. The <span class="math inline">\(T\)</span> agents, or users, will arrive sequentially. As the <span class="math inline">\(t\)</span>’th user arrives, they are recommended an action <span class="math inline">\(I_t\)</span> by the planner, which they are free to follow or not follow. After taking whichever action they choose, the user experiences some realized reward <span class="math inline">\(r_i \in [0, 1]\)</span>, which is stochastic i.i.d. with mean <span class="math inline">\(\mu_i\)</span>, and reports this reward back to the planner.</p>
<div id="fig-planner-agent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-planner-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/planner-agent-setup.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-planner-agent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Planner-agent setup
</figcaption>
</figure>
</div>
<p>So far, the model we have defined is equivalent to a multi-armed bandit model, which we have seen earlier in this chapter (<a href="#4optim" data-reference-type="ref" data-reference="4optim">1</a>). Under this model, well-known results in economics, operations research and computer science show that <span class="math inline">\(O(\sqrt{T})\)</span> regret is achievable <span class="citation" data-cites="russo2015informationtheoreticanalysisthompsonsampling auer_cesa-bianchi_fischer_2002 LAI19854">(<a href="#ref-russo2015informationtheoreticanalysisthompsonsampling" role="doc-biblioref">Russo and Roy 2015</a>; <a href="#ref-auer_cesa-bianchi_fischer_2002" role="doc-biblioref">Auer, Cesa-Bianchi, and Fischer 2002</a>; <a href="#ref-LAI19854" role="doc-biblioref">Lai and Robbins 1985</a>)</span> with algorithms such as Thompson sampling and UCB.</p>
<p>However, our agents are strategic and aim to maximize their own rewards. If they observe the rewards gained from actions taken by other previous users, they will simply take the action they believe will yield the highest reward given the previous actions; they would prefer to benefit from exploration done by other users rather than take the risk of exploring themselves. Therefore, exploration on an individual level, which the planner would like to facilitate, is not guaranteed under this paradigm.</p>
<p>In light of this, we also require that our model satisfy <strong>incentive compatibility</strong>, or that taking the action recommended by the planner has an expected utility that is as high as any other action the agent could take. Formally, <span class="math display">\[\forall i : \, E[\mu_i | I_t = i] \geq E[\mu_{i'} | I_t = i].\]</span> Note that this incentivizes the agents to actually take the actions recommended by the planner; if incentive compatibility is not satisfied, agents would simply ignore the planner and take whatever action they think will lead to the highest reward.</p>
<p>At a high level, the key to achieving incentive compatibility while still creating a policy for the planner that facilitates exploration is information asymmetry. Under this paradigm, the users only have access to their previous recommendations, actions, and rewards, and not to the recommendations, actions, and rewards of other users. Therefore, they are unsure of whether, after other users take certain actions and receive certain rewards, arms that they might have initially considered worse in practice outperform arms that they initially considered better. Only the planner has access to the previous actions and rewards of all users; the user only has access to their own recommendations and overall knowledge of the planner’s policy.</p>
<p>The main question we aim to answer for the rest of this section is, given this new constraint of incentive compatibility, is <span class="math inline">\(O(\sqrt{T})\)</span> regret still achievable? We illustrate such an algorithm in the following.</p>
</section>
<section id="black-box-reduction-algorithm" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="black-box-reduction-algorithm">Black-box Reduction Algorithm</h4>
<p>The main result for this chapter is a <strong>black-box reduction</strong> algorithm to turn any bandit algorithm into an <em>incentive compatible</em> one, with only a constant increase in Bayesian regret. Since, as mentioned earlier, there are bandit algorithms with <span class="math inline">\(O(\sqrt{T})\)</span> Bayesian regret, black-box reduction will also allow us to get incentive-compatible algorithms with <span class="math inline">\(O(\sqrt{T})\)</span> regret. The idea of black-box reduction will be to simulate <span class="math inline">\(T\)</span> steps of any bandit algorithm in an incentive-compatible way in <span class="math inline">\(c T\)</span> steps. This allows us to design incentive-compatible recommendation systems by using any bandit algorithm and then adapting it.</p>
<p>Consider the following setting: there are two possible actions, <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>. Assume the setting of <strong>deterministic rewards</strong>, where action 1 has reward <span class="math inline">\(\mu_1\)</span> with prior <span class="math inline">\(U[1/3, 1]\)</span> and mean <span class="math inline">\(\mathbb{E}[\mu_1] = 2/3\)</span>, and action 2 has reward <span class="math inline">\(\mu_2\)</span> with prior <span class="math inline">\(U[0, 1]\)</span> and mean <span class="math inline">\(\mathbb{E}[\mu_2] = 1/2\)</span>. Without the planner intervention and with full observability, users would simply always pick <span class="math inline">\(A_1\)</span>, so how can the planner <em>incentivize</em> users to play <span class="math inline">\(A_2\)</span>?</p>
<div id="fig-deterministic-guinea-pig" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deterministic-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/guinea_pig_fig.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-deterministic-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Illustration of black-box reduction algorithm when we have deterministic rewards.
</figcaption>
</figure>
</div>
<p>The key insight is going to be to <em>hide exploration in a pool of exploitation</em>. The users are only going to receive a recommendation from the planner, and no other observations. After deterministically recommending the action with the highest expected reward (<span class="math inline">\(A_1\)</span>), the planner will pick one <strong>guinea pig</strong> to recommend the exploratory action of <span class="math inline">\(A_2\)</span>. The users don’t know whether they are the guinea pig, so intuitively, as long as the planner picks guinea pigs uniformly at random and at low enough frequencies, the optimal decision for the users is still to follow the planner’s recommendation, even if it might go against their interest.</p>
<p>The planner will pick the user who will be recommended the exploratory action uniformly at random from the <span class="math inline">\(L\)</span> users that come after the first one (which deterministically gets recommended the exploitation action). Under this setting (illustrated in Figure <a href="#fig-deterministic-guinea-pig" data-reference-type="ref" data-reference="fig-deterministic-guinea-pig">1.2</a>), it is optimal for users to always follow the option that is recommended for them. More formally, if <span class="math inline">\(I_t\)</span> is the recommendation that a user receives at time <span class="math inline">\(t\)</span>, then we have that: <span class="math display">\[\begin{split}
    \mathbb{E}[\mu_1 - \mu_2 | I_t = 2] Pr[I_t = 2] &amp;= \frac{1}{L} (\mu_1 - \mu_2) \quad \text{(Gains if you are the unlucky guinea pig)}\\
    &amp;+ (1 - \frac{1}{L}) \mathbb{E}[\mu_1 - \mu_2 | \mu_1 &lt; \mu_2] Pr[\mu_1 &lt; \mu_2] \quad \text{(Loss if you are not and $\mu_1 &lt; \mu_2$)}\\
    &amp;\leq 0
\end{split}\]</span> This holds when <span class="math inline">\(L \geq 12\)</span>. It means that the gains from not taking the recommended action are <em>negative</em>, which implies that users should always take the recommendation.</p>
<p>So far we have considered the case where rewards are deterministic, but what about <strong>stochastic rewards</strong>? We are now going to consider the case where rewards are independent and identically distributed from some distribution, and where each action <span class="math inline">\(A_i\)</span> has some reward distribution <span class="math inline">\(r_i^t \sim D_i, \mathbb{E}[r_i^t] = \mu_i\)</span>. Back to the case where there are only two actions, we are going to adapt the prior algorithm of guinea pig-picking to the stochastic reward setting. Since one reward observation is not enough to fully know <span class="math inline">\(\mu_1\)</span> anymore, we’ll instead observe the outcome of the first action <span class="math inline">\(M\)</span> times to form a strong posterior <span class="math inline">\(\mathbb{E}[\mu_1 | r_1^1, \ldots r_1^M]\)</span>.</p>
<div id="fig-stochastic-guinea-pig" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stochastic-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/stochastic_guinea_pig.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stochastic-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Illustration of black-box reduction algorithm when we have stochastic rewards.
</figcaption>
</figure>
</div>
<p>Figure <a href="#fig-stochastic-guinea-pig" data-reference-type="ref" data-reference="fig-stochastic-guinea-pig">1.3</a> illustrates the algorithm that we can use with stochastic rewards when there are two actions. Similarly, as before, we pick one guinea pig uniformly at random from the next <span class="math inline">\(L\)</span> users and use the reward we get as the exploratory signal.<br>
In a very similar manner, we can generalize this algorithm from always having two actions to the general multi-armed bandit problem. Now suppose we have a general multi-armed bandit algorithm <span class="math inline">\(A\)</span>. We will wrap this algorithm around our black box reduction algorithm to make it incentive-compatible.</p>
<div id="fig-multi-armed-guinea-pig" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multi-armed-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/multi-armed-guinea-pig.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi-armed-guinea-pig-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.5: Illustration of black-box reduction algorithm for the general multi-armed bandit case.
</figcaption>
</figure>
</div>
<p>As Figure <a href="#fig-multi-armed-guinea-pig" data-reference-type="ref" data-reference="fig-multi-armed-guinea-pig">1.4</a> shows, we wrap every decision that <span class="math inline">\(A\)</span> would make by exactly <span class="math inline">\(L-1\)</span> recommendations of the action believed to be the best so far. This guarantees that the expected rewards for the users that are not chosen as guinea pigs are at least as good as <span class="math inline">\(A\)</span>’s reward at phase <span class="math inline">\(n\)</span>.</p>
</section>
</section>
</section>
<section id="preferential-bayesian-optimization" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="preferential-bayesian-optimization"><span class="header-section-number">4.2</span> Preferential Bayesian Optimization</h2>
<p>The traditional Bayesian optimization (BO) problem is described as follows. There is a black-box objective function <span class="math inline">\(g: \mathcal{X} \rightarrow \Re\)</span> defined on a bounded subset <span class="math inline">\(\mathcal{X} \subseteq \Re^q\)</span> such that direct queries to the function are expensive or not possible. However, we would like to solve the global optimization problem of finding <span class="math inline">\(\mathbf{x}_{\min }=\arg \min _{\mathbf{x} \in \mathcal{X}} g(\mathbf{x})\)</span>. This is highly analogous to modeling human preferences, since it is the case that direct access to a human’s latent preference function is not possible but we would still like to find its optimum, such as in A/B tests or recommender systems.</p>
<p>We approach this problem for human preferences with <em>Preferential Bayesian Optimization</em> (PBO), as the key difference is that we are able to query the preference function through pairwise comparisons of data points, i.e.&nbsp;<em>duels</em>. This is a form of indirect observation of the objective function, which models real-world scenarios closely: we commonly need to to optimize a function via data about preferences. With humans, it has been demonstrated that we are better at evaluating differences rather than absolute magnitudes <span class="citation" data-cites="kahneman_tversky_1979">(<a href="#ref-kahneman_tversky_1979" role="doc-biblioref">Kahneman and Tversky 1979</a>)</span> and therefore PBO models can be applied in various contexts.</p>
<section id="problem-statement" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="problem-statement"><span class="header-section-number">4.2.1</span> Problem statement</h3>
<p>The problem of finding the optimum of a latent preference function defined on <span class="math inline">\(\mathcal{X}\)</span> can be reduced to determining a sequence of duels on <span class="math inline">\(\mathcal{X} \times \mathcal{X}\)</span>. From each duel <span class="math inline">\(\left[\mathbf{x}, \mathbf{x}^{\prime}\right] \in\)</span> <span class="math inline">\(\mathcal{X} \times \mathcal{X}\)</span> we obtain binary feedback <span class="math inline">\(\{0,1\}\)</span> indicating whether or not <span class="math inline">\(\mathbf{x}\)</span> is preferred over <span class="math inline">\(\mathbf{x}^{\prime}\)</span> (<span class="math inline">\(g(\mathbf{x}) &lt; g(\mathbf{x}^{\prime})\)</span>). We consider that <span class="math inline">\(\mathbf{x}\)</span> is the winner of the duel if the output is <span class="math inline">\(\{1\}\)</span> and that <span class="math inline">\(\mathbf{x}^{\prime}\)</span> wins the duel if the output is <span class="math inline">\(\{0\}\)</span>. The aim is to find <span class="math inline">\(\mathbf{x}_{\min }\)</span> by reducing as much as possible the number of queried duels.</p>
<p>The key idea in PBO is to learn a preference function in the space of duels using a Gaussian process. We define a joint reward <span class="math inline">\(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)\)</span> on each duel which is never directly observed. Instead, the feedback we obtain after each pair is a binary output <span class="math inline">\(y \in\)</span> <span class="math inline">\(\{0,1\}\)</span> indicating which of the two inputs is preferred. One definition of f we will use (though others are possible) is <span class="math inline">\(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})\)</span>. The more <span class="math inline">\(\mathbf{x}^{\prime}\)</span> is preferred over <span class="math inline">\(\mathbf{x}\)</span>, the bigger the reward.</p>
<p>We define the model of preference using a Bernoulli likelihood, where <span class="math inline">\(p\left(y=1 \mid\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)\)</span> and <span class="math inline">\(p\left(y=0 \mid\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=\pi_f\left(\left[\mathbf{x}^{\prime}, \mathbf{x}\right]\right)\)</span> for some inverse link function <span class="math inline">\(\pi: \Re \times \Re \rightarrow[0,1]\)</span>. <span class="math inline">\(\pi_f\)</span> has the property that <span class="math inline">\(\pi_f\left(\left[\mathbf{x}^{\prime}, \mathbf{x}\right]\right)=1-\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)\)</span>. A natural choice for <span class="math inline">\(\pi_f\)</span> is the logistic function <span class="math display">\[\label{eq:bernoulli_pref}
\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=\sigma\left(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)\right)=\frac{1}{1+e^{-f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)}},\]</span> but others are possible. Therefore we have that for any duel <span class="math inline">\(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\)</span> in which <span class="math inline">\(g(\mathbf{x}) \leq g\left(\mathbf{x}^{\prime}\right)\)</span> it holds that <span class="math inline">\(\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right) \geq 0.5\)</span>. <span class="math inline">\(\pi_f\)</span> is a preference function that maps each query <span class="math inline">\(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\)</span> to the probability of having a preference on the left input <span class="math inline">\(\mathbf{x}\)</span> over the right input <span class="math inline">\(\mathbf{x}^{\prime}\)</span>.</p>
<p>When we marginalize over the right input <span class="math inline">\(\mathbf{x}^{\prime}\)</span> of <span class="math inline">\(f\)</span> (is this correct?), the global minimum of <span class="math inline">\(f\)</span> in <span class="math inline">\(\mathcal{X}\)</span> coincides with <span class="math inline">\(\mathbf{x}_{\min }\)</span>. We also introduce the definition of the <em>Copeland score function</em> for a point <span class="math inline">\(\mathbf{x}\)</span> as <span class="math display">\[S(\mathbf{x})=\operatorname{Vol}(\mathcal{X})^{-1} \int_{\mathcal{X}} \mathbb{I}_{\left\{\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right) \geq 0.5\right\}} d \mathbf{x}^{\prime}\]</span> where <span class="math inline">\(\operatorname{Vol}(\mathcal{X})=\int_{\mathcal{X}} d \mathbf{x}^{\prime}\)</span> is a normalizing constant that bounds <span class="math inline">\(S(\mathbf{x})\)</span> in the interval <span class="math inline">\([0,1]\)</span>. If <span class="math inline">\(\mathcal{X}\)</span> is a finite set, the Copeland score is simply the proportion of duels that a certain element <span class="math inline">\(\mathbf{x}\)</span> will win with probability larger than 0.5. A soft variant we will use instead of the Copeland score is the <em>soft-Copeland score</em>, defined as <span class="math display">\[\label{eq:soft-copeland}
C(\mathbf{x})=\operatorname{Vol}(\mathcal{X})^{-1} \int_{\mathcal{X}} \pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right) d \mathbf{x}^{\prime}\]</span> where the probability function <span class="math inline">\(\pi_f\)</span> is integrated over <span class="math inline">\(\mathcal{X}\)</span>. This score aims to capture the average probability of <span class="math inline">\(\mathbf{x}\)</span> being the winner of a duel.</p>
<p>We define the <em>Condorcet winner</em> <span class="math inline">\(\mathbf{x}_c\)</span> as the point with maximal soft-Copeland score. Note that this corresponds to the global minimum of <span class="math inline">\(f\)</span>, since the defining integral takes maximum value for points <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span> where <span class="math inline">\(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=\)</span> <span class="math inline">\(g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})&gt;0\)</span> or all <span class="math inline">\(\mathbf{x}^{\prime}\)</span>, occurring only if <span class="math inline">\(\mathbf{x}_c\)</span> is a minimum of <span class="math inline">\(f\)</span>. Therefore, if the preference function <span class="math inline">\(\pi_f\)</span> can be learned by observing the results of duels then our optimization problem of finding the minimum of <span class="math inline">\(f\)</span> can be solved by finding the Condorcet winner of the Copeland score.</p>
</section>
<section id="acquisition-functions-1" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="acquisition-functions-1"><span class="header-section-number">4.2.2</span> Acquisition Functions</h3>
<p>We describe several acquisition functions for sequential learning of the Condorcet winner. Our dataset <span class="math inline">\(\mathcal{D}=\left\{\left[\mathbf{x}_i, \mathbf{x}_i^{\prime}\right], y_i\right\}_{i=1}^N\)</span> represents the <span class="math inline">\(N\)</span> duels that have been performed so far. We aim to define a sequential policy <span class="math inline">\(\alpha\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right] ; \mathcal{D}_j, \theta\right)\)</span> for querying duels, where <span class="math inline">\(\theta\)</span> is a vector of model hyper-parameters, in order to find the minimum of the latent function <span class="math inline">\(g\)</span> as quickly as possible. Using Gaussian processes (GP) for classification with our dataset <span class="math inline">\(\mathcal{D}\)</span> allows us to perform inference over <span class="math inline">\(f\)</span> and <span class="math inline">\(\pi_f\)</span>.</p>
<section id="pure-exploration" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pure-exploration">Pure Exploration</h4>
<p>The output variable <span class="math inline">\(y_{\star}\)</span> of a prediction follows a Bernoulli distribution with probability given by the preference function <span class="math inline">\(\pi_f\)</span>. To carry out exploration as a policy, one method is to search for the duel where GP is most uncertain about the probability of the outcome (has the highest variance of <span class="math inline">\(\sigma\left(f_{\star}\right)\)</span> ), which is the result of transforming out epistemic uncertainty about <span class="math inline">\(f\)</span>, modeled by a GP, through the logistic function. The first order moment of this distribution coincides with the expectation of <span class="math inline">\(y_{\star}\)</span> but its variance is <span class="math display">\[\begin{aligned}
\mathbb{V}\left[\sigma\left(f_{\star}\right)\right] &amp; =\int\left(\sigma\left(f_{\star}\right)-\mathbb{E}\left[\sigma\left(f_{\star}\right)\right]\right)^2 p\left(f_{\star} \mid \mathcal{D},\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right) d f_{\star} \\
&amp; =\int \sigma\left(f_{\star}\right)^2 p\left(f_{\star} \mid \mathcal{D},\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right) d f_{\star}-\mathbb{E}\left[\sigma\left(f_{\star}\right)\right]^2
\end{aligned}\]</span> which explicitly takes into account the uncertainty over <span class="math inline">\(f\)</span>. Hence, pure exploration of duels space can be carried out by maximizing <span class="math display">\[\alpha_{\mathrm{PE}}\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right] \mid \mathcal{D}_j\right)=\mathbb{V}\left[\sigma\left(f_{\star}\right)\left|\left[\mathbf{x}_{\star}, \mathbf{x}_{\star}^{\prime}\right]\right| \mathcal{D}_j\right] .\]</span></p>
<p>Note that in this case, duels that have been already visited will have a lower chance of being visited again even in cases in which the objective takes similar values in both players. In practice, this acquisition functions requires computation of an intractable integral, that we approximate using Monte-Carlo.</p>
</section>
<section id="principled-optimistic-preferential-bayesian-optimization-pop-bo" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="principled-optimistic-preferential-bayesian-optimization-pop-bo">Principled Optimistic Preferential Bayesian Optimization (POP-BO)</h4>
<p>In a slightly modified problem setup <span class="citation" data-cites="xu2024principledpreferentialbayesianoptimization">(<a href="#ref-xu2024principledpreferentialbayesianoptimization" role="doc-biblioref">Xu et al. 2024</a>)</span>, the algorithm tries to solve for the MLE <span class="math inline">\(\hat{g}\)</span> and its confidence set <span class="math inline">\(\mathcal{B}_g\)</span> where <span class="math inline">\(g\)</span> is the ground truth black-box function. Assumptions include that <span class="math inline">\(g\)</span> is a member of a reproducing kernel Hilbert space (RKHS) <span class="math inline">\(\mathcal{H}_k\)</span> for some kernel function <span class="math inline">\(k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}\)</span>, and <span class="math inline">\(\|g\|_k \leq B\)</span> so that <span class="math inline">\(\mathcal{B}_g = \left\{\tilde{g} \in \mathcal{H}_k \mid\|\tilde{g}\|_k \leq B\right\}\)</span>. Similarly defining <span class="math inline">\(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})\)</span>, we model the preference function with a Bernoulli distribution as in Equation <a href="#eq:bernoulli_pref" data-reference-type="ref" data-reference="eq:bernoulli_pref">[eq:bernoulli_pref]</a> and also assume that probabilities follow the Bradley-Terry model, i.e. <span class="math display">\[\pi_f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)=\sigma\left(f\left(\left[\mathbf{x}, \mathbf{x}^{\prime}\right]\right)\right)=\frac{e^{g(\mathbf{x})}}{e^{g(\mathbf{x})}+e^{g\left(\mathbf{x^{\prime}}\right)}}\]</span></p>
<p>The update rule for MLE <span class="math inline">\(\hat{g}\)</span> is (equation 8,6,5) <span class="math display">\[\begin{aligned}
\hat{g}_t^{\text {MLE }}&amp;:= \arg \underset{\tilde{g} \in \mathcal{B}^t_g}{\max}\ell_t(\tilde{g}) \\
\ell_t(\tilde{g}) &amp;:= \log \prod_{\tau=1}^t y_\tau \pi_{\tilde{f}}([\mathbf{x_\tau}, \mathbf{x^{\prime}_\tau}])+\left(1-y_\tau\right)\left(1-\pi_{\tilde{f}}([\mathbf{x_\tau}, \mathbf{x^{\prime}_\tau}])\right) \\
&amp;=\sum_{\tau=1}^t \log \left(\frac{e^{\tilde{g}(\mathbf{x_\tau})} y_\tau+e^{\tilde{g}(\mathbf{x_\tau^\prime})}\left(1-y_\tau\right)}{e^{\tilde{g}(\mathbf{x_\tau})}+e^{\tilde{g}(\mathbf{x_\tau^\prime})}}\right) \\
&amp;=\sum_{\tau=1}^t\left(\tilde{g}(\mathbf{x_\tau}) y_\tau+\tilde{g}(\mathbf{x_\tau^\prime})\left(1-y_\tau\right)\right)-\sum_{\tau=1}^t \log \left(e^{\tilde{g}(\mathbf{x_\tau})}+e^{\tilde{g}(\mathbf{x_\tau^\prime})}\right)
\end{aligned}\]</span></p>
<p>(Eq 22 shows how to represent this as a convex optimisation problem so that it can be solved)</p>
<p>The update rule for the confidence set <span class="math inline">\(\mathcal{B}_f^{t+1}\)</span> is, (eq 9, 10?)</p>
<p><span class="math display">\[\begin{aligned}
&amp;\forall \epsilon, \delta &gt; 0 \\
&amp;\mathcal{B}_g^{t+1}:=\left\{\tilde{g} \in \mathcal{B}_g \mid \ell_t(\tilde{g}) \geq \ell_t\left(\hat{g}_t^{\mathrm{MLE}}\right)-\beta_1(\epsilon, \delta, t)\right\}
\end{aligned}\]</span> where <span class="math display">\[\beta_1(\epsilon, \delta, t):=\sqrt{32 t B^2 \log \frac{\pi^2 t^2 \mathcal{N}\left(\mathcal{B}_f, \epsilon,\|\cdot\|_{\infty}\right)}{6 \delta}}+ C_L \epsilon t=\mathcal{O}\left(\sqrt{t \log \frac{t \mathcal{N}\left(\mathcal{B}_f, \epsilon,\|\cdot\|_{\infty}\right)}{\delta}}+\epsilon t\right),\]</span> with <span class="math inline">\(C_L\)</span> a constant independent of <span class="math inline">\(\delta, t\)</span> and <span class="math inline">\(\epsilon\)</span>. <span class="math inline">\(\epsilon\)</span> is typically chosen to be <span class="math inline">\(1 / T\)</span>, where T is the running horizon of the algorithm. This satisfies the theorem that, <span class="math display">\[\mathbb{P}\left(g \in \mathcal{B}_g^{t+1}, \forall t \geq 1\right) \geq 1-\delta .\]</span></p>
<p>Intuitively, the confidence set <span class="math inline">\(\mathcal{B}_g^{t+1}\)</span> includes the functions with the log-likelihood value that is only ‘a little worse’ than the maximum likelihood estimator, and the theorem states that <span class="math inline">\(\mathcal{B}_g^{t+1}\)</span> contains the ground-truth function <span class="math inline">\(g\)</span> with high probability.</p>
<p>Inner level optimization in Line 4 of the algorithm can also be represented as a convex optimisation problem so that it can be solved, Eq 24, 25. The outer optimisation can be solved using grid search or Eq 26 for medium size problems.</p>
<div class="algorithm">
<div class="algorithmic">
<p>Given the initial point <span class="math inline">\(\mathbf{x_0} \in \mathcal{X}\)</span> and set <span class="math inline">\(\mathcal{B}_g^1 = \mathcal{B}_g\)</span> Set the reference point <span class="math inline">\(\mathbf{x_t^{\prime}} = \mathbf{x_{t-1}}\)</span> Compute <span class="math inline">\(\mathbf{x_t} \in \arg\max_{\mathbf{x} \in \mathcal{X}} \max_{\tilde{g} \in \mathcal{B}_g^t} (\tilde{g}(\mathbf{x}) - \tilde{g}(\mathbf{x_t^{\prime}}))\)</span>, with the inner optimal function denoted as <span class="math inline">\(\tilde{g}_t\)</span> Obtain the output of the duel <span class="math inline">\(y_t\)</span> and append the new data point to <span class="math inline">\(\mathcal{D}_t\)</span> Update the maximum likelihood estimator <span class="math inline">\(\hat{g}_t^{\mathrm{MLE}}\)</span> and the posterior confidence set <span class="math inline">\(\mathcal{B}_g^{t+1}\)</span>.</p>
</div>
</div>
</section>
<section id="qeubo-decision-theoretic-eubo" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qeubo-decision-theoretic-eubo">qEUBO: Decision-Theoretic EUBO</h4>
<p>qEUBO <span class="citation" data-cites="astudillo2023qeubodecisiontheoreticacquisitionfunction">(<a href="#ref-astudillo2023qeubodecisiontheoreticacquisitionfunction" role="doc-biblioref">Astudillo et al. 2023</a>)</span> derives an acquisition function that extends duels to <span class="math inline">\(q&gt;2\)</span> options which we call <em>queries</em>. Let <span class="math inline">\(X=\left(\mathbf{x_1}, \ldots, \mathbf{x_q}\right) \in \mathcal{X}^q\)</span> denote a query containing two points or more, and let <span class="math inline">\(g: \mathcal{X} \rightarrow \Re\)</span> be the latent preference function. Then after <span class="math inline">\(n\)</span> user queries, we define the <em>expected utility of the best option</em> (qEUBO) as <span class="math display">\[\mathrm{qEUBO}_n(X)=\mathbb{E}_n\left[\max \left\{g\left(x_1\right), \ldots, g\left(x_q\right)\right\}\right].\]</span></p>
<p>We now show that qEUBO is one-step Bayes optimal, meaning that each step chooses the query that maximises the expected utility received by the human. For a query <span class="math inline">\(X \in \mathcal{X}^q\)</span>, let <span class="math display">\[V_n(X)=\mathbb{E}_n\left[\max _{x \in \mathbb{X}} \mathbb{E}_{n+1}[g(x)] \mid X_{n+1}=X\right] .\]</span> Then <span class="math inline">\(V_n\)</span> defines the expected utility received if an additional query <span class="math inline">\(X_{n+1}=X\)</span> is performed, and maximizing <span class="math inline">\(V_n\)</span> is one-step Bayes optimal. Since <span class="math inline">\(\max _{x \in \mathbb{X}} \mathbb{E}_n[f(x)]\)</span> does not depend on <span class="math inline">\(X_{n+1}\)</span>, we can also equivalently maximize <span class="math display">\[\mathbb{E}_n\left[\max _{x \in \mathbb{X}} \mathbb{E}_{n+1}[g(x)]-\max _{x \in \mathbb{X}} \mathbb{E}_n[g(x)] \mid X_{n+1}=X\right],\]</span> which takes the same form as the knowledge gradient acquisition function <span class="citation" data-cites="wu2018parallelknowledgegradientmethod">(<a href="#ref-wu2018parallelknowledgegradientmethod" role="doc-biblioref">Wu and Frazier 2018</a>)</span> in standard Bayesian optimization.</p>
<p><span class="math inline">\(V_n\)</span> involves a nested stochastic optimization task, while qEUBO is a much simpler policy. When human responses are noise-free, we are able to use qEUBO as a sufficient policy due to the following theorem:</p>
<div class="theorem">
<p><span class="math display">\[\underset{X \in \mathbb{X}^q}{\operatorname{argmax}} \mathrm{qEUBO}_n(X) \subseteq \underset{X \in \mathbb{X}^q}{\operatorname{argmax}} V_n(X) .\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> For a query <span class="math inline">\(X \in \mathcal{X}^q\)</span>, let <span class="math inline">\(x^{+}(X, i) \in \operatorname{argmax}_{x \in \mathbb{X}} \mathbb{E}_n[g(x) \mid(X, i)]\)</span> and define <span class="math inline">\(X^{+}(X)=\)</span> <span class="math inline">\(\left(x^{+}(X, 1), \ldots, x^{+}(X, q)\right)\)</span>.</p>
<p><strong>Claim 1</strong> <span class="math inline">\(V_n(X) \leq \mathrm{qEUBO}_n\left(X^{+}(X)\right) .\)</span> We see that <span class="math display">\[\begin{aligned}
V_n(X) &amp; =\sum_{i=1}^q \mathbf{P}_n(r(X)=i) \mathbb{E}_n[g\left(x^{+}(X, i)\right) ] \\
&amp; \leq \sum_{i=1}^q \mathbf{P}_n(r(X)=i) \mathbb{E}_n[\max _{i=1, \ldots, q} g(x^{+}(X, i))] \\
&amp; =\mathbb{E}_n\left[\max _{i=1, \ldots, q} g\left(x^{+}(X, i)\right)\right] \\
&amp; =\mathrm{qEUBO}_n\left(X^{+}(X)\right),
\end{aligned}\]</span> as claimed.</p>
<p><strong>Claim 2</strong> <span class="math inline">\(\mathrm{qEUBO}_n(X) \leq V_n(X) .\)</span> For any given <span class="math inline">\(X \in \mathbb{X}^q\)</span> we have <span class="math display">\[\mathbb{E}_n\left[f\left(x_{r(X)}\right) \mid(X, r(X))\right] \leq \max _{x \in \mathbb{X}} \mathbb{E}_n[f(x) \mid(X, r(X))] .\]</span> Since <span class="math inline">\(f\left(x_{r(X)}\right)=\max _{i=1, \ldots, q} f\left(x_i\right)\)</span>, taking expectations over <span class="math inline">\(r(X)\)</span> on both sides obtains the required result.</p>
<p>Now building on the arguments above, let <span class="math inline">\(X^* \in \operatorname{argmax}_{X \in \mathbb{X}^q} \mathrm{qEUBO}_n(X)\)</span> and suppose for contradiction that <span class="math inline">\(X^* \notin \operatorname{argmax}_{X \in \mathbb{X}^q} V_n(X)\)</span>. Then, there exists <span class="math inline">\(\widetilde{X} \in \mathbb{X}^q\)</span> such that <span class="math inline">\(V_n(\widetilde{X})&gt;V_n\left(X^*\right)\)</span>. We have <span class="math display">\[\begin{aligned}
\operatorname{qEUBO}_n\left(X^{+}(\tilde{X})\right) &amp; \geq V_n(\tilde{X}) \\
&amp; &gt;V_n\left(X^*\right) \\
&amp; \geq \operatorname{qEUBO}_n\left(X^*\right) \\
&amp; \geq \operatorname{qEUBO}_n\left(X^{+}(\tilde{X})\right) .
\end{aligned}\]</span></p>
<p>The first inequality follows from (1). The second inequality is due to our supposition for contradiction. The third inequality is due to (2). Finally, the fourth inequality holds since <span class="math inline">\(X^* \in \operatorname{argmax}_{X \in \mathbb{X}^q} \mathrm{qEUBO}_n(X)\)</span>. This contradiction concludes the proof. ◻</p>
</div>
<p>Therefore a sufficient condition for following one-step Bayes optimality is by maximizing <span class="math inline">\(\text{qEUBO}_n\)</span>.</p>
<p>In experiments that were ran comparing qEUBO to other state-of-the-art acquisition functions, qEUBO consistently outperformed on most problems and was closely followed by qEI and qTS. These results also extended to experiments with multiple options when <span class="math inline">\(q&gt;2\)</span>. In fact, there is faster convergence in regret when using more options in human queries. [Prove Theorem 3: Regret analysis]</p>
</section>
<section id="qei-batch-expected-improvement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qei-batch-expected-improvement">qEI: Batch Expected Improvement</h4>
<p><span class="math display">\[\begin{aligned}
\mathrm{qEI}= &amp; \mathbb{E}_{\mathbf{y}}\left[\left(\max _{i \in[1, \ldots, q]}\left(\mu_{\min }-y_i\right)\right)_{+}\right] \\
= &amp; \sum_{i=1}^q \mathbb{E}_{\mathbf{y}}\left(\mu_{\min }-y_i \mid y_i \leq \mu_{\min }, y_i \leq y_j \forall j \neq i\right) \\
&amp; p\left(y_i \leq \mu_{\min }, y_i \leq y_j \forall j \neq i\right) .
\end{aligned}\]</span></p>
</section>
<section id="qts-batch-thompson-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qts-batch-thompson-sampling">qTS: Batch Thompson Sampling</h4>
<div class="algorithm">
<div class="algorithmic">
<p>Initial data <span class="math inline">\(\mathcal{D}_{\mathcal{I}(1)}=\{(\mathbf{x}_i, y_i)\}_{i \in \mathcal{I}(1)}\)</span> Compute current posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})\)</span> Sample <span class="math inline">\(\boldsymbol{\theta}\)</span> from <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})\)</span> Select <span class="math inline">\(k \leftarrow \arg \max_{j \notin \mathcal{I}(t)} \mathbb{E}[y_j \mid \mathbf{x}_j, \boldsymbol{\theta}]\)</span> Collect <span class="math inline">\(y_k\)</span> by evaluating <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}_k\)</span> <span class="math inline">\(\mathcal{D}_{\mathcal{I}(t+1)} \leftarrow \mathcal{D}_{\mathcal{I}(t)} \cup \{(\mathbf{x}_k, y_k)\}\)</span></p>
</div>
</div>
<div class="algorithm">
<div class="algorithmic">
<p>Initial data <span class="math inline">\(\mathcal{D}_{\mathcal{I}(1)}=\{\mathbf{x}_i, y_i\}_{i \in \mathcal{I}(1)}\)</span>, batch size <span class="math inline">\(S\)</span> Compute current posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})\)</span> Sample <span class="math inline">\(\boldsymbol{\theta}\)</span> from <span class="math inline">\(p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})\)</span> Select <span class="math inline">\(k(s) \leftarrow \arg \max_{j \notin \mathcal{I}(t)} \mathbb{E}[y_j \mid \mathbf{x}_j, \boldsymbol{\theta}]\)</span> <span class="math inline">\(\mathcal{D}_{\mathcal{I}(t+1)} = \mathcal{D}_{\mathcal{I}(t)} \cup \{\mathbf{x}_{k(s)}, y_{k(s)}\}_{s=1}^S\)</span></p>
</div>
</div>
</section>
</section>
<section id="regret-analysis" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="regret-analysis"><span class="header-section-number">4.2.3</span> Regret Analysis</h3>
<section id="qeubo-regret" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qeubo-regret">qEUBO Regret</h4>
<p>With the definition of Bayesian simple regret, we have that qEUBO converges to zero at a rate of <span class="math inline">\(o(1/n)\)</span>, i.e.</p>
<div class="theorem">
<p><span class="math display">\[\label{th:quebo_regret}
\mathbb{E}\left[f\left(x^*\right)-f\left(\widehat{x}_n^*\right)\right]=o(1 / n)\]</span></p>
</div>
<p>where <span class="math inline">\(x^*=\operatorname{argmax}_{x \in \mathrm{X}} f(x)\)</span> and <span class="math inline">\(\widehat{x}_n^* \in \operatorname{argmax}_{x \in \mathrm{X}} \mathbb{E}_n[f(x)]\)</span>.</p>
<p>This theorem holds under the following assumptions:</p>
<ol type="1">
<li><p><strong><span class="math inline">\(f\)</span> is injective</strong> <span class="math inline">\(\mathbf{P}(f(x)=f(y))=0\)</span> for any <span class="math inline">\(x, y \in \mathbb{X}\)</span> with <span class="math inline">\(x \neq y\)</span>.</p></li>
<li><p><strong><span class="math inline">\(f\)</span> represents the preferred option</strong> <span class="math inline">\(\exists a&gt;1 / 2\)</span> s.t. <span class="math inline">\(\mathbf{P}\left(r(X) \in \operatorname{argmax}_{i=1, \ldots, 2} f\left(x_i\right) \mid f(X)\right) \geq a \forall\)</span> <span class="math inline">\(X=\left(x_1, x_2\right) \in \mathbb{X}^2\)</span> with <span class="math inline">\(x_1 \neq x_2\)</span> almost surely under the prior on <span class="math inline">\(f\)</span>.</p></li>
<li><p><strong>Expected difference in utility is proportional to probability of greater utility</strong> <span class="math inline">\(\exists \Delta \geq \delta&gt;0\)</span> s.t. <span class="math inline">\(\forall \mathcal{D}^{(n)} \text{and} \forall x, y \in \mathbb{X}\)</span> (potentially depending on <span class="math inline">\(\mathcal{D}^{(n)}\)</span>), <span class="math display">\[\delta \mathbf{P}^{(n)}(f(x)&gt;f(y)) \leq \mathbb{E}^{(n)}\left[\{f(x)-f(y)\}^{+}\right] \leq \Delta \mathbf{P}^{(n)}(f(x)&gt;f(y))\]</span> almost surely under the prior on <span class="math inline">\(f\)</span>.</p></li>
</ol>
<p>Further lemmas leading to a proof of Theorem <a href="#th:quebo_regret" data-reference-type="ref" data-reference="th:quebo_regret">[th:quebo_regret]</a> is given in <span class="citation" data-cites="astudillo2023qeubodecisiontheoreticacquisitionfunction">(<a href="#ref-astudillo2023qeubodecisiontheoreticacquisitionfunction" role="doc-biblioref">Astudillo et al. 2023</a>)</span> Section B.</p>
</section>
<section id="qei-regret" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="qei-regret">qEI Regret</h4>
<p>The following theorem shows that, under the same assumptions used for qEUBO regret, simple regret of qEI can fail to converge to 0.</p>
<div class="theorem">
<p>There exists a problem instance (i.e., <span class="math inline">\(\mathbb{X}\)</span> and Bayesian prior distribution over f) satisfying the assumptions described in Theorem <a href="#th:quebo_regret" data-reference-type="ref" data-reference="th:quebo_regret">[th:quebo_regret]</a> such that if the sequence of queries is chosen by maximizing qEI, then <span class="math inline">\(\mathbb{E}\left[f\left(x^*\right)-\right.\)</span> <span class="math inline">\(\left.f\left(\widehat{x}_n^*\right)\right] \geq R\)</span> for all <span class="math inline">\(n\)</span>, for a constant <span class="math inline">\(R&gt;0\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> Let <span class="math inline">\(X = \{1, 2, 3, 4\}\)</span> and consider the functions <span class="math inline">\(f_i:X \rightarrow R\)</span>, for <span class="math inline">\(i=1,2,3,4\)</span>, given by <span class="math inline">\(f_i(1) = -1\)</span> and <span class="math inline">\(f_i(2) = 0\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math display">\[\begin{aligned}
    f_1(x) = \begin{cases}
    1, &amp;\ x=3\\
    \frac{1}{2}, &amp;\ x=4
    \end{cases},
\hspace{0.5cm}
f_2(x) = \begin{cases}
    \frac{1}{2}, &amp;\ x=3\\
    1, &amp;\ x=4
    \end{cases},
\hspace{0.5cm}
f_3(x) = \begin{cases}
    -\frac{1}{2}, &amp;\ x=3\\
    -1, &amp;\ x=4
    \end{cases},
\hspace{0.5cm}
f_4(x) = \begin{cases}
    -1, &amp;\ x=3\\
    -\frac{1}{2}, &amp;\ x=4
    \end{cases}.
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(p\)</span> be a number with <span class="math inline">\(0 &lt; p &lt; 1/3\)</span> and set <span class="math inline">\(q=1-p\)</span>. We consider a prior distribution on <span class="math inline">\(f\)</span> with support <span class="math inline">\(\{f_i\}_{i=1}^4\)</span> such that <span class="math display">\[\begin{aligned}
p_i = Pr(f=f_i) =
    \begin{cases}
        p/2, i =1,2,\\
        q/2, i=3,4.
    \end{cases}
\end{aligned}\]</span> We also assume the user’s response likelihood is given by <span class="math inline">\(Pr(r(X)=1\mid f(x_1) &gt; f(x_2)) = a\)</span> for some <span class="math inline">\(a\)</span> such that <span class="math inline">\(1/2 &lt; a &lt; 1\)</span>,</p>
<p>Let <span class="math inline">\(D^{(n)}\)</span> denote the set of observations up to time <span class="math inline">\(n\)</span> and let <span class="math inline">\(p_i^{(n)} = Pr(f=f_i \mid \mathbb{E}^{(n)})\)</span> for <span class="math inline">\(i=1,2,3,4\)</span>. We let the initial data set be <span class="math inline">\(\mathcal{D}^{(0)} = \{(X^{(0)}, r^{(0)})\}\)</span>, where <span class="math inline">\(X^{(0)}= (1,2)\)</span>. We will prove that the following statements are true for all <span class="math inline">\(n\geq 0\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(p_i^{(n)} &gt; 0\)</span> for <span class="math inline">\(i=1,2,3,4\)</span>.</p></li>
<li><p><span class="math inline">\(p_1^{(n)} &lt; \frac{1}{2}p_3^{(n)}\)</span> and <span class="math inline">\(p_2^{(n)} &lt; \frac{1}{2}p_4^{(n)}\)</span>.</p></li>
<li><p><span class="math inline">\(\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n)}[f(x)]=\{2\}\)</span>.</p></li>
<li><p><span class="math inline">\(\arg \max_{X\in\mathcal{X}^2}\text{qEI}^{(n)}(X) = \{(3, 4)\}\)</span>.</p></li>
</ol>
<p>We prove this by induction over <span class="math inline">\(n\)</span>. We begin by proving this for <span class="math inline">\(n=0\)</span>. Since <span class="math inline">\(f_i(1) &lt; f_i(2)\)</span> for all <span class="math inline">\(i\)</span>, the posterior distribution on <span class="math inline">\(f\)</span> given <span class="math inline">\(\mathcal{D}^{(0)}\)</span> remains the same as the prior; i.e., <span class="math inline">\(p_i^{(0)} = p_i\)</span> for <span class="math inline">\(i=1,2,3,4\)</span>. Using this, statements 1 and 2 can be easily verified. Now note that <span class="math inline">\(\mathbb{E}^{(0)}[f(1)]=-1\)</span>, <span class="math inline">\(\mathbb{E}^{(0)}[f(2)]=0\)</span>, and <span class="math inline">\(\mathbb{E}^{(0)}[f(3)] = \mathbb{E}^{(0)}[f(4)] = \frac{3}{2}(p - q)\)</span>. Since <span class="math inline">\(p &lt; q\)</span>, it follows that <span class="math inline">\(\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n)}[f(x)]=\{2\}\)</span>; i.e., statement 3 holds. Finally, since <span class="math inline">\(\max_{x\in\{1,2\}}\mathbb{E}^{(0)}[f(x)] = 0\)</span>, the qEI acquisition function at time <span class="math inline">\(n=0\)</span> is given by <span class="math inline">\(\text{qEI}^{(0)}(X) = \mathbb{E}^{(0)}[\{\max\{f(x_1), f(x_2)\}\}^+]\)</span>. A direct calculation can now be performed to verify that statement 4 holds. This completes the base case.</p>
<p>Now suppose statements 1-4 hold for some <span class="math inline">\(n\geq 0\)</span>. Since <span class="math inline">\(X^{(n+1)} = (3, 4)\)</span>, the posterior distribution on <span class="math inline">\(f\)</span> given <span class="math inline">\(D^{(n+1)}\)</span> is given by <span class="math display">\[\begin{aligned}
p_i^{(n+1)} \propto \begin{cases}
                        p_i^{(n)}\ell, \ i=1,3,\\
                         p_i^{(n)} (1 - \ell), \ i=2,4,
                        \end{cases}
\end{aligned}\]</span> where <span class="math display">\[\ell = a I\{r^{(n+1)} = 1\} + (1-a)I\{r^{(n+1)} = 2\}.\]</span> Observe that <span class="math inline">\(0&lt; \ell &lt; 1\)</span> since <span class="math inline">\(0 &lt; a &lt; 1\)</span>. Thus, <span class="math inline">\(\ell &gt; 0\)</span> and <span class="math inline">\(1-\ell &gt; 0\)</span>. Since <span class="math inline">\(p_i^{(n)} &gt; 0\)</span> by the induction hypothesis, it follows from this that <span class="math inline">\(p_i^{(n+1)} &gt; 0\)</span> for <span class="math inline">\(i=1,2,3,4\)</span>. Moreover, since <span class="math inline">\(p_i^{(n+1)} \propto p_i^{(n)}\ell\)</span> for <span class="math inline">\(i=1,3\)</span> and <span class="math inline">\(p_1^{(n)} &lt; \frac{1}{2}p_3^{(n)}\)</span> by the induction hypothesis, it follows that <span class="math inline">\(p_1^{(n+1)} &lt; \frac{1}{2}p_3^{(n+1)}\)</span>. Similarly, <span class="math inline">\(p_2^{(n+1)} &lt; \frac{1}{2}p_4^{(n+1)}\)</span>. Thus, statements 1 and 2 hold at time <span class="math inline">\(n+1\)</span>.</p>
<p>Now observe that <span class="math display">\[\begin{aligned}
    \mathbb{E}^{(n+1)}[f(3)] &amp;= p_1^{(n+1)} + \frac{1}{2}p_2^{(n+1)} - \frac{1}{2}p_3^{(n+1)} - p_4^{(n+1)}\\
    &amp;= \left(p_1^{(n+1)} - \frac{1}{2}p_3^{(n+1)}\right) + \left(\frac{1}{2}p_2^{(n+1)} - p_4^{(n+1)}\right)\\
    &amp;\leq \left(p_1^{(n+1)} - \frac{1}{2}p_3^{(n+1)}\right) + \left(p_2^{(n+1)} - \frac{1}{2}p_4^{(n+1)}\right)\\
    &amp;\leq 0,
\end{aligned}\]</span> where the last inequality holds since <span class="math inline">\(p_1^{(n+1)} &lt; \frac{1}{2}p_3^{(n+1)}\)</span> and <span class="math inline">\(p_2^{(n+1)} &lt; \frac{1}{2}p_4^{(n+1)}\)</span>. Similarly, we see that <span class="math inline">\(\mathbb{E}^{(n+1)}[f(4)] \leq 0\)</span>. Since <span class="math inline">\(\mathbb{E}^{(n+1)}[f(1)]=-1\)</span> and <span class="math inline">\(\mathbb{E}^{(n+1)}[f(2)]=0\)</span>, it follows that <span class="math inline">\(\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n+1)}[f(x)]=\{2\}\)</span>; i.e., statement 3 holds at time <span class="math inline">\(n+1\)</span>.</p>
<p>Since <span class="math inline">\(\max_{x\in\mathcal{X}}\mathbb{E}^{(0)}[f(x)] = 0\)</span>, the qEI acquisition function at time <span class="math inline">\(n+1\)</span> is given by <span class="math inline">\(\text{qEI}^{(n+1)}(X) = \mathbb{E}^{(n+1)}[\{\max\{f(x_1), f(x_2)\}\}^+]\)</span>. Since <span class="math inline">\(f(1) \leq f(x)\)</span> almost surely under the prior for all <span class="math inline">\(x\in\mathcal{X}\)</span>, there is always a maximizer of qEI that does not contain <span class="math inline">\(1\)</span>. Thus, to find the maximizer of qEI, it suffices to analyse its value at the pairs <span class="math inline">\((2, 3)\)</span>, <span class="math inline">\((3,4)\)</span> and <span class="math inline">\((4,2)\)</span>. We have <span class="math display">\[\text{qEI}^{(n+1)}(2, 3) = p_1^{(n+1)} + 1/2 p_2^{(n+1)},\]</span> <span class="math display">\[\operatorname{qEI}^{(n+1)}(3, 4) = p_1^{(n+1)} + p_2^{(n+1)}\]</span> and <span class="math display">\[\operatorname{qEI}^{(n+1)}(4, 2) = 1/2p_1^{(n+1)} + p_2^{(n+1)}.\]</span> Since <span class="math inline">\(p_1^{(n+1)} &gt; 0\)</span> and <span class="math inline">\(p_2^{(n+1)} &gt; 0\)</span>, it follows that <span class="math inline">\(\arg \max_{X \in X^2}\text{qEI}^{(n+1)}(X) = \{(3, 4)\}\)</span>, which concludes the proof by induction.</p>
<p>Finally, since <span class="math inline">\(\arg \max_{x\in X}\mathbb{E}^{(n)}[f(x)]=\{2\}\)</span> for all <span class="math inline">\(n\)</span>, the Bayesian simple regret of qEI is given by <span class="math display">\[\begin{aligned}
    \mathbb{E}\left[f(x^*) - f(2)\right] &amp;= \sum_{i=1}p_i\left(\max_{x\in X}f_i(x) - f_i(2)\right)\\
    &amp;= p
\end{aligned}\]</span> for all <span class="math inline">\(n\)</span>. ◻</p>
</div>
</section>
<section id="pop-bo-regret" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pop-bo-regret">POP-BO Regret</h4>
<p>Commonly used kernel functions within the RKHS are:</p>
<ol type="1">
<li><p>Linear: <span class="math display">\[k(x, \bar{x})=x^{\top} \bar{x} .\]</span></p></li>
<li><p>Squared Exponential (SE): <span class="math display">\[k(x, \bar{x})=\sigma_{\mathrm{SE}}^2 \exp \left\{-\frac{\|x-\bar{x}\|^2}{l^2}\right\},\]</span> where <span class="math inline">\(\sigma_{\mathrm{SE}}^2\)</span> is the variance parameter and <span class="math inline">\(l\)</span> is the lengthscale parameter.</p></li>
<li><p>Matérn: <span class="math display">\[k(x, \bar{x})=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x-\bar{x}\|}{\rho}\right)^\nu K_\nu\left(\sqrt{2 \nu} \frac{\|x-\bar{x}\|}{\rho}\right),\]</span> where <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\nu\)</span> are the two positive parameters of the kernel function, <span class="math inline">\(\Gamma\)</span> is the gamma function, and <span class="math inline">\(K_\nu\)</span> is the modified Bessel function of the second kind. <span class="math inline">\(\nu\)</span> captures the smoothness of the kernel function.</p></li>
</ol>
<p>With the definition of Bayesian simple regret, we have the following theorem defining the regret bound:</p>
<div class="theorem">
<p>With probability at least <span class="math inline">\(1-\delta\)</span>, the cumulative regret of POP-BO satisfies, <span class="math display">\[R_T=\mathcal{O}\left(\sqrt{\beta_T \gamma_T^{f f^{\prime}} T}\right),\]</span> where <span class="math display">\[\beta_T=\beta(1 / T, \delta, T)=\mathcal{O}\left(\sqrt{T \log \frac{T \mathcal{N}\left(\mathcal{B}_f, 1 / T,\|\cdot\|_{\infty}\right)}{\delta}}\right).\]</span></p>
</div>
<p>The guaranteed convergence rate is characterised as:</p>
<div class="theorem">
<p>[]{#th: popbo_converge label=“th: popbo_converge”} Let <span class="math inline">\(t^{\star}\)</span> be defined as in Eq. (19). With probability at least <span class="math inline">\(1-\delta\)</span>, <span class="math display">\[f\left(x^{\star}\right)-f\left(x_{t^{\star}}\right) \leq \mathcal{O}\left(\frac{\sqrt{\beta_T \gamma_T^{f f^{\prime}}}}{\sqrt{T}}\right)\]</span></p>
</div>
<p>Theorem <a href="#th:%20popbo_converge" data-reference-type="ref" data-reference="th: popbo_converge">[th: popbo_converge]</a> highlights that by minimizing the known term <span class="math inline">\(2\left(2 B+\lambda^{-1 / 2} \sqrt{\beta\left(\epsilon, \frac{\delta}{2}, t\right)}\right) \sigma_t^{f f^{\prime}}\left(\left(x_t, x_t^{\prime}\right)\right)\)</span>, the reported final solution <span class="math inline">\(x_{t^{\star}}\)</span> has a guaranteed convergence rate.</p>
<p>Further kernel-specific regret bounds for POP-BO are calculated as follows:</p>
<div class="theorem">
<p>Setting <span class="math inline">\(\epsilon=1 / T\)</span> and running our POP-BO algorithm in Alg. 1,</p>
<ol type="1">
<li><p>If <span class="math inline">\(k(x, y)=\langle x, y\rangle\)</span>, we have, <span class="math display">\[R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4}\right) .\]</span></p></li>
<li><p>If <span class="math inline">\(k(x, y)\)</span> is a squared exponential kernel, we have, <span class="math display">\[R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4(d+1)}\right) .\]</span></p></li>
<li><p>If <span class="math inline">\(k(x, y)\)</span> is a Matérn kernel, we have, <span class="math display">\[\left.R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4} T^{\frac{d}{\nu}\left(\frac{1}{4}+\frac{d+1}{4+2(d+1)^d / \nu}\right.}\right)\right).\]</span></p></li>
</ol>
</div>
</section>
</section>
</section>
<section id="exercises" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.3</span> Exercises</h2>
<section id="sec-question-1-preferential-bayesian-optimization-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-1-preferential-bayesian-optimization-30-points">Question 1: Preferential Bayesian Optimization (30 points)</h3>
<p><strong>Preferential Bayesian Optimization (PBO)</strong> is a variant of Bayesian Optimization (BO) designed to handle scenarios where feedback is provided in terms of preferences between alternatives rather than explicit numeric evaluations. Suppose you are optimizing an unknown function <span class="math inline">\(f\)</span> over a space <span class="math inline">\(\mathcal{X}\)</span>, but instead of receiving function values, you only receive pairwise comparisons between different points in the input space. That is, given two points <span class="math inline">\(x_1, x_2 \in \mathcal{X}\)</span>, you receive feedback in the form of a preference: <span class="math inline">\(x_1 \succ x_2\)</span> implies <span class="math inline">\(f(x_1) &gt; f(x_2)\)</span>.</p>
<p>The <strong>Gaussian Process (GP)</strong> framework is used to model <span class="math inline">\(f\)</span>, and the optimization is guided by this model. Let <span class="math inline">\(p(x_1 \succ x_2 | f)\)</span> be the probability that <span class="math inline">\(x_1\)</span> is preferred over <span class="math inline">\(x_2\)</span>, which can be modeled using a Bradley-Terry or Thurstone model based on the GP prior.</p>
<p>Using the paper “Preferential Bayesian Optimization” (<a href="https://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf" class="uri">https://proceedings.mlr.press/v70/gonzalez17a/gonzalez17a.pdf</a>), answer the following:</p>
<ol type="a">
<li><p><strong>Modeling Preferences (6 points)</strong></p>
<ol type="i">
<li><p><strong>Likelihood Derivation (Written, 2 points):</strong> Given two points <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and their corresponding latent function values <span class="math inline">\(f(x_1)\)</span> and <span class="math inline">\(f(x_2)\)</span>, derive the likelihood of a preference <span class="math inline">\(x_1 \succ x_2\)</span> using the Bradley-Terry model. Your solution here.</p></li>
<li><p><strong>Incorporating into GP (Written, 2 points):</strong> Explain how this likelihood can be incorporated into the GP framework to model preferences probabilistically. Specifically, describe how the covariance function of the GP affects the joint distribution of preferences and discuss any assumptions made regarding the smoothness or structure of <span class="math inline">\(f\)</span>.</p></li>
<li><p><strong>Posterior Update (Written, 2 points):</strong> Write out an expression for the posterior mean and variance at new query points by using the posterior predictive distribution based on previously observed preferences (no need to simplify since it’s intractable analytically). Suggest an approach that can be used to approximate the mean and variance.</p></li>
</ol></li>
<li><p><strong>Acquisition Function Adaptation (6 points)</strong></p>
<ol type="i">
<li><p><strong>Expected Improvement (EI) for Preferences (Written, 2 points):</strong> Explain how the Expected Improvement (EI) acquisition function is adapted in the context of PBO to handle preferences rather than absolute function values. Please read the paper for this.</p></li>
<li><p><strong>EI Computation for Pairwise Comparisons (Written, 2 points):</strong> Derive the expression for EI when dealing with pairwise comparisons. Show how the computation of EI differs from the standard BO setting and discuss how uncertainty in the GP model is used in this context.</p></li>
<li><p><strong>Selection Strategy (Written, 2 points):</strong> Describe how the acquisition function uses the pairwise preference data to select the next query point. Provide a rigorous justification for this selection strategy in terms of maximizing expected information gain.</p></li>
</ol></li>
<li><p><strong>Exploration-Exploitation Balance in PBO (6 points)</strong></p>
<ol type="i">
<li><p><strong>Exploration Mechanism (Written, 2 points):</strong> Explain how exploration is handled in the PBO framework. Describe how uncertainty in the preference model (the GP posterior) influences the selection of new points for evaluation.</p></li>
<li><p><strong>Uncertainty Quantification (Written, 2 points):</strong> Define how the variance in the GP posterior represents uncertainty in the model and show how this uncertainty is updated as new preferences are observed.</p></li>
<li><p><strong>Empirical Validation (Written, 2 points):</strong> Design an experiment to empirically validate the balance between exploration and exploitation in PBO. Describe the setup, including the objective function, the experimental conditions, and the evaluation metric for measuring the quality of exploration-exploitation balance.</p></li>
</ol></li>
<li><p><strong>Scalability and Practical Considerations (6 points)</strong></p>
<ol type="i">
<li><p><strong>Challenges in Preference Feedback (Written, 2 points):</strong> Discuss the challenges associated with preference feedback in real-world applications, such as inconsistency in user preferences and potential biases.</p></li>
<li><p><strong>GP Scalability (Written, 2 points):</strong> Explain how the scalability of the GP model affects the performance of PBO, especially as the number of observations increases. Include a discussion on computational complexity and possible solutions.</p></li>
<li><p><strong>Extensions for Large-Scale Problems (Written, 2 points):</strong> Propose potential extensions or modifications to improve the applicability of PBO to large-scale optimization problems. For example, discuss the feasibility of sparse GPs or other approximation techniques and evaluate their potential impact on PBO performance.</p></li>
</ol></li>
<li><p><strong>Empirical Experimentation (6 points)</strong></p>
<ol type="i">
<li><p><strong>Copeland Score (Coding, 2 points):</strong> Implement <code>compute_max_copeland_score</code> in<br>
<code>pbo/forrester_duel.py</code>.</p></li>
<li><p><strong>Copeland Acquisition (Coding, 4 points):</strong> Implement <code>copeland_acquisition</code>. Run <code>forrester_duel.py</code> and briefly discuss any patterns you observe in the chosen duels (black Xs on the heatmap).</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="f42dd2ba" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessClassifier</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF, ConstantKernel <span class="im">as</span> C</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># Define the Forrester function</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">def</span> forrester_function(x):</span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="co">"""</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">    Evaluates the Forrester function at the given input.</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">    </span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">    Args:</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">    - x (float or numpy.ndarray): Input value(s) in the range [0, 1].</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">    </span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">    Returns:</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">    - float or numpy.ndarray: Evaluated Forrester function value(s).</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="co">    """</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>    <span class="cf">return</span> (<span class="dv">6</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.sin(<span class="dv">12</span> <span class="op">*</span> x <span class="op">-</span> <span class="dv">4</span>)</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co"># Sigmoid function for probabilistic preferences</span></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb1-22"><a href="#cb1-22"></a>    <span class="co">"""</span></span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="co">    Computes the sigmoid function for the given input.</span></span>
<span id="cb1-24"><a href="#cb1-24"></a><span class="co">    </span></span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="co">    Args:</span></span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="co">    - x (float or numpy.ndarray): Input value(s).</span></span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="co">    </span></span>
<span id="cb1-28"><a href="#cb1-28"></a><span class="co">    Returns:</span></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="co">    - float or numpy.ndarray: Sigmoid-transformed value(s).</span></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="co">    """</span></span>
<span id="cb1-31"><a href="#cb1-31"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-32"><a href="#cb1-32"></a></span>
<span id="cb1-33"><a href="#cb1-33"></a><span class="co"># Simulate duel outcome probabilistically</span></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="kw">def</span> simulate_duel_outcome(x, x_prime):</span>
<span id="cb1-35"><a href="#cb1-35"></a>    <span class="co">"""</span></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="co">    Simulates the outcome of a duel between two candidates based on probabilistic preferences.</span></span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="co">    </span></span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="co">    Args:</span></span>
<span id="cb1-39"><a href="#cb1-39"></a><span class="co">    - x (float): First candidate's input value.</span></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="co">    - x_prime (float): Second candidate's input value.</span></span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="co">    </span></span>
<span id="cb1-42"><a href="#cb1-42"></a><span class="co">    Returns:</span></span>
<span id="cb1-43"><a href="#cb1-43"></a><span class="co">    - int: 1 if x wins, 0 otherwise.</span></span>
<span id="cb1-44"><a href="#cb1-44"></a><span class="co">    """</span></span>
<span id="cb1-45"><a href="#cb1-45"></a>    prob <span class="op">=</span> sigmoid(forrester_function(x_prime) <span class="op">-</span> forrester_function(x))  <span class="co"># Probability x beats x'</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>    <span class="cf">return</span> np.random.choice([<span class="dv">1</span>, <span class="dv">0</span>], p<span class="op">=</span>[prob, <span class="dv">1</span> <span class="op">-</span> prob])</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="co"># Compute the Soft Copeland score for all candidates (vectorized)</span></span>
<span id="cb1-49"><a href="#cb1-49"></a><span class="kw">def</span> compute_max_copeland_score(candidates, gp, landmarks):</span>
<span id="cb1-50"><a href="#cb1-50"></a>    <span class="co">"""</span></span>
<span id="cb1-51"><a href="#cb1-51"></a><span class="co">    Computes the maximum Copeland score for given candidates using predicted win probabilities.</span></span>
<span id="cb1-52"><a href="#cb1-52"></a><span class="co">    </span></span>
<span id="cb1-53"><a href="#cb1-53"></a><span class="co">    Args:</span></span>
<span id="cb1-54"><a href="#cb1-54"></a><span class="co">    - candidates (numpy.ndarray): Array of candidate points.</span></span>
<span id="cb1-55"><a href="#cb1-55"></a><span class="co">    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.</span></span>
<span id="cb1-56"><a href="#cb1-56"></a><span class="co">    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.</span></span>
<span id="cb1-57"><a href="#cb1-57"></a><span class="co">    </span></span>
<span id="cb1-58"><a href="#cb1-58"></a><span class="co">    Returns:</span></span>
<span id="cb1-59"><a href="#cb1-59"></a><span class="co">    - tuple: Maximum Copeland score and the best candidate.</span></span>
<span id="cb1-60"><a href="#cb1-60"></a><span class="co">    """</span></span>
<span id="cb1-61"><a href="#cb1-61"></a>    <span class="co"># YOUR CODE HERE (~6 lines)</span></span>
<span id="cb1-62"><a href="#cb1-62"></a>        <span class="co"># 1. Generate all pairs between candidates and landmarks.</span></span>
<span id="cb1-63"><a href="#cb1-63"></a>        <span class="co"># 2. Get win probabilities and average</span></span>
<span id="cb1-64"><a href="#cb1-64"></a>        <span class="co"># 3. Return appropriate maximum and best candidate.</span></span>
<span id="cb1-65"><a href="#cb1-65"></a>    <span class="cf">pass</span> </span>
<span id="cb1-66"><a href="#cb1-66"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a><span class="co"># Acquisition function with GP retraining and maximum Copeland score for each outcome</span></span>
<span id="cb1-69"><a href="#cb1-69"></a><span class="kw">def</span> copeland_acquisition(x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score):</span>
<span id="cb1-70"><a href="#cb1-70"></a>    <span class="co">"""</span></span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="co">    Computes the acquisition value for a candidate pair by simulating outcomes and retraining the GP.</span></span>
<span id="cb1-72"><a href="#cb1-72"></a><span class="co">    </span></span>
<span id="cb1-73"><a href="#cb1-73"></a><span class="co">    Args:</span></span>
<span id="cb1-74"><a href="#cb1-74"></a><span class="co">    - x (float): First value of duel.</span></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="co">    - x_prime (float): Second value of duel.</span></span>
<span id="cb1-76"><a href="#cb1-76"></a><span class="co">    - x_candidates (numpy.ndarray): Array of candidate points to evaluate soft Copeland on.</span></span>
<span id="cb1-77"><a href="#cb1-77"></a><span class="co">    - gp (GaussianProcessClassifier): Trained Gaussian process classifier for preference modeling.</span></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="co">    - train_X (numpy.ndarray): Current training input pairs.</span></span>
<span id="cb1-79"><a href="#cb1-79"></a><span class="co">    - train_y (numpy.ndarray): Current training labels.</span></span>
<span id="cb1-80"><a href="#cb1-80"></a><span class="co">    - landmarks (numpy.ndarray): Array of landmark points used for Monte Carlo approximation.</span></span>
<span id="cb1-81"><a href="#cb1-81"></a><span class="co">    - max_copeland_score (float): Maximum copeland score prior to acquiring any new pair</span></span>
<span id="cb1-82"><a href="#cb1-82"></a><span class="co">    </span></span>
<span id="cb1-83"><a href="#cb1-83"></a><span class="co">    Returns:</span></span>
<span id="cb1-84"><a href="#cb1-84"></a><span class="co">    - float: Acquisition value for the given pair (x, x_prime).</span></span>
<span id="cb1-85"><a href="#cb1-85"></a><span class="co">    """</span></span>
<span id="cb1-86"><a href="#cb1-86"></a>    <span class="co"># YOUR CODE HERE (~14-16 lines)</span></span>
<span id="cb1-87"><a href="#cb1-87"></a>        <span class="co"># 1. Predict dueling probabilities</span></span>
<span id="cb1-88"><a href="#cb1-88"></a>        <span class="co"># 2. Simulate adding (x, x') with y=1 (x beats x') and fit GP </span></span>
<span id="cb1-89"><a href="#cb1-89"></a>        <span class="co"># 3. Simulate adding (x, x') with y=0 (x' beats x) and fit GP </span></span>
<span id="cb1-90"><a href="#cb1-90"></a>        <span class="co"># 4. Compute expected improvement in max Copeland score</span></span>
<span id="cb1-91"><a href="#cb1-91"></a>        <span class="co"># 5. Return weighted acquisition value</span></span>
<span id="cb1-92"><a href="#cb1-92"></a>    <span class="cf">pass</span></span>
<span id="cb1-93"><a href="#cb1-93"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-94"><a href="#cb1-94"></a></span>
<span id="cb1-95"><a href="#cb1-95"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-96"><a href="#cb1-96"></a>    <span class="co"># Initialization</span></span>
<span id="cb1-97"><a href="#cb1-97"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-98"><a href="#cb1-98"></a>    kernel <span class="op">=</span> C(<span class="fl">28.0</span>, constant_value_bounds<span class="op">=</span><span class="st">'fixed'</span>) <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">0.15</span>, length_scale_bounds<span class="op">=</span><span class="st">'fixed'</span>)</span>
<span id="cb1-99"><a href="#cb1-99"></a>    gp <span class="op">=</span> GaussianProcessClassifier(kernel<span class="op">=</span>kernel)</span>
<span id="cb1-100"><a href="#cb1-100"></a></span>
<span id="cb1-101"><a href="#cb1-101"></a>    <span class="co"># Generate initial training data (random pairs)</span></span>
<span id="cb1-102"><a href="#cb1-102"></a>    train_X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>]]) <span class="co">#np.random.uniform(0, 1, (10, 2))  # 20 random dueling pairs [x, x']</span></span>
<span id="cb1-103"><a href="#cb1-103"></a>    train_y <span class="op">=</span> np.array([simulate_duel_outcome(pair[<span class="dv">0</span>], pair[<span class="dv">1</span>]) <span class="cf">for</span> pair <span class="kw">in</span> train_X])</span>
<span id="cb1-104"><a href="#cb1-104"></a></span>
<span id="cb1-105"><a href="#cb1-105"></a>    <span class="co"># Fixed landmark points and their function values</span></span>
<span id="cb1-106"><a href="#cb1-106"></a>    landmarks <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">30</span>)  <span class="co"># 10 fixed landmarks</span></span>
<span id="cb1-107"><a href="#cb1-107"></a></span>
<span id="cb1-108"><a href="#cb1-108"></a>    <span class="co"># Generate candidate pairs for optimization</span></span>
<span id="cb1-109"><a href="#cb1-109"></a>    x_candidates <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">30</span>)  <span class="co"># Reduced grid for efficiency</span></span>
<span id="cb1-110"><a href="#cb1-110"></a>    X, X_prime <span class="op">=</span> np.meshgrid(x_candidates, x_candidates)</span>
<span id="cb1-111"><a href="#cb1-111"></a>    candidate_pairs <span class="op">=</span> np.c_[X.ravel(), X_prime.ravel()]</span>
<span id="cb1-112"><a href="#cb1-112"></a></span>
<span id="cb1-113"><a href="#cb1-113"></a>    <span class="co"># Optimization loop</span></span>
<span id="cb1-114"><a href="#cb1-114"></a>    n_iterations <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-115"><a href="#cb1-115"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb1-116"><a href="#cb1-116"></a>        <span class="co"># Retrain the GP with current training data</span></span>
<span id="cb1-117"><a href="#cb1-117"></a>        gp.fit(train_X, train_y)</span>
<span id="cb1-118"><a href="#cb1-118"></a></span>
<span id="cb1-119"><a href="#cb1-119"></a>        <span class="co"># Compute global maximum Copeland score</span></span>
<span id="cb1-120"><a href="#cb1-120"></a>        max_copeland_score, condorcet_winner <span class="op">=</span> compute_max_copeland_score(x_candidates, gp, landmarks)</span>
<span id="cb1-121"><a href="#cb1-121"></a>        <span class="bu">print</span>(<span class="ss">f"Condorcet winner iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>condorcet_winner<span class="sc">}</span><span class="ss"> with soft-Copeland score </span><span class="sc">{</span>max_copeland_score<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-122"><a href="#cb1-122"></a></span>
<span id="cb1-123"><a href="#cb1-123"></a>        <span class="co"># Evaluate acquisition values for all candidate pairs</span></span>
<span id="cb1-124"><a href="#cb1-124"></a>        acquisition_values <span class="op">=</span> np.zeros(<span class="bu">len</span>(candidate_pairs))</span>
<span id="cb1-125"><a href="#cb1-125"></a>        <span class="cf">for</span> idx, (x, x_prime) <span class="kw">in</span> tqdm(<span class="bu">enumerate</span>(candidate_pairs), total<span class="op">=</span><span class="bu">len</span>(candidate_pairs)):</span>
<span id="cb1-126"><a href="#cb1-126"></a>            acquisition_values[idx] <span class="op">=</span> copeland_acquisition(</span>
<span id="cb1-127"><a href="#cb1-127"></a>                x, x_prime, x_candidates, gp, train_X, train_y, landmarks, max_copeland_score</span>
<span id="cb1-128"><a href="#cb1-128"></a>            )</span>
<span id="cb1-129"><a href="#cb1-129"></a></span>
<span id="cb1-130"><a href="#cb1-130"></a>        <span class="co"># Select the pair with the highest acquisition value</span></span>
<span id="cb1-131"><a href="#cb1-131"></a>        best_idx <span class="op">=</span> np.argmax(acquisition_values)</span>
<span id="cb1-132"><a href="#cb1-132"></a>        next_x, next_x_prime <span class="op">=</span> candidate_pairs[best_idx]</span>
<span id="cb1-133"><a href="#cb1-133"></a></span>
<span id="cb1-134"><a href="#cb1-134"></a>        <span class="co"># Simulate the actual outcome of the duel</span></span>
<span id="cb1-135"><a href="#cb1-135"></a>        outcome <span class="op">=</span> simulate_duel_outcome(next_x, next_x_prime)</span>
<span id="cb1-136"><a href="#cb1-136"></a></span>
<span id="cb1-137"><a href="#cb1-137"></a>        <span class="co"># Update training data with the new duel outcome</span></span>
<span id="cb1-138"><a href="#cb1-138"></a>        train_X <span class="op">=</span> np.vstack([train_X, [next_x, next_x_prime]])</span>
<span id="cb1-139"><a href="#cb1-139"></a>        train_y <span class="op">=</span> np.append(train_y, outcome)</span>
<span id="cb1-140"><a href="#cb1-140"></a></span>
<span id="cb1-141"><a href="#cb1-141"></a>    <span class="co"># Generate heatmaps</span></span>
<span id="cb1-142"><a href="#cb1-142"></a>    x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb1-143"><a href="#cb1-143"></a>    X, X_prime <span class="op">=</span> np.meshgrid(x, x)</span>
<span id="cb1-144"><a href="#cb1-144"></a>    pairs <span class="op">=</span> np.c_[X.ravel(), X_prime.ravel()]</span>
<span id="cb1-145"><a href="#cb1-145"></a></span>
<span id="cb1-146"><a href="#cb1-146"></a>    <span class="co"># Ground Truth Preference Probabilities</span></span>
<span id="cb1-147"><a href="#cb1-147"></a>    gt_preferences <span class="op">=</span> np.array([</span>
<span id="cb1-148"><a href="#cb1-148"></a>        sigmoid(forrester_function(x_prime) <span class="op">-</span> forrester_function(x))</span>
<span id="cb1-149"><a href="#cb1-149"></a>        <span class="cf">for</span> x, x_prime <span class="kw">in</span> pairs</span>
<span id="cb1-150"><a href="#cb1-150"></a>    ]).reshape(X.shape)</span>
<span id="cb1-151"><a href="#cb1-151"></a></span>
<span id="cb1-152"><a href="#cb1-152"></a>    <span class="co"># GP-Predicted Preferences</span></span>
<span id="cb1-153"><a href="#cb1-153"></a>    gp_predictions <span class="op">=</span> gp.predict_proba(pairs)[:, <span class="dv">1</span>].reshape(X.shape)</span>
<span id="cb1-154"><a href="#cb1-154"></a></span>
<span id="cb1-155"><a href="#cb1-155"></a>    <span class="co"># Plot Ground Truth Preference Heatmap</span></span>
<span id="cb1-156"><a href="#cb1-156"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb1-157"><a href="#cb1-157"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-158"><a href="#cb1-158"></a>    plt.contourf(X, X_prime, gt_preferences, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb1-159"><a href="#cb1-159"></a>    plt.colorbar(label<span class="op">=</span><span class="st">"Ground Truth Preference Probability"</span>)</span>
<span id="cb1-160"><a href="#cb1-160"></a>    plt.title(<span class="st">"Ground Truth Preference Heatmap"</span>)</span>
<span id="cb1-161"><a href="#cb1-161"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-162"><a href="#cb1-162"></a>    plt.ylabel(<span class="st">"x'"</span>)</span>
<span id="cb1-163"><a href="#cb1-163"></a></span>
<span id="cb1-164"><a href="#cb1-164"></a>    <span class="bu">print</span>(<span class="ss">f'Chosen duels: </span><span class="sc">{</span>train_X[<span class="op">-</span>n_iterations:]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-165"><a href="#cb1-165"></a></span>
<span id="cb1-166"><a href="#cb1-166"></a>    <span class="co"># Plot GP-Predicted Preference Heatmap</span></span>
<span id="cb1-167"><a href="#cb1-167"></a>    plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-168"><a href="#cb1-168"></a>    plt.contourf(X, X_prime, gp_predictions, levels<span class="op">=</span><span class="dv">50</span>, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb1-169"><a href="#cb1-169"></a>    plt.colorbar(label<span class="op">=</span><span class="st">"GP-Predicted Preference Probability"</span>)</span>
<span id="cb1-170"><a href="#cb1-170"></a>    plt.scatter(train_X[<span class="op">-</span>n_iterations:, <span class="dv">0</span>], train_X[<span class="op">-</span>n_iterations:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">'black'</span>, label<span class="op">=</span><span class="st">"Last Iterations"</span>, s<span class="op">=</span><span class="dv">30</span>, marker<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb1-171"><a href="#cb1-171"></a>    plt.title(<span class="st">"GP-Predicted Preference Heatmap"</span>)</span>
<span id="cb1-172"><a href="#cb1-172"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb1-173"><a href="#cb1-173"></a>    plt.ylabel(<span class="st">"x'"</span>)</span>
<span id="cb1-174"><a href="#cb1-174"></a></span>
<span id="cb1-175"><a href="#cb1-175"></a>    plt.tight_layout()</span>
<span id="cb1-176"><a href="#cb1-176"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-2-linear-dueling-bandit-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-2-linear-dueling-bandit-30-points">Question 2: Linear Dueling Bandit (30 points)</h3>
<p>In the linear dueling bandit problem, feedback is provided through pairwise comparisons between actions, rather than direct rewards. Consider a finite set of <span class="math inline">\(K\)</span> actions, each represented by a feature vector <span class="math inline">\(x_1, x_2, \dots, x_K \in \mathbb{R}^d\)</span>. Let the unknown preference scores be <span class="math inline">\(f(x_i) = \theta^\top x_i\)</span> and <span class="math inline">\(f(x_j) = \theta^\top x_j\)</span>, where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> is an unknown parameter vector. The goal is to identify the best action by iteratively comparing pairs of actions while minimizing cumulative regret. Using qEUBO from <a href="https://arxiv.org/pdf/2303.15746" class="uri">https://arxiv.org/pdf/2303.15746</a>, complete the following:</p>
<ol type="a">
<li><p><strong>Acquisition Functions for Regret Minimization (Written, 10 points)</strong>: Write out the expression for the acquisition function Expected Improvement discussed in Q1 and qEUBO in the context of the linear dueling bandit. Discuss conditions under which each acquisition function could outperform the others in minimizing cumulative regret.</p></li>
<li><p><strong>Experimental Evaluation of Acquisition Functions (Written + Coding, 10 points)</strong>: Benchmark the performance of the two acquisition functions experimentally.</p>
<ol type="i">
<li><p>Finish implementing the acquisition functions in a linear dueling bandit simulation with <span class="math inline">\(K = 10\)</span> and <span class="math inline">\(d = 5\)</span>, using synthetic data by completing the function <code>calculate_regret_from_gp</code> in <code>linear_dueling/run.py</code>.</p></li>
<li><p>Measure and compare cumulative regret over <span class="math inline">\(T = 200\)</span> rounds for each acquisition function.</p></li>
<li><p>Report and analyze the empirical regret curves, discussing any notable performance differences.</p></li>
</ol></li>
<li><p><strong>Effect of Dimensionality on Regret (Written + Coding, 10 points)</strong>: Analyze how increasing feature dimensionality impacts regret.</p>
<ol type="i">
<li><p>Experimentally evaluate the regret for different values of <span class="math inline">\(d\)</span> (e.g., <span class="math inline">\(d = 5, 10, 20\)</span>) while keeping <span class="math inline">\(K\)</span> constant.</p></li>
<li><p>Plot the regret against <span class="math inline">\(d\)</span> and explain any observed trends.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="e465df2b" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> __future__ <span class="im">import</span> annotations</span>
<span id="cb2-2"><a href="#cb2-2"></a></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">from</span> typing <span class="im">import</span> Optional</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">import</span> itertools</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="im">import</span> torch</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="im">from</span> botorch.acquisition.preference <span class="im">import</span> qExpectedUtilityOfBestOption</span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="im">from</span> botorch.acquisition.logei <span class="im">import</span> qLogExpectedImprovement</span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="im">from</span> botorch.fit <span class="im">import</span> fit_gpytorch_mll</span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="im">from</span> botorch.models.gpytorch <span class="im">import</span> GPyTorchModel</span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="im">from</span> botorch.utils.sampling <span class="im">import</span> draw_sobol_samples</span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="im">from</span> botorch.sampling <span class="im">import</span> SobolQMCNormalSampler</span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="im">from</span> botorch.posteriors.gpytorch <span class="im">import</span> GPyTorchPosterior</span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="im">from</span> gpytorch.distributions <span class="im">import</span> base_distributions</span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="im">from</span> gpytorch.likelihoods <span class="im">import</span> Likelihood</span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="im">from</span> gpytorch.distributions <span class="im">import</span> MultivariateNormal</span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="im">from</span> gpytorch.kernels <span class="im">import</span> Kernel, RBFKernel, ScaleKernel</span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="im">from</span> gpytorch.mlls.variational_elbo <span class="im">import</span> VariationalELBO</span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="im">from</span> gpytorch.means <span class="im">import</span> ConstantMean</span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="im">from</span> gpytorch.models <span class="im">import</span> ApproximateGP</span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="im">from</span> gpytorch.priors.torch_priors <span class="im">import</span> GammaPrior</span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="im">from</span> gpytorch.variational <span class="im">import</span> (</span>
<span id="cb2-26"><a href="#cb2-26"></a>    CholeskyVariationalDistribution,</span>
<span id="cb2-27"><a href="#cb2-27"></a>    UnwhitenedVariationalStrategy,</span>
<span id="cb2-28"><a href="#cb2-28"></a>    VariationalStrategy,</span>
<span id="cb2-29"><a href="#cb2-29"></a>)</span>
<span id="cb2-30"><a href="#cb2-30"></a></span>
<span id="cb2-31"><a href="#cb2-31"></a></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="kw">class</span> PreferentialSoftmaxLikelihood(Likelihood):</span>
<span id="cb2-33"><a href="#cb2-33"></a>    <span class="co">r"""</span></span>
<span id="cb2-34"><a href="#cb2-34"></a><span class="co">    Implements the softmax likelihood used for GP-based preference learning.</span></span>
<span id="cb2-35"><a href="#cb2-35"></a></span>
<span id="cb2-36"><a href="#cb2-36"></a><span class="co">    .. math::</span></span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="co">        p(\mathbf y \mid \mathbf f) = </span><span class="ch">\t</span><span class="co">ext{Softmax} \left( \mathbf f </span><span class="ch">\r</span><span class="co">ight)</span></span>
<span id="cb2-38"><a href="#cb2-38"></a></span>
<span id="cb2-39"><a href="#cb2-39"></a><span class="co">    :param int num_alternatives: Number of alternatives (i.e., q).</span></span>
<span id="cb2-40"><a href="#cb2-40"></a><span class="co">    """</span></span>
<span id="cb2-41"><a href="#cb2-41"></a></span>
<span id="cb2-42"><a href="#cb2-42"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_alternatives):</span>
<span id="cb2-43"><a href="#cb2-43"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-44"><a href="#cb2-44"></a>        <span class="va">self</span>.num_alternatives <span class="op">=</span> num_alternatives</span>
<span id="cb2-45"><a href="#cb2-45"></a>        <span class="va">self</span>.noise <span class="op">=</span> torch.tensor(<span class="fl">1e-4</span>)  <span class="co"># This is only used to draw RFFs-based</span></span>
<span id="cb2-46"><a href="#cb2-46"></a>        <span class="co"># samples. We set it close to zero because we want noise-free samples</span></span>
<span id="cb2-47"><a href="#cb2-47"></a>        <span class="va">self</span>.sampler <span class="op">=</span> SobolQMCNormalSampler(</span>
<span id="cb2-48"><a href="#cb2-48"></a>            sample_shape<span class="op">=</span>torch.Size([<span class="dv">512</span>]))  <span class="co"># This allows for</span></span>
<span id="cb2-49"><a href="#cb2-49"></a>        <span class="co"># SAA-based optimization of the ELBO</span></span>
<span id="cb2-50"><a href="#cb2-50"></a></span>
<span id="cb2-51"><a href="#cb2-51"></a>    <span class="kw">def</span> _draw_likelihood_samples(</span>
<span id="cb2-52"><a href="#cb2-52"></a>        <span class="va">self</span>, function_dist, <span class="op">*</span>args, sample_shape<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs</span>
<span id="cb2-53"><a href="#cb2-53"></a>    ):</span>
<span id="cb2-54"><a href="#cb2-54"></a>        function_samples <span class="op">=</span> <span class="va">self</span>.sampler(</span>
<span id="cb2-55"><a href="#cb2-55"></a>            GPyTorchPosterior(function_dist)).squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb2-56"><a href="#cb2-56"></a>        <span class="cf">return</span> <span class="va">self</span>.forward(function_samples, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb2-57"><a href="#cb2-57"></a></span>
<span id="cb2-58"><a href="#cb2-58"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, function_samples, <span class="op">*</span>params, <span class="op">**</span>kwargs):</span>
<span id="cb2-59"><a href="#cb2-59"></a>        function_samples <span class="op">=</span> function_samples.reshape(</span>
<span id="cb2-60"><a href="#cb2-60"></a>            function_samples.shape[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-61"><a href="#cb2-61"></a>            <span class="op">+</span> torch.Size(</span>
<span id="cb2-62"><a href="#cb2-62"></a>                (</span>
<span id="cb2-63"><a href="#cb2-63"></a>                    <span class="bu">int</span>(function_samples.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">/</span> <span class="va">self</span>.num_alternatives),</span>
<span id="cb2-64"><a href="#cb2-64"></a>                    <span class="va">self</span>.num_alternatives,</span>
<span id="cb2-65"><a href="#cb2-65"></a>                )</span>
<span id="cb2-66"><a href="#cb2-66"></a>            )</span>
<span id="cb2-67"><a href="#cb2-67"></a>        )  <span class="co"># Reshape samples as if they came from a multi-output model (with `q` outputs)</span></span>
<span id="cb2-68"><a href="#cb2-68"></a>        num_alternatives <span class="op">=</span> function_samples.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-69"><a href="#cb2-69"></a></span>
<span id="cb2-70"><a href="#cb2-70"></a>        <span class="cf">if</span> num_alternatives <span class="op">!=</span> <span class="va">self</span>.num_alternatives:</span>
<span id="cb2-71"><a href="#cb2-71"></a>            <span class="cf">raise</span> <span class="pp">RuntimeError</span>(<span class="st">"There should be </span><span class="sc">%d</span><span class="st"> points"</span> <span class="op">%</span></span>
<span id="cb2-72"><a href="#cb2-72"></a>                               <span class="va">self</span>.num_alternatives)</span>
<span id="cb2-73"><a href="#cb2-73"></a></span>
<span id="cb2-74"><a href="#cb2-74"></a>        res <span class="op">=</span> base_distributions.Categorical(</span>
<span id="cb2-75"><a href="#cb2-75"></a>            logits<span class="op">=</span>function_samples)  <span class="co"># Passing the</span></span>
<span id="cb2-76"><a href="#cb2-76"></a>        <span class="co"># function values as logits recovers the softmax likelihood</span></span>
<span id="cb2-77"><a href="#cb2-77"></a>        <span class="cf">return</span> res</span>
<span id="cb2-78"><a href="#cb2-78"></a></span>
<span id="cb2-79"><a href="#cb2-79"></a></span>
<span id="cb2-80"><a href="#cb2-80"></a><span class="kw">class</span> VariationalPreferentialGP(GPyTorchModel, ApproximateGP):</span>
<span id="cb2-81"><a href="#cb2-81"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb2-82"><a href="#cb2-82"></a>        <span class="va">self</span>,</span>
<span id="cb2-83"><a href="#cb2-83"></a>        queries: Tensor,</span>
<span id="cb2-84"><a href="#cb2-84"></a>        responses: Tensor,</span>
<span id="cb2-85"><a href="#cb2-85"></a>        use_withening: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb2-86"><a href="#cb2-86"></a>        covar_module: Optional[Kernel] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb2-87"><a href="#cb2-87"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-88"><a href="#cb2-88"></a>        <span class="co">r"""</span></span>
<span id="cb2-89"><a href="#cb2-89"></a><span class="co">        Args:</span></span>
<span id="cb2-90"><a href="#cb2-90"></a><span class="co">            queries: A `n x q x d` tensor of training inputs. Each of the `n` queries is constituted</span></span>
<span id="cb2-91"><a href="#cb2-91"></a><span class="co">                by `q` `d`-dimensional decision vectors.</span></span>
<span id="cb2-92"><a href="#cb2-92"></a><span class="co">            responses: A `n x 1` tensor of training outputs. Each of the `n` responses is an integer</span></span>
<span id="cb2-93"><a href="#cb2-93"></a><span class="co">                between 0 and `q-1` indicating the decision vector selected by the user.</span></span>
<span id="cb2-94"><a href="#cb2-94"></a><span class="co">            use_withening: If true, use withening to enhance variational inference.</span></span>
<span id="cb2-95"><a href="#cb2-95"></a><span class="co">            covar_module: The module computing the covariance matrix.</span></span>
<span id="cb2-96"><a href="#cb2-96"></a><span class="co">        """</span></span>
<span id="cb2-97"><a href="#cb2-97"></a>        <span class="va">self</span>.queries <span class="op">=</span> queries</span>
<span id="cb2-98"><a href="#cb2-98"></a>        <span class="va">self</span>.responses <span class="op">=</span> responses</span>
<span id="cb2-99"><a href="#cb2-99"></a>        <span class="va">self</span>.input_dim <span class="op">=</span> queries.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb2-100"><a href="#cb2-100"></a>        <span class="va">self</span>.q <span class="op">=</span> queries.shape[<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb2-101"><a href="#cb2-101"></a>        <span class="va">self</span>.num_data <span class="op">=</span> queries.shape[<span class="op">-</span><span class="dv">3</span>]</span>
<span id="cb2-102"><a href="#cb2-102"></a>        train_x <span class="op">=</span> queries.reshape(</span>
<span id="cb2-103"><a href="#cb2-103"></a>            queries.shape[<span class="dv">0</span>] <span class="op">*</span> queries.shape[<span class="dv">1</span>], queries.shape[<span class="dv">2</span>]</span>
<span id="cb2-104"><a href="#cb2-104"></a>        )  <span class="co"># Reshape queries in the form of "standard training inputs"</span></span>
<span id="cb2-105"><a href="#cb2-105"></a>        train_y <span class="op">=</span> responses.squeeze(<span class="op">-</span><span class="dv">1</span>)  <span class="co"># Squeeze out output dimension</span></span>
<span id="cb2-106"><a href="#cb2-106"></a>        bounds <span class="op">=</span> torch.tensor(</span>
<span id="cb2-107"><a href="#cb2-107"></a>            [[<span class="dv">0</span>, <span class="dv">1</span>] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.input_dim)], dtype<span class="op">=</span>torch.double</span>
<span id="cb2-108"><a href="#cb2-108"></a>        ).T  <span class="co"># This assumes the input space has been normalized beforehand</span></span>
<span id="cb2-109"><a href="#cb2-109"></a>        <span class="co"># Construct variational distribution and strategy</span></span>
<span id="cb2-110"><a href="#cb2-110"></a>        <span class="cf">if</span> use_withening:</span>
<span id="cb2-111"><a href="#cb2-111"></a>            inducing_points <span class="op">=</span> draw_sobol_samples(</span>
<span id="cb2-112"><a href="#cb2-112"></a>                bounds<span class="op">=</span>bounds,</span>
<span id="cb2-113"><a href="#cb2-113"></a>                n<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> <span class="va">self</span>.input_dim,</span>
<span id="cb2-114"><a href="#cb2-114"></a>                q<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-115"><a href="#cb2-115"></a>                seed<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb2-116"><a href="#cb2-116"></a>            ).squeeze(<span class="dv">1</span>)</span>
<span id="cb2-117"><a href="#cb2-117"></a>            inducing_points <span class="op">=</span> torch.cat([inducing_points, train_x], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-118"><a href="#cb2-118"></a>            variational_distribution <span class="op">=</span> CholeskyVariationalDistribution(</span>
<span id="cb2-119"><a href="#cb2-119"></a>                inducing_points.size(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb2-120"><a href="#cb2-120"></a>            )</span>
<span id="cb2-121"><a href="#cb2-121"></a>            variational_strategy <span class="op">=</span> VariationalStrategy(</span>
<span id="cb2-122"><a href="#cb2-122"></a>                <span class="va">self</span>,</span>
<span id="cb2-123"><a href="#cb2-123"></a>                inducing_points,</span>
<span id="cb2-124"><a href="#cb2-124"></a>                variational_distribution,</span>
<span id="cb2-125"><a href="#cb2-125"></a>                learn_inducing_locations<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-126"><a href="#cb2-126"></a>            )</span>
<span id="cb2-127"><a href="#cb2-127"></a>        <span class="cf">else</span>:</span>
<span id="cb2-128"><a href="#cb2-128"></a>            inducing_points <span class="op">=</span> train_x</span>
<span id="cb2-129"><a href="#cb2-129"></a>            variational_distribution <span class="op">=</span> CholeskyVariationalDistribution(</span>
<span id="cb2-130"><a href="#cb2-130"></a>                inducing_points.size(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb2-131"><a href="#cb2-131"></a>            )</span>
<span id="cb2-132"><a href="#cb2-132"></a>            variational_strategy <span class="op">=</span> UnwhitenedVariationalStrategy(</span>
<span id="cb2-133"><a href="#cb2-133"></a>                <span class="va">self</span>,</span>
<span id="cb2-134"><a href="#cb2-134"></a>                inducing_points,</span>
<span id="cb2-135"><a href="#cb2-135"></a>                variational_distribution,</span>
<span id="cb2-136"><a href="#cb2-136"></a>                learn_inducing_locations<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-137"><a href="#cb2-137"></a>            )</span>
<span id="cb2-138"><a href="#cb2-138"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(variational_strategy)</span>
<span id="cb2-139"><a href="#cb2-139"></a>        <span class="va">self</span>.likelihood <span class="op">=</span> PreferentialSoftmaxLikelihood(</span>
<span id="cb2-140"><a href="#cb2-140"></a>            num_alternatives<span class="op">=</span><span class="va">self</span>.q)</span>
<span id="cb2-141"><a href="#cb2-141"></a>        <span class="va">self</span>.mean_module <span class="op">=</span> ConstantMean()</span>
<span id="cb2-142"><a href="#cb2-142"></a>        scales <span class="op">=</span> bounds[<span class="dv">1</span>, :] <span class="op">-</span> bounds[<span class="dv">0</span>, :]</span>
<span id="cb2-143"><a href="#cb2-143"></a></span>
<span id="cb2-144"><a href="#cb2-144"></a>        <span class="cf">if</span> covar_module <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-145"><a href="#cb2-145"></a>            <span class="va">self</span>.covar_module <span class="op">=</span> ScaleKernel(</span>
<span id="cb2-146"><a href="#cb2-146"></a>                RBFKernel(</span>
<span id="cb2-147"><a href="#cb2-147"></a>                    ard_num_dims<span class="op">=</span><span class="va">self</span>.input_dim,</span>
<span id="cb2-148"><a href="#cb2-148"></a>                    lengthscale_prior<span class="op">=</span>GammaPrior(<span class="fl">3.0</span>, <span class="fl">6.0</span> <span class="op">/</span> scales),</span>
<span id="cb2-149"><a href="#cb2-149"></a>                ),</span>
<span id="cb2-150"><a href="#cb2-150"></a>                outputscale_prior<span class="op">=</span>GammaPrior(<span class="fl">2.0</span>, <span class="fl">0.15</span>),</span>
<span id="cb2-151"><a href="#cb2-151"></a>            )</span>
<span id="cb2-152"><a href="#cb2-152"></a>        <span class="cf">else</span>:</span>
<span id="cb2-153"><a href="#cb2-153"></a>            <span class="va">self</span>.covar_module <span class="op">=</span> covar_module</span>
<span id="cb2-154"><a href="#cb2-154"></a>        <span class="va">self</span>._num_outputs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-155"><a href="#cb2-155"></a>        <span class="va">self</span>.train_inputs <span class="op">=</span> (train_x,)</span>
<span id="cb2-156"><a href="#cb2-156"></a>        <span class="va">self</span>.train_targets <span class="op">=</span> train_y</span>
<span id="cb2-157"><a href="#cb2-157"></a></span>
<span id="cb2-158"><a href="#cb2-158"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X: Tensor) <span class="op">-&gt;</span> MultivariateNormal:</span>
<span id="cb2-159"><a href="#cb2-159"></a>        mean_X <span class="op">=</span> <span class="va">self</span>.mean_module(X)</span>
<span id="cb2-160"><a href="#cb2-160"></a>        covar_X <span class="op">=</span> <span class="va">self</span>.covar_module(X)</span>
<span id="cb2-161"><a href="#cb2-161"></a>        <span class="cf">return</span> MultivariateNormal(mean_X, covar_X)</span>
<span id="cb2-162"><a href="#cb2-162"></a></span>
<span id="cb2-163"><a href="#cb2-163"></a>    <span class="at">@property</span></span>
<span id="cb2-164"><a href="#cb2-164"></a>    <span class="kw">def</span> num_outputs(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb2-165"><a href="#cb2-165"></a>        <span class="co">r"""The number of outputs of the model."""</span></span>
<span id="cb2-166"><a href="#cb2-166"></a>        <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-167"><a href="#cb2-167"></a></span>
<span id="cb2-168"><a href="#cb2-168"></a></span>
<span id="cb2-169"><a href="#cb2-169"></a><span class="co"># Objective function for pairwise comparisons</span></span>
<span id="cb2-170"><a href="#cb2-170"></a><span class="kw">def</span> f(x):</span>
<span id="cb2-171"><a href="#cb2-171"></a>    <span class="co">"""</span></span>
<span id="cb2-172"><a href="#cb2-172"></a><span class="co">    Computes the preference score for a given action.</span></span>
<span id="cb2-173"><a href="#cb2-173"></a></span>
<span id="cb2-174"><a href="#cb2-174"></a><span class="co">    Args:</span></span>
<span id="cb2-175"><a href="#cb2-175"></a><span class="co">        x (torch.Tensor): A feature vector of dimension `d`.</span></span>
<span id="cb2-176"><a href="#cb2-176"></a></span>
<span id="cb2-177"><a href="#cb2-177"></a><span class="co">    Returns:</span></span>
<span id="cb2-178"><a href="#cb2-178"></a><span class="co">        torch.Tensor: The computed preference score.</span></span>
<span id="cb2-179"><a href="#cb2-179"></a><span class="co">    """</span></span>
<span id="cb2-180"><a href="#cb2-180"></a>    <span class="cf">return</span> x <span class="op">@</span> theta_true</span>
<span id="cb2-181"><a href="#cb2-181"></a></span>
<span id="cb2-182"><a href="#cb2-182"></a><span class="co"># Simulate pairwise comparisons</span></span>
<span id="cb2-183"><a href="#cb2-183"></a></span>
<span id="cb2-184"><a href="#cb2-184"></a></span>
<span id="cb2-185"><a href="#cb2-185"></a><span class="kw">def</span> simulate_comparison(x1, x2):</span>
<span id="cb2-186"><a href="#cb2-186"></a>    <span class="co">"""</span></span>
<span id="cb2-187"><a href="#cb2-187"></a><span class="co">    Simulates a pairwise comparison between two actions based on their preference scores.</span></span>
<span id="cb2-188"><a href="#cb2-188"></a></span>
<span id="cb2-189"><a href="#cb2-189"></a><span class="co">    Args:</span></span>
<span id="cb2-190"><a href="#cb2-190"></a><span class="co">        x1 (torch.Tensor): Feature vector of the first action.</span></span>
<span id="cb2-191"><a href="#cb2-191"></a><span class="co">        x2 (torch.Tensor): Feature vector of the second action.</span></span>
<span id="cb2-192"><a href="#cb2-192"></a></span>
<span id="cb2-193"><a href="#cb2-193"></a><span class="co">    Returns:</span></span>
<span id="cb2-194"><a href="#cb2-194"></a><span class="co">        torch.Tensor: The feature vector of the preferred action.</span></span>
<span id="cb2-195"><a href="#cb2-195"></a><span class="co">    """</span></span>
<span id="cb2-196"><a href="#cb2-196"></a>    prob_x1 <span class="op">=</span> torch.sigmoid(f(x1) <span class="op">-</span> f(x2))</span>
<span id="cb2-197"><a href="#cb2-197"></a>    <span class="cf">return</span> x1 <span class="cf">if</span> torch.rand(<span class="dv">1</span>).item() <span class="op">&lt;</span> prob_x1 <span class="cf">else</span> x2</span>
<span id="cb2-198"><a href="#cb2-198"></a></span>
<span id="cb2-199"><a href="#cb2-199"></a><span class="co"># Function to fit a Variational GP model</span></span>
<span id="cb2-200"><a href="#cb2-200"></a></span>
<span id="cb2-201"><a href="#cb2-201"></a></span>
<span id="cb2-202"><a href="#cb2-202"></a><span class="kw">def</span> fit_variational_gp(train_X, train_Y):</span>
<span id="cb2-203"><a href="#cb2-203"></a>    <span class="co">"""</span></span>
<span id="cb2-204"><a href="#cb2-204"></a><span class="co">    Fits a Variational Gaussian Process (GP) model to the given training data.</span></span>
<span id="cb2-205"><a href="#cb2-205"></a></span>
<span id="cb2-206"><a href="#cb2-206"></a><span class="co">    Args:</span></span>
<span id="cb2-207"><a href="#cb2-207"></a><span class="co">        train_X (torch.Tensor): Training feature pairs of shape [n, 2, d].</span></span>
<span id="cb2-208"><a href="#cb2-208"></a><span class="co">        train_Y (torch.Tensor): Training preferences of shape [n, 1].</span></span>
<span id="cb2-209"><a href="#cb2-209"></a></span>
<span id="cb2-210"><a href="#cb2-210"></a><span class="co">    Returns:</span></span>
<span id="cb2-211"><a href="#cb2-211"></a><span class="co">        VariationalPreferentialGP: A fitted GP model.</span></span>
<span id="cb2-212"><a href="#cb2-212"></a><span class="co">    """</span></span>
<span id="cb2-213"><a href="#cb2-213"></a>    queries <span class="op">=</span> train_X.reshape(train_X.shape[<span class="dv">0</span>], <span class="dv">2</span>, d)</span>
<span id="cb2-214"><a href="#cb2-214"></a>    responses <span class="op">=</span> train_Y</span>
<span id="cb2-215"><a href="#cb2-215"></a>    <span class="cf">return</span> fit_model(queries, responses)</span>
<span id="cb2-216"><a href="#cb2-216"></a></span>
<span id="cb2-217"><a href="#cb2-217"></a></span>
<span id="cb2-218"><a href="#cb2-218"></a><span class="kw">def</span> fit_model(queries, responses):</span>
<span id="cb2-219"><a href="#cb2-219"></a>    <span class="co">"""</span></span>
<span id="cb2-220"><a href="#cb2-220"></a><span class="co">    Internal helper to train a VariationalPreferentialGP.</span></span>
<span id="cb2-221"><a href="#cb2-221"></a></span>
<span id="cb2-222"><a href="#cb2-222"></a><span class="co">    Args:</span></span>
<span id="cb2-223"><a href="#cb2-223"></a><span class="co">        queries (torch.Tensor): Training feature pairs.</span></span>
<span id="cb2-224"><a href="#cb2-224"></a><span class="co">        responses (torch.Tensor): Training responses (preferences).</span></span>
<span id="cb2-225"><a href="#cb2-225"></a></span>
<span id="cb2-226"><a href="#cb2-226"></a><span class="co">    Returns:</span></span>
<span id="cb2-227"><a href="#cb2-227"></a><span class="co">        VariationalPreferentialGP: Trained GP model.</span></span>
<span id="cb2-228"><a href="#cb2-228"></a><span class="co">    """</span></span>
<span id="cb2-229"><a href="#cb2-229"></a>    model <span class="op">=</span> VariationalPreferentialGP(queries, responses)</span>
<span id="cb2-230"><a href="#cb2-230"></a>    model.train()</span>
<span id="cb2-231"><a href="#cb2-231"></a>    model.likelihood.train()</span>
<span id="cb2-232"><a href="#cb2-232"></a>    mll <span class="op">=</span> VariationalELBO(</span>
<span id="cb2-233"><a href="#cb2-233"></a>        likelihood<span class="op">=</span>model.likelihood,</span>
<span id="cb2-234"><a href="#cb2-234"></a>        model<span class="op">=</span>model,</span>
<span id="cb2-235"><a href="#cb2-235"></a>        num_data<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> model.num_data,</span>
<span id="cb2-236"><a href="#cb2-236"></a>    )</span>
<span id="cb2-237"><a href="#cb2-237"></a>    fit_gpytorch_mll(mll)</span>
<span id="cb2-238"><a href="#cb2-238"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb2-239"><a href="#cb2-239"></a>    model.likelihood.<span class="bu">eval</span>()</span>
<span id="cb2-240"><a href="#cb2-240"></a>    <span class="cf">return</span> model</span>
<span id="cb2-241"><a href="#cb2-241"></a></span>
<span id="cb2-242"><a href="#cb2-242"></a><span class="co"># Acquisition function definition</span></span>
<span id="cb2-243"><a href="#cb2-243"></a></span>
<span id="cb2-244"><a href="#cb2-244"></a></span>
<span id="cb2-245"><a href="#cb2-245"></a><span class="kw">def</span> get_acquisition_functions(gp):</span>
<span id="cb2-246"><a href="#cb2-246"></a>    <span class="co">"""</span></span>
<span id="cb2-247"><a href="#cb2-247"></a><span class="co">    Returns acquisition functions (qLogEI and qEUBO) for a given GP model.</span></span>
<span id="cb2-248"><a href="#cb2-248"></a></span>
<span id="cb2-249"><a href="#cb2-249"></a><span class="co">    Args:</span></span>
<span id="cb2-250"><a href="#cb2-250"></a><span class="co">        gp (VariationalPreferentialGP): The fitted GP model.</span></span>
<span id="cb2-251"><a href="#cb2-251"></a></span>
<span id="cb2-252"><a href="#cb2-252"></a><span class="co">    Returns:</span></span>
<span id="cb2-253"><a href="#cb2-253"></a><span class="co">        tuple: qLogExpectedImprovement and qExpectedUtilityOfBestOption acquisition functions.</span></span>
<span id="cb2-254"><a href="#cb2-254"></a><span class="co">    """</span></span>
<span id="cb2-255"><a href="#cb2-255"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-256"><a href="#cb2-256"></a>        posterior <span class="op">=</span> gp.posterior(gp.train_inputs[<span class="dv">0</span>])</span>
<span id="cb2-257"><a href="#cb2-257"></a>        best_f <span class="op">=</span> posterior.mean.squeeze(<span class="op">-</span><span class="dv">1</span>).<span class="bu">max</span>()</span>
<span id="cb2-258"><a href="#cb2-258"></a></span>
<span id="cb2-259"><a href="#cb2-259"></a>    qLogEI <span class="op">=</span> qLogExpectedImprovement(model<span class="op">=</span>gp, best_f<span class="op">=</span>best_f)</span>
<span id="cb2-260"><a href="#cb2-260"></a>    qEUBO <span class="op">=</span> qExpectedUtilityOfBestOption(pref_model<span class="op">=</span>gp)</span>
<span id="cb2-261"><a href="#cb2-261"></a>    <span class="cf">return</span> qLogEI, qEUBO</span>
<span id="cb2-262"><a href="#cb2-262"></a></span>
<span id="cb2-263"><a href="#cb2-263"></a><span class="co"># Evaluate acquisition function on pairs</span></span>
<span id="cb2-264"><a href="#cb2-264"></a></span>
<span id="cb2-265"><a href="#cb2-265"></a></span>
<span id="cb2-266"><a href="#cb2-266"></a><span class="kw">def</span> evaluate_acquisition_on_pairs(acq_function, arms):</span>
<span id="cb2-267"><a href="#cb2-267"></a>    <span class="co">"""</span></span>
<span id="cb2-268"><a href="#cb2-268"></a><span class="co">    Computes acquisition values for all possible pairs of arms.</span></span>
<span id="cb2-269"><a href="#cb2-269"></a></span>
<span id="cb2-270"><a href="#cb2-270"></a><span class="co">    Args:</span></span>
<span id="cb2-271"><a href="#cb2-271"></a><span class="co">        acq_function: The acquisition function to evaluate.</span></span>
<span id="cb2-272"><a href="#cb2-272"></a><span class="co">        arms (torch.Tensor): All available arms (feature vectors).</span></span>
<span id="cb2-273"><a href="#cb2-273"></a></span>
<span id="cb2-274"><a href="#cb2-274"></a><span class="co">    Returns:</span></span>
<span id="cb2-275"><a href="#cb2-275"></a><span class="co">        tuple: A list of pairs and their corresponding acquisition values.</span></span>
<span id="cb2-276"><a href="#cb2-276"></a><span class="co">    """</span></span>
<span id="cb2-277"><a href="#cb2-277"></a>    pairs <span class="op">=</span> <span class="bu">list</span>(itertools.combinations(arms, <span class="dv">2</span>))</span>
<span id="cb2-278"><a href="#cb2-278"></a>    pair_values <span class="op">=</span> []</span>
<span id="cb2-279"><a href="#cb2-279"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-280"><a href="#cb2-280"></a>        <span class="cf">for</span> x1, x2 <span class="kw">in</span> pairs:</span>
<span id="cb2-281"><a href="#cb2-281"></a>            pair <span class="op">=</span> torch.stack([x1, x2]).unsqueeze(<span class="dv">0</span>)</span>
<span id="cb2-282"><a href="#cb2-282"></a>            pair_values.append(acq_function(pair))</span>
<span id="cb2-283"><a href="#cb2-283"></a>    <span class="cf">return</span> pairs, torch.tensor(pair_values)</span>
<span id="cb2-284"><a href="#cb2-284"></a></span>
<span id="cb2-285"><a href="#cb2-285"></a><span class="co"># Regret calculation</span></span>
<span id="cb2-286"><a href="#cb2-286"></a></span>
<span id="cb2-287"><a href="#cb2-287"></a></span>
<span id="cb2-288"><a href="#cb2-288"></a><span class="kw">def</span> calculate_regret_from_gp(gp, actions):</span>
<span id="cb2-289"><a href="#cb2-289"></a>    <span class="co">"""</span></span>
<span id="cb2-290"><a href="#cb2-290"></a><span class="co">    Computes the regret for the current GP model.</span></span>
<span id="cb2-291"><a href="#cb2-291"></a></span>
<span id="cb2-292"><a href="#cb2-292"></a><span class="co">    Args:</span></span>
<span id="cb2-293"><a href="#cb2-293"></a><span class="co">        gp (VariationalPreferentialGP): The fitted GP model.</span></span>
<span id="cb2-294"><a href="#cb2-294"></a><span class="co">        actions (torch.Tensor): Feature vectors of arms.</span></span>
<span id="cb2-295"><a href="#cb2-295"></a></span>
<span id="cb2-296"><a href="#cb2-296"></a><span class="co">    Returns:</span></span>
<span id="cb2-297"><a href="#cb2-297"></a><span class="co">        torch.Tensor: The calculated regret.</span></span>
<span id="cb2-298"><a href="#cb2-298"></a><span class="co">    """</span></span>
<span id="cb2-299"><a href="#cb2-299"></a>    <span class="co"># YOUR CODE HERE (~6 lines)</span></span>
<span id="cb2-300"><a href="#cb2-300"></a>    <span class="co"># Compare the ground truth optimal arm to the GP's believed best arm</span></span>
<span id="cb2-301"><a href="#cb2-301"></a>    <span class="co"># Hint: To find GP believed best arm in expectation, use gp.posterior which returns with a mean property.</span></span>
<span id="cb2-302"><a href="#cb2-302"></a>    <span class="cf">pass</span></span>
<span id="cb2-303"><a href="#cb2-303"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-304"><a href="#cb2-304"></a></span>
<span id="cb2-305"><a href="#cb2-305"></a></span>
<span id="cb2-306"><a href="#cb2-306"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb2-307"><a href="#cb2-307"></a>    <span class="co"># Set default tensor precision</span></span>
<span id="cb2-308"><a href="#cb2-308"></a>    torch.set_default_dtype(torch.double)</span>
<span id="cb2-309"><a href="#cb2-309"></a></span>
<span id="cb2-310"><a href="#cb2-310"></a>    <span class="co"># Problem settings</span></span>
<span id="cb2-311"><a href="#cb2-311"></a>    torch.manual_seed(<span class="dv">55</span>)</span>
<span id="cb2-312"><a href="#cb2-312"></a>    K <span class="op">=</span> <span class="dv">30</span>  <span class="co"># Number of arms (discrete choices)</span></span>
<span id="cb2-313"><a href="#cb2-313"></a>    d <span class="op">=</span> <span class="dv">2</span>   <span class="co"># Dimensionality of feature vectors</span></span>
<span id="cb2-314"><a href="#cb2-314"></a>    T <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Number of rounds (iterations)</span></span>
<span id="cb2-315"><a href="#cb2-315"></a>    bounds <span class="op">=</span> torch.tensor([[<span class="fl">0.0</span>] <span class="op">*</span> d, [<span class="fl">1.0</span>] <span class="op">*</span> d])  <span class="co"># Bounds for action space</span></span>
<span id="cb2-316"><a href="#cb2-316"></a></span>
<span id="cb2-317"><a href="#cb2-317"></a>    <span class="co"># Generate random actions (feature vectors)</span></span>
<span id="cb2-318"><a href="#cb2-318"></a>    actions <span class="op">=</span> torch.rand(K, d)</span>
<span id="cb2-319"><a href="#cb2-319"></a></span>
<span id="cb2-320"><a href="#cb2-320"></a>    <span class="co"># Ground-truth preference parameter (unknown to the model)</span></span>
<span id="cb2-321"><a href="#cb2-321"></a>    theta_true <span class="op">=</span> torch.ones(d)</span>
<span id="cb2-322"><a href="#cb2-322"></a></span>
<span id="cb2-323"><a href="#cb2-323"></a>    <span class="co"># Generate initial observations</span></span>
<span id="cb2-324"><a href="#cb2-324"></a>    n_initial <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-325"><a href="#cb2-325"></a>    indices <span class="op">=</span> torch.randint(<span class="dv">0</span>, K, (n_initial, <span class="dv">2</span>))</span>
<span id="cb2-326"><a href="#cb2-326"></a>    train_X_logei <span class="op">=</span> actions[indices]  <span class="co"># Shape: [n_initial, 2, d]</span></span>
<span id="cb2-327"><a href="#cb2-327"></a>    train_X_qeubo <span class="op">=</span> train_X_logei.clone()</span>
<span id="cb2-328"><a href="#cb2-328"></a>    train_X_random <span class="op">=</span> train_X_logei.clone()</span>
<span id="cb2-329"><a href="#cb2-329"></a>    train_Y_logei <span class="op">=</span> torch.tensor([[<span class="fl">0.0</span> <span class="cf">if</span> simulate_comparison(x1, x2).equal(x1) <span class="cf">else</span> <span class="fl">1.0</span>]</span>
<span id="cb2-330"><a href="#cb2-330"></a>                                  <span class="cf">for</span> x1, x2 <span class="kw">in</span> train_X_logei])</span>
<span id="cb2-331"><a href="#cb2-331"></a>    train_Y_qeubo <span class="op">=</span> train_Y_logei.clone()</span>
<span id="cb2-332"><a href="#cb2-332"></a>    train_Y_random <span class="op">=</span> train_Y_logei.clone()</span>
<span id="cb2-333"><a href="#cb2-333"></a></span>
<span id="cb2-334"><a href="#cb2-334"></a>    <span class="co"># Optimization loop</span></span>
<span id="cb2-335"><a href="#cb2-335"></a>    cumulative_regret_logei <span class="op">=</span> []</span>
<span id="cb2-336"><a href="#cb2-336"></a>    cumulative_regret_qeubo <span class="op">=</span> []</span>
<span id="cb2-337"><a href="#cb2-337"></a>    cumulative_regret_random <span class="op">=</span> []</span>
<span id="cb2-338"><a href="#cb2-338"></a></span>
<span id="cb2-339"><a href="#cb2-339"></a>    <span class="cf">for</span> t <span class="kw">in</span> tqdm(<span class="bu">range</span>(T)):</span>
<span id="cb2-340"><a href="#cb2-340"></a>        <span class="co"># Fit GP models</span></span>
<span id="cb2-341"><a href="#cb2-341"></a>        gp_logei <span class="op">=</span> fit_variational_gp(train_X_logei, train_Y_logei)</span>
<span id="cb2-342"><a href="#cb2-342"></a>        gp_qeubo <span class="op">=</span> fit_variational_gp(train_X_qeubo, train_Y_qeubo)</span>
<span id="cb2-343"><a href="#cb2-343"></a>        gp_random <span class="op">=</span> fit_variational_gp(train_X_random, train_Y_random)</span>
<span id="cb2-344"><a href="#cb2-344"></a></span>
<span id="cb2-345"><a href="#cb2-345"></a>        <span class="co"># Define acquisition functions</span></span>
<span id="cb2-346"><a href="#cb2-346"></a>        qLogEI, _ <span class="op">=</span> get_acquisition_functions(gp_logei)</span>
<span id="cb2-347"><a href="#cb2-347"></a>        _, qEUBO <span class="op">=</span> get_acquisition_functions(gp_qeubo)</span>
<span id="cb2-348"><a href="#cb2-348"></a></span>
<span id="cb2-349"><a href="#cb2-349"></a>        <span class="co"># Evaluate acquisition functions</span></span>
<span id="cb2-350"><a href="#cb2-350"></a>        pairs_logei, acq_values_logei <span class="op">=</span> evaluate_acquisition_on_pairs(</span>
<span id="cb2-351"><a href="#cb2-351"></a>            qLogEI, actions)</span>
<span id="cb2-352"><a href="#cb2-352"></a>        pairs_qeubo, acq_values_qeubo <span class="op">=</span> evaluate_acquisition_on_pairs(</span>
<span id="cb2-353"><a href="#cb2-353"></a>            qEUBO, actions)</span>
<span id="cb2-354"><a href="#cb2-354"></a></span>
<span id="cb2-355"><a href="#cb2-355"></a>        <span class="co"># Select pairs based on acquisition values</span></span>
<span id="cb2-356"><a href="#cb2-356"></a>        best_pair_idx_logei <span class="op">=</span> torch.argmax(acq_values_logei)</span>
<span id="cb2-357"><a href="#cb2-357"></a>        best_pair_idx_qeubo <span class="op">=</span> torch.argmax(acq_values_qeubo)</span>
<span id="cb2-358"><a href="#cb2-358"></a>        x1_logei, x2_logei <span class="op">=</span> pairs_logei[best_pair_idx_logei]</span>
<span id="cb2-359"><a href="#cb2-359"></a>        x1_qeubo, x2_qeubo <span class="op">=</span> pairs_qeubo[best_pair_idx_qeubo]</span>
<span id="cb2-360"><a href="#cb2-360"></a></span>
<span id="cb2-361"><a href="#cb2-361"></a>        <span class="co"># Random pair selection</span></span>
<span id="cb2-362"><a href="#cb2-362"></a>        random_indices <span class="op">=</span> torch.randint(<span class="dv">0</span>, K, (<span class="dv">2</span>,))</span>
<span id="cb2-363"><a href="#cb2-363"></a>        x1_random <span class="op">=</span> actions[random_indices[<span class="dv">0</span>]]</span>
<span id="cb2-364"><a href="#cb2-364"></a>        x2_random <span class="op">=</span> actions[random_indices[<span class="dv">1</span>]]</span>
<span id="cb2-365"><a href="#cb2-365"></a></span>
<span id="cb2-366"><a href="#cb2-366"></a>        <span class="co"># Simulate comparisons</span></span>
<span id="cb2-367"><a href="#cb2-367"></a>        selected_logei <span class="op">=</span> simulate_comparison(x1_logei, x2_logei)</span>
<span id="cb2-368"><a href="#cb2-368"></a>        selected_qeubo <span class="op">=</span> simulate_comparison(x1_qeubo, x2_qeubo)</span>
<span id="cb2-369"><a href="#cb2-369"></a>        selected_random <span class="op">=</span> simulate_comparison(x1_random, x2_random)</span>
<span id="cb2-370"><a href="#cb2-370"></a></span>
<span id="cb2-371"><a href="#cb2-371"></a>        <span class="co"># Update training data</span></span>
<span id="cb2-372"><a href="#cb2-372"></a>        train_X_logei <span class="op">=</span> torch.cat(</span>
<span id="cb2-373"><a href="#cb2-373"></a>            [train_X_logei, torch.stack([x1_logei, x2_logei]).unsqueeze(<span class="dv">0</span>)])</span>
<span id="cb2-374"><a href="#cb2-374"></a>        train_Y_logei <span class="op">=</span> torch.cat([train_Y_logei, torch.tensor(</span>
<span id="cb2-375"><a href="#cb2-375"></a>            [[<span class="fl">0.0</span> <span class="cf">if</span> selected_logei.equal(x1_logei) <span class="cf">else</span> <span class="fl">1.0</span>]])])</span>
<span id="cb2-376"><a href="#cb2-376"></a>        train_X_qeubo <span class="op">=</span> torch.cat(</span>
<span id="cb2-377"><a href="#cb2-377"></a>            [train_X_qeubo, torch.stack([x1_qeubo, x2_qeubo]).unsqueeze(<span class="dv">0</span>)])</span>
<span id="cb2-378"><a href="#cb2-378"></a>        train_Y_qeubo <span class="op">=</span> torch.cat([train_Y_qeubo, torch.tensor(</span>
<span id="cb2-379"><a href="#cb2-379"></a>            [[<span class="fl">0.0</span> <span class="cf">if</span> selected_qeubo.equal(x1_qeubo) <span class="cf">else</span> <span class="fl">1.0</span>]])])</span>
<span id="cb2-380"><a href="#cb2-380"></a>        train_X_random <span class="op">=</span> torch.cat(</span>
<span id="cb2-381"><a href="#cb2-381"></a>            [train_X_random, torch.stack([x1_random, x2_random]).unsqueeze(<span class="dv">0</span>)])</span>
<span id="cb2-382"><a href="#cb2-382"></a>        train_Y_random <span class="op">=</span> torch.cat([train_Y_random, torch.tensor(</span>
<span id="cb2-383"><a href="#cb2-383"></a>            [[<span class="fl">0.0</span> <span class="cf">if</span> selected_random.equal(x1_random) <span class="cf">else</span> <span class="fl">1.0</span>]])])</span>
<span id="cb2-384"><a href="#cb2-384"></a></span>
<span id="cb2-385"><a href="#cb2-385"></a>        <span class="co"># Calculate regrets</span></span>
<span id="cb2-386"><a href="#cb2-386"></a>        regret_logei <span class="op">=</span> calculate_regret_from_gp(gp_logei, actions)</span>
<span id="cb2-387"><a href="#cb2-387"></a>        regret_qeubo <span class="op">=</span> calculate_regret_from_gp(gp_qeubo, actions)</span>
<span id="cb2-388"><a href="#cb2-388"></a>        regret_random <span class="op">=</span> calculate_regret_from_gp(gp_random, actions)</span>
<span id="cb2-389"><a href="#cb2-389"></a></span>
<span id="cb2-390"><a href="#cb2-390"></a>        <span class="bu">print</span>(<span class="ss">f'Regret LogEI: </span><span class="sc">{</span>regret_logei<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-391"><a href="#cb2-391"></a>        <span class="bu">print</span>(<span class="ss">f'Regret qEUBO: </span><span class="sc">{</span>regret_qeubo<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-392"><a href="#cb2-392"></a>        <span class="bu">print</span>(<span class="ss">f'Regret Random: </span><span class="sc">{</span>regret_random<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-393"><a href="#cb2-393"></a></span>
<span id="cb2-394"><a href="#cb2-394"></a>        cumulative_regret_logei.append(regret_logei)</span>
<span id="cb2-395"><a href="#cb2-395"></a>        cumulative_regret_qeubo.append(regret_qeubo)</span>
<span id="cb2-396"><a href="#cb2-396"></a>        cumulative_regret_random.append(regret_random)</span>
<span id="cb2-397"><a href="#cb2-397"></a></span>
<span id="cb2-398"><a href="#cb2-398"></a>    <span class="co"># Plot cumulative regret</span></span>
<span id="cb2-399"><a href="#cb2-399"></a>    plt.plot(torch.cumsum(torch.tensor(</span>
<span id="cb2-400"><a href="#cb2-400"></a>        cumulative_regret_logei), dim<span class="op">=</span><span class="dv">0</span>), label<span class="op">=</span><span class="st">'qLogEI'</span>)</span>
<span id="cb2-401"><a href="#cb2-401"></a>    plt.plot(torch.cumsum(torch.tensor(</span>
<span id="cb2-402"><a href="#cb2-402"></a>        cumulative_regret_qeubo), dim<span class="op">=</span><span class="dv">0</span>), label<span class="op">=</span><span class="st">'qEUBO'</span>)</span>
<span id="cb2-403"><a href="#cb2-403"></a>    plt.plot(torch.cumsum(torch.tensor(</span>
<span id="cb2-404"><a href="#cb2-404"></a>        cumulative_regret_random), dim<span class="op">=</span><span class="dv">0</span>), label<span class="op">=</span><span class="st">'Random'</span>)</span>
<span id="cb2-405"><a href="#cb2-405"></a>    plt.xlabel(<span class="st">'Round'</span>)</span>
<span id="cb2-406"><a href="#cb2-406"></a>    plt.ylabel(<span class="st">'Cumulative Regret'</span>)</span>
<span id="cb2-407"><a href="#cb2-407"></a>    plt.legend()</span>
<span id="cb2-408"><a href="#cb2-408"></a>    plt.title(<span class="st">'Comparison of qLogEI, qEUBO, and Random Sampling'</span>)</span>
<span id="cb2-409"><a href="#cb2-409"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-3-multi-objective-thompson-sampling-in-linear-contextual-bandits-30-points">Question 3: Multi-Objective Thompson Sampling in Linear Contextual Bandits (30 points)</h3>
<p>Thompson Sampling (TS) is commonly used for reward maximization in multi-armed bandit problems, optimizing for the expected reward across actions. However, in many real-world scenarios, other objectives, such as the interpretability or reusability of learned parameters, are equally valuable. This is particularly relevant when modeling unknown reward functions with parameters that might offer insights or inform future experiments. A purely reward-focused Thompson Sampling approach may result in increased false positive rates due to aggressive exploitation, whereas a pure exploration approach—such as those used in active learning—might better suit the goal of parameter learning.</p>
<p>Assume a multi-objective setting where the goal is to not only maximize the cumulative reward but also to accurately learn the parameters of the reward function itself in a linear contextual bandit setting. Let each arm be represented by a feature vector <span class="math inline">\(x \in \mathbb{R}^d\)</span>, with rewards generated by an unknown linear model <span class="math inline">\(r = \theta^\top x + \epsilon\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>. Given these considerations, answer the following:</p>
<ol type="a">
<li><p><strong>Theoretical Analysis of Multi-Objective Thompson Sampling (8 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 3 points).</strong> Define a cumulative regret objective that balances maximizing the expected reward and minimizing the parameter estimation error <span class="math inline">\(\|\theta - \hat{\theta}\|_2\)</span>. Explain how this multi-objective regret differs from the single-objective regret typically used in linear bandits.</p></li>
<li><p><strong>(Written, 3 points).</strong> Derive the expected regret bounds for Thompson Sampling in the single-objective case and describe the additional challenges posed when extending these bounds to the multi-objective case.</p></li>
<li><p><strong>(Written, 2 points).</strong> Suppose you were to use a pure exploration approach for parameter estimation. Provide an upper bound for the parameter error <span class="math inline">\(\|\theta - \hat{\theta}\|_2\)</span> over <span class="math inline">\(T\)</span> rounds.</p></li>
</ol></li>
<li><p><strong>Acquisition Strategies for Multi-Objective Optimization (8 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 3 points).</strong> Explain how to adapt the Upper Confidence Bound (UCB) acquisition function to balance exploration and exploitation for parameter learning alongside reward maximization. Discuss the effect of tuning parameters on exploration.</p></li>
<li><p><strong>(Written + Coding, 3 points).</strong> Implement a Thompson Sampling acquisition strategy that alternates between reward maximization and parameter-focused exploration using a multi-objective UCB. Implement the <code>select_arm</code> function of <code>multi_obj_thompson/bandit.py</code>.</p></li>
<li><p><strong>(Written, 2 points).</strong> Describe the impact of this alternating acquisition strategy on false positive rates and regret in comparison to standard Thompson Sampling.</p></li>
</ol></li>
<li><p><strong>Posterior Distribution Analysis (8 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 2 points).</strong> Given a prior distribution for <span class="math inline">\(\theta\)</span> and observed rewards, derive the posterior distribution of <span class="math inline">\(\theta\)</span> at each time step in the context of multi-objective Thompson Sampling. Explain any assumptions needed for computational tractability.</p></li>
<li><p><strong>(Coding, 4 points).</strong> Implement a Bayesian update for the posterior of <span class="math inline">\(\theta\)</span> following each observation. Do this in <code>update</code>.</p></li>
<li><p><strong>(Written, 2 points).</strong> Explain how this posterior update accommodates both exploration for parameter estimation and exploitation for reward maximization.</p></li>
</ol></li>
<li><p><strong>Empirical Evaluation (6 points)</strong></p>
<ol type="i">
<li><p><strong>(Coding, 3 points).</strong> Design and conduct an experiment comparing standard Thompson Sampling, pure exploration, and your multi-objective TS algorithm. Run this experiment on a synthetic dataset with <span class="math inline">\(d = 5\)</span> features and <span class="math inline">\(K = 10\)</span> arms by executing <code>run.py</code>.</p></li>
<li><p><strong>(Written, 3 points).</strong> Report and interpret the results by comparing the cumulative reward and parameter estimation error across methods. Provide insights on the trade-offs observed and any patterns in the rate of regret reduction.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="4e67360c" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="kw">class</span> MultiObjectiveThompsonSamplingBandit:</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="co">"""</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">    A class that implements a multi-objective Thompson sampling bandit.</span></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">    Attributes:</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">    - d (int): Dimension of the feature vector x.</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">    - lambda_prior (float): Regularization parameter for the prior covariance matrix.</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">    - sigma_noise (float): Standard deviation of the noise in rewards.</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    - mu (np.array): Prior mean of theta (initialized as a zero vector).</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    - Sigma (np.array): Prior covariance of theta (initialized as a scaled identity matrix).</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">    """</span></span>
<span id="cb3-15"><a href="#cb3-15"></a></span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d, lambda_prior<span class="op">=</span><span class="fl">1.0</span>, sigma_noise<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb3-17"><a href="#cb3-17"></a>        <span class="co">"""</span></span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="co">        Initializes the bandit with a prior on theta and noise variance.</span></span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="co">        Parameters:</span></span>
<span id="cb3-21"><a href="#cb3-21"></a><span class="co">        - d (int): Dimension of the feature vector x.</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="co">        - lambda_prior (float): Regularization parameter for the prior covariance matrix.</span></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">        - sigma_noise (float): Standard deviation of the noise in rewards.</span></span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="co">        """</span></span>
<span id="cb3-25"><a href="#cb3-25"></a>        <span class="va">self</span>.d <span class="op">=</span> d</span>
<span id="cb3-26"><a href="#cb3-26"></a>        <span class="va">self</span>.lambda_prior <span class="op">=</span> lambda_prior</span>
<span id="cb3-27"><a href="#cb3-27"></a>        <span class="va">self</span>.sigma_noise <span class="op">=</span> sigma_noise</span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a>        <span class="co"># Initialize prior mean and covariance matrix</span></span>
<span id="cb3-30"><a href="#cb3-30"></a>        <span class="va">self</span>.mu <span class="op">=</span> np.zeros(d)  <span class="co"># Prior mean of theta</span></span>
<span id="cb3-31"><a href="#cb3-31"></a>        <span class="va">self</span>.Sigma <span class="op">=</span> lambda_prior <span class="op">*</span> np.eye(d)  <span class="co"># Prior covariance of theta</span></span>
<span id="cb3-32"><a href="#cb3-32"></a></span>
<span id="cb3-33"><a href="#cb3-33"></a>    <span class="kw">def</span> select_arm(<span class="va">self</span>, arms, mode):</span>
<span id="cb3-34"><a href="#cb3-34"></a>        <span class="co">"""</span></span>
<span id="cb3-35"><a href="#cb3-35"></a><span class="co">        Selects an arm (action) based on the specified mode.</span></span>
<span id="cb3-36"><a href="#cb3-36"></a></span>
<span id="cb3-37"><a href="#cb3-37"></a><span class="co">        Parameters:</span></span>
<span id="cb3-38"><a href="#cb3-38"></a><span class="co">        - arms (np.array): A 2D NumPy array of shape (K, d) representing the feature vectors of K arms.</span></span>
<span id="cb3-39"><a href="#cb3-39"></a><span class="co">        - mode (str): Selection mode, either 'exploit' (reward maximization) or 'explore' (focus on reducing uncertainty in theta).</span></span>
<span id="cb3-40"><a href="#cb3-40"></a></span>
<span id="cb3-41"><a href="#cb3-41"></a><span class="co">        Returns:</span></span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="co">        - selected_arm (np.array): The feature vector of the selected arm.</span></span>
<span id="cb3-43"><a href="#cb3-43"></a><span class="co">        - arm_index (int): The index of the selected arm.</span></span>
<span id="cb3-44"><a href="#cb3-44"></a><span class="co">        """</span></span>
<span id="cb3-45"><a href="#cb3-45"></a>        <span class="co"># Sample a belief for theta from the current posterior</span></span>
<span id="cb3-46"><a href="#cb3-46"></a>        theta_sample <span class="op">=</span> np.random.multivariate_normal(<span class="va">self</span>.mu, <span class="va">self</span>.Sigma)</span>
<span id="cb3-47"><a href="#cb3-47"></a></span>
<span id="cb3-48"><a href="#cb3-48"></a>        <span class="co"># Generate reward noise for the arms</span></span>
<span id="cb3-49"><a href="#cb3-49"></a>        reward_noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="va">self</span>.sigma_noise, size<span class="op">=</span><span class="bu">len</span>(arms))</span>
<span id="cb3-50"><a href="#cb3-50"></a></span>
<span id="cb3-51"><a href="#cb3-51"></a>        <span class="cf">if</span> mode <span class="op">==</span> <span class="st">'exploit'</span>:</span>
<span id="cb3-52"><a href="#cb3-52"></a>            <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb3-53"><a href="#cb3-53"></a>                <span class="co"># 1. Compute expected rewards with noise</span></span>
<span id="cb3-54"><a href="#cb3-54"></a>                <span class="co"># 2. Select the arm with the highest expected reward</span></span>
<span id="cb3-55"><a href="#cb3-55"></a>                <span class="cf">pass</span> </span>
<span id="cb3-56"><a href="#cb3-56"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-57"><a href="#cb3-57"></a>        <span class="cf">elif</span> mode <span class="op">==</span> <span class="st">'explore'</span>:</span>
<span id="cb3-58"><a href="#cb3-58"></a>            <span class="co"># Compute posterior covariance norms to evaluate exploration potential for each arm</span></span>
<span id="cb3-59"><a href="#cb3-59"></a>            posterior_cov_norms <span class="op">=</span> []</span>
<span id="cb3-60"><a href="#cb3-60"></a>            <span class="cf">for</span> x <span class="kw">in</span> arms:</span>
<span id="cb3-61"><a href="#cb3-61"></a>                x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Reshape to column vector</span></span>
<span id="cb3-62"><a href="#cb3-62"></a></span>
<span id="cb3-63"><a href="#cb3-63"></a>                <span class="co"># Find posterior covariance hypothetically and get its norm</span></span>
<span id="cb3-64"><a href="#cb3-64"></a>                <span class="co"># YOUR CODE HERE (~4 lines)</span></span>
<span id="cb3-65"><a href="#cb3-65"></a>                <span class="cf">pass</span></span>
<span id="cb3-66"><a href="#cb3-66"></a>                <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-67"><a href="#cb3-67"></a></span>
<span id="cb3-68"><a href="#cb3-68"></a>                posterior_cov_norms.append(norm)</span>
<span id="cb3-69"><a href="#cb3-69"></a></span>
<span id="cb3-70"><a href="#cb3-70"></a>            <span class="co"># Select the arm that minimizes the posterior covariance norm</span></span>
<span id="cb3-71"><a href="#cb3-71"></a>            arm_index <span class="op">=</span> np.argmin(posterior_cov_norms)</span>
<span id="cb3-72"><a href="#cb3-72"></a></span>
<span id="cb3-73"><a href="#cb3-73"></a>        <span class="cf">else</span>:</span>
<span id="cb3-74"><a href="#cb3-74"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Mode must be either 'exploit' or 'explore'."</span>)</span>
<span id="cb3-75"><a href="#cb3-75"></a></span>
<span id="cb3-76"><a href="#cb3-76"></a>        <span class="cf">return</span> arms[arm_index], arm_index, posterior_cov_norms <span class="cf">if</span> mode <span class="op">==</span> <span class="st">'explore'</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-77"><a href="#cb3-77"></a></span>
<span id="cb3-78"><a href="#cb3-78"></a>    <span class="kw">def</span> update(<span class="va">self</span>, x_t, r_t):</span>
<span id="cb3-79"><a href="#cb3-79"></a>        <span class="co">"""</span></span>
<span id="cb3-80"><a href="#cb3-80"></a><span class="co">        Updates the posterior distribution of theta given a new observation.</span></span>
<span id="cb3-81"><a href="#cb3-81"></a></span>
<span id="cb3-82"><a href="#cb3-82"></a><span class="co">        Parameters:</span></span>
<span id="cb3-83"><a href="#cb3-83"></a><span class="co">        - x_t (np.array): Feature vector of the selected arm at time t.</span></span>
<span id="cb3-84"><a href="#cb3-84"></a><span class="co">        - r_t (float): Observed reward at time t.</span></span>
<span id="cb3-85"><a href="#cb3-85"></a><span class="co">        """</span></span>
<span id="cb3-86"><a href="#cb3-86"></a>        x_t <span class="op">=</span> x_t.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Reshape to column vector</span></span>
<span id="cb3-87"><a href="#cb3-87"></a></span>
<span id="cb3-88"><a href="#cb3-88"></a>        <span class="co"># YOUR CODE HERE (~4 lines)</span></span>
<span id="cb3-89"><a href="#cb3-89"></a>        <span class="co"># Obtain mu_new and Sigma_new of theta posterior. This requires doing some math!</span></span>
<span id="cb3-90"><a href="#cb3-90"></a>        <span class="cf">pass</span></span>
<span id="cb3-91"><a href="#cb3-91"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-92"><a href="#cb3-92"></a></span>
<span id="cb3-93"><a href="#cb3-93"></a>        <span class="co"># Update internal state</span></span>
<span id="cb3-94"><a href="#cb3-94"></a>        <span class="va">self</span>.mu <span class="op">=</span> mu_new.flatten()</span>
<span id="cb3-95"><a href="#cb3-95"></a>        <span class="va">self</span>.Sigma <span class="op">=</span> Sigma_new</span>
<span id="cb3-96"><a href="#cb3-96"></a></span>
<span id="cb3-97"><a href="#cb3-97"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb3-98"><a href="#cb3-98"></a>    <span class="co"># Number of features (dimension) and arms</span></span>
<span id="cb3-99"><a href="#cb3-99"></a>    d <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Feature dimension</span></span>
<span id="cb3-100"><a href="#cb3-100"></a>    K <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of arms</span></span>
<span id="cb3-101"><a href="#cb3-101"></a></span>
<span id="cb3-102"><a href="#cb3-102"></a>    <span class="co"># Generate random arms (feature vectors)</span></span>
<span id="cb3-103"><a href="#cb3-103"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb3-104"><a href="#cb3-104"></a>    arms <span class="op">=</span> np.random.randn(K, d)</span>
<span id="cb3-105"><a href="#cb3-105"></a></span>
<span id="cb3-106"><a href="#cb3-106"></a>    <span class="co"># True theta (unknown to the bandit)</span></span>
<span id="cb3-107"><a href="#cb3-107"></a>    theta_true <span class="op">=</span> np.random.randn(d)</span>
<span id="cb3-108"><a href="#cb3-108"></a></span>
<span id="cb3-109"><a href="#cb3-109"></a>    <span class="co"># Initialize the bandit</span></span>
<span id="cb3-110"><a href="#cb3-110"></a>    bandit <span class="op">=</span> MultiObjectiveThompsonSamplingBandit(d)</span>
<span id="cb3-111"><a href="#cb3-111"></a></span>
<span id="cb3-112"><a href="#cb3-112"></a>    <span class="co"># Number of rounds</span></span>
<span id="cb3-113"><a href="#cb3-113"></a>    T <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-114"><a href="#cb3-114"></a></span>
<span id="cb3-115"><a href="#cb3-115"></a>    <span class="co"># Lists to store results</span></span>
<span id="cb3-116"><a href="#cb3-116"></a>    regrets <span class="op">=</span> []  <span class="co"># Store the regret at each round</span></span>
<span id="cb3-117"><a href="#cb3-117"></a>    theta_errors <span class="op">=</span> []  <span class="co"># Store the error between estimated and true theta</span></span>
<span id="cb3-118"><a href="#cb3-118"></a></span>
<span id="cb3-119"><a href="#cb3-119"></a>    <span class="co"># Simulation loop</span></span>
<span id="cb3-120"><a href="#cb3-120"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb3-121"><a href="#cb3-121"></a>        <span class="co"># Alternate between 'exploit' and 'explore' modes</span></span>
<span id="cb3-122"><a href="#cb3-122"></a>        mode <span class="op">=</span> <span class="st">'exploit'</span> <span class="cf">if</span> t <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'explore'</span></span>
<span id="cb3-123"><a href="#cb3-123"></a></span>
<span id="cb3-124"><a href="#cb3-124"></a>        <span class="co"># Select an arm based on the current mode</span></span>
<span id="cb3-125"><a href="#cb3-125"></a>        x_t, arm_index, _ <span class="op">=</span> bandit.select_arm(arms, mode<span class="op">=</span>mode)</span>
<span id="cb3-126"><a href="#cb3-126"></a></span>
<span id="cb3-127"><a href="#cb3-127"></a>        <span class="co"># Observe the reward with noise</span></span>
<span id="cb3-128"><a href="#cb3-128"></a>        r_t <span class="op">=</span> theta_true <span class="op">@</span> x_t <span class="op">+</span> np.random.normal(<span class="dv">0</span>, bandit.sigma_noise)</span>
<span id="cb3-129"><a href="#cb3-129"></a></span>
<span id="cb3-130"><a href="#cb3-130"></a>        <span class="co"># Update the bandit with the new observation</span></span>
<span id="cb3-131"><a href="#cb3-131"></a>        bandit.update(x_t, r_t)</span>
<span id="cb3-132"><a href="#cb3-132"></a></span>
<span id="cb3-133"><a href="#cb3-133"></a>        <span class="co"># Compute regret (difference between optimal reward and received reward)</span></span>
<span id="cb3-134"><a href="#cb3-134"></a>        optimal_reward <span class="op">=</span> np.<span class="bu">max</span>(arms <span class="op">@</span> theta_true)  <span class="co"># Best possible reward</span></span>
<span id="cb3-135"><a href="#cb3-135"></a>        regret <span class="op">=</span> optimal_reward <span class="op">-</span> (theta_true <span class="op">@</span> x_t)  <span class="co"># Regret for this round</span></span>
<span id="cb3-136"><a href="#cb3-136"></a>        regrets.append(regret)</span>
<span id="cb3-137"><a href="#cb3-137"></a></span>
<span id="cb3-138"><a href="#cb3-138"></a>        <span class="co"># Compute parameter estimation error (distance between true and estimated theta)</span></span>
<span id="cb3-139"><a href="#cb3-139"></a>        theta_error <span class="op">=</span> np.linalg.norm(theta_true <span class="op">-</span> bandit.mu)</span>
<span id="cb3-140"><a href="#cb3-140"></a>        theta_errors.append(theta_error)</span>
<span id="cb3-141"><a href="#cb3-141"></a></span>
<span id="cb3-142"><a href="#cb3-142"></a>    <span class="co"># Final estimates after all rounds</span></span>
<span id="cb3-143"><a href="#cb3-143"></a>    mu_estimate, Sigma_estimate <span class="op">=</span> bandit.mu, bandit.Sigma</span>
<span id="cb3-144"><a href="#cb3-144"></a></span>
<span id="cb3-145"><a href="#cb3-145"></a>    <span class="co"># Print results</span></span>
<span id="cb3-146"><a href="#cb3-146"></a>    <span class="bu">print</span>(<span class="st">"Estimated theta:"</span>, mu_estimate)</span>
<span id="cb3-147"><a href="#cb3-147"></a>    <span class="bu">print</span>(<span class="st">"True theta:"</span>, theta_true)</span>
<span id="cb3-148"><a href="#cb3-148"></a>    <span class="bu">print</span>(<span class="st">"Cumulative regret:"</span>, np.<span class="bu">sum</span>(regrets))</span>
<span id="cb3-149"><a href="#cb3-149"></a>    <span class="bu">print</span>(<span class="st">"Final covariance norm:"</span>, np.linalg.norm(Sigma_estimate))</span>
<span id="cb3-150"><a href="#cb3-150"></a></span>
<span id="cb3-151"><a href="#cb3-151"></a>    <span class="co"># Visualization of results</span></span>
<span id="cb3-152"><a href="#cb3-152"></a></span>
<span id="cb3-153"><a href="#cb3-153"></a>    <span class="co"># Plot cumulative regret over time</span></span>
<span id="cb3-154"><a href="#cb3-154"></a>    plt.figure()</span>
<span id="cb3-155"><a href="#cb3-155"></a>    plt.plot(np.cumsum(regrets))</span>
<span id="cb3-156"><a href="#cb3-156"></a>    plt.title(<span class="st">'Cumulative Regret over Time'</span>)</span>
<span id="cb3-157"><a href="#cb3-157"></a>    plt.xlabel(<span class="st">'Rounds'</span>)</span>
<span id="cb3-158"><a href="#cb3-158"></a>    plt.ylabel(<span class="st">'Cumulative Regret'</span>)</span>
<span id="cb3-159"><a href="#cb3-159"></a>    plt.show()</span>
<span id="cb3-160"><a href="#cb3-160"></a></span>
<span id="cb3-161"><a href="#cb3-161"></a>    <span class="co"># Plot estimation error over time</span></span>
<span id="cb3-162"><a href="#cb3-162"></a>    plt.figure()</span>
<span id="cb3-163"><a href="#cb3-163"></a>    plt.plot(theta_errors)</span>
<span id="cb3-164"><a href="#cb3-164"></a>    plt.title(<span class="st">'Theta Estimation Error over Time'</span>)</span>
<span id="cb3-165"><a href="#cb3-165"></a>    plt.xlabel(<span class="st">'Rounds'</span>)</span>
<span id="cb3-166"><a href="#cb3-166"></a>    plt.ylabel(<span class="st">'Estimation Error (L2 Norm)'</span>)</span>
<span id="cb3-167"><a href="#cb3-167"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-4-mechanism-design-in-preference-learning-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-4-mechanism-design-in-preference-learning-30-points">Question 4: Mechanism Design in Preference Learning (30 points)</h3>
<p>In mechanism design, a central challenge is optimizing resource allocation while accounting for user preferences, which may be private and complex. This problem can be addressed using learning techniques to infer user preferences, thereby enabling the designer to make informed pricing and allocation decisions. Consider a scenario where a designer allocates a divisible resource <span class="math inline">\(B\)</span> among <span class="math inline">\(N\)</span> players, each with a private, continuous, concave utility function <span class="math inline">\(U_i(x_i)\)</span> over their allocated share <span class="math inline">\(x_i\)</span>, where <span class="math inline">\(x = [x_1, x_2, \dots, x_N]\)</span> denotes the allocation vector. The designer aims to maximize social welfare while ensuring full resource utilization.</p>
<ol type="a">
<li><p><strong>Modeling User Preferences (7 points)</strong>:</p>
<ol type="i">
<li><p><strong>(Written, 1 point)</strong> Provide a realistic scenario in which we estimate a utility function through eliciting preferences in the context of the mechanism.</p></li>
<li><p><strong>(Written, 3 point)</strong> Explain how elliptical slice sampling can be used with a GP in order to estimate a utility function through preferences.</p></li>
<li><p><strong>(Written, 3 point)</strong> How can the elliptical slice posterior samples be used to obtain the mean of the posterior predictive for test points? (Hint: Read page <span class="math inline">\(44\)</span> of <a href="https://gaussianprocess.org/gpml/chapters/RW.pdf" class="uri">https://gaussianprocess.org/gpml/chapters/RW.pdf</a>.)</p></li>
</ol></li>
<li><p><strong>Optimization with Learned Preferences (10 points)</strong>:</p>
<ol type="i">
<li><p><strong>(Written, 3 point)</strong> Formulate the designer’s optimization problem, maximizing social welfare <span class="math inline">\(\sum_{i=1}^N U_i(x_i)\)</span> subject to the constraint <span class="math inline">\(\sum_{i=1}^N x_i \leq B\)</span>.</p></li>
<li><p><strong>(Written, 4 point)</strong> Using the Lagrange multiplier method, derive the conditions that must be met for optimal allocation and pricing.</p></li>
<li><p><strong>(Written, 3 point)</strong> As an alternative approach to Lagrange multipliers, explain how projected gradient descent (PGD) can be used to solve the designer’s optimization problem.</p></li>
</ol></li>
<li><p><strong>Benchmarking Learning and Allocation Efficiency (13 points)</strong>:</p>
<ol type="i">
<li><p><strong>(Coding, 3 point)</strong> Implement <code>preference_loglik</code> in the file <code>gp_mechanism/preference_gp.py</code>.</p></li>
<li><p><strong>(Coding, 3 point)</strong> Implement <code>predictive_function</code>.</p></li>
<li><p><strong>(Coding, 3 point)</strong> Implement <code>optimize_allocations</code> inside <code>gp_mechanism/run.py</code>.</p></li>
<li><p><strong>(Written, 4 point)</strong> Compare GP-approximated utility allocations through PGD, exact utility allocations through PGD, and the optimal Lagrange-based allocation done by hand with each other for your choice of utility functions <span class="math inline">\(U_i\)</span>. Make sure your utilities are continuous and concave.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="d45986e1" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">import</span> torch  <span class="co"># Import PyTorch</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="kw">class</span> EllipticalSliceSampler:</span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb4-11"><a href="#cb4-11"></a>                 prior_cov: np.ndarray,</span>
<span id="cb4-12"><a href="#cb4-12"></a>                 loglik: Callable):</span>
<span id="cb4-13"><a href="#cb4-13"></a>        <span class="co">"""</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co">        Initializes the Elliptical Slice Sampler.</span></span>
<span id="cb4-15"><a href="#cb4-15"></a></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co">        Args:</span></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co">        - prior_cov (np.ndarray): Prior covariance matrix.</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">        - loglik (Callable): Log-likelihood function.</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">        """</span></span>
<span id="cb4-20"><a href="#cb4-20"></a>        <span class="va">self</span>.prior_cov <span class="op">=</span> prior_cov</span>
<span id="cb4-21"><a href="#cb4-21"></a>        <span class="va">self</span>.loglik <span class="op">=</span> loglik</span>
<span id="cb4-22"><a href="#cb4-22"></a></span>
<span id="cb4-23"><a href="#cb4-23"></a>        <span class="va">self</span>._n <span class="op">=</span> prior_cov.shape[<span class="dv">0</span>]  <span class="co"># Dimensionality of the space</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>        <span class="co"># Cache Cholesky decomposition</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>        <span class="va">self</span>._chol <span class="op">=</span> np.linalg.cholesky(prior_cov)</span>
<span id="cb4-26"><a href="#cb4-26"></a></span>
<span id="cb4-27"><a href="#cb4-27"></a>        <span class="co"># Initialize state and cache previous states</span></span>
<span id="cb4-28"><a href="#cb4-28"></a>        <span class="va">self</span>._state_f <span class="op">=</span> <span class="va">self</span>._chol <span class="op">@</span> np.random.randn(<span class="va">self</span>._n)</span>
<span id="cb4-29"><a href="#cb4-29"></a></span>
<span id="cb4-30"><a href="#cb4-30"></a>    <span class="kw">def</span> _indiv_sample(<span class="va">self</span>):</span>
<span id="cb4-31"><a href="#cb4-31"></a>        <span class="co">"""</span></span>
<span id="cb4-32"><a href="#cb4-32"></a><span class="co">        Main algorithm for generating an individual sample using Elliptical Slice Sampling.</span></span>
<span id="cb4-33"><a href="#cb4-33"></a><span class="co">        """</span></span>
<span id="cb4-34"><a href="#cb4-34"></a>        f <span class="op">=</span> <span class="va">self</span>._state_f  <span class="co"># Previous state</span></span>
<span id="cb4-35"><a href="#cb4-35"></a>        <span class="co"># Sample from prior for the ellipse</span></span>
<span id="cb4-36"><a href="#cb4-36"></a>        nu <span class="op">=</span> <span class="va">self</span>._chol <span class="op">@</span> np.random.randn(<span class="va">self</span>._n)</span>
<span id="cb4-37"><a href="#cb4-37"></a>        log_y <span class="op">=</span> <span class="va">self</span>.loglik(f) <span class="op">+</span> np.log(np.random.uniform()</span>
<span id="cb4-38"><a href="#cb4-38"></a>                                        )  <span class="co"># Log-likelihood threshold</span></span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a>        theta <span class="op">=</span> np.random.uniform(<span class="fl">0.</span>, <span class="dv">2</span> <span class="op">*</span> np.pi)  <span class="co"># Initial proposal angle</span></span>
<span id="cb4-41"><a href="#cb4-41"></a>        theta_min, theta_max <span class="op">=</span> theta <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.pi, theta  <span class="co"># Define bracketing interval</span></span>
<span id="cb4-42"><a href="#cb4-42"></a></span>
<span id="cb4-43"><a href="#cb4-43"></a>        <span class="co"># Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb4-45"><a href="#cb4-45"></a>            <span class="co"># YOUR CODE HERE (~10 lines)</span></span>
<span id="cb4-46"><a href="#cb4-46"></a>            <span class="co"># Generate a new sample point based on the current angle.</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>            f_prime <span class="op">=</span> f <span class="op">*</span> np.cos(theta) <span class="op">+</span> nu <span class="op">*</span> np.sin(theta)</span>
<span id="cb4-48"><a href="#cb4-48"></a></span>
<span id="cb4-49"><a href="#cb4-49"></a>            <span class="co"># Check if the proposed point meets the acceptance criterion.</span></span>
<span id="cb4-50"><a href="#cb4-50"></a>            <span class="cf">if</span> <span class="va">self</span>.loglik(f_prime) <span class="op">&gt;</span> log_y:  <span class="co"># Accept the sample</span></span>
<span id="cb4-51"><a href="#cb4-51"></a>                <span class="va">self</span>._state_f <span class="op">=</span> f_prime</span>
<span id="cb4-52"><a href="#cb4-52"></a>                <span class="cf">return</span></span>
<span id="cb4-53"><a href="#cb4-53"></a></span>
<span id="cb4-54"><a href="#cb4-54"></a>            <span class="cf">else</span>:  <span class="co"># If not accepted, adjust the bracket and select a new angle.</span></span>
<span id="cb4-55"><a href="#cb4-55"></a>                <span class="cf">if</span> theta <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb4-56"><a href="#cb4-56"></a>                    theta_min <span class="op">=</span> theta</span>
<span id="cb4-57"><a href="#cb4-57"></a>                <span class="cf">else</span>:</span>
<span id="cb4-58"><a href="#cb4-58"></a>                    theta_max <span class="op">=</span> theta</span>
<span id="cb4-59"><a href="#cb4-59"></a>                theta <span class="op">=</span> np.random.uniform(theta_min, theta_max)</span>
<span id="cb4-60"><a href="#cb4-60"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-61"><a href="#cb4-61"></a></span>
<span id="cb4-62"><a href="#cb4-62"></a>    <span class="kw">def</span> sample(<span class="va">self</span>,</span>
<span id="cb4-63"><a href="#cb4-63"></a>               n_samples: <span class="bu">int</span>,</span>
<span id="cb4-64"><a href="#cb4-64"></a>               n_burn: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb4-65"><a href="#cb4-65"></a>        <span class="co">"""</span></span>
<span id="cb4-66"><a href="#cb4-66"></a><span class="co">        Generates samples using Elliptical Slice Sampling.</span></span>
<span id="cb4-67"><a href="#cb4-67"></a></span>
<span id="cb4-68"><a href="#cb4-68"></a><span class="co">        Args:</span></span>
<span id="cb4-69"><a href="#cb4-69"></a><span class="co">        - n_samples (int): Total number of samples to return.</span></span>
<span id="cb4-70"><a href="#cb4-70"></a><span class="co">        - n_burn (int): Number of initial samples to discard (burn-in).</span></span>
<span id="cb4-71"><a href="#cb4-71"></a></span>
<span id="cb4-72"><a href="#cb4-72"></a><span class="co">        Returns:</span></span>
<span id="cb4-73"><a href="#cb4-73"></a><span class="co">        - np.ndarray: Array of samples after burn-in.</span></span>
<span id="cb4-74"><a href="#cb4-74"></a><span class="co">        """</span></span>
<span id="cb4-75"><a href="#cb4-75"></a>        samples <span class="op">=</span> []</span>
<span id="cb4-76"><a href="#cb4-76"></a>        <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_samples), desc<span class="op">=</span><span class="st">"Sampling"</span>):</span>
<span id="cb4-77"><a href="#cb4-77"></a>            <span class="va">self</span>._indiv_sample()</span>
<span id="cb4-78"><a href="#cb4-78"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> n_burn:</span>
<span id="cb4-79"><a href="#cb4-79"></a>                <span class="co"># Store sample post burn-in</span></span>
<span id="cb4-80"><a href="#cb4-80"></a>                samples.append(<span class="va">self</span>._state_f.copy())</span>
<span id="cb4-81"><a href="#cb4-81"></a></span>
<span id="cb4-82"><a href="#cb4-82"></a>        <span class="cf">return</span> np.stack(samples)</span>
<span id="cb4-83"><a href="#cb4-83"></a></span>
<span id="cb4-84"><a href="#cb4-84"></a></span>
<span id="cb4-85"><a href="#cb4-85"></a><span class="kw">def</span> squared_exponential_cov_torch(X1, X2, length_scale<span class="op">=</span><span class="fl">1.0</span>, variance<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb4-86"><a href="#cb4-86"></a>    <span class="co">"""</span></span>
<span id="cb4-87"><a href="#cb4-87"></a><span class="co">    Squared Exponential (RBF) Covariance Function using PyTorch tensors.</span></span>
<span id="cb4-88"><a href="#cb4-88"></a></span>
<span id="cb4-89"><a href="#cb4-89"></a><span class="co">    Args:</span></span>
<span id="cb4-90"><a href="#cb4-90"></a><span class="co">        X1 (torch.Tensor): First set of input points.</span></span>
<span id="cb4-91"><a href="#cb4-91"></a><span class="co">        X2 (torch.Tensor): Second set of input points.</span></span>
<span id="cb4-92"><a href="#cb4-92"></a><span class="co">        length_scale (float): Length scale of the kernel.</span></span>
<span id="cb4-93"><a href="#cb4-93"></a><span class="co">        variance (float): Variance (amplitude) of the kernel.</span></span>
<span id="cb4-94"><a href="#cb4-94"></a></span>
<span id="cb4-95"><a href="#cb4-95"></a><span class="co">    Returns:</span></span>
<span id="cb4-96"><a href="#cb4-96"></a><span class="co">        torch.Tensor: Covariance matrix between X1 and X2.</span></span>
<span id="cb4-97"><a href="#cb4-97"></a><span class="co">    """</span></span>
<span id="cb4-98"><a href="#cb4-98"></a>    X1 <span class="op">=</span> X1.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-99"><a href="#cb4-99"></a>    X2 <span class="op">=</span> X2.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-100"><a href="#cb4-100"></a>    dists <span class="op">=</span> torch.<span class="bu">sum</span>(X1<span class="op">**</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb4-101"><a href="#cb4-101"></a>        torch.<span class="bu">sum</span>(X2<span class="op">**</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> torch.mm(X1, X2.T)</span>
<span id="cb4-102"><a href="#cb4-102"></a>    <span class="cf">return</span> variance <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> dists <span class="op">/</span> length_scale<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-103"><a href="#cb4-103"></a></span>
<span id="cb4-104"><a href="#cb4-104"></a></span>
<span id="cb4-105"><a href="#cb4-105"></a><span class="kw">def</span> generate_preferences(x_pairs, utility_fn):</span>
<span id="cb4-106"><a href="#cb4-106"></a>    <span class="co">"""</span></span>
<span id="cb4-107"><a href="#cb4-107"></a><span class="co">    Generates preference labels based on the Bradley-Terry model.</span></span>
<span id="cb4-108"><a href="#cb4-108"></a></span>
<span id="cb4-109"><a href="#cb4-109"></a><span class="co">    Args:</span></span>
<span id="cb4-110"><a href="#cb4-110"></a><span class="co">        x_pairs (np.array): Array of preference pairs, shape [n_pairs, 2].</span></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="co">        utility_fn (function): Ground truth utility function.</span></span>
<span id="cb4-112"><a href="#cb4-112"></a></span>
<span id="cb4-113"><a href="#cb4-113"></a><span class="co">    Returns:</span></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="co">        np.array: Preference labels (1 if the first item in the pair is preferred, 0 otherwise).</span></span>
<span id="cb4-115"><a href="#cb4-115"></a><span class="co">    """</span></span>
<span id="cb4-116"><a href="#cb4-116"></a>    preference_labels <span class="op">=</span> []</span>
<span id="cb4-117"><a href="#cb4-117"></a>    <span class="cf">for</span> x1, x2 <span class="kw">in</span> x_pairs:</span>
<span id="cb4-118"><a href="#cb4-118"></a>        u1, u2 <span class="op">=</span> utility_fn(x1), utility_fn(x2)</span>
<span id="cb4-119"><a href="#cb4-119"></a>        prob <span class="op">=</span> np.exp(u1) <span class="op">/</span> (np.exp(u1) <span class="op">+</span> np.exp(u2))</span>
<span id="cb4-120"><a href="#cb4-120"></a>        preference_labels.append(<span class="dv">1</span> <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> prob <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb4-121"><a href="#cb4-121"></a>    <span class="cf">return</span> np.array(preference_labels)</span>
<span id="cb4-122"><a href="#cb4-122"></a></span>
<span id="cb4-123"><a href="#cb4-123"></a></span>
<span id="cb4-124"><a href="#cb4-124"></a><span class="kw">def</span> create_predictive_function(ground_truth_utility, num_pairs<span class="op">=</span><span class="dv">3000</span>, n_samples<span class="op">=</span><span class="dv">100</span>, n_burn<span class="op">=</span><span class="dv">50</span>, length_scale<span class="op">=</span><span class="fl">2.0</span>, variance<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb4-125"><a href="#cb4-125"></a>    <span class="co">"""</span></span>
<span id="cb4-126"><a href="#cb4-126"></a><span class="co">    Creates a predictive function to compute the posterior predictive mean of a Gaussian Process.</span></span>
<span id="cb4-127"><a href="#cb4-127"></a></span>
<span id="cb4-128"><a href="#cb4-128"></a><span class="co">    Args:</span></span>
<span id="cb4-129"><a href="#cb4-129"></a><span class="co">        ground_truth_utility (function): The ground truth utility function for generating preferences.</span></span>
<span id="cb4-130"><a href="#cb4-130"></a><span class="co">        num_pairs (int): Number of random preference pairs to generate.</span></span>
<span id="cb4-131"><a href="#cb4-131"></a><span class="co">        n_samples (int): Number of samples for Elliptical Slice Sampling.</span></span>
<span id="cb4-132"><a href="#cb4-132"></a><span class="co">        n_burn (int): Number of burn-in samples for Elliptical Slice Sampling.</span></span>
<span id="cb4-133"><a href="#cb4-133"></a><span class="co">        length_scale (float): Length scale for the Squared Exponential Kernel.</span></span>
<span id="cb4-134"><a href="#cb4-134"></a><span class="co">        variance (float): Variance (amplitude) of the Squared Exponential Kernel.</span></span>
<span id="cb4-135"><a href="#cb4-135"></a></span>
<span id="cb4-136"><a href="#cb4-136"></a><span class="co">    Returns:</span></span>
<span id="cb4-137"><a href="#cb4-137"></a><span class="co">        function: A predictive function that computes the posterior predictive mean.</span></span>
<span id="cb4-138"><a href="#cb4-138"></a><span class="co">    """</span></span>
<span id="cb4-139"><a href="#cb4-139"></a>    <span class="co"># Generate random preference pairs</span></span>
<span id="cb4-140"><a href="#cb4-140"></a>    np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-141"><a href="#cb4-141"></a>    x_pairs <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">10</span>, size<span class="op">=</span>(num_pairs, <span class="dv">2</span>))</span>
<span id="cb4-142"><a href="#cb4-142"></a>    X_flat <span class="op">=</span> x_pairs.flatten()</span>
<span id="cb4-143"><a href="#cb4-143"></a></span>
<span id="cb4-144"><a href="#cb4-144"></a>    <span class="co"># Generate preference labels</span></span>
<span id="cb4-145"><a href="#cb4-145"></a>    preference_labels <span class="op">=</span> generate_preferences(x_pairs, ground_truth_utility)</span>
<span id="cb4-146"><a href="#cb4-146"></a></span>
<span id="cb4-147"><a href="#cb4-147"></a>    <span class="co"># Convert X_flat to PyTorch tensor</span></span>
<span id="cb4-148"><a href="#cb4-148"></a>    X_flat_torch <span class="op">=</span> torch.tensor(X_flat, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb4-149"><a href="#cb4-149"></a></span>
<span id="cb4-150"><a href="#cb4-150"></a>    <span class="co"># GP Prior (using PyTorch)</span></span>
<span id="cb4-151"><a href="#cb4-151"></a>    K_torch <span class="op">=</span> squared_exponential_cov_torch(</span>
<span id="cb4-152"><a href="#cb4-152"></a>        X_flat_torch, X_flat_torch, length_scale<span class="op">=</span>length_scale, variance<span class="op">=</span>variance)</span>
<span id="cb4-153"><a href="#cb4-153"></a>    <span class="co"># Add jitter for numerical stability</span></span>
<span id="cb4-154"><a href="#cb4-154"></a>    K_torch <span class="op">+=</span> <span class="fl">1e-2</span> <span class="op">*</span> torch.eye(<span class="bu">len</span>(X_flat_torch))</span>
<span id="cb4-155"><a href="#cb4-155"></a>    prior_cov <span class="op">=</span> K_torch.numpy()  <span class="co"># Convert back to numpy for the sampler</span></span>
<span id="cb4-156"><a href="#cb4-156"></a></span>
<span id="cb4-157"><a href="#cb4-157"></a>    <span class="co"># Log-likelihood function</span></span>
<span id="cb4-158"><a href="#cb4-158"></a>    <span class="kw">def</span> preference_loglik(f):</span>
<span id="cb4-159"><a href="#cb4-159"></a>        <span class="co">"""</span></span>
<span id="cb4-160"><a href="#cb4-160"></a><span class="co">        Computes the log-likelihood of the preferences under the Bradley-Terry model.</span></span>
<span id="cb4-161"><a href="#cb4-161"></a></span>
<span id="cb4-162"><a href="#cb4-162"></a><span class="co">        Args:</span></span>
<span id="cb4-163"><a href="#cb4-163"></a><span class="co">            f (np.array): Latent utility values.</span></span>
<span id="cb4-164"><a href="#cb4-164"></a></span>
<span id="cb4-165"><a href="#cb4-165"></a><span class="co">        Returns:</span></span>
<span id="cb4-166"><a href="#cb4-166"></a><span class="co">            float: Log-likelihood of the given latent utilities.</span></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="co">        """</span></span>
<span id="cb4-168"><a href="#cb4-168"></a>        log_likelihood <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb4-169"><a href="#cb4-169"></a>        <span class="cf">for</span> (x1, x2), label <span class="kw">in</span> <span class="bu">zip</span>(x_pairs, preference_labels):</span>
<span id="cb4-170"><a href="#cb4-170"></a>            idx1 <span class="op">=</span> np.where(X_flat <span class="op">==</span> x1)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb4-171"><a href="#cb4-171"></a>            idx2 <span class="op">=</span> np.where(X_flat <span class="op">==</span> x2)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb4-172"><a href="#cb4-172"></a>            f1, f2 <span class="op">=</span> f[idx1], f[idx2]</span>
<span id="cb4-173"><a href="#cb4-173"></a></span>
<span id="cb4-174"><a href="#cb4-174"></a>            <span class="co"># YOUR CODE HERE (~4 lines)</span></span>
<span id="cb4-175"><a href="#cb4-175"></a>            <span class="co"># Add datapoint log likelihood using Bradley-Terry model</span></span>
<span id="cb4-176"><a href="#cb4-176"></a>            <span class="cf">pass</span></span>
<span id="cb4-177"><a href="#cb4-177"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-178"><a href="#cb4-178"></a>        <span class="cf">return</span> log_likelihood</span>
<span id="cb4-179"><a href="#cb4-179"></a></span>
<span id="cb4-180"><a href="#cb4-180"></a>    <span class="co"># Elliptical Slice Sampling</span></span>
<span id="cb4-181"><a href="#cb4-181"></a>    sampler <span class="op">=</span> EllipticalSliceSampler(</span>
<span id="cb4-182"><a href="#cb4-182"></a>        prior_cov<span class="op">=</span>prior_cov, loglik<span class="op">=</span>preference_loglik)</span>
<span id="cb4-183"><a href="#cb4-183"></a>    posterior_samples <span class="op">=</span> sampler.sample(n_samples<span class="op">=</span>n_samples, n_burn<span class="op">=</span>n_burn)</span>
<span id="cb4-184"><a href="#cb4-184"></a>    posterior_mean <span class="op">=</span> np.mean(posterior_samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-185"><a href="#cb4-185"></a></span>
<span id="cb4-186"><a href="#cb4-186"></a>    <span class="co"># Convert posterior_mean to PyTorch tensor</span></span>
<span id="cb4-187"><a href="#cb4-187"></a>    posterior_mean_torch <span class="op">=</span> torch.tensor(posterior_mean, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb4-188"><a href="#cb4-188"></a></span>
<span id="cb4-189"><a href="#cb4-189"></a>    <span class="co"># Compute K_inv using PyTorch</span></span>
<span id="cb4-190"><a href="#cb4-190"></a>    K_inv_torch <span class="op">=</span> torch.inverse(K_torch)</span>
<span id="cb4-191"><a href="#cb4-191"></a></span>
<span id="cb4-192"><a href="#cb4-192"></a>    <span class="co"># Define the predictive function</span></span>
<span id="cb4-193"><a href="#cb4-193"></a>    <span class="kw">def</span> predictive_function(x):</span>
<span id="cb4-194"><a href="#cb4-194"></a>        <span class="co">"""</span></span>
<span id="cb4-195"><a href="#cb4-195"></a><span class="co">        Predicts the utility for new input points.</span></span>
<span id="cb4-196"><a href="#cb4-196"></a></span>
<span id="cb4-197"><a href="#cb4-197"></a><span class="co">        Args:</span></span>
<span id="cb4-198"><a href="#cb4-198"></a><span class="co">            x (torch.Tensor): Input points to predict utilities for.</span></span>
<span id="cb4-199"><a href="#cb4-199"></a></span>
<span id="cb4-200"><a href="#cb4-200"></a><span class="co">        Returns:</span></span>
<span id="cb4-201"><a href="#cb4-201"></a><span class="co">            torch.Tensor: Predicted expected utilities.</span></span>
<span id="cb4-202"><a href="#cb4-202"></a><span class="co">        """</span></span>
<span id="cb4-203"><a href="#cb4-203"></a>        <span class="cf">if</span> <span class="kw">not</span> torch.is_tensor(x):</span>
<span id="cb4-204"><a href="#cb4-204"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'Predictive function must take in torch.tensor'</span>)</span>
<span id="cb4-205"><a href="#cb4-205"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-206"><a href="#cb4-206"></a>        X_flat_torch_reshaped <span class="op">=</span> X_flat_torch.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-207"><a href="#cb4-207"></a></span>
<span id="cb4-208"><a href="#cb4-208"></a>        <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb4-209"><a href="#cb4-209"></a>        <span class="co"># Implement equation (3.21) on page 44 of https://gaussianprocess.org/gpml/chapters/RW.pdf</span></span>
<span id="cb4-210"><a href="#cb4-210"></a>        <span class="cf">pass</span></span>
<span id="cb4-211"><a href="#cb4-211"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-212"><a href="#cb4-212"></a></span>
<span id="cb4-213"><a href="#cb4-213"></a>    <span class="cf">return</span> predictive_function</span>
<span id="cb4-214"><a href="#cb4-214"></a></span>
<span id="cb4-215"><a href="#cb4-215"></a></span>
<span id="cb4-216"><a href="#cb4-216"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-217"><a href="#cb4-217"></a>    <span class="co"># Ground truth utility function</span></span>
<span id="cb4-218"><a href="#cb4-218"></a>    <span class="kw">def</span> ground_truth_utility(x): <span class="cf">return</span> np.sin(x)</span>
<span id="cb4-219"><a href="#cb4-219"></a></span>
<span id="cb4-220"><a href="#cb4-220"></a>    <span class="co"># Create the predictive function</span></span>
<span id="cb4-221"><a href="#cb4-221"></a>    predictive_fn <span class="op">=</span> create_predictive_function(ground_truth_utility)</span>
<span id="cb4-222"><a href="#cb4-222"></a></span>
<span id="cb4-223"><a href="#cb4-223"></a>    <span class="co"># Test the predictive function</span></span>
<span id="cb4-224"><a href="#cb4-224"></a>    X_test <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">50</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Test points</span></span>
<span id="cb4-225"><a href="#cb4-225"></a>    posterior_means <span class="op">=</span> predictive_fn(</span>
<span id="cb4-226"><a href="#cb4-226"></a>        X_test).detach().numpy()  <span class="co"># Predicted posterior means</span></span>
<span id="cb4-227"><a href="#cb4-227"></a></span>
<span id="cb4-228"><a href="#cb4-228"></a>    <span class="co"># Ground truth utilities</span></span>
<span id="cb4-229"><a href="#cb4-229"></a>    ground_truth_utilities <span class="op">=</span> ground_truth_utility(X_test.numpy())</span>
<span id="cb4-230"><a href="#cb4-230"></a></span>
<span id="cb4-231"><a href="#cb4-231"></a>    <span class="co"># Plot results</span></span>
<span id="cb4-232"><a href="#cb4-232"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-233"><a href="#cb4-233"></a>    plt.title(<span class="st">"GP Posterior Predictive Mean (Utility Approximation)"</span>)</span>
<span id="cb4-234"><a href="#cb4-234"></a>    plt.plot(X_test.numpy(), posterior_means,</span>
<span id="cb4-235"><a href="#cb4-235"></a>             label<span class="op">=</span><span class="st">"Posterior Predictive Mean"</span>, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb4-236"><a href="#cb4-236"></a>    plt.scatter(X_test.numpy(), ground_truth_utilities,</span>
<span id="cb4-237"><a href="#cb4-237"></a>                label<span class="op">=</span><span class="st">"Ground Truth Utility"</span>, color<span class="op">=</span><span class="st">"blue"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-238"><a href="#cb4-238"></a>    plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb4-239"><a href="#cb4-239"></a>    plt.ylabel(<span class="st">"Utility"</span>)</span>
<span id="cb4-240"><a href="#cb4-240"></a>    plt.legend()</span>
<span id="cb4-241"><a href="#cb4-241"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="bf9b5236" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">from</span> preference_gp <span class="im">import</span> create_predictive_function</span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co"># Feel free to play around with continuous, concave utility functions!</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="kw">def</span> utility_1(x):</span>
<span id="cb5-6"><a href="#cb5-6"></a>    <span class="co">"""</span></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co">    Utility function 1: 3 * log(x + 1)</span></span>
<span id="cb5-8"><a href="#cb5-8"></a><span class="co">    Args:</span></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co">        x (torch.Tensor): Input tensor of allocations.</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="co">    Returns:</span></span>
<span id="cb5-11"><a href="#cb5-11"></a><span class="co">        torch.Tensor: Computed utility values.</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">    """</span></span>
<span id="cb5-13"><a href="#cb5-13"></a>    <span class="cf">return</span> <span class="dv">3</span> <span class="op">*</span> torch.log(x <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-14"><a href="#cb5-14"></a></span>
<span id="cb5-15"><a href="#cb5-15"></a><span class="kw">def</span> utility_2(x):</span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="co">"""</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">    Utility function 2: 5 * log(x + 2)</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">    Args:</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co">        x (torch.Tensor): Input tensor of allocations.</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="co">    Returns:</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co">        torch.Tensor: Computed utility values.</span></span>
<span id="cb5-22"><a href="#cb5-22"></a><span class="co">    """</span></span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="cf">return</span> <span class="dv">5</span> <span class="op">*</span> torch.log(x <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb5-24"><a href="#cb5-24"></a></span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="kw">def</span> utility_3(x):</span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span class="co">"""</span></span>
<span id="cb5-27"><a href="#cb5-27"></a><span class="co">    Utility function 3: 8 * log(x + 3)</span></span>
<span id="cb5-28"><a href="#cb5-28"></a><span class="co">    Args:</span></span>
<span id="cb5-29"><a href="#cb5-29"></a><span class="co">        x (torch.Tensor): Input tensor of allocations.</span></span>
<span id="cb5-30"><a href="#cb5-30"></a><span class="co">    Returns:</span></span>
<span id="cb5-31"><a href="#cb5-31"></a><span class="co">        torch.Tensor: Computed utility values.</span></span>
<span id="cb5-32"><a href="#cb5-32"></a><span class="co">    """</span></span>
<span id="cb5-33"><a href="#cb5-33"></a>    <span class="cf">return</span> <span class="dv">8</span> <span class="op">*</span> torch.log(x <span class="op">+</span> <span class="dv">3</span>)</span>
<span id="cb5-34"><a href="#cb5-34"></a></span>
<span id="cb5-35"><a href="#cb5-35"></a><span class="kw">def</span> project(x, B):</span>
<span id="cb5-36"><a href="#cb5-36"></a>    <span class="co">"""</span></span>
<span id="cb5-37"><a href="#cb5-37"></a><span class="co">    Projects the allocation vector `x` onto the feasible set {z | sum(z) = B, z &gt;= 0}.</span></span>
<span id="cb5-38"><a href="#cb5-38"></a><span class="co">    This ensures that the allocations respect the resource constraint.</span></span>
<span id="cb5-39"><a href="#cb5-39"></a></span>
<span id="cb5-40"><a href="#cb5-40"></a><span class="co">    Args:</span></span>
<span id="cb5-41"><a href="#cb5-41"></a><span class="co">        x (torch.Tensor): Current allocation vector.</span></span>
<span id="cb5-42"><a href="#cb5-42"></a><span class="co">        B (float): Total available resource.</span></span>
<span id="cb5-43"><a href="#cb5-43"></a></span>
<span id="cb5-44"><a href="#cb5-44"></a><span class="co">    Returns:</span></span>
<span id="cb5-45"><a href="#cb5-45"></a><span class="co">        torch.Tensor: Projected allocation vector.</span></span>
<span id="cb5-46"><a href="#cb5-46"></a><span class="co">    """</span></span>
<span id="cb5-47"><a href="#cb5-47"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-48"><a href="#cb5-48"></a>        <span class="co"># Sort x in descending order</span></span>
<span id="cb5-49"><a href="#cb5-49"></a>        sorted_x, _ <span class="op">=</span> torch.sort(x, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-50"><a href="#cb5-50"></a>        </span>
<span id="cb5-51"><a href="#cb5-51"></a>        <span class="co"># Compute cumulative sum adjusted by B</span></span>
<span id="cb5-52"><a href="#cb5-52"></a>        cumulative_sum <span class="op">=</span> torch.cumsum(sorted_x, dim<span class="op">=</span><span class="dv">0</span>) <span class="op">-</span> B</span>
<span id="cb5-53"><a href="#cb5-53"></a>        </span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="co"># Find the threshold (water-filling algorithm)</span></span>
<span id="cb5-55"><a href="#cb5-55"></a>        rho <span class="op">=</span> torch.where(sorted_x <span class="op">-</span> (cumulative_sum <span class="op">/</span> torch.arange(<span class="dv">1</span>, <span class="bu">len</span>(x) <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32)) <span class="op">&gt;</span> <span class="dv">0</span>)[<span class="dv">0</span>].<span class="bu">max</span>().item()</span>
<span id="cb5-56"><a href="#cb5-56"></a>        theta <span class="op">=</span> cumulative_sum[<span class="bu">int</span>(rho)] <span class="op">/</span> (rho <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-57"><a href="#cb5-57"></a>        </span>
<span id="cb5-58"><a href="#cb5-58"></a>        <span class="co"># Compute the projected allocation</span></span>
<span id="cb5-59"><a href="#cb5-59"></a>        <span class="cf">return</span> torch.clamp(x <span class="op">-</span> theta, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-60"><a href="#cb5-60"></a></span>
<span id="cb5-61"><a href="#cb5-61"></a><span class="kw">def</span> optimize_allocations(utilities, B, learning_rate, num_iterations):</span>
<span id="cb5-62"><a href="#cb5-62"></a>    <span class="co">"""</span></span>
<span id="cb5-63"><a href="#cb5-63"></a><span class="co">    Optimizes the allocation of resources to maximize the total utility.</span></span>
<span id="cb5-64"><a href="#cb5-64"></a></span>
<span id="cb5-65"><a href="#cb5-65"></a><span class="co">    Args:</span></span>
<span id="cb5-66"><a href="#cb5-66"></a><span class="co">        utilities (list): List of utility functions or GP-based predictive functions.</span></span>
<span id="cb5-67"><a href="#cb5-67"></a><span class="co">        B (float): Total available resource.</span></span>
<span id="cb5-68"><a href="#cb5-68"></a><span class="co">        learning_rate (float): Step size for gradient ascent.</span></span>
<span id="cb5-69"><a href="#cb5-69"></a><span class="co">        num_iterations (int): Number of optimization iterations.</span></span>
<span id="cb5-70"><a href="#cb5-70"></a></span>
<span id="cb5-71"><a href="#cb5-71"></a><span class="co">    Returns:</span></span>
<span id="cb5-72"><a href="#cb5-72"></a><span class="co">        torch.Tensor: Final resource allocations.</span></span>
<span id="cb5-73"><a href="#cb5-73"></a><span class="co">    """</span></span>
<span id="cb5-74"><a href="#cb5-74"></a>    <span class="co"># Initialize resource allocations equally</span></span>
<span id="cb5-75"><a href="#cb5-75"></a>    x <span class="op">=</span> torch.tensor([<span class="fl">1.0</span>] <span class="op">*</span> <span class="bu">len</span>(utilities), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-76"><a href="#cb5-76"></a></span>
<span id="cb5-77"><a href="#cb5-77"></a>    <span class="co"># Optimization loop</span></span>
<span id="cb5-78"><a href="#cb5-78"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb5-79"><a href="#cb5-79"></a>        <span class="co"># YOUR CODE HERE (~6 lines)</span></span>
<span id="cb5-80"><a href="#cb5-80"></a>        <span class="co"># 1. Compute total utility and backprop</span></span>
<span id="cb5-81"><a href="#cb5-81"></a>        <span class="co"># 2. Update x directly with x.grad</span></span>
<span id="cb5-82"><a href="#cb5-82"></a>        <span class="co"># 3. Project onto convex constraint set since we are using Projected Gradient Descent (PGD)</span></span>
<span id="cb5-83"><a href="#cb5-83"></a>        <span class="cf">pass</span></span>
<span id="cb5-84"><a href="#cb5-84"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb5-85"><a href="#cb5-85"></a>        </span>
<span id="cb5-86"><a href="#cb5-86"></a>        <span class="co"># Log progress every 10 iterations or at the last iteration</span></span>
<span id="cb5-87"><a href="#cb5-87"></a>        <span class="cf">if</span> iteration <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> iteration <span class="op">==</span> num_iterations <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb5-88"><a href="#cb5-88"></a>            <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration<span class="sc">}</span><span class="ss">: Total Utility = </span><span class="sc">{</span>total_utility<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">, Allocations = </span><span class="sc">{</span>x<span class="sc">.</span>data<span class="sc">.</span>numpy()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-89"><a href="#cb5-89"></a>    </span>
<span id="cb5-90"><a href="#cb5-90"></a>    <span class="cf">return</span> x</span>
<span id="cb5-91"><a href="#cb5-91"></a></span>
<span id="cb5-92"><a href="#cb5-92"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb5-93"><a href="#cb5-93"></a>    <span class="co"># Generate GP models for each utility</span></span>
<span id="cb5-94"><a href="#cb5-94"></a>    gp_1 <span class="op">=</span> create_predictive_function(<span class="kw">lambda</span> x: utility_1(torch.tensor(x)).numpy())</span>
<span id="cb5-95"><a href="#cb5-95"></a>    gp_2 <span class="op">=</span> create_predictive_function(<span class="kw">lambda</span> x: utility_2(torch.tensor(x)).numpy())</span>
<span id="cb5-96"><a href="#cb5-96"></a>    gp_3 <span class="op">=</span> create_predictive_function(<span class="kw">lambda</span> x: utility_3(torch.tensor(x)).numpy())</span>
<span id="cb5-97"><a href="#cb5-97"></a></span>
<span id="cb5-98"><a href="#cb5-98"></a>    <span class="co"># Combine utility GPs into a list for optimization</span></span>
<span id="cb5-99"><a href="#cb5-99"></a>    utilities <span class="op">=</span> [gp_1, gp_2, gp_3]  <span class="co"># Use [utility_1, utility_2, utility_3] for exact utility functions</span></span>
<span id="cb5-100"><a href="#cb5-100"></a></span>
<span id="cb5-101"><a href="#cb5-101"></a>    <span class="co"># Resource constraint and optimization settings</span></span>
<span id="cb5-102"><a href="#cb5-102"></a>    B <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Total available resource</span></span>
<span id="cb5-103"><a href="#cb5-103"></a>    learning_rate <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Gradient ascent step size</span></span>
<span id="cb5-104"><a href="#cb5-104"></a>    num_iterations <span class="op">=</span> <span class="dv">2000</span>  <span class="co"># Number of iterations</span></span>
<span id="cb5-105"><a href="#cb5-105"></a></span>
<span id="cb5-106"><a href="#cb5-106"></a>    <span class="co"># Optimize allocations</span></span>
<span id="cb5-107"><a href="#cb5-107"></a>    final_allocations <span class="op">=</span> optimize_allocations(utilities, B, learning_rate, num_iterations)</span>
<span id="cb5-108"><a href="#cb5-108"></a></span>
<span id="cb5-109"><a href="#cb5-109"></a>    <span class="co"># Final results</span></span>
<span id="cb5-110"><a href="#cb5-110"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Final allocations:"</span>)</span>
<span id="cb5-111"><a href="#cb5-111"></a>    <span class="bu">print</span>(final_allocations.data.numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-astudillo2023qeubodecisiontheoreticacquisitionfunction" class="csl-entry" role="listitem">
Astudillo, Raul, Zhiyuan Jerry Lin, Eytan Bakshy, and Peter I. Frazier. 2023. <span>“qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization.”</span> <a href="https://arxiv.org/abs/2303.15746">https://arxiv.org/abs/2303.15746</a>.
</div>
<div id="ref-auer_cesa-bianchi_fischer_2002" class="csl-entry" role="listitem">
Auer, Peter, Nicolò Cesa-Bianchi, and Paul Fischer. 2002. <span>“Finite-Time Analysis of the Multiarmed Bandit Problem.”</span> <em>Machine Learning</em> 47 (2). <a href="https://doi.org/10.1023/A:1013689704352">https://doi.org/10.1023/A:1013689704352</a>.
</div>
<div id="ref-bastani2020online" class="csl-entry" role="listitem">
Bastani, Hamsa, and Mohsen Bayati. 2020. <span>“Online Decision Making with High-Dimensional Covariates.”</span> <em>Operations Research</em> 68 (1): 276–94. <a href="https://doi.org/10.1287/opre.2019.1902">https://doi.org/10.1287/opre.2019.1902</a>.
</div>
<div id="ref-bouneffouf2012a" class="csl-entry" role="listitem">
Bouneffouf, Djallel, Amel Bouzeghoub, and Alda Lopes Gançarski. 2012. <span>“A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System.”</span> In <em>Neural Information Processing</em>, edited by Tingwen Huang, Zhigang Zeng, Chuandong Li, and Chi Sing Leung, 324–31. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-bouneffouf2020survey" class="csl-entry" role="listitem">
Bouneffouf, Djallel, Irina Rish, and Charu Aggarwal. 2020. <span>“Survey on Applications of Multi-Armed and Contextual Bandits.”</span> In <em>2020 IEEE Congress on Evolutionary Computation (CEC)</em>, 1–8. Glasgow, United Kingdom: IEEE Press. <a href="https://doi.org/10.1109/CEC48606.2020.9185782">https://doi.org/10.1109/CEC48606.2020.9185782</a>.
</div>
<div id="ref-bouneffouf2017bandit" class="csl-entry" role="listitem">
Bouneffouf, Djallel, Irina Rish, and Guillermo A. Cecchi. 2017. <span>“Bandit Models of Human Behavior: Reward Processing in Mental Disorders.”</span> In <em>Artificial General Intelligence</em>, edited by Tom Everitt, Ben Goertzel, and Alexey Potapov, 237–48. Cham: Springer International Publishing.
</div>
<div id="ref-ding2019interactive" class="csl-entry" role="listitem">
Ding, Kaize, Jundong Li, and Huan Liu. 2019. <span>“Interactive Anomaly Detection on Attributed Networks.”</span> In <em>Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>, 357–65. WSDM ’19. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3289600.3290964">https://doi.org/10.1145/3289600.3290964</a>.
</div>
<div id="ref-huo2017risk" class="csl-entry" role="listitem">
Huo, Xiaoguang, and Feng Fu. 2017. <span>“Risk-Aware Multi-Armed Bandit Problem with Application to Portfolio Selection.”</span> <em>Royal Society Open Science</em> 4 (November). <a href="https://doi.org/10.1098/rsos.171377">https://doi.org/10.1098/rsos.171377</a>.
</div>
<div id="ref-kahneman_tversky_1979" class="csl-entry" role="listitem">
Kahneman, Daniel, and Amos Tversky. 1979. <span>“Prospect Theory: Analysis of Decision Under Risk.”</span> <em>Econometrica</em> 47 (2). <a href="https://doi.org/10.2307/1914185">https://doi.org/10.2307/1914185</a>.
</div>
<div id="ref-LAI19854" class="csl-entry" role="listitem">
Lai, T. L, and Herbert Robbins. 1985. <span>“Asymptotically Efficient Adaptive Allocation Rules.”</span> <em>Advances in Applied Mathematics</em> 6 (1): 4–22. https://doi.org/<a href="https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>.
</div>
<div id="ref-liu2018customized" class="csl-entry" role="listitem">
Liu, Bing, Tong Yu, Ian Lane, and Ole J. Mengshoel. 2018. <span>“Customized Nonlinear Bandits for Online Response Selection in Neural Conversation Models.”</span> In <em>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence</em>. AAAI’18/IAAI’18/EAAI’18. New Orleans, Louisiana, USA: AAAI Press.
</div>
<div id="ref-mansour2019bayesianincentivecompatiblebanditexploration" class="csl-entry" role="listitem">
Mansour, Yishay, Aleksandrs Slivkins, and Vasilis Syrgkanis. 2019. <span>“Bayesian Incentive-Compatible Bandit Exploration.”</span> <a href="https://arxiv.org/abs/1502.04147">https://arxiv.org/abs/1502.04147</a>.
</div>
<div id="ref-mansour2021bayesianexplorationincentivizingexploration" class="csl-entry" role="listitem">
Mansour, Yishay, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. 2021. <span>“Bayesian Exploration: Incentivizing Exploration in Bayesian Games.”</span> <a href="https://arxiv.org/abs/1602.07570">https://arxiv.org/abs/1602.07570</a>.
</div>
<div id="ref-misra2019dynamic" class="csl-entry" role="listitem">
Misra, Kanishka, Eric M. Schwartz, and Jacob Abernethy. 2019. <span>“Dynamic Online Pricing with Incomplete Information Using Multiarmed Bandit Experiments.”</span> <em>Marketing Science</em> 38 (2): 226–52. <a href="https://doi.org/10.1287/mksc.2018.1129">https://doi.org/10.1287/mksc.2018.1129</a>.
</div>
<div id="ref-perez2018contextual" class="csl-entry" role="listitem">
perez, julien, and Tomi Silander. 2018. <span>“Contextual Memory Bandit for Pro-Active Dialog Engagement.”</span> <a href="https://openreview.net/forum?id=SJiHOSeR-">https://openreview.net/forum?id=SJiHOSeR-</a>.
</div>
<div id="ref-russo2015informationtheoreticanalysisthompsonsampling" class="csl-entry" role="listitem">
Russo, Daniel, and Benjamin Van Roy. 2015. <span>“An Information-Theoretic Analysis of Thompson Sampling.”</span> <a href="https://arxiv.org/abs/1403.5341">https://arxiv.org/abs/1403.5341</a>.
</div>
<div id="ref-shen2015portfolio" class="csl-entry" role="listitem">
Shen, Weiwei, Jun Wang, Yu-Gang Jiang, and Hongyuan Zha. 2015. <span>“Portfolio Choices with Orthogonal Bandit Learning.”</span> In <em>Proceedings of the 24th International Conference on Artificial Intelligence</em>, 974–80. IJCAI’15. Buenos Aires, Argentina: AAAI Press.
</div>
<div id="ref-advancements_dueling" class="csl-entry" role="listitem">
Sui, Yanan, Masrour Zoghi, Katja Hofmann, and Yisong Yue. 2018. <span>“Advancements in Dueling Bandits.”</span> <em>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</em>. <a href="https://doi.org/10.24963/ijcai.2018/776">https://doi.org/10.24963/ijcai.2018/776</a>.
</div>
<div id="ref-upadhyay2019a" class="csl-entry" role="listitem">
Upadhyay, Sohini, Mayank Agarwal, Djallel Bouneffouf, and Yasaman Khazaeni. 2019. <span>“A Bandit Approach to Posterior Dialog Orchestration Under a Budget.”</span>
</div>
<div id="ref-wu2018parallelknowledgegradientmethod" class="csl-entry" role="listitem">
Wu, Jian, and Peter I. Frazier. 2018. <span>“The Parallel Knowledge Gradient Method for Batch Bayesian Optimization.”</span> <a href="https://arxiv.org/abs/1606.04414">https://arxiv.org/abs/1606.04414</a>.
</div>
<div id="ref-xu2024principledpreferentialbayesianoptimization" class="csl-entry" role="listitem">
Xu, Wenjie, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, and Colin N. Jones. 2024. <span>“Principled Preferential Bayesian Optimization.”</span> <a href="https://arxiv.org/abs/2402.05367">https://arxiv.org/abs/2402.05367</a>.
</div>
<div id="ref-YUE20121538" class="csl-entry" role="listitem">
Yue, Yisong, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. <span>“The k-Armed Dueling Bandits Problem.”</span> <em>Journal of Computer and System Sciences</em> 78 (5): 1538–56. https://doi.org/<a href="https://doi.org/10.1016/j.jcss.2011.12.028">https://doi.org/10.1016/j.jcss.2011.12.028</a>.
</div>
<div id="ref-IR" class="csl-entry" role="listitem">
Yue, Yisong, and Thorsten Joachims. 2009. <span>“Interactively Optimizing Information Retrieval Systems as a Dueling Bandits Problem.”</span> <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>. <a href="https://doi.org/10.1145/1553374.1553527">https://doi.org/10.1145/1553374.1553527</a>.
</div>
<div id="ref-fgts_cdb" class="csl-entry" role="listitem">
Zhang, Tong. 2021. <span>“Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement Learning.”</span> <em>CoRR</em> abs/2110.00871. <a href="https://arxiv.org/abs/2110.00871">https://arxiv.org/abs/2110.00871</a>.
</div>
<div id="ref-zhou2017large" class="csl-entry" role="listitem">
Zhou, Qian, XiaoFang Zhang, Jin Xu, and Bin Liang. 2017. <span>“Large-Scale Bandit Approaches for Recommender Systems.”</span> In <em>Neural Information Processing</em>, edited by Derong Liu, Shengli Xie, Yuanqing Li, Dongbin Zhao, and El-Sayed M. El-Alfy, 811–21. Cham: Springer International Publishing.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/003-measure.html" class="pagination-link" aria-label="Model-Based Preference Optimization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/005-align.html" class="pagination-link" aria-label="Human Values and AI Alignment">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu"># Model-Free Preference Optimization {#sec-model-free}</span></span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a>::: {.content-visible when-format="html"}</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>&lt;iframe</span>
<span id="cb6-6"><a href="#cb6-6"></a>  src="https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/"</span>
<span id="cb6-7"><a href="#cb6-7"></a>  style="width:45%; height:225px;"</span>
<span id="cb6-8"><a href="#cb6-8"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb6-9"><a href="#cb6-9"></a>&lt;iframe</span>
<span id="cb6-10"><a href="#cb6-10"></a>  src="https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/"</span>
<span id="cb6-11"><a href="#cb6-11"></a>  style="width:45%; height:225px;"</span>
<span id="cb6-12"><a href="#cb6-12"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co">[</span><span class="ot">Fullscreen Part 1</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/4.1.dueling_bandit/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="co">[</span><span class="ot">Fullscreen Part 2</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/4.2.preferential_baysian_optimization/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a>:::</span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a><span class="fu">## Individual Preference Optimization via Dueling Bandit</span></span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="fu">### Introduction to Dueling Bandit Problem and Its Extension</span></span>
<span id="cb6-21"><a href="#cb6-21"></a>The multi-armed bandit (MAB) problem involves a gambler deciding which lever to pull on an MAB </span>
<span id="cb6-22"><a href="#cb6-22"></a>machine to maximize the winning rate, despite not knowing which machine is the most rewarding. </span>
<span id="cb6-23"><a href="#cb6-23"></a>This scenario highlights the need to balance exploration (trying new machines to discover potential </span>
<span id="cb6-24"><a href="#cb6-24"></a>higher rewards) and exploitation (using current knowledge to maximize gains). MAB algorithms </span>
<span id="cb6-25"><a href="#cb6-25"></a>address this dilemma by making decisions under uncertainty to achieve the best possible outcomes </span>
<span id="cb6-26"><a href="#cb6-26"></a>based on gathered data. At the core of the MAB problem is a set of actions, or 'arms,' denoted by </span>
<span id="cb6-27"><a href="#cb6-27"></a>$\mathcal{A} = <span class="sc">\{</span>1, 2, \ldots, K<span class="sc">\}</span>$, where $K$ signifies the total number of arms. For each round $t$, </span>
<span id="cb6-28"><a href="#cb6-28"></a>the agent selects an arm $a_t \in \mathcal{A}$ and receives a reward $r_t$, sampled from an arm-specific,</span>
<span id="cb6-29"><a href="#cb6-29"></a>unknown probability distribution. The expected reward of pulling arm $a$ is represented as $\mu_a = \mathbb{E}<span class="co">[</span><span class="ot">r_t | a</span><span class="co">]</span>$.</span>
<span id="cb6-30"><a href="#cb6-30"></a></span>
<span id="cb6-31"><a href="#cb6-31"></a>The multi-armed bandit framework can be extended in various ways to model more complex scenarios. </span>
<span id="cb6-32"><a href="#cb6-32"></a>In the infinite-armed bandit problem, the set of possible arms $\mathcal{A}$ is either very large </span>
<span id="cb6-33"><a href="#cb6-33"></a>or infinite. This introduces significant challenges in exploration, as the agent cannot afford to </span>
<span id="cb6-34"><a href="#cb6-34"></a>explore each arm even once. Algorithms for infinite-armed bandits typically assume some regularity </span>
<span id="cb6-35"><a href="#cb6-35"></a>or structure of the reward function across arms to make the problem tractable. The contextual bandit </span>
<span id="cb6-36"><a href="#cb6-36"></a>problem extends the bandit framework by incorporating observable external states or contexts that </span>
<span id="cb6-37"><a href="#cb6-37"></a>influence the reward distributions of arms. The agent's task is to learn policies that map contexts </span>
<span id="cb6-38"><a href="#cb6-38"></a>to arms to maximize reward. This model is particularly powerful for personalized recommendations, where </span>
<span id="cb6-39"><a href="#cb6-39"></a>the context can include user features or historical interactions. In dueling bandit problems, the agent </span>
<span id="cb6-40"><a href="#cb6-40"></a>chooses two arms to pull simultaneously and receives feedback only on which of the two is better, not </span>
<span id="cb6-41"><a href="#cb6-41"></a>the actual reward values. This pairwise comparison model is especially useful in scenarios where absolute </span>
<span id="cb6-42"><a href="#cb6-42"></a>evaluations are difficult, but relative preferences are easier to determine, such as in ranking systems.</span>
<span id="cb6-43"><a href="#cb6-43"></a></span>
<span id="cb6-44"><a href="#cb6-44"></a>Contextual bandits extend the multi-armed bandits by making decisions conditional on the state of the </span>
<span id="cb6-45"><a href="#cb6-45"></a>environment and previous observations. The benefit of such a model is that observing the environment </span>
<span id="cb6-46"><a href="#cb6-46"></a>can provide additional information, potentially leading to better rewards and outcomes. In each </span>
<span id="cb6-47"><a href="#cb6-47"></a>iteration, the agent is presented with the context of the environment, then decides on an action </span>
<span id="cb6-48"><a href="#cb6-48"></a>based on the context and previous observations. Finally, the agent observes the action's outcome and </span>
<span id="cb6-49"><a href="#cb6-49"></a>reward. Throughout this process, the agent aims to maximize the expected reward.</span>
<span id="cb6-50"><a href="#cb6-50"></a></span>
<span id="cb6-51"><a href="#cb6-51"></a>In many real-world contexts, one may not have a real-valued reward (or at least a reliable one) </span>
<span id="cb6-52"><a href="#cb6-52"></a>associated with a decision. Instead, we may only have observations indicating which of a set of </span>
<span id="cb6-53"><a href="#cb6-53"></a>bandits was optimal in a given scenario. The assumption is that within these observations of </span>
<span id="cb6-54"><a href="#cb6-54"></a>preferred choices among a set of options, there is an implicit reward or payoff encapsulated in that </span>
<span id="cb6-55"><a href="#cb6-55"></a>decision. Consider the following examples:</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a><span class="ss">1.  </span>**Dietary preferences**: When providing food recommendations to humans, it is often not possible </span>
<span id="cb6-58"><a href="#cb6-58"></a>    to quantify an explicit reward from recommending a specific food item. Instead, we can offer meal </span>
<span id="cb6-59"><a href="#cb6-59"></a>    options and observe which one the person selects.</span>
<span id="cb6-60"><a href="#cb6-60"></a></span>
<span id="cb6-61"><a href="#cb6-61"></a><span class="ss">2.  </span>**Video recommendation**: Websites like YouTube and TikTok recommend specific videos to users. It is </span>
<span id="cb6-62"><a href="#cb6-62"></a>    typically not feasible to measure the reward a person gains from watching a video. However, we can infer </span>
<span id="cb6-63"><a href="#cb6-63"></a>    that a user preferred one video over another. From these relative preference observations, we can develop </span>
<span id="cb6-64"><a href="#cb6-64"></a>    a strategy to recommend videos they are likely to enjoy.</span>
<span id="cb6-65"><a href="#cb6-65"></a></span>
<span id="cb6-66"><a href="#cb6-66"></a><span class="ss">3.  </span>**Exoskeleton gait optimization**: Tucker et al. (2020) created a framework that uses human-evaluated </span>
<span id="cb6-67"><a href="#cb6-67"></a>    preferences for an exoskeleton gait algorithm to develop an optimal strategy for the exoskeleton to assist</span>
<span id="cb6-68"><a href="#cb6-68"></a>    a human in walking. A human cannot reliably produce a numerical value for how well the exoskeleton helped </span>
<span id="cb6-69"><a href="#cb6-69"></a>    them walk but can reliably indicate which option performed best according to their preferences.</span>
<span id="cb6-70"><a href="#cb6-70"></a></span>
<span id="cb6-71"><a href="#cb6-71"></a>Generally, we assume access to a set of actions. A noteworthy assumption is that any observations we make</span>
<span id="cb6-72"><a href="#cb6-72"></a>are unbiased estimates of the payoff. This means that if we observe a human preferred one option over another </span>
<span id="cb6-73"><a href="#cb6-73"></a>(or several others), the preferred option had a higher implicit reward or payoff than the alternatives. In the </span>
<span id="cb6-74"><a href="#cb6-74"></a>case of dietary preferences, this may mean that a human liked the preferred option; in the case of video </span>
<span id="cb6-75"><a href="#cb6-75"></a>recommendations, a user was more entertained, satisfied, or educated by the video they selected than the other options.</span>
<span id="cb6-76"><a href="#cb6-76"></a></span>
<span id="cb6-77"><a href="#cb6-77"></a>The overarching context is that we do not have direct or reliable access to rewards. We may not have a </span>
<span id="cb6-78"><a href="#cb6-78"></a>reward at all (for some decisions, it may be impossible to define a real value to the outcome), or it may be </span>
<span id="cb6-79"><a href="#cb6-79"></a>noisy (for example, if we ask a human to rate their satisfaction on a scale of 1 to 10). We use relative </span>
<span id="cb6-80"><a href="#cb6-80"></a>comparisons to evaluate the best of multiple options in this case. Our goal is to minimize total regret in </span>
<span id="cb6-81"><a href="#cb6-81"></a>the face of noisy comparisons. Humans may not always provide consistent observations (since human decision-making </span>
<span id="cb6-82"><a href="#cb6-82"></a>is not guaranteed to be consistent). However, we can still determine an optimal strategy with the observed </span>
<span id="cb6-83"><a href="#cb6-83"></a>comparisons. We aim to minimize the frequency of sub-optimal decisions according to human preferences. In practice, </span>
<span id="cb6-84"><a href="#cb6-84"></a>many formulations of bandits can allow for infinitely many bandits (for example, in continuous-value and high-dimensional </span>
<span id="cb6-85"><a href="#cb6-85"></a>spaces). However, this situation can be intractable when determining an optimal decision strategy. With </span>
<span id="cb6-86"><a href="#cb6-86"></a>infinite options, how can we always ensure we have chosen the best? We will constrain our bandits to a </span>
<span id="cb6-87"><a href="#cb6-87"></a>discrete space to enable efficient exploration. We will assume that we have $k$ bandits, $b_i, i \in <span class="co">[</span><span class="ot">1, k</span><span class="co">]</span>$, </span>
<span id="cb6-88"><a href="#cb6-88"></a>and our task is to choose the one that will minimize regret.</span>
<span id="cb6-89"><a href="#cb6-89"></a></span>
<span id="cb6-90"><a href="#cb6-90"></a>With the framework outlined, we now define our approach more formally. This method was introduced by </span>
<span id="cb6-91"><a href="#cb6-91"></a> <span class="co">[</span><span class="ot">@YUE20121538</span><span class="co">]</span>, and proofs for the guarantees and derivations of parameters can be found in their work.</span>
<span id="cb6-92"><a href="#cb6-92"></a></span>
<span id="cb6-93"><a href="#cb6-93"></a>To determine the optimal action, we will compare pairwise to ascertain the probability that an action $b_i$ is </span>
<span id="cb6-94"><a href="#cb6-94"></a>preferred over another $b_j$, where $i \ne j$. Concretely, we assume access to a function $\epsilon$ that helps </span>
<span id="cb6-95"><a href="#cb6-95"></a>determine this probability; in practice, this can be done with an oracle, such as asking a human which of two </span>
<span id="cb6-96"><a href="#cb6-96"></a>options they prefer: $$P(b_i &gt; b_j) = \varepsilon(b_i, b_j) + \frac{1}{2}.$$ With this model, three basic properties </span>
<span id="cb6-97"><a href="#cb6-97"></a>govern the values provided by $\epsilon$:</span>
<span id="cb6-98"><a href="#cb6-98"></a>$$\epsilon(b_i, b_j) = -\epsilon(b_j, b_i), \epsilon(b_i, b_i) = 0, \epsilon(b_i, b_j) \in \left(-\frac{1}{2}, \frac{1}{2} \right).$$</span>
<span id="cb6-99"><a href="#cb6-99"></a></span>
<span id="cb6-100"><a href="#cb6-100"></a>We assume there is a total ordering of bandits, such that $b_i \succ b_j$ implies $\epsilon(b_i, b_j) &gt; 0$. We </span>
<span id="cb6-101"><a href="#cb6-101"></a>impose two constraints to properly model comparisons:</span>
<span id="cb6-102"><a href="#cb6-102"></a></span>
<span id="cb6-103"><a href="#cb6-103"></a><span class="ss">-   </span>**Strong Stochastic Transitivity**: We must maintain our total ordering of bandits, and as such, the comparison </span>
<span id="cb6-104"><a href="#cb6-104"></a>    model also respects this ordering:</span>
<span id="cb6-105"><a href="#cb6-105"></a>    $$b_i \succ b_j \succ b_k \Rightarrow \epsilon(b_i, b_k) \ge \text{max}<span class="sc">\{</span>\epsilon(b_i, b_j), \epsilon(b_j, b_k)<span class="sc">\}</span>.$$ {#eq-stochastic-transitivity}</span>
<span id="cb6-106"><a href="#cb6-106"></a></span>
<span id="cb6-107"><a href="#cb6-107"></a><span class="ss">-   </span>**Stochastic Triangle Inequality**: We also impose a triangle inequality, which captures the condition that the probability </span>
<span id="cb6-108"><a href="#cb6-108"></a>    of a bandit winning (or losing) a comparison will exhibit diminishing returns as it becomes increasingly superior (or inferior) </span>
<span id="cb6-109"><a href="#cb6-109"></a>    to the competing bandit:</span>
<span id="cb6-110"><a href="#cb6-110"></a>    $$b_i \succ b_j \succ b_k \Rightarrow \epsilon(b_i, b_k) \le \epsilon(b_i, b_j) + \epsilon(b_j, b_k).$$ {#eq-triangle-inequality}</span>
<span id="cb6-111"><a href="#cb6-111"></a></span>
<span id="cb6-112"><a href="#cb6-112"></a>These assumptions may initially seem limiting; however, common models for comparisons satisfy these constraints. </span>
<span id="cb6-113"><a href="#cb6-113"></a>For example, the Bradley-Terry Model follows $P(b_i &gt; b_j) = \frac{\mu_i}{\mu_i + \mu_j}$. </span>
<span id="cb6-114"><a href="#cb6-114"></a>The Gaussian model with unit variance also satisfies these constraints: $P(b_i &gt; b_j) = P(X_i - X_j &gt; 0)$, </span>
<span id="cb6-115"><a href="#cb6-115"></a>where $X_i - X_j \sim N(\mu_i - \mu_j, 2)$.</span>
<span id="cb6-116"><a href="#cb6-116"></a></span>
<span id="cb6-117"><a href="#cb6-117"></a>To accurately model the preferences between bandits in our framework of pairwise bandit comparisons and regret, </span>
<span id="cb6-118"><a href="#cb6-118"></a>we must track certain parameters in our algorithm. First, we will maintain a running empirical estimate of the probability </span>
<span id="cb6-119"><a href="#cb6-119"></a>of bandit preferences based on our observations. It is important to note that we do not have direct access to an $\epsilon$ </span>
<span id="cb6-120"><a href="#cb6-120"></a>function. Instead, we must present two bandits to a human, who selects a winner. To do this, we define:</span>
<span id="cb6-121"><a href="#cb6-121"></a>$$\hat{P}_{i, j} = \frac{<span class="sc">\#</span> b_i\ \text{wins}}{<span class="sc">\#</span> \text{comparisons between}\ i \text{and}\ j}.$$</span>
<span id="cb6-122"><a href="#cb6-122"></a></span>
<span id="cb6-123"><a href="#cb6-123"></a>We will also compute confidence intervals at each timestep for each of the entries in $\hat{P}$ as</span>
<span id="cb6-124"><a href="#cb6-124"></a>$$\hat{C}_t = \left( \hat{P}_t - c_t, \hat{P}_t + c_t \right),$$</span>
<span id="cb6-125"><a href="#cb6-125"></a>where $c_t = \sqrt{\frac{4\log(\frac{1}{\delta})}{t}}$. Note that $\delta = \frac{1}{TK^2}$, where $T$ is the </span>
<span id="cb6-126"><a href="#cb6-126"></a>time horizon and $K$ is the number of bandits.</span>
<span id="cb6-127"><a href="#cb6-127"></a></span>
<span id="cb6-128"><a href="#cb6-128"></a>Previously, we discussed approaches for finding the best action in a specific context. Now, we consider changing contexts, </span>
<span id="cb6-129"><a href="#cb6-129"></a>which means there is no longer a static hidden preference matrix $P$. Instead, at every time step, there is a preference </span>
<span id="cb6-130"><a href="#cb6-130"></a>matrix $P_C$ depending on context $C$. We consider a context $C$ and a preference matrix $P_C$ to be chosen by nature as a </span>
<span id="cb6-131"><a href="#cb6-131"></a>result of the given environment (Yue et al., 2012). The goal of a contextual bandits algorithm is to find a policy $\pi$ that </span>
<span id="cb6-132"><a href="#cb6-132"></a>maps contexts to a Von Neumann winner distribution over our bandits. That is, our policy $\pi$ should map any context to some </span>
<span id="cb6-133"><a href="#cb6-133"></a>distribution over our bandits such that sampling from that distribution is preferred to a random action for that context.</span>
<span id="cb6-134"><a href="#cb6-134"></a></span>
<span id="cb6-135"><a href="#cb6-135"></a><span class="fu">### Regret</span></span>
<span id="cb6-136"><a href="#cb6-136"></a></span>
<span id="cb6-137"><a href="#cb6-137"></a>The agent aims to pick a sequence of arms $(a_1, a_2, \ldots, a_T)$</span>
<span id="cb6-138"><a href="#cb6-138"></a>across a succession of time steps $t = 1$ to $t = T$ to maximize the</span>
<span id="cb6-139"><a href="#cb6-139"></a>total accumulated reward. Formally, the strategy seeks to maximize the</span>
<span id="cb6-140"><a href="#cb6-140"></a>sum of the expected rewards:</span>
<span id="cb6-141"><a href="#cb6-141"></a>$\max_{a_1, \ldots, a_T} \mathbb{E} \left<span class="co">[</span><span class="ot">\sum_{t=1}^{T} r_t\right</span><span class="co">]</span>$.</span>
<span id="cb6-142"><a href="#cb6-142"></a>Regret is defined as the difference between the cumulative reward that</span>
<span id="cb6-143"><a href="#cb6-143"></a>could have been obtained by always pulling the best arm (in hindsight,</span>
<span id="cb6-144"><a href="#cb6-144"></a>after knowing the reward distributions) and the cumulative reward</span>
<span id="cb6-145"><a href="#cb6-145"></a>actually obtained by the algorithm. Formally, if $\mu^*$ is the expected</span>
<span id="cb6-146"><a href="#cb6-146"></a>reward of the best arm and $\mu_{a_t}$ is the expected reward of the arm</span>
<span id="cb6-147"><a href="#cb6-147"></a>chosen at time $t$, the regret after $T$ time steps is given by</span>
<span id="cb6-148"><a href="#cb6-148"></a>$R(T) = T \cdot \mu^* - \sum_{t=1}^{T} \mu_{a_t}$. The objective of a</span>
<span id="cb6-149"><a href="#cb6-149"></a>bandit algorithm is to minimize this regret over time, effectively</span>
<span id="cb6-150"><a href="#cb6-150"></a>learning to make decisions that are as close as possible to the</span>
<span id="cb6-151"><a href="#cb6-151"></a>decisions of an oracle that knows the reward distributions beforehand.</span>
<span id="cb6-152"><a href="#cb6-152"></a>Low regret indicates an algorithm that has often learned to choose</span>
<span id="cb6-153"><a href="#cb6-153"></a>well-performing arms, balancing the exploration of unknown arms with the</span>
<span id="cb6-154"><a href="#cb6-154"></a>exploitation of arms that are already known to perform well. Thus, an</span>
<span id="cb6-155"><a href="#cb6-155"></a>efficient bandit algorithm exhibits sub-linear regret growth, meaning</span>
<span id="cb6-156"><a href="#cb6-156"></a>that the average regret per round tends to zero as the number of rounds</span>
<span id="cb6-157"><a href="#cb6-157"></a>$T$ goes to infinity: $\lim_{T \to \infty} \frac{R(T)}{T} = 0$.</span>
<span id="cb6-158"><a href="#cb6-158"></a>Minimizing regret is a cornerstone in the design of bandit algorithms,</span>
<span id="cb6-159"><a href="#cb6-159"></a>and its analysis helps in understanding the long-term efficiency and</span>
<span id="cb6-160"><a href="#cb6-160"></a>effectiveness of different bandit strategies.</span>
<span id="cb6-161"><a href="#cb6-161"></a></span>
<span id="cb6-162"><a href="#cb6-162"></a>As previously discussed, our goal is to select the bandit that minimizes</span>
<span id="cb6-163"><a href="#cb6-163"></a>a quantity that reflects regret or the cost of not selecting the optimal</span>
<span id="cb6-164"><a href="#cb6-164"></a>bandit at all times. We can leverage our comparison model to define a</span>
<span id="cb6-165"><a href="#cb6-165"></a>quantity for regret over some time horizon $T$, which is the number of</span>
<span id="cb6-166"><a href="#cb6-166"></a>decisions we make (selecting what we think is the best bandit at each</span>
<span id="cb6-167"><a href="#cb6-167"></a>iteration). Assuming we know the best bandit $b^*$ (and we know that</span>
<span id="cb6-168"><a href="#cb6-168"></a>there *is* a best bandit, since there is a total ordering of our</span>
<span id="cb6-169"><a href="#cb6-169"></a>discrete bandits), we can define two notions of regret:</span>
<span id="cb6-170"><a href="#cb6-170"></a></span>
<span id="cb6-171"><a href="#cb6-171"></a><span class="ss">-   </span>Strong regret: aims to capture the fraction of users who would</span>
<span id="cb6-172"><a href="#cb6-172"></a>    prefer the optimal bandit $b^*$ over the *worse* of the options</span>
<span id="cb6-173"><a href="#cb6-173"></a>    $b_1, b_2$ we provide at a given</span>
<span id="cb6-174"><a href="#cb6-174"></a>    step:$R_T = \sum_{t = 1}^T \text{max} \left<span class="sc">\{</span> \epsilon(b^*, b_1^{(t)}), \epsilon(b^*, b_2^{(t)}) \right<span class="sc">\}</span>$</span>
<span id="cb6-175"><a href="#cb6-175"></a></span>
<span id="cb6-176"><a href="#cb6-176"></a><span class="ss">-   </span>Weak regret: aims to capture the fraction of users who would prefer</span>
<span id="cb6-177"><a href="#cb6-177"></a>    the optimal bandit $b^*$ over the *better* of the options $b_1, b_2$</span>
<span id="cb6-178"><a href="#cb6-178"></a>    we provide at a given</span>
<span id="cb6-179"><a href="#cb6-179"></a>    step:$\tilde{R}_T = \sum_{t = 1}^T \text{min} \left<span class="sc">\{</span> \epsilon(b^*, b_1^{(t)}), \epsilon(b^*, b_2^{(t)}) \right<span class="sc">\}</span>$</span>
<span id="cb6-180"><a href="#cb6-180"></a></span>
<span id="cb6-181"><a href="#cb6-181"></a>The best bandit described in our regret definition is called a</span>
<span id="cb6-182"><a href="#cb6-182"></a>**Condorcet Winner**. This is the strongest form of winner. It's the</span>
<span id="cb6-183"><a href="#cb6-183"></a>action **$A_{i}$** which is preferred to each other action **$A_j$**</span>
<span id="cb6-184"><a href="#cb6-184"></a>with $p &gt; 0.5$ in a head-to-head election. While the above introduced</span>
<span id="cb6-185"><a href="#cb6-185"></a>notions of regret assume an overall best bandit to exist, there might be</span>
<span id="cb6-186"><a href="#cb6-186"></a>settings, where no bandit wins more than half head-to-head duels. A set</span>
<span id="cb6-187"><a href="#cb6-187"></a>of actions without a Condorcet winner is described by the following</span>
<span id="cb6-188"><a href="#cb6-188"></a>preference matrix, where each entry $\Delta_{jk}$ is</span>
<span id="cb6-189"><a href="#cb6-189"></a>$p(j \succ k) - 0.5$, the probability that action $j$ is preferred over</span>
<span id="cb6-190"><a href="#cb6-190"></a>action $k$ minus 0.5. There is no Condorcet winner as there is no action</span>
<span id="cb6-191"><a href="#cb6-191"></a>that is preferred with $p &gt; 0.5$ over all other actions. Imagine, you</span>
<span id="cb6-192"><a href="#cb6-192"></a>want to find the best pizza to eat (=action). There may not be a pizza</span>
<span id="cb6-193"><a href="#cb6-193"></a>that wins more than half of the head-to-head duels against every other</span>
<span id="cb6-194"><a href="#cb6-194"></a>pizza.</span>
<span id="cb6-195"><a href="#cb6-195"></a></span>
<span id="cb6-196"><a href="#cb6-196"></a>However, we might still have an intuition of the best pizza. Therefore</span>
<span id="cb6-197"><a href="#cb6-197"></a>Sui et al., 2018 introduce the concepts of different $\textit{winners}$</span>
<span id="cb6-198"><a href="#cb6-198"></a>in dueling bandit problems <span class="co">[</span><span class="ot">@advancements_dueling</span><span class="co">]</span>. In this example, we</span>
<span id="cb6-199"><a href="#cb6-199"></a>might define the best pizza as the most popular one. We call the Pizza</span>
<span id="cb6-200"><a href="#cb6-200"></a>receiving the most votes in a public vote the **Borda Winner**, or</span>
<span id="cb6-201"><a href="#cb6-201"></a>formally, Borda winner</span>
<span id="cb6-202"><a href="#cb6-202"></a>$j = \arg\max_{i \in A, i \neq j} \left(\sum p(j \succ i)\right)$. In</span>
<span id="cb6-203"><a href="#cb6-203"></a>contrast to the Condorcet Winner setting, there is always guaranteed to</span>
<span id="cb6-204"><a href="#cb6-204"></a>be one or more (in the case of a tie) Borda winners for a set of</span>
<span id="cb6-205"><a href="#cb6-205"></a>actions. However - if there is a Condorcet Winner, this might not</span>
<span id="cb6-206"><a href="#cb6-206"></a>necessarily be the same as a Borda Winner: In our Pizza example, a</span>
<span id="cb6-207"><a href="#cb6-207"></a>Pepperoni Pizza might win more than half of its head-to-head duels,</span>
<span id="cb6-208"><a href="#cb6-208"></a>while the Cheese-Pizza is still the most popular in a public poll.</span>
<span id="cb6-209"><a href="#cb6-209"></a></span>
<span id="cb6-210"><a href="#cb6-210"></a>A more generic concept of winner is the **Von Neumann Winner**, which</span>
<span id="cb6-211"><a href="#cb6-211"></a>describes a probability distribution rather than a single bandit winner.</span>
<span id="cb6-212"><a href="#cb6-212"></a>A Von Neumann winner simply prescribes a probability distribution $W$</span>
<span id="cb6-213"><a href="#cb6-213"></a>such that sampling from this distribution 'beats' an action from the</span>
<span id="cb6-214"><a href="#cb6-214"></a>random uniform distribution with $p &gt; 0.5$. In our pizza example, this</span>
<span id="cb6-215"><a href="#cb6-215"></a>would correspond to trusting a friend to order whichever Pizza he likes,</span>
<span id="cb6-216"><a href="#cb6-216"></a>because this may still be preferred to ordering randomly. Formally, $W$</span>
<span id="cb6-217"><a href="#cb6-217"></a>is a Von Neumann if $(j \sim W, k \sim R) <span class="co">[</span><span class="ot">p(p(j \succ k) &gt; 0.5) &gt; 0.5</span><span class="co">]</span>$</span>
<span id="cb6-218"><a href="#cb6-218"></a>where $R$ describes the uniform probability distribution over our</span>
<span id="cb6-219"><a href="#cb6-219"></a>actions. The concept of a Von Neumann winner is useful in contextual</span>
<span id="cb6-220"><a href="#cb6-220"></a>bandits, which will be introduced later. In these settings, the</span>
<span id="cb6-221"><a href="#cb6-221"></a>preference matrix depends on different context, which may have different</span>
<span id="cb6-222"><a href="#cb6-222"></a>Borda winners, just as different parties may vote for different pizzas.</span>
<span id="cb6-223"><a href="#cb6-223"></a></span>
<span id="cb6-224"><a href="#cb6-224"></a>::: {#fig-condorcet_violation}</span>
<span id="cb6-225"><a href="#cb6-225"></a>         A        B           C         D       E      F</span>
<span id="cb6-226"><a href="#cb6-226"></a>  --- ------- ---------- ----------- ------- ------- ------</span>
<span id="cb6-227"><a href="#cb6-227"></a>   A     0     **0.03**   **-0.02**   0.06    0.10    0.11</span>
<span id="cb6-228"><a href="#cb6-228"></a>   B   -0.03      0       **0.03**    0.05    0.08    0.11</span>
<span id="cb6-229"><a href="#cb6-229"></a>   C            -0.03         0       0.04    0.07    0.09</span>
<span id="cb6-230"><a href="#cb6-230"></a>   D   -0.06    -0.05       -0.04       0     0.05    0.07</span>
<span id="cb6-231"><a href="#cb6-231"></a>   E   -0.10    -0.08       -0.07     -0.05     0     0.03</span>
<span id="cb6-232"><a href="#cb6-232"></a>   F   -0.11    -0.11       -0.09     -0.07   -0.03    0</span>
<span id="cb6-233"><a href="#cb6-233"></a></span>
<span id="cb6-234"><a href="#cb6-234"></a>  : Violation of Condorcet Winner. Highlighted entries are different</span>
<span id="cb6-235"><a href="#cb6-235"></a>  from Table 1. No Condorcet winner exists as no arm could beat every</span>
<span id="cb6-236"><a href="#cb6-236"></a>  other arm.</span>
<span id="cb6-237"><a href="#cb6-237"></a>:::</span>
<span id="cb6-238"><a href="#cb6-238"></a></span>
<span id="cb6-239"><a href="#cb6-239"></a>Next, we introduce two performance measures for the planner. The</span>
<span id="cb6-240"><a href="#cb6-240"></a>**asymptotic ex-post regret** is defined as</span>
<span id="cb6-241"><a href="#cb6-241"></a>$$\text{Regret}(\mu_1, \ldots \mu_K) = T\cdot \max_i \mu_i - \sum_{i=1}^T E<span class="co">[</span><span class="ot">\mu_{I_t}</span><span class="co">]</span>.$$</span>
<span id="cb6-242"><a href="#cb6-242"></a></span>
<span id="cb6-243"><a href="#cb6-243"></a>Intuitively, this represents the difference between the reward achieved</span>
<span id="cb6-244"><a href="#cb6-244"></a>by always taking the action with the highest possible reward and the</span>
<span id="cb6-245"><a href="#cb6-245"></a>expected welfare of the recommendation algorithm (based on the actions</span>
<span id="cb6-246"><a href="#cb6-246"></a>it recommends at each timestep).</span>
<span id="cb6-247"><a href="#cb6-247"></a></span>
<span id="cb6-248"><a href="#cb6-248"></a>We also define a weaker performance measure, the **Bayesian regret**,</span>
<span id="cb6-249"><a href="#cb6-249"></a>which is defined as</span>
<span id="cb6-250"><a href="#cb6-250"></a>$$\text {Bayesian regret}=E_{\mu_1, \ldots, \mu_K \sim \text {Prior}}\left<span class="co">[</span><span class="ot">\operatorname{Regret}\left(\mu_1, \ldots, \mu_K\right)\right</span><span class="co">]</span>$$</span>
<span id="cb6-251"><a href="#cb6-251"></a></span>
<span id="cb6-252"><a href="#cb6-252"></a>With a Bayesian optimal policy, we would like either definition of</span>
<span id="cb6-253"><a href="#cb6-253"></a>regret to vanish as $T\to \infty$; we are considering "large-market</span>
<span id="cb6-254"><a href="#cb6-254"></a>optimal\" settings where there are many short-lived, rather than a few</span>
<span id="cb6-255"><a href="#cb6-255"></a>long-term, users. Note the fact that ex-post regret is prior-free makes</span>
<span id="cb6-256"><a href="#cb6-256"></a>it robust to inaccuracies on the prior.</span>
<span id="cb6-257"><a href="#cb6-257"></a></span>
<span id="cb6-258"><a href="#cb6-258"></a><span class="fu">### Acquisition Functions</span></span>
<span id="cb6-259"><a href="#cb6-259"></a></span>
<span id="cb6-260"><a href="#cb6-260"></a>Various strategies have been developed to balance the</span>
<span id="cb6-261"><a href="#cb6-261"></a>exploration-exploitation trade-off. These strategies differ in selecting</span>
<span id="cb6-262"><a href="#cb6-262"></a>arms based on past experiences and rewards.</span>
<span id="cb6-263"><a href="#cb6-263"></a></span>
<span id="cb6-264"><a href="#cb6-264"></a><span class="fu">#### Classical Acquisition Functions</span></span>
<span id="cb6-265"><a href="#cb6-265"></a></span>
<span id="cb6-266"><a href="#cb6-266"></a>**Uniform** acquisition function is the most straightforward approach where</span>
<span id="cb6-267"><a href="#cb6-267"></a>each arm is selected uniformly randomly over time. This strategy does</span>
<span id="cb6-268"><a href="#cb6-268"></a>not consider the past rewards and treats each arm equally promising</span>
<span id="cb6-269"><a href="#cb6-269"></a>regardless of the observed outcomes. It is a purely explorative strategy</span>
<span id="cb6-270"><a href="#cb6-270"></a>that ensures each arm is sampled enough to estimate its expected reward,</span>
<span id="cb6-271"><a href="#cb6-271"></a>but it does not exploit the information to optimize rewards. In</span>
<span id="cb6-272"><a href="#cb6-272"></a>mathematical terms, if $N_t(a)$ denotes the number of times arm $a$ has</span>
<span id="cb6-273"><a href="#cb6-273"></a>been selected up to time $t$, the Uniform Strategy would ensure that</span>
<span id="cb6-274"><a href="#cb6-274"></a>$N_t(a) \approx \frac{t}{K}$ for all arms $a$ as $t$ grows large:</span>
<span id="cb6-275"><a href="#cb6-275"></a>$P(a_t = a) = \frac{1}{K}$</span>
<span id="cb6-276"><a href="#cb6-276"></a></span>
<span id="cb6-277"><a href="#cb6-277"></a>The **Epsilon Greedy** is a popular method that introduces a</span>
<span id="cb6-278"><a href="#cb6-278"></a>balance between exploration and exploitation. With a small probability</span>
<span id="cb6-279"><a href="#cb6-279"></a>$\epsilon$, it explores by choosing an arm at random, and with a</span>
<span id="cb6-280"><a href="#cb6-280"></a>probability $1 - \epsilon$, it exploits by selecting the arm with the</span>
<span id="cb6-281"><a href="#cb6-281"></a>highest estimated reward so far. This strategy incrementally favors</span>
<span id="cb6-282"><a href="#cb6-282"></a>actions that have historically yielded higher rewards, but still allows</span>
<span id="cb6-283"><a href="#cb6-283"></a>for occasional exploration to discover better options potentially. The</span>
<span id="cb6-284"><a href="#cb6-284"></a>parameter $\epsilon$ is chosen based on the desired exploration level,</span>
<span id="cb6-285"><a href="#cb6-285"></a>often set between 0.01 and 0.1. $$P(a_t = a) =</span>
<span id="cb6-286"><a href="#cb6-286"></a>\begin{cases} </span>
<span id="cb6-287"><a href="#cb6-287"></a>\frac{\epsilon}{K} + 1 - \epsilon &amp; \text{if } a = \arg\max_{a'} \hat{\mu}_{a'} <span class="sc">\\</span></span>
<span id="cb6-288"><a href="#cb6-288"></a>\frac{\epsilon}{K} &amp; \text{otherwise}</span>
<span id="cb6-289"><a href="#cb6-289"></a>\end{cases}$$</span>
<span id="cb6-290"><a href="#cb6-290"></a></span>
<span id="cb6-291"><a href="#cb6-291"></a>**Upper Confidence Bound** (UCB) acquisition function takes a more</span>
<span id="cb6-292"><a href="#cb6-292"></a>sophisticated approach to the exploration-exploitation dilemma. It</span>
<span id="cb6-293"><a href="#cb6-293"></a>selects arms based on both the estimated rewards and the uncertainty or</span>
<span id="cb6-294"><a href="#cb6-294"></a>variance associated with those estimates. Specifically, it favors arms</span>
<span id="cb6-295"><a href="#cb6-295"></a>with high upper confidence bounds on the estimated rewards, which is a</span>
<span id="cb6-296"><a href="#cb6-296"></a>sum of the estimated mean and a confidence interval that decreases with</span>
<span id="cb6-297"><a href="#cb6-297"></a>the number of times the arm has been played. This ensures that arms with</span>
<span id="cb6-298"><a href="#cb6-298"></a>less certainty (those played less often) are considered more often,</span>
<span id="cb6-299"><a href="#cb6-299"></a>naturally balancing exploration with exploitation as the uncertainty is</span>
<span id="cb6-300"><a href="#cb6-300"></a>reduced over time.</span>
<span id="cb6-301"><a href="#cb6-301"></a></span>
<span id="cb6-302"><a href="#cb6-302"></a>$$P(a_t = a) =</span>
<span id="cb6-303"><a href="#cb6-303"></a>\begin{cases} </span>
<span id="cb6-304"><a href="#cb6-304"></a>1 &amp; \text{if } a = \arg\max_{a'} \left( \hat{\mu}_{a'} + \sqrt{\frac{2 \ln t}{N_t(a')}} \right) <span class="sc">\\</span></span>
<span id="cb6-305"><a href="#cb6-305"></a>0 &amp; \text{otherwise}</span>
<span id="cb6-306"><a href="#cb6-306"></a>\end{cases}$$</span>
<span id="cb6-307"><a href="#cb6-307"></a></span>
<span id="cb6-308"><a href="#cb6-308"></a><span class="fu">#### Interleaved Filter</span></span>
<span id="cb6-309"><a href="#cb6-309"></a></span>
<span id="cb6-310"><a href="#cb6-310"></a>This algorithm tries to find the best bandit (Condorcet Winner) in a</span>
<span id="cb6-311"><a href="#cb6-311"></a>discrete, limited bandit-space via pairwise comparisons of the bandits.</span>
<span id="cb6-312"><a href="#cb6-312"></a>We will now introduce the algorithm for the Interleaved Filter as</span>
<span id="cb6-313"><a href="#cb6-313"></a>provided in <span class="co">[</span><span class="ot">@YUE20121538</span><span class="co">]</span> to solve a dueling bandit setup. It starts</span>
<span id="cb6-314"><a href="#cb6-314"></a>with a randomly defined *best bandit* $\hat{b}$ and iteratively compares</span>
<span id="cb6-315"><a href="#cb6-315"></a>it to set $W$ containing the remaining bandits $b$ resulting in winning</span>
<span id="cb6-316"><a href="#cb6-316"></a>probabilities $\hat{P}_{\hat{b},b}$ and confidence interval</span>
<span id="cb6-317"><a href="#cb6-317"></a>$\hat{C}_{\hat{b},b}$. If a bandit $b$ is *confidently worse* than</span>
<span id="cb6-318"><a href="#cb6-318"></a>$\hat{b}$, it is removed from $W$. If a bandit $b'$ is *confidently</span>
<span id="cb6-319"><a href="#cb6-319"></a>better* than $\hat{b}$, it is set as new *best bandit* $\hat{b}$ and</span>
<span id="cb6-320"><a href="#cb6-320"></a>bandit $\hat{b}$ as well as every other bandit $b$ *worse* than</span>
<span id="cb6-321"><a href="#cb6-321"></a>$\hat{b}$ are removed from $W$. This is done, until $W$ is empty,</span>
<span id="cb6-322"><a href="#cb6-322"></a>leaving the final $\hat{b}$ as the predicted best bandit.</span>
<span id="cb6-323"><a href="#cb6-323"></a></span>
<span id="cb6-324"><a href="#cb6-324"></a>:::: algorithm</span>
<span id="cb6-325"><a href="#cb6-325"></a>::: algorithmic</span>
<span id="cb6-326"><a href="#cb6-326"></a>**input:** $T$, $B=<span class="sc">\{</span>b_1, \dots, b_k<span class="sc">\}</span>$ $\delta \gets 1/(TK^2)$ </span>
<span id="cb6-327"><a href="#cb6-327"></a>Choose $\hat{b} \in B$ randomly</span>
<span id="cb6-328"><a href="#cb6-328"></a>$W \gets <span class="sc">\{</span>b_1, \dots, b_k<span class="sc">\}</span> \backslash <span class="sc">\{</span>\hat{b}<span class="sc">\}</span>$ $\forall b \in W$,</span>
<span id="cb6-329"><a href="#cb6-329"></a>maintain estimate $\hat{P}_{\hat{b},b}$ of $P(\hat{b} &gt; b)$ according to</span>
<span id="cb6-330"><a href="#cb6-330"></a>(6) $\forall b \in W$, maintain $1 - \delta$ confidence interval</span>
<span id="cb6-331"><a href="#cb6-331"></a>$\hat{C}_{\hat{b},b}$ of $\hat{P}_{\hat{b},b}$ according to (7), (8)</span>
<span id="cb6-332"><a href="#cb6-332"></a>compare $\hat{b}$ and $b$ update $\hat{P}_{\hat{b},b}$,</span>
<span id="cb6-333"><a href="#cb6-333"></a>$\hat{C}_{\hat{b},b}$ $W \gets W \backslash <span class="sc">\{</span>b<span class="sc">\}</span>$</span>
<span id="cb6-334"><a href="#cb6-334"></a></span>
<span id="cb6-335"><a href="#cb6-335"></a>$W \gets W \backslash <span class="sc">\{</span>b<span class="sc">\}</span>$ $\hat{b} \gets b'$,</span>
<span id="cb6-336"><a href="#cb6-336"></a>$W \gets W \backslash <span class="sc">\{</span>b'<span class="sc">\}</span>$ $\forall b \in W$, reset</span>
<span id="cb6-337"><a href="#cb6-337"></a>$\hat{P}_{\hat{b},b}$ and $\hat{C}_{\hat{b},b}$ $\hat{T} \gets$ Total</span>
<span id="cb6-338"><a href="#cb6-338"></a>Comparisons Made $(\hat{b}, \hat{T})$</span>
<span id="cb6-339"><a href="#cb6-339"></a>:::</span>
<span id="cb6-340"><a href="#cb6-340"></a>::::</span>
<span id="cb6-341"><a href="#cb6-341"></a></span>
<span id="cb6-342"><a href="#cb6-342"></a>Parameter Initialization</span>
<span id="cb6-343"><a href="#cb6-343"></a></span>
<span id="cb6-344"><a href="#cb6-344"></a>:   In lines 1-6 of the algorithm, we take the inputs and first compute</span>
<span id="cb6-345"><a href="#cb6-345"></a>    the value $\delta$ which is used to compute our confidence</span>
<span id="cb6-346"><a href="#cb6-346"></a>    intervals. We select an initial guess of an optimal bandit $\hat{b}$</span>
<span id="cb6-347"><a href="#cb6-347"></a>    by uniformly sampling from all bandits $\mathcal{B}$. We also keep a</span>
<span id="cb6-348"><a href="#cb6-348"></a>    running set of bandit candidates $W$, which is initialized to be</span>
<span id="cb6-349"><a href="#cb6-349"></a>    $\mathcal{B} \setminus <span class="sc">\{</span>\hat{b}<span class="sc">\}</span>$. At this point, we also</span>
<span id="cb6-350"><a href="#cb6-350"></a>    initialize our empirical estimates for $\hat{P}, \hat{C}$.</span>
<span id="cb6-351"><a href="#cb6-351"></a></span>
<span id="cb6-352"><a href="#cb6-352"></a><span class="in">    Next, we will repeat several steps until our working set of bandit</span></span>
<span id="cb6-353"><a href="#cb6-353"></a><span class="in">    candidates $W$ is empty.</span></span>
<span id="cb6-354"><a href="#cb6-354"></a></span>
<span id="cb6-355"><a href="#cb6-355"></a>Update Estimates Based on Comparisons</span>
<span id="cb6-356"><a href="#cb6-356"></a></span>
<span id="cb6-357"><a href="#cb6-357"></a>:   The first step at each iteration (lines 8-11) is to look at all</span>
<span id="cb6-358"><a href="#cb6-358"></a>    candidates in $W$, and compare them to our current guess $\hat{b}$</span>
<span id="cb6-359"><a href="#cb6-359"></a>    using an oracle (e.g. by asking a human which of $\hat{b}$ or</span>
<span id="cb6-360"><a href="#cb6-360"></a>    $b \in W$ is preferred). With this new set of wins and comparisons,</span>
<span id="cb6-361"><a href="#cb6-361"></a>    we update our estimates of $\hat{P}, \hat{C}$.</span>
<span id="cb6-362"><a href="#cb6-362"></a></span>
<span id="cb6-363"><a href="#cb6-363"></a>Prune Suboptimal Bandits</span>
<span id="cb6-364"><a href="#cb6-364"></a></span>
<span id="cb6-365"><a href="#cb6-365"></a>:   In lines 12-13, with updated comparison win probabilities and</span>
<span id="cb6-366"><a href="#cb6-366"></a>    corresponding confidence intervals, we can remove bandit candidates</span>
<span id="cb6-367"><a href="#cb6-367"></a>    from $W$ that we are *confident* $\hat{b}$ is better than. The</span>
<span id="cb6-368"><a href="#cb6-368"></a>    intuition here is that we are mostly sure that our current best</span>
<span id="cb6-369"><a href="#cb6-369"></a>    guess is better than some of the candidates, and we don't need to</span>
<span id="cb6-370"><a href="#cb6-370"></a>    consider those candidates in future iterations.</span>
<span id="cb6-371"><a href="#cb6-371"></a></span>
<span id="cb6-372"><a href="#cb6-372"></a>Check for Better Bandits from Candidate Set</span>
<span id="cb6-373"><a href="#cb6-373"></a></span>
<span id="cb6-374"><a href="#cb6-374"></a>:   Now that our candidate set of bandits may be smaller, in lines 15-21</span>
<span id="cb6-375"><a href="#cb6-375"></a>    we check if there are any bandits $b'$ that we are *confident* are</span>
<span id="cb6-376"><a href="#cb6-376"></a>    better than our current best guess. If we do find such a candidate,</span>
<span id="cb6-377"><a href="#cb6-377"></a>    we remove bandits which $\hat{P}$ indicates $b$ is *likely* worse</span>
<span id="cb6-378"><a href="#cb6-378"></a>    than $\hat{b}$. Note that in this step, we do not require the</span>
<span id="cb6-379"><a href="#cb6-379"></a>    probability to be outside the confidence interval, since we already</span>
<span id="cb6-380"><a href="#cb6-380"></a>    found one we believe to be significantly closer to optimal than our</span>
<span id="cb6-381"><a href="#cb6-381"></a>    current best guess.</span>
<span id="cb6-382"><a href="#cb6-382"></a></span>
<span id="cb6-383"><a href="#cb6-383"></a><span class="in">    Once we remove the candidates *likely* worse than $\hat{b}$, we</span></span>
<span id="cb6-384"><a href="#cb6-384"></a><span class="in">    crown $b'$ as the new best guess, e.g. $\hat{b} := b'$.</span></span>
<span id="cb6-385"><a href="#cb6-385"></a><span class="in">    Consequently, we remove $b'$ from $W$ and reset our empirical win</span></span>
<span id="cb6-386"><a href="#cb6-386"></a><span class="in">    counters $\hat{P}, \hat{C}$.</span></span>
<span id="cb6-387"><a href="#cb6-387"></a></span>
<span id="cb6-388"><a href="#cb6-388"></a>With this algorithm defined, let us look at some provisions of the</span>
<span id="cb6-389"><a href="#cb6-389"></a>method with respect to identifying the optimal strategy. Note that the</span>
<span id="cb6-390"><a href="#cb6-390"></a>proofs and derivations for these quantities are provided in</span>
<span id="cb6-391"><a href="#cb6-391"></a><span class="co">[</span><span class="ot">@YUE20121538</span><span class="co">]</span>.</span>
<span id="cb6-392"><a href="#cb6-392"></a></span>
<span id="cb6-393"><a href="#cb6-393"></a>First, the method guarantees that for the provided time horizon $T$, the</span>
<span id="cb6-394"><a href="#cb6-394"></a>algorithm returns the correct bandit with probability</span>
<span id="cb6-395"><a href="#cb6-395"></a>$P \ge 1 - \frac{1}{T}$. It is interesting and useful to note that if</span>
<span id="cb6-396"><a href="#cb6-396"></a>one has a strict requirement for the probability of identifying the</span>
<span id="cb6-397"><a href="#cb6-397"></a>correct bandit, one can compute the time horizon $T$ that guarantees</span>
<span id="cb6-398"><a href="#cb6-398"></a>this outcome at that probability. Furthermore, a time horizon of 1</span>
<span id="cb6-399"><a href="#cb6-399"></a>leaves no probabilistic guarantee of a successful outcome, and</span>
<span id="cb6-400"><a href="#cb6-400"></a>increasing $T$ has diminishing returns. Second, in the event that the</span>
<span id="cb6-401"><a href="#cb6-401"></a>algorithm returns an incorrect bandit, the maximal regret incurred is</span>
<span id="cb6-402"><a href="#cb6-402"></a>linear with respect to $T$, e.g. $\mathcal(O)(T)$. This is also a useful</span>
<span id="cb6-403"><a href="#cb6-403"></a>provision as it allows us to estimate the overall cost in the worst case</span>
<span id="cb6-404"><a href="#cb6-404"></a>outcome. Based on these two provisions, we can compute the expected</span>
<span id="cb6-405"><a href="#cb6-405"></a>cumulative regret from running the Interleaved Filter algorithm, which</span>
<span id="cb6-406"><a href="#cb6-406"></a>is:</span>
<span id="cb6-407"><a href="#cb6-407"></a>$$\mathbb{E}\left<span class="co">[</span><span class="ot">R_T\right</span><span class="co">]</span> \le \left(1 - \frac{1}{T}\right) \mathbb{E}\left<span class="co">[</span><span class="ot"> R_T^{IF} \right</span><span class="co">]</span> + \frac{1}{T}\mathcal{O}(T) <span class="sc">\\</span></span>
<span id="cb6-408"><a href="#cb6-408"></a>= \mathcal{O}\left(\mathbb{E}\left<span class="co">[</span><span class="ot"> R_T^{IF} \right</span><span class="co">]</span> + 1\right)$$</span>
<span id="cb6-409"><a href="#cb6-409"></a></span>
<span id="cb6-410"><a href="#cb6-410"></a>Interestingly, the original work shows that these bounds hold for both</span>
<span id="cb6-411"><a href="#cb6-411"></a>strong and weak regret. As demonstrated, the Interleaved Filter</span>
<span id="cb6-412"><a href="#cb6-412"></a>algorithm <span class="co">[</span><span class="ot">\[fig-if\]</span><span class="co">](#fig-if)</span>{reference-type="ref" reference="fig-if"}</span>
<span id="cb6-413"><a href="#cb6-413"></a>provides a robust method to ascertain the optimal bandit or strategy</span>
<span id="cb6-414"><a href="#cb6-414"></a>given a set of options and only noisy comparisons. In most real-world</span>
<span id="cb6-415"><a href="#cb6-415"></a>scenarios for modeling human preferences, it is not possible to observe</span>
<span id="cb6-416"><a href="#cb6-416"></a>a real-world reward value, or at least a reliable one and as such this</span>
<span id="cb6-417"><a href="#cb6-417"></a>method is a useful way to properly model human preferences.</span>
<span id="cb6-418"><a href="#cb6-418"></a></span>
<span id="cb6-419"><a href="#cb6-419"></a>Furthermore, the algorithm provides strong guarantees for the</span>
<span id="cb6-420"><a href="#cb6-420"></a>probability of selecting the correct bandit, maximal regret, and the</span>
<span id="cb6-421"><a href="#cb6-421"></a>number of comparisons required. It is even more impressive that the</span>
<span id="cb6-422"><a href="#cb6-422"></a>method can do so without severely limiting constraints; as demonstrated,</span>
<span id="cb6-423"><a href="#cb6-423"></a>the most commonly used models satisfy the imposed constraints.</span>
<span id="cb6-424"><a href="#cb6-424"></a></span>
<span id="cb6-425"><a href="#cb6-425"></a>As we look to model human preferences, we can certainly leverage this</span>
<span id="cb6-426"><a href="#cb6-426"></a>method for k-armed dueling bandits to identify the best strategy to</span>
<span id="cb6-427"><a href="#cb6-427"></a>solve human-centric challenges, from video recommendation to meal</span>
<span id="cb6-428"><a href="#cb6-428"></a>selection and exoskeleton-assisted walking.</span>
<span id="cb6-429"><a href="#cb6-429"></a></span>
<span id="cb6-430"><a href="#cb6-430"></a><span class="fu">#### Dueling Bandit Gradient Descent</span></span>
<span id="cb6-431"><a href="#cb6-431"></a></span>
<span id="cb6-432"><a href="#cb6-432"></a>This algorithm tries to find the best bandit in a continuous</span>
<span id="cb6-433"><a href="#cb6-433"></a>bandit-space. Here, the set of all bandits is regarded as an</span>
<span id="cb6-434"><a href="#cb6-434"></a>Information-Retrieval (IR) system with infinite bandits uniquely defined</span>
<span id="cb6-435"><a href="#cb6-435"></a>by $w$. We will cover the *Dueling Bandit Gradient Descent* algorithm</span>
<span id="cb6-436"><a href="#cb6-436"></a>from Yue and Joachims 2009 <span class="co">[</span><span class="ot">@IR</span><span class="co">]</span>. Yue and Joachims use the dueling</span>
<span id="cb6-437"><a href="#cb6-437"></a>bandits formulation for online IR optimization. They propose a retrieval</span>
<span id="cb6-438"><a href="#cb6-438"></a>system parameterized by a set of continuous variables lying in $W$, a</span>
<span id="cb6-439"><a href="#cb6-439"></a>$d$-dimensional unit-sphere. The DBGD algorithm adapts the current</span>
<span id="cb6-440"><a href="#cb6-440"></a>parameters $w_t$ of IR system by comparison with slightly altered</span>
<span id="cb6-441"><a href="#cb6-441"></a>parameters $w_t'$ both querying query $q_t$. Only if the IR outcome</span>
<span id="cb6-442"><a href="#cb6-442"></a>using $w_t'$ is preferred, the parameters are changed in their</span>
<span id="cb6-443"><a href="#cb6-443"></a>direction. We will now discuss the algorithm more detailed.</span>
<span id="cb6-444"><a href="#cb6-444"></a></span>
<span id="cb6-445"><a href="#cb6-445"></a>:::: algorithm</span>
<span id="cb6-446"><a href="#cb6-446"></a>::: algorithmic</span>
<span id="cb6-447"><a href="#cb6-447"></a>**input:** $\gamma$, $\delta$, $w_1$ </span>
<span id="cb6-448"><a href="#cb6-448"></a></span>
<span id="cb6-449"><a href="#cb6-449"></a>Sample unit vector $u_t$ uniformly</span>
<span id="cb6-450"><a href="#cb6-450"></a></span>
<span id="cb6-451"><a href="#cb6-451"></a>$w_t' \gets P_W(w_t + \delta u_t)$ </span>
<span id="cb6-452"><a href="#cb6-452"></a></span>
<span id="cb6-453"><a href="#cb6-453"></a>Compare $w_t$ and $w_t'$</span>
<span id="cb6-454"><a href="#cb6-454"></a></span>
<span id="cb6-455"><a href="#cb6-455"></a>$w_{t+1} \gets P_W(w_t + \gamma u_t)$ </span>
<span id="cb6-456"><a href="#cb6-456"></a></span>
<span id="cb6-457"><a href="#cb6-457"></a>$w_{t+1} \gets w_t$</span>
<span id="cb6-458"><a href="#cb6-458"></a>:::</span>
<span id="cb6-459"><a href="#cb6-459"></a>::::</span>
<span id="cb6-460"><a href="#cb6-460"></a></span>
<span id="cb6-461"><a href="#cb6-461"></a>We first choose exploration step length $\delta$, exploitation step</span>
<span id="cb6-462"><a href="#cb6-462"></a>length $\gamma$, and starting point (in unit-sphere) $w_1$. Choose a</span>
<span id="cb6-463"><a href="#cb6-463"></a>query and sample a random unit vector $u_t$. We duel $w_t$ and $w_t'$,</span>
<span id="cb6-464"><a href="#cb6-464"></a>where $w_t$ is our current point in the sphere, and $w_t'$ is our</span>
<span id="cb6-465"><a href="#cb6-465"></a>exploratory comparison, which is generated by taking a random step of</span>
<span id="cb6-466"><a href="#cb6-466"></a>length $\delta$, such that $w_t' = w_t + \delta u_t$. The objective of</span>
<span id="cb6-467"><a href="#cb6-467"></a>this duel is to ascertain the binary preference of users with respect to</span>
<span id="cb6-468"><a href="#cb6-468"></a>the results yielded by the IR systems parameterized by $w_t$ and $w_t'$</span>
<span id="cb6-469"><a href="#cb6-469"></a>respectively, taking query $q_t$ as an input. The parameters that get</span>
<span id="cb6-470"><a href="#cb6-470"></a>the majority of the votes in the head to head win. If $w_t$ wins, then</span>
<span id="cb6-471"><a href="#cb6-471"></a>we keep the parameters for the next iteration. If $w_t'$ wins the duel,</span>
<span id="cb6-472"><a href="#cb6-472"></a>we update our parameters in the direction of $u_t$ by taking a step of</span>
<span id="cb6-473"><a href="#cb6-473"></a>length $\gamma$. Note that the algorithm describes projection operation</span>
<span id="cb6-474"><a href="#cb6-474"></a>$P_W(\overrightarrow{v})$. Since $u_t$ is chosen randomly,</span>
<span id="cb6-475"><a href="#cb6-475"></a>$w_t + \delta u_t$ or $w_t + \gamma u_t$ could exist outside of the unit</span>
<span id="cb6-476"><a href="#cb6-476"></a>sphere where all possible parameter configurations lie. In this case, we</span>
<span id="cb6-477"><a href="#cb6-477"></a>simply project the point back onto the sphere using said projection</span>
<span id="cb6-478"><a href="#cb6-478"></a>$P_W(\overrightarrow{v})$.</span>
<span id="cb6-479"><a href="#cb6-479"></a></span>
<span id="cb6-480"><a href="#cb6-480"></a>Yue and Joachims show that this algorithm has sublinear regret in $T$,</span>
<span id="cb6-481"><a href="#cb6-481"></a>the number of iterations. We note that the algorithm assumes that there</span>
<span id="cb6-482"><a href="#cb6-482"></a>exists a hidden reward function $R(w)$ that maps system parameters $w_t$</span>
<span id="cb6-483"><a href="#cb6-483"></a>to a reward value which is smooth and strictly concave over the input</span>
<span id="cb6-484"><a href="#cb6-484"></a>space $W$.</span>
<span id="cb6-485"><a href="#cb6-485"></a></span>
<span id="cb6-486"><a href="#cb6-486"></a>Lastly, we would also like to give motivation behind $\delta$ and</span>
<span id="cb6-487"><a href="#cb6-487"></a>$\gamma$ being different values. We need a $\delta$ that is sufficiently</span>
<span id="cb6-488"><a href="#cb6-488"></a>large that the comparison between a system parameterized by $w_t$ and</span>
<span id="cb6-489"><a href="#cb6-489"></a>$w_t'$ is meaningful. On the other hand, we may wish to take a smaller</span>
<span id="cb6-490"><a href="#cb6-490"></a>step in the direction of $w_t'$ during our update step, as during a</span>
<span id="cb6-491"><a href="#cb6-491"></a>duel, we only score $w_t$ against $w_t'$ over the results on one query</span>
<span id="cb6-492"><a href="#cb6-492"></a>$q_t$. Having $\delta &gt; \gamma$ allows us to get reward signal from</span>
<span id="cb6-493"><a href="#cb6-493"></a>meaningfully different points while also updating our belief of the best</span>
<span id="cb6-494"><a href="#cb6-494"></a>point $w_{\text{best}}$ gradually.</span>
<span id="cb6-495"><a href="#cb6-495"></a></span>
<span id="cb6-496"><a href="#cb6-496"></a><span class="fu">#### Sparring EXP4 {#sparring-exp4 .unnumbered}</span></span>
<span id="cb6-497"><a href="#cb6-497"></a></span>
<span id="cb6-498"><a href="#cb6-498"></a>Zoghi et al. 2015 propose one algorithm for this problem --- sparring</span>
<span id="cb6-499"><a href="#cb6-499"></a>EXP4, which duels two traditional EXP4 - algorithms. The (traditional)</span>
<span id="cb6-500"><a href="#cb6-500"></a>EXP4 algorithm solves the traditional contextual bandits --- the case</span>
<span id="cb6-501"><a href="#cb6-501"></a>where we can directly observe a reward for a choice of bandit given a</span>
<span id="cb6-502"><a href="#cb6-502"></a>context. The EXP4 algorithm embeds each bandit as a vector. When the</span>
<span id="cb6-503"><a href="#cb6-503"></a>algorithm sees the context (called 'advice' in this formulation), it</span>
<span id="cb6-504"><a href="#cb6-504"></a>produces a probability distribution over the choices based on an</span>
<span id="cb6-505"><a href="#cb6-505"></a>adjusted softmax function on the inner product between the context and</span>
<span id="cb6-506"><a href="#cb6-506"></a>the bandit vectors. The probability function is different from a softmax</span>
<span id="cb6-507"><a href="#cb6-507"></a>as we assign some minimum probability that any action gets chosen to</span>
<span id="cb6-508"><a href="#cb6-508"></a>enforce exploration. A reward is then observed for the choice and</span>
<span id="cb6-509"><a href="#cb6-509"></a>propagated back through the embedding of the chosen bandit.</span>
<span id="cb6-510"><a href="#cb6-510"></a></span>
<span id="cb6-511"><a href="#cb6-511"></a>Sparring EXP4 runs two instances of the EXP4 algorithm against each</span>
<span id="cb6-512"><a href="#cb6-512"></a>other. Each EXP4 instance samples an action given a context, and then</span>
<span id="cb6-513"><a href="#cb6-513"></a>these choices are 'dueled' against each other. Instead of directly</span>
<span id="cb6-514"><a href="#cb6-514"></a>observing a reward, as for traditional EXP4, we instead observe two</span>
<span id="cb6-515"><a href="#cb6-515"></a>converse reward --- a positive reward for the choice that won the duel</span>
<span id="cb6-516"><a href="#cb6-516"></a>and a negative reward to the choice that lost. The reward is</span>
<span id="cb6-517"><a href="#cb6-517"></a>proportional to the degree to which the bandit wins the duel, i.e. how</span>
<span id="cb6-518"><a href="#cb6-518"></a>likely the bandit is to be preferred over the other when users are</span>
<span id="cb6-519"><a href="#cb6-519"></a>queried for binary preferences. Like in traditional EXP4, the reward or</span>
<span id="cb6-520"><a href="#cb6-520"></a>negative reward is then propagated back through the representations of</span>
<span id="cb6-521"><a href="#cb6-521"></a>the bandits.</span>
<span id="cb6-522"><a href="#cb6-522"></a></span>
<span id="cb6-523"><a href="#cb6-523"></a><span class="fu">#### Feel-good Thompson sampling</span></span>
<span id="cb6-524"><a href="#cb6-524"></a></span>
<span id="cb6-525"><a href="#cb6-525"></a>This algorithm is a solution for the contextual dueling bandit setting,</span>
<span id="cb6-526"><a href="#cb6-526"></a>and tries to minimize cumulative average regret (= find WHAT WINNER?!Von</span>
<span id="cb6-527"><a href="#cb6-527"></a>Neumann???):</span>
<span id="cb6-528"><a href="#cb6-528"></a>$$\text{Regret}(T) := \sum_{t=1}^{T} \left<span class="co">[</span><span class="ot"> r_{*}(x_t, a_{t}^{*}) - \frac{r_{*}(x_t, a_{t}^{1}) + r_{*}(x_t, a_{t}^{2})}{2} \right</span><span class="co">]</span>,$$</span>
<span id="cb6-529"><a href="#cb6-529"></a>where $r_{*}(x_t, a_{t})$ is the true, hidden reward function of a</span>
<span id="cb6-530"><a href="#cb6-530"></a>context $x_t$ and action $a_t$. Thompson sampling is an iterative</span>
<span id="cb6-531"><a href="#cb6-531"></a>process of receiving preference over two actions, each maximizing a</span>
<span id="cb6-532"><a href="#cb6-532"></a>different approximation of the reward function based on past data and</span>
<span id="cb6-533"><a href="#cb6-533"></a>adding this new information to the data.</span>
<span id="cb6-534"><a href="#cb6-534"></a></span>
<span id="cb6-535"><a href="#cb6-535"></a>Finding good approximations of the reward function at time $t$ is done</span>
<span id="cb6-536"><a href="#cb6-536"></a>by sampling two reward function parameters $\theta_t^{j=1}$ and</span>
<span id="cb6-537"><a href="#cb6-537"></a>$\theta_t^{j=2}$ from a posterior distribution based on all previous</span>
<span id="cb6-538"><a href="#cb6-538"></a>data $p_j(\cdot \mid S_{t-1})$. This posterior distribution is</span>
<span id="cb6-539"><a href="#cb6-539"></a>proportional to the multiplication of the prior and the likelihood</span>
<span id="cb6-540"><a href="#cb6-540"></a>function, which is a Gaussian in standard Thompson sampling. In</span>
<span id="cb6-541"><a href="#cb6-541"></a>Feel-Good Thompson sampling, an additional term called \"Feel-good</span>
<span id="cb6-542"><a href="#cb6-542"></a>exploration\" encourages parameters $\theta$ with a large maximum reward</span>
<span id="cb6-543"><a href="#cb6-543"></a>in previous rounds. This change to the likelihood function may increase</span>
<span id="cb6-544"><a href="#cb6-544"></a>probabilities in uncertain areas, thus exploring those regions. All</span>
<span id="cb6-545"><a href="#cb6-545"></a>that's left is to select an action maximizing each reward function</span>
<span id="cb6-546"><a href="#cb6-546"></a>approximation and receive a preference $y_t$ on one of them to add the</span>
<span id="cb6-547"><a href="#cb6-547"></a>new information to the dataset<span class="co">[</span><span class="ot">@fgts_cdb</span><span class="co">]</span>.</span>
<span id="cb6-548"><a href="#cb6-548"></a></span>
<span id="cb6-549"><a href="#cb6-549"></a>:::: algorithm</span>
<span id="cb6-550"><a href="#cb6-550"></a>::: algorithmic</span>
<span id="cb6-551"><a href="#cb6-551"></a>Initialize $S_0 = \varnothing$. Receive prompt $x_t$ and action space</span>
<span id="cb6-552"><a href="#cb6-552"></a>$\mathcal{A}_t$. Sample model parameter $\theta_t^j$ from the posterior</span>
<span id="cb6-553"><a href="#cb6-553"></a>distribution $p^j(\cdot \mid S_{t-1})$ Select response</span>
<span id="cb6-554"><a href="#cb6-554"></a>$a_t^j = \arg\max_{a \in \mathcal{A}_t} \langle \theta_t^j, \phi(x_t, a) \rangle$.</span>
<span id="cb6-555"><a href="#cb6-555"></a>Receive preference $y_t$. Update dataset</span>
<span id="cb6-556"><a href="#cb6-556"></a>$S_t \leftarrow S_{t-1} \cup <span class="sc">\{</span>(x_t, a_t^1, a_t^2, y_t)<span class="sc">\}</span>$.</span>
<span id="cb6-557"><a href="#cb6-557"></a>:::</span>
<span id="cb6-558"><a href="#cb6-558"></a>::::</span>
<span id="cb6-559"><a href="#cb6-559"></a></span>
<span id="cb6-560"><a href="#cb6-560"></a><span class="fu">### Applications</span></span>
<span id="cb6-561"><a href="#cb6-561"></a></span>
<span id="cb6-562"><a href="#cb6-562"></a>There are many applications where contextual bandits are used. Many of</span>
<span id="cb6-563"><a href="#cb6-563"></a>these applications can utilize human preferences. One particular</span>
<span id="cb6-564"><a href="#cb6-564"></a>application illustrates the benefits a contextual bandit would have over</span>
<span id="cb6-565"><a href="#cb6-565"></a>a multi-armed bandit: a website deciding which app to show someone</span>
<span id="cb6-566"><a href="#cb6-566"></a>visiting the website. A multi-armed bandit might decide to show someone</span>
<span id="cb6-567"><a href="#cb6-567"></a>an ad for a swimsuit because the swimsuit ads have gotten the most user</span>
<span id="cb6-568"><a href="#cb6-568"></a>clicks (which indicates human preference). A contextual bandit might</span>
<span id="cb6-569"><a href="#cb6-569"></a>choose differently, however. A contextual bandit will also take into</span>
<span id="cb6-570"><a href="#cb6-570"></a>account the context, which in this case might mean information about the</span>
<span id="cb6-571"><a href="#cb6-571"></a>user (location, previously visited pages, and device information). If it</span>
<span id="cb6-572"><a href="#cb6-572"></a>discovers the user lives in a cold environment, for example, it might</span>
<span id="cb6-573"><a href="#cb6-573"></a>suggest a sweater ad for the user instead and get a better chance of a</span>
<span id="cb6-574"><a href="#cb6-574"></a>click. There are many more examples of where contextual bandits can be</span>
<span id="cb6-575"><a href="#cb6-575"></a>applied. They can be applied in other web applications, such as to</span>
<span id="cb6-576"><a href="#cb6-576"></a>optimize search results, medical applications, such as how much of a</span>
<span id="cb6-577"><a href="#cb6-577"></a>medication to prescribe based on a patient's history, and gaming</span>
<span id="cb6-578"><a href="#cb6-578"></a>applications, such as basing moves off of the state of a chess board to</span>
<span id="cb6-579"><a href="#cb6-579"></a>try to win. In each of the above examples, human feedback could have</span>
<span id="cb6-580"><a href="#cb6-580"></a>been introduced during training and leveraged to learn a reward</span>
<span id="cb6-581"><a href="#cb6-581"></a>function.</span>
<span id="cb6-582"><a href="#cb6-582"></a></span>
<span id="cb6-583"><a href="#cb6-583"></a>We explored different versions of bandits that address the</span>
<span id="cb6-584"><a href="#cb6-584"></a>exploration-exploitation trade-off in various real-world scenarios.</span>
<span id="cb6-585"><a href="#cb6-585"></a>These models have been employed across various fields, including but not</span>
<span id="cb6-586"><a href="#cb6-586"></a>limited to healthcare, finance, dynamic pricing, and anomaly detection.</span>
<span id="cb6-587"><a href="#cb6-587"></a>This section provides a deep dive into some real-world applications,</span>
<span id="cb6-588"><a href="#cb6-588"></a>emphasizing the value and advancements achieved by incorporating bandit</span>
<span id="cb6-589"><a href="#cb6-589"></a>methodologies. The content of this section draws upon the findings from</span>
<span id="cb6-590"><a href="#cb6-590"></a>the survey cited in reference <span class="co">[</span><span class="ot">@bouneffouf2020survey</span><span class="co">]</span>.</span>
<span id="cb6-591"><a href="#cb6-591"></a></span>
<span id="cb6-592"><a href="#cb6-592"></a>In healthcare, researchers have been applying bandits to address</span>
<span id="cb6-593"><a href="#cb6-593"></a>challenges in clinical trials and behavioral</span>
<span id="cb6-594"><a href="#cb6-594"></a>modeling <span class="co">[</span><span class="ot">@bouneffouf2017bandit; @bastani2020online</span><span class="co">]</span>. One of the</span>
<span id="cb6-595"><a href="#cb6-595"></a>examples is drug dosing. Warfarin, an oral anticoagulant, has</span>
<span id="cb6-596"><a href="#cb6-596"></a>traditionally been administered using fixed dosing protocols. Physicians</span>
<span id="cb6-597"><a href="#cb6-597"></a>would then make subsequent adjustments based on the patient's emerging</span>
<span id="cb6-598"><a href="#cb6-598"></a>symptoms. Nonetheless, inaccuracies in the initial dosage---whether too</span>
<span id="cb6-599"><a href="#cb6-599"></a>low or too high---can lead to serious complications like strokes and</span>
<span id="cb6-600"><a href="#cb6-600"></a>internal bleeding. In a pivotal study, researchers</span>
<span id="cb6-601"><a href="#cb6-601"></a>in <span class="co">[</span><span class="ot">@bastani2020online</span><span class="co">]</span> modeled the Warfarin initial dosing as a</span>
<span id="cb6-602"><a href="#cb6-602"></a>contextual bandit problem to assign dosages to individual patients</span>
<span id="cb6-603"><a href="#cb6-603"></a>appropriately based on their medication history. Their contributions</span>
<span id="cb6-604"><a href="#cb6-604"></a>include the adaptation of the LASSO estimator to the bandit setting,</span>
<span id="cb6-605"><a href="#cb6-605"></a>achieving a theoretical regret bound of $O({s_0}^2 \log^2(dT)$, where</span>
<span id="cb6-606"><a href="#cb6-606"></a>$d$ represents the number of covariates, $s_0 &lt;&lt; d$ signifies the number</span>
<span id="cb6-607"><a href="#cb6-607"></a>of pertinent covariates, and $T$ indicates the total number of users.</span>
<span id="cb6-608"><a href="#cb6-608"></a>Additionally, they conducted empirical experiments to validate the</span>
<span id="cb6-609"><a href="#cb6-609"></a>robustness of their methodology.</span>
<span id="cb6-610"><a href="#cb6-610"></a></span>
<span id="cb6-611"><a href="#cb6-611"></a>Within the finance sector, bandits have been instrumental in reshaping</span>
<span id="cb6-612"><a href="#cb6-612"></a>the landscape of portfolio optimization. Portfolio optimization is an</span>
<span id="cb6-613"><a href="#cb6-613"></a>approach to designing a portfolio based on the investor's return and</span>
<span id="cb6-614"><a href="#cb6-614"></a>risk criteria, which fits the exploration-exploitation nature of the</span>
<span id="cb6-615"><a href="#cb6-615"></a>bandit problems. <span class="co">[</span><span class="ot">@shen2015portfolio</span><span class="co">]</span> utilized multi-armed bandits to</span>
<span id="cb6-616"><a href="#cb6-616"></a>exploit correlations between the instruments. They constructed</span>
<span id="cb6-617"><a href="#cb6-617"></a>orthogonal portfolios and integrated them with the UCB policy to achieve</span>
<span id="cb6-618"><a href="#cb6-618"></a>a cumulative regret bound of $\frac{8n}{\Delta*} \ln(m) + 5n$, where</span>
<span id="cb6-619"><a href="#cb6-619"></a>$n$, $m$, and $\Delta*$ denotes the number of available assets, total</span>
<span id="cb6-620"><a href="#cb6-620"></a>time steps, and the gap between the best-expected reward and the</span>
<span id="cb6-621"><a href="#cb6-621"></a>expected reward. On the other hand, <span class="co">[</span><span class="ot">@huo2017risk</span><span class="co">]</span> focused on</span>
<span id="cb6-622"><a href="#cb6-622"></a>risk-awareness online portfolio optimization by incorporating a compute</span>
<span id="cb6-623"><a href="#cb6-623"></a>of the minimum spanning tree in the bipartite graph, which encodes a</span>
<span id="cb6-624"><a href="#cb6-624"></a>combination of financial institutions and assets that helps diversify</span>
<span id="cb6-625"><a href="#cb6-625"></a>and reduce exposure to systematic risk during the financial crisis.</span>
<span id="cb6-626"><a href="#cb6-626"></a></span>
<span id="cb6-627"><a href="#cb6-627"></a>Dynamic pricing, also known as demand-based pricing, refers to the</span>
<span id="cb6-628"><a href="#cb6-628"></a>strategy of setting flexible prices for products or services based on</span>
<span id="cb6-629"><a href="#cb6-629"></a>current market demands. The application of bandits in dynamic pricing</span>
<span id="cb6-630"><a href="#cb6-630"></a>offers a systematic approach to making real-time pricing decisions while</span>
<span id="cb6-631"><a href="#cb6-631"></a>balancing the trade-off between exploring new price points and</span>
<span id="cb6-632"><a href="#cb6-632"></a>exploiting known optimal prices. <span class="co">[</span><span class="ot">@misra2019dynamic</span><span class="co">]</span> proposed a policy</span>
<span id="cb6-633"><a href="#cb6-633"></a>where the company has only incomplete demand information. They derived</span>
<span id="cb6-634"><a href="#cb6-634"></a>an algorithm that balances immediate and future profits by combining</span>
<span id="cb6-635"><a href="#cb6-635"></a>multi-armed bandits with partial identification of consumer demand from</span>
<span id="cb6-636"><a href="#cb6-636"></a>economic theory.</span>
<span id="cb6-637"><a href="#cb6-637"></a></span>
<span id="cb6-638"><a href="#cb6-638"></a>are essential components of numerous online platforms, guiding users</span>
<span id="cb6-639"><a href="#cb6-639"></a>through vast content landscapes to deliver tailored suggestions. These</span>
<span id="cb6-640"><a href="#cb6-640"></a>systems are instrumental in platforms like e-commerce sites, streaming</span>
<span id="cb6-641"><a href="#cb6-641"></a>platforms, and social media networks. However, the challenge of</span>
<span id="cb6-642"><a href="#cb6-642"></a>effectively recommending items to users is non-trivial, given the</span>
<span id="cb6-643"><a href="#cb6-643"></a>dynamic nature of user preferences and the vast amount of content</span>
<span id="cb6-644"><a href="#cb6-644"></a>available.</span>
<span id="cb6-645"><a href="#cb6-645"></a></span>
<span id="cb6-646"><a href="#cb6-646"></a>One of the most significant challenges in recommendation systems is the</span>
<span id="cb6-647"><a href="#cb6-647"></a>\"cold start\" problem. This issue arises when a new user joins a</span>
<span id="cb6-648"><a href="#cb6-648"></a>platform, and the system has limited or no information about the user's</span>
<span id="cb6-649"><a href="#cb6-649"></a>preferences. Traditional recommendation algorithms struggle in such</span>
<span id="cb6-650"><a href="#cb6-650"></a>scenarios since they rely on historical user-item interactions. As</span>
<span id="cb6-651"><a href="#cb6-651"></a>discussed in <span class="co">[</span><span class="ot">@zhou2017large</span><span class="co">]</span>, the bandit setting is particularly</span>
<span id="cb6-652"><a href="#cb6-652"></a>suitable for large-scale recommender systems with a vast number of</span>
<span id="cb6-653"><a href="#cb6-653"></a>items. By continuously exploring user preferences and exploiting known</span>
<span id="cb6-654"><a href="#cb6-654"></a>interactions, bandit-based recommender systems can quickly adapt to new</span>
<span id="cb6-655"><a href="#cb6-655"></a>users, ensuring relevant recommendations in a few interactions. The</span>
<span id="cb6-656"><a href="#cb6-656"></a>continuous exploration inherent in bandit approaches also means that as</span>
<span id="cb6-657"><a href="#cb6-657"></a>a user's preferences evolve, the system can adapt, ensuring that</span>
<span id="cb6-658"><a href="#cb6-658"></a>recommendations remain relevant. Recommending content that is up to date</span>
<span id="cb6-659"><a href="#cb6-659"></a>is also another important aspect of a recommendation system.</span>
<span id="cb6-660"><a href="#cb6-660"></a>In <span class="co">[</span><span class="ot">@bouneffouf2012a</span><span class="co">]</span>, the concept of \"freshness\" in content is</span>
<span id="cb6-661"><a href="#cb6-661"></a>explored through the lens of the bandit problem. The Freshness-Aware</span>
<span id="cb6-662"><a href="#cb6-662"></a>Thompson Sampling algorithm introduced in this study aims to manage the</span>
<span id="cb6-663"><a href="#cb6-663"></a>recommendation of fresh documents according to the user's risk of the</span>
<span id="cb6-664"><a href="#cb6-664"></a>situation.</span>
<span id="cb6-665"><a href="#cb6-665"></a></span>
<span id="cb6-666"><a href="#cb6-666"></a>Dialogue systems, often termed conversational agents or chatbots, aim to</span>
<span id="cb6-667"><a href="#cb6-667"></a>simulate human-like conversations with users. These systems are deployed</span>
<span id="cb6-668"><a href="#cb6-668"></a>across various platforms, including customer support, virtual</span>
<span id="cb6-669"><a href="#cb6-669"></a>assistants, and entertainment applications, and they are crucial for</span>
<span id="cb6-670"><a href="#cb6-670"></a>enhancing user experience and engagement. Response selection is</span>
<span id="cb6-671"><a href="#cb6-671"></a>fundamental to creating a natural and coherent dialogue flow.</span>
<span id="cb6-672"><a href="#cb6-672"></a>Traditional dialogue systems rely on a predefined set of responses or</span>
<span id="cb6-673"><a href="#cb6-673"></a>rules, which can make interactions feel scripted and inauthentic.</span>
<span id="cb6-674"><a href="#cb6-674"></a>In <span class="co">[</span><span class="ot">@liu2018customized</span><span class="co">]</span>, the authors proposed a contextual multi-armed</span>
<span id="cb6-675"><a href="#cb6-675"></a>bandit model for online learning of response selection. Specifically,</span>
<span id="cb6-676"><a href="#cb6-676"></a>they utilized bidirectional LSTM to produce the distributed</span>
<span id="cb6-677"><a href="#cb6-677"></a>representations of a dialogue context and responses and customized the</span>
<span id="cb6-678"><a href="#cb6-678"></a>Thompson sampling method.</span>
<span id="cb6-679"><a href="#cb6-679"></a></span>
<span id="cb6-680"><a href="#cb6-680"></a>To create a more engaging and dynamic interaction, there's a growing</span>
<span id="cb6-681"><a href="#cb6-681"></a>interest in developing pro-active dialogue systems that can initiate</span>
<span id="cb6-682"><a href="#cb6-682"></a>conversations without user initiation. <span class="co">[</span><span class="ot">@perez2018contextual</span><span class="co">]</span> proposed a</span>
<span id="cb6-683"><a href="#cb6-683"></a>novel approach to this challenge with contextual bandits. By introducing</span>
<span id="cb6-684"><a href="#cb6-684"></a>memory models into the bandit framework, the system can recall past</span>
<span id="cb6-685"><a href="#cb6-685"></a>interactions, making its proactive responses more contextually relevant.</span>
<span id="cb6-686"><a href="#cb6-686"></a>Their contributions include the Contextual Attentive Memory Network,</span>
<span id="cb6-687"><a href="#cb6-687"></a>which implements a differentiable attention mechanism over past</span>
<span id="cb6-688"><a href="#cb6-688"></a>interactions.</span>
<span id="cb6-689"><a href="#cb6-689"></a></span>
<span id="cb6-690"><a href="#cb6-690"></a><span class="co">[</span><span class="ot">@upadhyay2019a</span><span class="co">]</span> addressed the challenge of orchestrating multiple</span>
<span id="cb6-691"><a href="#cb6-691"></a>independently trained dialogue agents or skills in a unified system.</span>
<span id="cb6-692"><a href="#cb6-692"></a>They attempted online posterior dialogue orchestration, defining it as</span>
<span id="cb6-693"><a href="#cb6-693"></a>selecting the most suitable subset of skills in response to a user's</span>
<span id="cb6-694"><a href="#cb6-694"></a>input, which studying a context-attentive bandit model that operates</span>
<span id="cb6-695"><a href="#cb6-695"></a>under a skill execution budget, ensuring efficient and accurate response</span>
<span id="cb6-696"><a href="#cb6-696"></a>selection.</span>
<span id="cb6-697"><a href="#cb6-697"></a></span>
<span id="cb6-698"><a href="#cb6-698"></a>Anomaly detection refers to the task of identifying samples that behave</span>
<span id="cb6-699"><a href="#cb6-699"></a>differently from the majority. In <span class="co">[</span><span class="ot">@ding2019interactive</span><span class="co">]</span>, the authors</span>
<span id="cb6-700"><a href="#cb6-700"></a>delve into anomaly detection in an interactive setting, allowing the</span>
<span id="cb6-701"><a href="#cb6-701"></a>system to actively engage with human experts through a limited number of</span>
<span id="cb6-702"><a href="#cb6-702"></a>queries about genuine anomalies. The goal is to present as many true</span>
<span id="cb6-703"><a href="#cb6-703"></a>anomalies to the human expert as possible after a fixed query budget is</span>
<span id="cb6-704"><a href="#cb6-704"></a>used up. They applied the multi-armed contextual bandit framework to</span>
<span id="cb6-705"><a href="#cb6-705"></a>address this issue. This algorithm adeptly integrates both nodal</span>
<span id="cb6-706"><a href="#cb6-706"></a>attributes and node dependencies into a unified model, efficiently</span>
<span id="cb6-707"><a href="#cb6-707"></a>managing the exploration-exploitation trade-off during anomaly queries.</span>
<span id="cb6-708"><a href="#cb6-708"></a></span>
<span id="cb6-709"><a href="#cb6-709"></a>There are many challenges associated with contextual bandits. The first</span>
<span id="cb6-710"><a href="#cb6-710"></a>challenge is that each action only reveals the reward for that</span>
<span id="cb6-711"><a href="#cb6-711"></a>particular action. Therefore, the algorithm has to work with incomplete</span>
<span id="cb6-712"><a href="#cb6-712"></a>information. This leads to the dilemma of exploitation versus</span>
<span id="cb6-713"><a href="#cb6-713"></a>exploration: when should the algorithm choose the best-known option</span>
<span id="cb6-714"><a href="#cb6-714"></a>versus trying new options for potentially better outcomes? Another</span>
<span id="cb6-715"><a href="#cb6-715"></a>significant challenge for contextual bandits is using context</span>
<span id="cb6-716"><a href="#cb6-716"></a>effectively. The context the environment gives needs to be explored to</span>
<span id="cb6-717"><a href="#cb6-717"></a>figure out which action is best for each context.</span>
<span id="cb6-718"><a href="#cb6-718"></a></span>
<span id="cb6-719"><a href="#cb6-719"></a>The overarching goal in systems designed for recommending options of</span>
<span id="cb6-720"><a href="#cb6-720"></a>high value to users is to achieve an optimal balance between exploration</span>
<span id="cb6-721"><a href="#cb6-721"></a>and exploitation. This dual approach is crucial in environments where</span>
<span id="cb6-722"><a href="#cb6-722"></a>user preferences and needs are dynamic and diverse. Exploration refers</span>
<span id="cb6-723"><a href="#cb6-723"></a>to the process of seeking out new options, learning about untried</span>
<span id="cb6-724"><a href="#cb6-724"></a>possibilities, and gathering fresh information that could lead to</span>
<span id="cb6-725"><a href="#cb6-725"></a>high-value recommendations. In contrast, exploitation involves utilizing</span>
<span id="cb6-726"><a href="#cb6-726"></a>existing knowledge and past experiences to recommend the best options</span>
<span id="cb6-727"><a href="#cb6-727"></a>currently known. This balance is key to maintaining a system that</span>
<span id="cb6-728"><a href="#cb6-728"></a>continuously adapts to changing user preferences while ensuring the</span>
<span id="cb6-729"><a href="#cb6-729"></a>reliability of its recommendations.</span>
<span id="cb6-730"><a href="#cb6-730"></a></span>
<span id="cb6-731"><a href="#cb6-731"></a>A key observation in such systems is the dual role of users as both</span>
<span id="cb6-732"><a href="#cb6-732"></a>producers and consumers of information. Each user's experience</span>
<span id="cb6-733"><a href="#cb6-733"></a>contributes valuable data that informs future recommendations for</span>
<span id="cb6-734"><a href="#cb6-734"></a>others. For instance, platforms like Waze, Netflix, and Trip Advisor</span>
<span id="cb6-735"><a href="#cb6-735"></a>rely heavily on user input and feedback. Waze uses real-time traffic</span>
<span id="cb6-736"><a href="#cb6-736"></a>data from drivers to recommend optimal routes; Netflix suggests movies</span>
<span id="cb6-737"><a href="#cb6-737"></a>and shows based on viewing histories and ratings; Trip Advisor relies on</span>
<span id="cb6-738"><a href="#cb6-738"></a>traveler reviews to guide future tourists. In these examples, the</span>
<span id="cb6-739"><a href="#cb6-739"></a>balance between gathering new information (exploration) and recommending</span>
<span id="cb6-740"><a href="#cb6-740"></a>the best-known options (exploitation) is dynamically managed to enhance</span>
<span id="cb6-741"><a href="#cb6-741"></a>user experience and satisfaction. This approach underscores the</span>
<span id="cb6-742"><a href="#cb6-742"></a>importance of user engagement in systems where monetary incentives are</span>
<span id="cb6-743"><a href="#cb6-743"></a>not (or can not be) the primary driver.</span>
<span id="cb6-744"><a href="#cb6-744"></a></span>
<span id="cb6-745"><a href="#cb6-745"></a>Recommendation systems often face the challenge of overcoming user</span>
<span id="cb6-746"><a href="#cb6-746"></a>biases that can lead to a narrow exploration of options. Users come with</span>
<span id="cb6-747"><a href="#cb6-747"></a>preconceived notions and preferences, which can cause them to overlook</span>
<span id="cb6-748"><a href="#cb6-748"></a>potentially valuable options that initially appear inferior or unaligned</span>
<span id="cb6-749"><a href="#cb6-749"></a>with their interests. This predisposition can significantly limit the</span>
<span id="cb6-750"><a href="#cb6-750"></a>effectiveness of recommendation systems, as users might miss out on</span>
<span id="cb6-751"><a href="#cb6-751"></a>high-value choices simply due to their existing biases.</span>
<span id="cb6-752"><a href="#cb6-752"></a></span>
<span id="cb6-753"><a href="#cb6-753"></a>To counteract this, it is crucial for recommendation systems to actively</span>
<span id="cb6-754"><a href="#cb6-754"></a>incentivize exploration among users. One innovative approach to achieve</span>
<span id="cb6-755"><a href="#cb6-755"></a>this is through the strategic use of **information asymmetry**. By</span>
<span id="cb6-756"><a href="#cb6-756"></a>controlling and selectively presenting information, these systems can</span>
<span id="cb6-757"><a href="#cb6-757"></a>guide users to explore options they might not typically consider. This</span>
<span id="cb6-758"><a href="#cb6-758"></a>method aims to reveal the true potential of various options by nudging</span>
<span id="cb6-759"><a href="#cb6-759"></a>users out of their comfort zones and encouraging a broader exploration</span>
<span id="cb6-760"><a href="#cb6-760"></a>of available choices. An important note here is that the system is not</span>
<span id="cb6-761"><a href="#cb6-761"></a>lying to users - it only selectively reveals information it has.</span>
<span id="cb6-762"><a href="#cb6-762"></a></span>
<span id="cb6-763"><a href="#cb6-763"></a>The concept of incentivizing exploration becomes even more complex when</span>
<span id="cb6-764"><a href="#cb6-764"></a>considering different types of users. For instance, systems often</span>
<span id="cb6-765"><a href="#cb6-765"></a>encounter short-lived users who have little to gain from contributing to</span>
<span id="cb6-766"><a href="#cb6-766"></a>the system's learning process, as their interactions are infrequent or</span>
<span id="cb6-767"><a href="#cb6-767"></a>based on immediate needs. Similarly, some users may operate under a</span>
<span id="cb6-768"><a href="#cb6-768"></a>'greedy' principle, primarily seeking immediate gratification rather</span>
<span id="cb6-769"><a href="#cb6-769"></a>than contributing to the long-term accuracy and effectiveness of the</span>
<span id="cb6-770"><a href="#cb6-770"></a>system. In such scenarios, managing information asymmetry can be a</span>
<span id="cb6-771"><a href="#cb6-771"></a>powerful tool. By selectively revealing information, recommendation</span>
<span id="cb6-772"><a href="#cb6-772"></a>systems can create a sense of novelty and interest, prompting even the</span>
<span id="cb6-773"><a href="#cb6-773"></a>most transient or self-interested users to engage in exploration,</span>
<span id="cb6-774"><a href="#cb6-774"></a>thereby enhancing the system's overall knowledge base and recommendation</span>
<span id="cb6-775"><a href="#cb6-775"></a>quality.</span>
<span id="cb6-776"><a href="#cb6-776"></a></span>
<span id="cb6-777"><a href="#cb6-777"></a><span class="fu">### Incentive-Compatible Online Learning</span></span>
<span id="cb6-778"><a href="#cb6-778"></a></span>
<span id="cb6-779"><a href="#cb6-779"></a>To address this problem, we seek to create a model. But first, it is</span>
<span id="cb6-780"><a href="#cb6-780"></a>useful to outline the key criteria that our model must achieve.</span>
<span id="cb6-781"><a href="#cb6-781"></a></span>
<span id="cb6-782"><a href="#cb6-782"></a><span class="ss">-   </span>The *core* of the model revolves around repeated interactions</span>
<span id="cb6-783"><a href="#cb6-783"></a>    between a planner (the system) and multiple agents (the users). Each</span>
<span id="cb6-784"><a href="#cb6-784"></a>    agent, upon arrival in the system, is presented with a set of</span>
<span id="cb6-785"><a href="#cb6-785"></a>    available options to choose from. These options could vary widely</span>
<span id="cb6-786"><a href="#cb6-786"></a>    depending on the application of the model, such as routes in a</span>
<span id="cb6-787"><a href="#cb6-787"></a>    transportation network, a selection of hotels in a travel booking</span>
<span id="cb6-788"><a href="#cb6-788"></a>    system, or even entertainment choices in a streaming service.</span>
<span id="cb6-789"><a href="#cb6-789"></a></span>
<span id="cb6-790"><a href="#cb6-790"></a><span class="ss">-   </span>The *interaction process* is straightforward but crucial: agents</span>
<span id="cb6-791"><a href="#cb6-791"></a>    arrive, select an action from the provided options, and then report</span>
<span id="cb6-792"><a href="#cb6-792"></a>    feedback based on their experience. This feedback is vital as it</span>
<span id="cb6-793"><a href="#cb6-793"></a>    forms the basis upon which the planner improves and evolves its</span>
<span id="cb6-794"><a href="#cb6-794"></a>    recommendations. The agents in this model are considered strategic;</span>
<span id="cb6-795"><a href="#cb6-795"></a>    they aim to maximize their reward based on the information available</span>
<span id="cb6-796"><a href="#cb6-796"></a>    to them. This aspect of the model acknowledges the real-world</span>
<span id="cb6-797"><a href="#cb6-797"></a>    scenario where users are typically self-interested and seek to</span>
<span id="cb6-798"><a href="#cb6-798"></a>    optimize their own outcomes.</span>
<span id="cb6-799"><a href="#cb6-799"></a></span>
<span id="cb6-800"><a href="#cb6-800"></a><span class="ss">-   </span>The *planner*, on the other hand, has a broader objective. It aims</span>
<span id="cb6-801"><a href="#cb6-801"></a>    to learn which alternatives are best in a given context and works to</span>
<span id="cb6-802"><a href="#cb6-802"></a>    maximize the overall welfare of all agents. This involves a complex</span>
<span id="cb6-803"><a href="#cb6-803"></a>    balancing act: the planner must accurately interpret feedback from a</span>
<span id="cb6-804"><a href="#cb6-804"></a>    diverse set of agents, each with their own preferences and biases,</span>
<span id="cb6-805"><a href="#cb6-805"></a>    and use this information to refine and improve the set of options</span>
<span id="cb6-806"><a href="#cb6-806"></a>    available. The ultimate goal of the planner is to create a dynamic,</span>
<span id="cb6-807"><a href="#cb6-807"></a>    responsive system that not only caters to the immediate needs of</span>
<span id="cb6-808"><a href="#cb6-808"></a>    individual agents but also enhances the collective experience over</span>
<span id="cb6-809"><a href="#cb6-809"></a>    time, leading to a continually improving recommendation ecosystem.</span>
<span id="cb6-810"><a href="#cb6-810"></a></span>
<span id="cb6-811"><a href="#cb6-811"></a>Let's break this up into a set of tangible research questions that we</span>
<span id="cb6-812"><a href="#cb6-812"></a>seek to answer in the rest of this chapter.</span>
<span id="cb6-813"><a href="#cb6-813"></a></span>
<span id="cb6-814"><a href="#cb6-814"></a><span class="ss">-   </span>**Planner Limitations**: We seek to address the inherent limitations</span>
<span id="cb6-815"><a href="#cb6-815"></a>    faced by the planner, particularly in scenarios where monetary</span>
<span id="cb6-816"><a href="#cb6-816"></a>    transfers are not an option, and the only tool at its disposal is</span>
<span id="cb6-817"><a href="#cb6-817"></a>    the control over the flow of information between agents. This</span>
<span id="cb6-818"><a href="#cb6-818"></a>    inquiry aims to understand the extent to which these limitations</span>
<span id="cb6-819"><a href="#cb6-819"></a>    impact the planner's ability to effectively guide and influence</span>
<span id="cb6-820"><a href="#cb6-820"></a>    agent behavior.</span>
<span id="cb6-821"><a href="#cb6-821"></a></span>
<span id="cb6-822"><a href="#cb6-822"></a><span class="ss">-   </span>**Inducing Exploration**: A critical question is whether the planner</span>
<span id="cb6-823"><a href="#cb6-823"></a>    can successfully induce exploration among agents, especially in the</span>
<span id="cb6-824"><a href="#cb6-824"></a>    absence of financial incentives. This involves investigating</span>
<span id="cb6-825"><a href="#cb6-825"></a>    strategies to encourage users to try less obvious or popular</span>
<span id="cb6-826"><a href="#cb6-826"></a>    options, thus broadening the scope of feedback and enhancing the</span>
<span id="cb6-827"><a href="#cb6-827"></a>    system's ability to learn and identify the best alternatives.</span>
<span id="cb6-828"><a href="#cb6-828"></a></span>
<span id="cb6-829"><a href="#cb6-829"></a><span class="ss">-   </span>**Rate of Learning**: Another essential research area is</span>
<span id="cb6-830"><a href="#cb6-830"></a>    understanding the rate at which the planner learns from agent</span>
<span id="cb6-831"><a href="#cb6-831"></a>    interactions. This encompasses examining how different agent</span>
<span id="cb6-832"><a href="#cb6-832"></a>    incentives, their willingness to explore, and their feedback impact</span>
<span id="cb6-833"><a href="#cb6-833"></a>    the speed and efficiency with which the planner can identify optimal</span>
<span id="cb6-834"><a href="#cb6-834"></a>    recommendations.</span>
<span id="cb6-835"><a href="#cb6-835"></a></span>
<span id="cb6-836"><a href="#cb6-836"></a><span class="ss">-   </span>**Model Extensions**: The model can be extended in several</span>
<span id="cb6-837"><a href="#cb6-837"></a>    directions, each raising its own set of questions.</span>
<span id="cb6-838"><a href="#cb6-838"></a></span>
<span id="cb6-839"><a href="#cb6-839"></a><span class="ss">    1.  </span>Multiple Agents with Interconnected Payoffs: When multiple</span>
<span id="cb6-840"><a href="#cb6-840"></a>        agents arrive simultaneously, their choices and payoffs become</span>
<span id="cb6-841"><a href="#cb6-841"></a>        interconnected, resembling a game. The research question here</span>
<span id="cb6-842"><a href="#cb6-842"></a>        focuses on how these interdependencies affect individual and</span>
<span id="cb6-843"><a href="#cb6-843"></a>        collective decision-making.</span>
<span id="cb6-844"><a href="#cb6-844"></a></span>
<span id="cb6-845"><a href="#cb6-845"></a><span class="ss">    2.  </span>Planner with Arbitrary Objective Function: Investigating</span>
<span id="cb6-846"><a href="#cb6-846"></a>        scenarios where the planner operates under an arbitrary</span>
<span id="cb6-847"><a href="#cb6-847"></a>        objective function, which might not align with maximizing</span>
<span id="cb6-848"><a href="#cb6-848"></a>        overall welfare or learning the best alternative.</span>
<span id="cb6-849"><a href="#cb6-849"></a></span>
<span id="cb6-850"><a href="#cb6-850"></a><span class="ss">    3.  </span>Observed Heterogeneity Among Agents: This involves situations</span>
<span id="cb6-851"><a href="#cb6-851"></a>        where differences among agents are observable and known, akin to</span>
<span id="cb6-852"><a href="#cb6-852"></a>        contextual bandits in machine learning. The research question</span>
<span id="cb6-853"><a href="#cb6-853"></a>        revolves around how these observable traits can be used to</span>
<span id="cb6-854"><a href="#cb6-854"></a>        tailor recommendations more effectively.</span>
<span id="cb6-855"><a href="#cb6-855"></a></span>
<span id="cb6-856"><a href="#cb6-856"></a><span class="ss">    4.  </span>Unobserved Heterogeneity Among Agents: This aspect delves into</span>
<span id="cb6-857"><a href="#cb6-857"></a>        scenarios where differences among agents are not directly</span>
<span id="cb6-858"><a href="#cb6-858"></a>        observable, necessitating the use of causal inference techniques</span>
<span id="cb6-859"><a href="#cb6-859"></a>        to understand and cater to diverse user needs.</span>
<span id="cb6-860"><a href="#cb6-860"></a></span>
<span id="cb6-861"><a href="#cb6-861"></a><span class="fu">#### Bayesian Incentive-Compatible Bandit Model {#bayesian-incentive-compatible-bandit-model .unnumbered}</span></span>
<span id="cb6-862"><a href="#cb6-862"></a></span>
<span id="cb6-863"><a href="#cb6-863"></a>In this section, we introduce the main model of study in this chapter</span>
<span id="cb6-864"><a href="#cb6-864"></a><span class="co">[</span><span class="ot">@mansour2019bayesianincentivecompatiblebanditexploration; @mansour2021bayesianexplorationincentivizingexploration</span><span class="co">]</span>.</span>
<span id="cb6-865"><a href="#cb6-865"></a>In our setup, there is a "planner,\" which aims to increase exploration,</span>
<span id="cb6-866"><a href="#cb6-866"></a>and many independent "agents,\" which will act selfishly (in a way that</span>
<span id="cb6-867"><a href="#cb6-867"></a>they believe will maximize their individual reward).</span>
<span id="cb6-868"><a href="#cb6-868"></a></span>
<span id="cb6-869"><a href="#cb6-869"></a>Under our model shown in Figure</span>
<span id="cb6-870"><a href="#cb6-870"></a><span class="co">[</span><span class="ot">1.1</span><span class="co">](#fig-planner-agent)</span>{reference-type="ref"</span>
<span id="cb6-871"><a href="#cb6-871"></a>reference="fig-planner-agent"}, there are $K$ possible actions that all</span>
<span id="cb6-872"><a href="#cb6-872"></a>users can take, and each action has some mean reward $\mu_i \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$.</span>
<span id="cb6-873"><a href="#cb6-873"></a>In addition, there is a common prior belief on each $\mu_i$ across all</span>
<span id="cb6-874"><a href="#cb6-874"></a>users.. The $T$ agents, or users, will arrive sequentially. As the</span>
<span id="cb6-875"><a href="#cb6-875"></a>$t$'th user arrives, they are recommended an action $I_t$ by the</span>
<span id="cb6-876"><a href="#cb6-876"></a>planner, which they are free to follow or not follow. After taking</span>
<span id="cb6-877"><a href="#cb6-877"></a>whichever action they choose, the user experiences some realized reward</span>
<span id="cb6-878"><a href="#cb6-878"></a>$r_i \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$, which is stochastic i.i.d. with mean $\mu_i$, and</span>
<span id="cb6-879"><a href="#cb6-879"></a>reports this reward back to the planner.</span>
<span id="cb6-880"><a href="#cb6-880"></a></span>
<span id="cb6-881"><a href="#cb6-881"></a>![Planner-agent</span>
<span id="cb6-882"><a href="#cb6-882"></a>setup](Figures/planner-agent-setup.png){#fig-planner-agent width="60%"}</span>
<span id="cb6-883"><a href="#cb6-883"></a></span>
<span id="cb6-884"><a href="#cb6-884"></a>So far, the model we have defined is equivalent to a multi-armed bandit</span>
<span id="cb6-885"><a href="#cb6-885"></a>model, which we have seen earlier in this chapter</span>
<span id="cb6-886"><a href="#cb6-886"></a>(<span class="co">[</span><span class="ot">1</span><span class="co">](#4optim)</span>{reference-type="ref" reference="4optim"}). Under this</span>
<span id="cb6-887"><a href="#cb6-887"></a>model, well-known results in economics, operations research and computer</span>
<span id="cb6-888"><a href="#cb6-888"></a>science show that $O(\sqrt{T})$ regret is achievable</span>
<span id="cb6-889"><a href="#cb6-889"></a><span class="co">[</span><span class="ot">@russo2015informationtheoreticanalysisthompsonsampling; @auer_cesa-bianchi_fischer_2002; @LAI19854</span><span class="co">]</span></span>
<span id="cb6-890"><a href="#cb6-890"></a>with algorithms such as Thompson sampling and UCB.</span>
<span id="cb6-891"><a href="#cb6-891"></a></span>
<span id="cb6-892"><a href="#cb6-892"></a>However, our agents are strategic and aim to maximize their own rewards.</span>
<span id="cb6-893"><a href="#cb6-893"></a>If they observe the rewards gained from actions taken by other previous</span>
<span id="cb6-894"><a href="#cb6-894"></a>users, they will simply take the action they believe will yield the</span>
<span id="cb6-895"><a href="#cb6-895"></a>highest reward given the previous actions; they would prefer to benefit</span>
<span id="cb6-896"><a href="#cb6-896"></a>from exploration done by other users rather than take the risk of</span>
<span id="cb6-897"><a href="#cb6-897"></a>exploring themselves. Therefore, exploration on an individual level,</span>
<span id="cb6-898"><a href="#cb6-898"></a>which the planner would like to facilitate, is not guaranteed under this</span>
<span id="cb6-899"><a href="#cb6-899"></a>paradigm.</span>
<span id="cb6-900"><a href="#cb6-900"></a></span>
<span id="cb6-901"><a href="#cb6-901"></a>In light of this, we also require that our model satisfy **incentive</span>
<span id="cb6-902"><a href="#cb6-902"></a>compatibility**, or that taking the action recommended by the planner</span>
<span id="cb6-903"><a href="#cb6-903"></a>has an expected utility that is as high as any other action the agent</span>
<span id="cb6-904"><a href="#cb6-904"></a>could take. Formally,</span>
<span id="cb6-905"><a href="#cb6-905"></a>$$\forall i : \, E<span class="co">[</span><span class="ot">\mu_i | I_t = i</span><span class="co">]</span> \geq E<span class="co">[</span><span class="ot">\mu_{i'} | I_t = i</span><span class="co">]</span>.$$ Note</span>
<span id="cb6-906"><a href="#cb6-906"></a>that this incentivizes the agents to actually take the actions</span>
<span id="cb6-907"><a href="#cb6-907"></a>recommended by the planner; if incentive compatibility is not satisfied,</span>
<span id="cb6-908"><a href="#cb6-908"></a>agents would simply ignore the planner and take whatever action they</span>
<span id="cb6-909"><a href="#cb6-909"></a>think will lead to the highest reward.</span>
<span id="cb6-910"><a href="#cb6-910"></a></span>
<span id="cb6-911"><a href="#cb6-911"></a>At a high level, the key to achieving incentive compatibility while</span>
<span id="cb6-912"><a href="#cb6-912"></a>still creating a policy for the planner that facilitates exploration is</span>
<span id="cb6-913"><a href="#cb6-913"></a>information asymmetry. Under this paradigm, the users only have access</span>
<span id="cb6-914"><a href="#cb6-914"></a>to their previous recommendations, actions, and rewards, and not to the</span>
<span id="cb6-915"><a href="#cb6-915"></a>recommendations, actions, and rewards of other users. Therefore, they</span>
<span id="cb6-916"><a href="#cb6-916"></a>are unsure of whether, after other users take certain actions and</span>
<span id="cb6-917"><a href="#cb6-917"></a>receive certain rewards, arms that they might have initially considered</span>
<span id="cb6-918"><a href="#cb6-918"></a>worse in practice outperform arms that they initially considered better.</span>
<span id="cb6-919"><a href="#cb6-919"></a>Only the planner has access to the previous actions and rewards of all</span>
<span id="cb6-920"><a href="#cb6-920"></a>users; the user only has access to their own recommendations and overall</span>
<span id="cb6-921"><a href="#cb6-921"></a>knowledge of the planner's policy.</span>
<span id="cb6-922"><a href="#cb6-922"></a></span>
<span id="cb6-923"><a href="#cb6-923"></a>The main question we aim to answer for the rest of this section is,</span>
<span id="cb6-924"><a href="#cb6-924"></a>given this new constraint of incentive compatibility, is $O(\sqrt{T})$</span>
<span id="cb6-925"><a href="#cb6-925"></a>regret still achievable? We illustrate such an algorithm in the</span>
<span id="cb6-926"><a href="#cb6-926"></a>following.</span>
<span id="cb6-927"><a href="#cb6-927"></a></span>
<span id="cb6-928"><a href="#cb6-928"></a><span class="fu">#### Black-box Reduction Algorithm {#black-box-reduction-algorithm .unnumbered}</span></span>
<span id="cb6-929"><a href="#cb6-929"></a></span>
<span id="cb6-930"><a href="#cb6-930"></a>The main result for this chapter is a **black-box reduction** algorithm</span>
<span id="cb6-931"><a href="#cb6-931"></a>to turn any bandit algorithm into an *incentive compatible* one, with</span>
<span id="cb6-932"><a href="#cb6-932"></a>only a constant increase in Bayesian regret. Since, as mentioned</span>
<span id="cb6-933"><a href="#cb6-933"></a>earlier, there are bandit algorithms with $O(\sqrt{T})$ Bayesian regret,</span>
<span id="cb6-934"><a href="#cb6-934"></a>black-box reduction will also allow us to get incentive-compatible</span>
<span id="cb6-935"><a href="#cb6-935"></a>algorithms with $O(\sqrt{T})$ regret. The idea of black-box reduction</span>
<span id="cb6-936"><a href="#cb6-936"></a>will be to simulate $T$ steps of any bandit algorithm in an</span>
<span id="cb6-937"><a href="#cb6-937"></a>incentive-compatible way in $c T$ steps. This allows us to design</span>
<span id="cb6-938"><a href="#cb6-938"></a>incentive-compatible recommendation systems by using any bandit</span>
<span id="cb6-939"><a href="#cb6-939"></a>algorithm and then adapting it.</span>
<span id="cb6-940"><a href="#cb6-940"></a></span>
<span id="cb6-941"><a href="#cb6-941"></a>Consider the following setting: there are two possible actions, $A_1$</span>
<span id="cb6-942"><a href="#cb6-942"></a>and $A_2$. Assume the setting of **deterministic rewards**, where action</span>
<span id="cb6-943"><a href="#cb6-943"></a>1 has reward $\mu_1$ with prior $U<span class="co">[</span><span class="ot">1/3, 1</span><span class="co">]</span>$ and mean</span>
<span id="cb6-944"><a href="#cb6-944"></a>$\mathbb{E}<span class="co">[</span><span class="ot">\mu_1</span><span class="co">]</span> = 2/3$, and action 2 has reward $\mu_2$ with prior</span>
<span id="cb6-945"><a href="#cb6-945"></a>$U<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ and mean $\mathbb{E}<span class="co">[</span><span class="ot">\mu_2</span><span class="co">]</span> = 1/2$. Without the planner</span>
<span id="cb6-946"><a href="#cb6-946"></a>intervention and with full observability, users would simply always pick</span>
<span id="cb6-947"><a href="#cb6-947"></a>$A_1$, so how can the planner *incentivize* users to play $A_2$?</span>
<span id="cb6-948"><a href="#cb6-948"></a></span>
<span id="cb6-949"><a href="#cb6-949"></a>![Illustration of black-box reduction algorithm when we have</span>
<span id="cb6-950"><a href="#cb6-950"></a>deterministic</span>
<span id="cb6-951"><a href="#cb6-951"></a>rewards.](Figures/guinea_pig_fig.png){#fig-deterministic-guinea-pig}</span>
<span id="cb6-952"><a href="#cb6-952"></a></span>
<span id="cb6-953"><a href="#cb6-953"></a>The key insight is going to be to *hide exploration in a pool of</span>
<span id="cb6-954"><a href="#cb6-954"></a>exploitation*. The users are only going to receive a recommendation from</span>
<span id="cb6-955"><a href="#cb6-955"></a>the planner, and no other observations. After deterministically</span>
<span id="cb6-956"><a href="#cb6-956"></a>recommending the action with the highest expected reward ($A_1$), the</span>
<span id="cb6-957"><a href="#cb6-957"></a>planner will pick one **guinea pig** to recommend the exploratory action</span>
<span id="cb6-958"><a href="#cb6-958"></a>of $A_2$. The users don't know whether they are the guinea pig, so</span>
<span id="cb6-959"><a href="#cb6-959"></a>intuitively, as long as the planner picks guinea pigs uniformly at</span>
<span id="cb6-960"><a href="#cb6-960"></a>random and at low enough frequencies, the optimal decision for the users</span>
<span id="cb6-961"><a href="#cb6-961"></a>is still to follow the planner's recommendation, even if it might go</span>
<span id="cb6-962"><a href="#cb6-962"></a>against their interest.</span>
<span id="cb6-963"><a href="#cb6-963"></a></span>
<span id="cb6-964"><a href="#cb6-964"></a>The planner will pick the user who will be recommended the exploratory</span>
<span id="cb6-965"><a href="#cb6-965"></a>action uniformly at random from the $L$ users that come after the first</span>
<span id="cb6-966"><a href="#cb6-966"></a>one (which deterministically gets recommended the exploitation action).</span>
<span id="cb6-967"><a href="#cb6-967"></a>Under this setting (illustrated in Figure</span>
<span id="cb6-968"><a href="#cb6-968"></a><span class="co">[</span><span class="ot">1.2</span><span class="co">](#fig-deterministic-guinea-pig)</span>{reference-type="ref"</span>
<span id="cb6-969"><a href="#cb6-969"></a>reference="fig-deterministic-guinea-pig"}), it is optimal for users to</span>
<span id="cb6-970"><a href="#cb6-970"></a>always follow the option that is recommended for them. More formally, if</span>
<span id="cb6-971"><a href="#cb6-971"></a>$I_t$ is the recommendation that a user receives at time $t$, then we</span>
<span id="cb6-972"><a href="#cb6-972"></a>have that: $$\begin{split}</span>
<span id="cb6-973"><a href="#cb6-973"></a>    \mathbb{E}<span class="co">[</span><span class="ot">\mu_1 - \mu_2 | I_t = 2</span><span class="co">]</span> Pr<span class="co">[</span><span class="ot">I_t = 2</span><span class="co">]</span> &amp;= \frac{1}{L} (\mu_1 - \mu_2) \quad \text{(Gains if you are the unlucky guinea pig)}<span class="sc">\\</span></span>
<span id="cb6-974"><a href="#cb6-974"></a>    &amp;+ (1 - \frac{1}{L}) \mathbb{E}<span class="co">[</span><span class="ot">\mu_1 - \mu_2 | \mu_1 &lt; \mu_2</span><span class="co">]</span> Pr<span class="co">[</span><span class="ot">\mu_1 &lt; \mu_2</span><span class="co">]</span> \quad \text{(Loss if you are not and $\mu_1 &lt; \mu_2$)}<span class="sc">\\</span></span>
<span id="cb6-975"><a href="#cb6-975"></a>    &amp;\leq 0</span>
<span id="cb6-976"><a href="#cb6-976"></a>\end{split}$$ This holds when $L \geq 12$. It means that the gains from</span>
<span id="cb6-977"><a href="#cb6-977"></a>not taking the recommended action are *negative*, which implies that</span>
<span id="cb6-978"><a href="#cb6-978"></a>users should always take the recommendation.</span>
<span id="cb6-979"><a href="#cb6-979"></a></span>
<span id="cb6-980"><a href="#cb6-980"></a>So far we have considered the case where rewards are deterministic, but</span>
<span id="cb6-981"><a href="#cb6-981"></a>what about **stochastic rewards**? We are now going to consider the case</span>
<span id="cb6-982"><a href="#cb6-982"></a>where rewards are independent and identically distributed from some</span>
<span id="cb6-983"><a href="#cb6-983"></a>distribution, and where each action $A_i$ has some reward distribution</span>
<span id="cb6-984"><a href="#cb6-984"></a>$r_i^t \sim D_i, \mathbb{E}<span class="co">[</span><span class="ot">r_i^t</span><span class="co">]</span> = \mu_i$. Back to the case where</span>
<span id="cb6-985"><a href="#cb6-985"></a>there are only two actions, we are going to adapt the prior algorithm of</span>
<span id="cb6-986"><a href="#cb6-986"></a>guinea pig-picking to the stochastic reward setting. Since one reward</span>
<span id="cb6-987"><a href="#cb6-987"></a>observation is not enough to fully know $\mu_1$ anymore, we'll instead</span>
<span id="cb6-988"><a href="#cb6-988"></a>observe the outcome of the first action $M$ times to form a strong</span>
<span id="cb6-989"><a href="#cb6-989"></a>posterior $\mathbb{E}<span class="co">[</span><span class="ot">\mu_1 | r_1^1, \ldots r_1^M</span><span class="co">]</span>$.</span>
<span id="cb6-990"><a href="#cb6-990"></a></span>
<span id="cb6-991"><a href="#cb6-991"></a>![Illustration of black-box reduction algorithm when we have stochastic</span>
<span id="cb6-992"><a href="#cb6-992"></a>rewards.](Figures/stochastic_guinea_pig.png){#fig-stochastic-guinea-pig}</span>
<span id="cb6-993"><a href="#cb6-993"></a></span>
<span id="cb6-994"><a href="#cb6-994"></a>Figure <span class="co">[</span><span class="ot">1.3</span><span class="co">](#fig-stochastic-guinea-pig)</span>{reference-type="ref"</span>
<span id="cb6-995"><a href="#cb6-995"></a>reference="fig-stochastic-guinea-pig"} illustrates the algorithm that we</span>
<span id="cb6-996"><a href="#cb6-996"></a>can use with stochastic rewards when there are two actions. Similarly,</span>
<span id="cb6-997"><a href="#cb6-997"></a>as before, we pick one guinea pig uniformly at random from the next $L$</span>
<span id="cb6-998"><a href="#cb6-998"></a>users and use the reward we get as the exploratory signal.\</span>
<span id="cb6-999"><a href="#cb6-999"></a>In a very similar manner, we can generalize this algorithm from always</span>
<span id="cb6-1000"><a href="#cb6-1000"></a>having two actions to the general multi-armed bandit problem. Now</span>
<span id="cb6-1001"><a href="#cb6-1001"></a>suppose we have a general multi-armed bandit algorithm $A$. We will wrap</span>
<span id="cb6-1002"><a href="#cb6-1002"></a>this algorithm around our black box reduction algorithm to make it</span>
<span id="cb6-1003"><a href="#cb6-1003"></a>incentive-compatible.</span>
<span id="cb6-1004"><a href="#cb6-1004"></a></span>
<span id="cb6-1005"><a href="#cb6-1005"></a>![Illustration of black-box reduction algorithm for the general</span>
<span id="cb6-1006"><a href="#cb6-1006"></a>multi-armed bandit</span>
<span id="cb6-1007"><a href="#cb6-1007"></a>case.](Figures/multi-armed-guinea-pig.png){#fig-multi-armed-guinea-pig}</span>
<span id="cb6-1008"><a href="#cb6-1008"></a></span>
<span id="cb6-1009"><a href="#cb6-1009"></a>As Figure <span class="co">[</span><span class="ot">1.4</span><span class="co">](#fig-multi-armed-guinea-pig)</span>{reference-type="ref"</span>
<span id="cb6-1010"><a href="#cb6-1010"></a>reference="fig-multi-armed-guinea-pig"} shows, we wrap every decision</span>
<span id="cb6-1011"><a href="#cb6-1011"></a>that $A$ would make by exactly $L-1$ recommendations of the action</span>
<span id="cb6-1012"><a href="#cb6-1012"></a>believed to be the best so far. This guarantees that the expected</span>
<span id="cb6-1013"><a href="#cb6-1013"></a>rewards for the users that are not chosen as guinea pigs are at least as</span>
<span id="cb6-1014"><a href="#cb6-1014"></a>good as $A$'s reward at phase $n$.</span>
<span id="cb6-1015"><a href="#cb6-1015"></a></span>
<span id="cb6-1016"><a href="#cb6-1016"></a><span class="fu">## Preferential Bayesian Optimization</span></span>
<span id="cb6-1017"><a href="#cb6-1017"></a></span>
<span id="cb6-1018"><a href="#cb6-1018"></a>The traditional Bayesian optimization (BO) problem is described as</span>
<span id="cb6-1019"><a href="#cb6-1019"></a>follows. There is a black-box objective function</span>
<span id="cb6-1020"><a href="#cb6-1020"></a>$g: \mathcal{X} \rightarrow \Re$ defined on a bounded subset</span>
<span id="cb6-1021"><a href="#cb6-1021"></a>$\mathcal{X} \subseteq \Re^q$ such that direct queries to the function</span>
<span id="cb6-1022"><a href="#cb6-1022"></a>are expensive or not possible. However, we would like to solve the</span>
<span id="cb6-1023"><a href="#cb6-1023"></a>global optimization problem of finding</span>
<span id="cb6-1024"><a href="#cb6-1024"></a>$\mathbf{x}_{\min }=\arg \min _{\mathbf{x} \in \mathcal{X}} g(\mathbf{x})$.</span>
<span id="cb6-1025"><a href="#cb6-1025"></a>This is highly analogous to modeling human preferences, since it is the</span>
<span id="cb6-1026"><a href="#cb6-1026"></a>case that direct access to a human's latent preference function is not</span>
<span id="cb6-1027"><a href="#cb6-1027"></a>possible but we would still like to find its optimum, such as in A/B</span>
<span id="cb6-1028"><a href="#cb6-1028"></a>tests or recommender systems.</span>
<span id="cb6-1029"><a href="#cb6-1029"></a></span>
<span id="cb6-1030"><a href="#cb6-1030"></a>We approach this problem for human preferences with *Preferential</span>
<span id="cb6-1031"><a href="#cb6-1031"></a>Bayesian Optimization* (PBO), as the key difference is that we are able</span>
<span id="cb6-1032"><a href="#cb6-1032"></a>to query the preference function through pairwise comparisons of data</span>
<span id="cb6-1033"><a href="#cb6-1033"></a>points, i.e. *duels*. This is a form of indirect observation of the</span>
<span id="cb6-1034"><a href="#cb6-1034"></a>objective function, which models real-world scenarios closely: we</span>
<span id="cb6-1035"><a href="#cb6-1035"></a>commonly need to to optimize a function via data about preferences. With</span>
<span id="cb6-1036"><a href="#cb6-1036"></a>humans, it has been demonstrated that we are better at evaluating</span>
<span id="cb6-1037"><a href="#cb6-1037"></a>differences rather than absolute magnitudes <span class="co">[</span><span class="ot">@kahneman_tversky_1979</span><span class="co">]</span> and</span>
<span id="cb6-1038"><a href="#cb6-1038"></a>therefore PBO models can be applied in various contexts.</span>
<span id="cb6-1039"><a href="#cb6-1039"></a></span>
<span id="cb6-1040"><a href="#cb6-1040"></a><span class="fu">### Problem statement</span></span>
<span id="cb6-1041"><a href="#cb6-1041"></a></span>
<span id="cb6-1042"><a href="#cb6-1042"></a>The problem of finding the optimum of a latent preference function</span>
<span id="cb6-1043"><a href="#cb6-1043"></a>defined on $\mathcal{X}$ can be reduced to determining a sequence of</span>
<span id="cb6-1044"><a href="#cb6-1044"></a>duels on $\mathcal{X} \times \mathcal{X}$. From each duel</span>
<span id="cb6-1045"><a href="#cb6-1045"></a>$\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span> \in$</span>
<span id="cb6-1046"><a href="#cb6-1046"></a>$\mathcal{X} \times \mathcal{X}$ we obtain binary feedback $<span class="sc">\{</span>0,1<span class="sc">\}</span>$</span>
<span id="cb6-1047"><a href="#cb6-1047"></a>indicating whether or not $\mathbf{x}$ is preferred over</span>
<span id="cb6-1048"><a href="#cb6-1048"></a>$\mathbf{x}^{\prime}$ ($g(\mathbf{x}) &lt; g(\mathbf{x}^{\prime})$). We</span>
<span id="cb6-1049"><a href="#cb6-1049"></a>consider that $\mathbf{x}$ is the winner of the duel if the output is</span>
<span id="cb6-1050"><a href="#cb6-1050"></a>$<span class="sc">\{</span>1<span class="sc">\}</span>$ and that $\mathbf{x}^{\prime}$ wins the duel if the output is</span>
<span id="cb6-1051"><a href="#cb6-1051"></a>$<span class="sc">\{</span>0<span class="sc">\}</span>$. The aim is to find $\mathbf{x}_{\min }$ by reducing as much as</span>
<span id="cb6-1052"><a href="#cb6-1052"></a>possible the number of queried duels.</span>
<span id="cb6-1053"><a href="#cb6-1053"></a></span>
<span id="cb6-1054"><a href="#cb6-1054"></a>The key idea in PBO is to learn a preference function in the space of</span>
<span id="cb6-1055"><a href="#cb6-1055"></a>duels using a Gaussian process. We define a joint reward</span>
<span id="cb6-1056"><a href="#cb6-1056"></a>$f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)$ on each</span>
<span id="cb6-1057"><a href="#cb6-1057"></a>duel which is never directly observed. Instead, the feedback we obtain</span>
<span id="cb6-1058"><a href="#cb6-1058"></a>after each pair is a binary output $y \in$ $<span class="sc">\{</span>0,1<span class="sc">\}</span>$ indicating which of</span>
<span id="cb6-1059"><a href="#cb6-1059"></a>the two inputs is preferred. One definition of f we will use (though</span>
<span id="cb6-1060"><a href="#cb6-1060"></a>others are possible) is</span>
<span id="cb6-1061"><a href="#cb6-1061"></a>$f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})$.</span>
<span id="cb6-1062"><a href="#cb6-1062"></a>The more $\mathbf{x}^{\prime}$ is preferred over $\mathbf{x}$, the</span>
<span id="cb6-1063"><a href="#cb6-1063"></a>bigger the reward.</span>
<span id="cb6-1064"><a href="#cb6-1064"></a></span>
<span id="cb6-1065"><a href="#cb6-1065"></a>We define the model of preference using a Bernoulli likelihood, where</span>
<span id="cb6-1066"><a href="#cb6-1066"></a>$p\left(y=1 \mid\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)$</span>
<span id="cb6-1067"><a href="#cb6-1067"></a>and</span>
<span id="cb6-1068"><a href="#cb6-1068"></a>$p\left(y=0 \mid\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}^{\prime}, \mathbf{x}\right</span><span class="co">]</span>\right)$</span>
<span id="cb6-1069"><a href="#cb6-1069"></a>for some inverse link function $\pi: \Re \times \Re \rightarrow<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$.</span>
<span id="cb6-1070"><a href="#cb6-1070"></a>$\pi_f$ has the property that</span>
<span id="cb6-1071"><a href="#cb6-1071"></a>$\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}^{\prime}, \mathbf{x}\right</span><span class="co">]</span>\right)=1-\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)$.</span>
<span id="cb6-1072"><a href="#cb6-1072"></a>A natural choice for $\pi_f$ is the logistic function</span>
<span id="cb6-1073"><a href="#cb6-1073"></a>$$\label{eq:bernoulli_pref}</span>
<span id="cb6-1074"><a href="#cb6-1074"></a>\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=\sigma\left(f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)\right)=\frac{1}{1+e^{-f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)}},$$</span>
<span id="cb6-1075"><a href="#cb6-1075"></a>but others are possible. Therefore we have that for any duel</span>
<span id="cb6-1076"><a href="#cb6-1076"></a>$\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>$ in which</span>
<span id="cb6-1077"><a href="#cb6-1077"></a>$g(\mathbf{x}) \leq g\left(\mathbf{x}^{\prime}\right)$ it holds that</span>
<span id="cb6-1078"><a href="#cb6-1078"></a>$\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right) \geq 0.5$.</span>
<span id="cb6-1079"><a href="#cb6-1079"></a>$\pi_f$ is a preference function that maps each query</span>
<span id="cb6-1080"><a href="#cb6-1080"></a>$\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>$ to the probability of</span>
<span id="cb6-1081"><a href="#cb6-1081"></a>having a preference on the left input $\mathbf{x}$ over the right input</span>
<span id="cb6-1082"><a href="#cb6-1082"></a>$\mathbf{x}^{\prime}$.</span>
<span id="cb6-1083"><a href="#cb6-1083"></a></span>
<span id="cb6-1084"><a href="#cb6-1084"></a>When we marginalize over the right input $\mathbf{x}^{\prime}$ of $f$</span>
<span id="cb6-1085"><a href="#cb6-1085"></a>(is this correct?), the global minimum of $f$ in $\mathcal{X}$ coincides</span>
<span id="cb6-1086"><a href="#cb6-1086"></a>with $\mathbf{x}_{\min }$. We also introduce the definition of the</span>
<span id="cb6-1087"><a href="#cb6-1087"></a>*Copeland score function* for a point $\mathbf{x}$ as</span>
<span id="cb6-1088"><a href="#cb6-1088"></a>$$S(\mathbf{x})=\operatorname{Vol}(\mathcal{X})^{-1} \int_{\mathcal{X}} \mathbb{I}_{\left<span class="sc">\{</span>\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right) \geq 0.5\right<span class="sc">\}</span>} d \mathbf{x}^{\prime}$$</span>
<span id="cb6-1089"><a href="#cb6-1089"></a>where</span>
<span id="cb6-1090"><a href="#cb6-1090"></a>$\operatorname{Vol}(\mathcal{X})=\int_{\mathcal{X}} d \mathbf{x}^{\prime}$</span>
<span id="cb6-1091"><a href="#cb6-1091"></a>is a normalizing constant that bounds $S(\mathbf{x})$ in the interval</span>
<span id="cb6-1092"><a href="#cb6-1092"></a>$<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. If $\mathcal{X}$ is a finite set, the Copeland score is simply</span>
<span id="cb6-1093"><a href="#cb6-1093"></a>the proportion of duels that a certain element $\mathbf{x}$ will win</span>
<span id="cb6-1094"><a href="#cb6-1094"></a>with probability larger than 0.5. A soft variant we will use instead of</span>
<span id="cb6-1095"><a href="#cb6-1095"></a>the Copeland score is the *soft-Copeland score*, defined as</span>
<span id="cb6-1096"><a href="#cb6-1096"></a>$$\label{eq:soft-copeland}</span>
<span id="cb6-1097"><a href="#cb6-1097"></a>C(\mathbf{x})=\operatorname{Vol}(\mathcal{X})^{-1} \int_{\mathcal{X}} \pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right) d \mathbf{x}^{\prime}$$</span>
<span id="cb6-1098"><a href="#cb6-1098"></a>where the probability function $\pi_f$ is integrated over $\mathcal{X}$.</span>
<span id="cb6-1099"><a href="#cb6-1099"></a>This score aims to capture the average probability of $\mathbf{x}$ being</span>
<span id="cb6-1100"><a href="#cb6-1100"></a>the winner of a duel.</span>
<span id="cb6-1101"><a href="#cb6-1101"></a></span>
<span id="cb6-1102"><a href="#cb6-1102"></a>We define the *Condorcet winner* $\mathbf{x}_c$ as the point with</span>
<span id="cb6-1103"><a href="#cb6-1103"></a>maximal soft-Copeland score. Note that this corresponds to the global</span>
<span id="cb6-1104"><a href="#cb6-1104"></a>minimum of $f$, since the defining integral takes maximum value for</span>
<span id="cb6-1105"><a href="#cb6-1105"></a>points $\mathbf{x} \in \mathcal{X}$ where</span>
<span id="cb6-1106"><a href="#cb6-1106"></a>$f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=$</span>
<span id="cb6-1107"><a href="#cb6-1107"></a>$g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})&gt;0$ or all</span>
<span id="cb6-1108"><a href="#cb6-1108"></a>$\mathbf{x}^{\prime}$, occurring only if $\mathbf{x}_c$ is a minimum of</span>
<span id="cb6-1109"><a href="#cb6-1109"></a>$f$. Therefore, if the preference function $\pi_f$ can be learned by</span>
<span id="cb6-1110"><a href="#cb6-1110"></a>observing the results of duels then our optimization problem of finding</span>
<span id="cb6-1111"><a href="#cb6-1111"></a>the minimum of $f$ can be solved by finding the Condorcet winner of the</span>
<span id="cb6-1112"><a href="#cb6-1112"></a>Copeland score.</span>
<span id="cb6-1113"><a href="#cb6-1113"></a></span>
<span id="cb6-1114"><a href="#cb6-1114"></a><span class="fu">### Acquisition Functions</span></span>
<span id="cb6-1115"><a href="#cb6-1115"></a></span>
<span id="cb6-1116"><a href="#cb6-1116"></a>We describe several acquisition functions for sequential learning of the</span>
<span id="cb6-1117"><a href="#cb6-1117"></a>Condorcet winner. Our dataset</span>
<span id="cb6-1118"><a href="#cb6-1118"></a>$\mathcal{D}=\left<span class="sc">\{</span>\left<span class="co">[</span><span class="ot">\mathbf{x}_i, \mathbf{x}_i^{\prime}\right</span><span class="co">]</span>, y_i\right<span class="sc">\}</span>_{i=1}^N$</span>
<span id="cb6-1119"><a href="#cb6-1119"></a>represents the $N$ duels that have been performed so far. We aim to</span>
<span id="cb6-1120"><a href="#cb6-1120"></a>define a sequential policy</span>
<span id="cb6-1121"><a href="#cb6-1121"></a>$\alpha\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span> ; \mathcal{D}_j, \theta\right)$</span>
<span id="cb6-1122"><a href="#cb6-1122"></a>for querying duels, where $\theta$ is a vector of model</span>
<span id="cb6-1123"><a href="#cb6-1123"></a>hyper-parameters, in order to find the minimum of the latent function</span>
<span id="cb6-1124"><a href="#cb6-1124"></a>$g$ as quickly as possible. Using Gaussian processes (GP) for</span>
<span id="cb6-1125"><a href="#cb6-1125"></a>classification with our dataset $\mathcal{D}$ allows us to perform</span>
<span id="cb6-1126"><a href="#cb6-1126"></a>inference over $f$ and $\pi_f$.</span>
<span id="cb6-1127"><a href="#cb6-1127"></a></span>
<span id="cb6-1128"><a href="#cb6-1128"></a><span class="fu">#### Pure Exploration {#pure-exploration .unnumbered}</span></span>
<span id="cb6-1129"><a href="#cb6-1129"></a></span>
<span id="cb6-1130"><a href="#cb6-1130"></a>The output variable $y_{\star}$ of a prediction follows a Bernoulli</span>
<span id="cb6-1131"><a href="#cb6-1131"></a>distribution with probability given by the preference function $\pi_f$.</span>
<span id="cb6-1132"><a href="#cb6-1132"></a>To carry out exploration as a policy, one method is to search for the</span>
<span id="cb6-1133"><a href="#cb6-1133"></a>duel where GP is most uncertain about the probability of the outcome</span>
<span id="cb6-1134"><a href="#cb6-1134"></a>(has the highest variance of $\sigma\left(f_{\star}\right)$ ), which is</span>
<span id="cb6-1135"><a href="#cb6-1135"></a>the result of transforming out epistemic uncertainty about $f$, modeled</span>
<span id="cb6-1136"><a href="#cb6-1136"></a>by a GP, through the logistic function. The first order moment of this</span>
<span id="cb6-1137"><a href="#cb6-1137"></a>distribution coincides with the expectation of $y_{\star}$ but its</span>
<span id="cb6-1138"><a href="#cb6-1138"></a>variance is $$\begin{aligned}</span>
<span id="cb6-1139"><a href="#cb6-1139"></a>\mathbb{V}\left<span class="co">[</span><span class="ot">\sigma\left(f_{\star}\right)\right</span><span class="co">]</span> &amp; =\int\left(\sigma\left(f_{\star}\right)-\mathbb{E}\left<span class="co">[</span><span class="ot">\sigma\left(f_{\star}\right)\right</span><span class="co">]</span>\right)^2 p\left(f_{\star} \mid \mathcal{D},\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right) d f_{\star} <span class="sc">\\</span></span>
<span id="cb6-1140"><a href="#cb6-1140"></a>&amp; =\int \sigma\left(f_{\star}\right)^2 p\left(f_{\star} \mid \mathcal{D},\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right) d f_{\star}-\mathbb{E}\left<span class="co">[</span><span class="ot">\sigma\left(f_{\star}\right)\right</span><span class="co">]</span>^2</span>
<span id="cb6-1141"><a href="#cb6-1141"></a>\end{aligned}$$ which explicitly takes into account the uncertainty over</span>
<span id="cb6-1142"><a href="#cb6-1142"></a>$f$. Hence, pure exploration of duels space can be carried out by</span>
<span id="cb6-1143"><a href="#cb6-1143"></a>maximizing</span>
<span id="cb6-1144"><a href="#cb6-1144"></a>$$\alpha_{\mathrm{PE}}\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span> \mid \mathcal{D}_j\right)=\mathbb{V}\left[\sigma\left(f_{\star}\right)\left|\left[\mathbf{x}_{\star}, \mathbf{x}_{\star}^{\prime}\right]\right| \mathcal{D}_j\right] .$$</span>
<span id="cb6-1145"><a href="#cb6-1145"></a></span>
<span id="cb6-1146"><a href="#cb6-1146"></a>Note that in this case, duels that have been already visited will have a</span>
<span id="cb6-1147"><a href="#cb6-1147"></a>lower chance of being visited again even in cases in which the objective</span>
<span id="cb6-1148"><a href="#cb6-1148"></a>takes similar values in both players. In practice, this acquisition</span>
<span id="cb6-1149"><a href="#cb6-1149"></a>functions requires computation of an intractable integral, that we</span>
<span id="cb6-1150"><a href="#cb6-1150"></a>approximate using Monte-Carlo.</span>
<span id="cb6-1151"><a href="#cb6-1151"></a></span>
<span id="cb6-1152"><a href="#cb6-1152"></a><span class="fu">#### Principled Optimistic Preferential Bayesian Optimization (POP-BO) {#principled-optimistic-preferential-bayesian-optimization-pop-bo .unnumbered}</span></span>
<span id="cb6-1153"><a href="#cb6-1153"></a></span>
<span id="cb6-1154"><a href="#cb6-1154"></a>In a slightly modified problem setup</span>
<span id="cb6-1155"><a href="#cb6-1155"></a><span class="co">[</span><span class="ot">@xu2024principledpreferentialbayesianoptimization</span><span class="co">]</span>, the algorithm tries</span>
<span id="cb6-1156"><a href="#cb6-1156"></a>to solve for the MLE $\hat{g}$ and its confidence set $\mathcal{B}_g$</span>
<span id="cb6-1157"><a href="#cb6-1157"></a>where $g$ is the ground truth black-box function. Assumptions include</span>
<span id="cb6-1158"><a href="#cb6-1158"></a>that $g$ is a member of a reproducing kernel Hilbert space (RKHS)</span>
<span id="cb6-1159"><a href="#cb6-1159"></a>$\mathcal{H}_k$ for some kernel function</span>
<span id="cb6-1160"><a href="#cb6-1160"></a>$k: \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}$, and</span>
<span id="cb6-1161"><a href="#cb6-1161"></a>$\|g\|_k \leq B$ so that</span>
<span id="cb6-1162"><a href="#cb6-1162"></a>$\mathcal{B}_g = \left<span class="sc">\{</span>\tilde{g} \in \mathcal{H}_k \mid\|\tilde{g}\|_k \leq B\right<span class="sc">\}</span>$.</span>
<span id="cb6-1163"><a href="#cb6-1163"></a>Similarly defining</span>
<span id="cb6-1164"><a href="#cb6-1164"></a>$f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=g\left(\mathbf{x}^{\prime}\right)-g(\mathbf{x})$,</span>
<span id="cb6-1165"><a href="#cb6-1165"></a>we model the preference function with a Bernoulli distribution as in</span>
<span id="cb6-1166"><a href="#cb6-1166"></a>Equation</span>
<span id="cb6-1167"><a href="#cb6-1167"></a><span class="co">[</span><span class="ot">\[eq:bernoulli_pref\]</span><span class="co">](#eq:bernoulli_pref)</span>{reference-type="ref"</span>
<span id="cb6-1168"><a href="#cb6-1168"></a>reference="eq:bernoulli_pref"} and also assume that probabilities follow</span>
<span id="cb6-1169"><a href="#cb6-1169"></a>the Bradley-Terry model, i.e.</span>
<span id="cb6-1170"><a href="#cb6-1170"></a>$$\pi_f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)=\sigma\left(f\left(\left<span class="co">[</span><span class="ot">\mathbf{x}, \mathbf{x}^{\prime}\right</span><span class="co">]</span>\right)\right)=\frac{e^{g(\mathbf{x})}}{e^{g(\mathbf{x})}+e^{g\left(\mathbf{x^{\prime}}\right)}}$$</span>
<span id="cb6-1171"><a href="#cb6-1171"></a></span>
<span id="cb6-1172"><a href="#cb6-1172"></a>The update rule for MLE $\hat{g}$ is (equation 8,6,5) $$\begin{aligned}</span>
<span id="cb6-1173"><a href="#cb6-1173"></a>\hat{g}_t^{\text {MLE }}&amp;:= \arg \underset{\tilde{g} \in \mathcal{B}^t_g}{\max}\ell_t(\tilde{g}) <span class="sc">\\</span></span>
<span id="cb6-1174"><a href="#cb6-1174"></a>\ell_t(\tilde{g}) &amp;:= \log \prod_{\tau=1}^t y_\tau \pi_{\tilde{f}}(<span class="co">[</span><span class="ot">\mathbf{x_\tau}, \mathbf{x^{\prime}_\tau}</span><span class="co">]</span>)+\left(1-y_\tau\right)\left(1-\pi_{\tilde{f}}(<span class="co">[</span><span class="ot">\mathbf{x_\tau}, \mathbf{x^{\prime}_\tau}</span><span class="co">]</span>)\right) <span class="sc">\\</span></span>
<span id="cb6-1175"><a href="#cb6-1175"></a>&amp;=\sum_{\tau=1}^t \log \left(\frac{e^{\tilde{g}(\mathbf{x_\tau})} y_\tau+e^{\tilde{g}(\mathbf{x_\tau^\prime})}\left(1-y_\tau\right)}{e^{\tilde{g}(\mathbf{x_\tau})}+e^{\tilde{g}(\mathbf{x_\tau^\prime})}}\right) <span class="sc">\\</span></span>
<span id="cb6-1176"><a href="#cb6-1176"></a>&amp;=\sum_{\tau=1}^t\left(\tilde{g}(\mathbf{x_\tau}) y_\tau+\tilde{g}(\mathbf{x_\tau^\prime})\left(1-y_\tau\right)\right)-\sum_{\tau=1}^t \log \left(e^{\tilde{g}(\mathbf{x_\tau})}+e^{\tilde{g}(\mathbf{x_\tau^\prime})}\right)</span>
<span id="cb6-1177"><a href="#cb6-1177"></a>\end{aligned}$$</span>
<span id="cb6-1178"><a href="#cb6-1178"></a></span>
<span id="cb6-1179"><a href="#cb6-1179"></a>(Eq 22 shows how to represent this as a convex optimisation problem so</span>
<span id="cb6-1180"><a href="#cb6-1180"></a>that it can be solved)</span>
<span id="cb6-1181"><a href="#cb6-1181"></a></span>
<span id="cb6-1182"><a href="#cb6-1182"></a>The update rule for the confidence set $\mathcal{B}_f^{t+1}$ is, (eq 9,</span>
<span id="cb6-1183"><a href="#cb6-1183"></a>10?)</span>
<span id="cb6-1184"><a href="#cb6-1184"></a></span>
<span id="cb6-1185"><a href="#cb6-1185"></a>$$\begin{aligned}</span>
<span id="cb6-1186"><a href="#cb6-1186"></a>&amp;\forall \epsilon, \delta &gt; 0 <span class="sc">\\</span></span>
<span id="cb6-1187"><a href="#cb6-1187"></a>&amp;\mathcal{B}_g^{t+1}:=\left<span class="sc">\{</span>\tilde{g} \in \mathcal{B}_g \mid \ell_t(\tilde{g}) \geq \ell_t\left(\hat{g}_t^{\mathrm{MLE}}\right)-\beta_1(\epsilon, \delta, t)\right<span class="sc">\}</span></span>
<span id="cb6-1188"><a href="#cb6-1188"></a>\end{aligned}$$ where</span>
<span id="cb6-1189"><a href="#cb6-1189"></a>$$\beta_1(\epsilon, \delta, t):=\sqrt{32 t B^2 \log \frac{\pi^2 t^2 \mathcal{N}\left(\mathcal{B}_f, \epsilon,\|\cdot\|_{\infty}\right)}{6 \delta}}+ C_L \epsilon t=\mathcal{O}\left(\sqrt{t \log \frac{t \mathcal{N}\left(\mathcal{B}_f, \epsilon,\|\cdot\|_{\infty}\right)}{\delta}}+\epsilon t\right),$$</span>
<span id="cb6-1190"><a href="#cb6-1190"></a>with $C_L$ a constant independent of $\delta, t$ and $\epsilon$.</span>
<span id="cb6-1191"><a href="#cb6-1191"></a>$\epsilon$ is typically chosen to be $1 / T$, where T is the running</span>
<span id="cb6-1192"><a href="#cb6-1192"></a>horizon of the algorithm. This satisfies the theorem that,</span>
<span id="cb6-1193"><a href="#cb6-1193"></a>$$\mathbb{P}\left(g \in \mathcal{B}_g^{t+1}, \forall t \geq 1\right) \geq 1-\delta .$$</span>
<span id="cb6-1194"><a href="#cb6-1194"></a></span>
<span id="cb6-1195"><a href="#cb6-1195"></a>Intuitively, the confidence set $\mathcal{B}_g^{t+1}$ includes the</span>
<span id="cb6-1196"><a href="#cb6-1196"></a>functions with the log-likelihood value that is only 'a little worse'</span>
<span id="cb6-1197"><a href="#cb6-1197"></a>than the maximum likelihood estimator, and the theorem states that</span>
<span id="cb6-1198"><a href="#cb6-1198"></a>$\mathcal{B}_g^{t+1}$ contains the ground-truth function $g$ with high</span>
<span id="cb6-1199"><a href="#cb6-1199"></a>probability.</span>
<span id="cb6-1200"><a href="#cb6-1200"></a></span>
<span id="cb6-1201"><a href="#cb6-1201"></a>Inner level optimization in Line 4 of the algorithm can also be</span>
<span id="cb6-1202"><a href="#cb6-1202"></a>represented as a convex optimisation problem so that it can be solved,</span>
<span id="cb6-1203"><a href="#cb6-1203"></a>Eq 24, 25. The outer optimisation can be solved using grid search or Eq</span>
<span id="cb6-1204"><a href="#cb6-1204"></a>26 for medium size problems.</span>
<span id="cb6-1205"><a href="#cb6-1205"></a></span>
<span id="cb6-1206"><a href="#cb6-1206"></a>:::: algorithm</span>
<span id="cb6-1207"><a href="#cb6-1207"></a>::: algorithmic</span>
<span id="cb6-1208"><a href="#cb6-1208"></a>Given the initial point $\mathbf{x_0} \in \mathcal{X}$ and set</span>
<span id="cb6-1209"><a href="#cb6-1209"></a>$\mathcal{B}_g^1 = \mathcal{B}_g$ Set the reference point</span>
<span id="cb6-1210"><a href="#cb6-1210"></a>$\mathbf{x_t^{\prime}} = \mathbf{x_{t-1}}$ Compute</span>
<span id="cb6-1211"><a href="#cb6-1211"></a>$\mathbf{x_t} \in \arg\max_{\mathbf{x} \in \mathcal{X}} \max_{\tilde{g} \in \mathcal{B}_g^t} (\tilde{g}(\mathbf{x}) - \tilde{g}(\mathbf{x_t^{\prime}}))$,</span>
<span id="cb6-1212"><a href="#cb6-1212"></a>with the inner optimal function denoted as $\tilde{g}_t$ Obtain the</span>
<span id="cb6-1213"><a href="#cb6-1213"></a>output of the duel $y_t$ and append the new data point to</span>
<span id="cb6-1214"><a href="#cb6-1214"></a>$\mathcal{D}_t$ Update the maximum likelihood estimator</span>
<span id="cb6-1215"><a href="#cb6-1215"></a>$\hat{g}_t^{\mathrm{MLE}}$ and the posterior confidence set</span>
<span id="cb6-1216"><a href="#cb6-1216"></a>$\mathcal{B}_g^{t+1}$.</span>
<span id="cb6-1217"><a href="#cb6-1217"></a>:::</span>
<span id="cb6-1218"><a href="#cb6-1218"></a>::::</span>
<span id="cb6-1219"><a href="#cb6-1219"></a></span>
<span id="cb6-1220"><a href="#cb6-1220"></a><span class="fu">#### qEUBO: Decision-Theoretic EUBO {#qeubo-decision-theoretic-eubo .unnumbered}</span></span>
<span id="cb6-1221"><a href="#cb6-1221"></a></span>
<span id="cb6-1222"><a href="#cb6-1222"></a>qEUBO <span class="co">[</span><span class="ot">@astudillo2023qeubodecisiontheoreticacquisitionfunction</span><span class="co">]</span> derives</span>
<span id="cb6-1223"><a href="#cb6-1223"></a>an acquisition function that extends duels to $q&gt;2$ options which we</span>
<span id="cb6-1224"><a href="#cb6-1224"></a>call *queries*. Let</span>
<span id="cb6-1225"><a href="#cb6-1225"></a>$X=\left(\mathbf{x_1}, \ldots, \mathbf{x_q}\right) \in \mathcal{X}^q$</span>
<span id="cb6-1226"><a href="#cb6-1226"></a>denote a query containing two points or more, and let</span>
<span id="cb6-1227"><a href="#cb6-1227"></a>$g: \mathcal{X} \rightarrow \Re$ be the latent preference function. Then</span>
<span id="cb6-1228"><a href="#cb6-1228"></a>after $n$ user queries, we define the *expected utility of the best</span>
<span id="cb6-1229"><a href="#cb6-1229"></a>option* (qEUBO) as</span>
<span id="cb6-1230"><a href="#cb6-1230"></a>$$\mathrm{qEUBO}_n(X)=\mathbb{E}_n\left<span class="co">[</span><span class="ot">\max \left\{g\left(x_1\right), \ldots, g\left(x_q\right)\right\}\right</span><span class="co">]</span>.$$</span>
<span id="cb6-1231"><a href="#cb6-1231"></a></span>
<span id="cb6-1232"><a href="#cb6-1232"></a>We now show that qEUBO is one-step Bayes optimal, meaning that each step</span>
<span id="cb6-1233"><a href="#cb6-1233"></a>chooses the query that maximises the expected utility received by the</span>
<span id="cb6-1234"><a href="#cb6-1234"></a>human. For a query $X \in \mathcal{X}^q$, let</span>
<span id="cb6-1235"><a href="#cb6-1235"></a>$$V_n(X)=\mathbb{E}_n\left[\max _{x \in \mathbb{X}} \mathbb{E}_{n+1}[g(x)] \mid X_{n+1}=X\right] .$$</span>
<span id="cb6-1236"><a href="#cb6-1236"></a>Then $V_n$ defines the expected utility received if an additional query</span>
<span id="cb6-1237"><a href="#cb6-1237"></a>$X_{n+1}=X$ is performed, and maximizing $V_n$ is one-step Bayes</span>
<span id="cb6-1238"><a href="#cb6-1238"></a>optimal. Since $\max _{x \in \mathbb{X}} \mathbb{E}_n<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>$ does not</span>
<span id="cb6-1239"><a href="#cb6-1239"></a>depend on $X_{n+1}$, we can also equivalently maximize</span>
<span id="cb6-1240"><a href="#cb6-1240"></a>$$\mathbb{E}_n\left[\max _{x \in \mathbb{X}} \mathbb{E}_{n+1}[g(x)]-\max _{x \in \mathbb{X}} \mathbb{E}_n[g(x)] \mid X_{n+1}=X\right],$$</span>
<span id="cb6-1241"><a href="#cb6-1241"></a>which takes the same form as the knowledge gradient acquisition function</span>
<span id="cb6-1242"><a href="#cb6-1242"></a><span class="co">[</span><span class="ot">@wu2018parallelknowledgegradientmethod</span><span class="co">]</span> in standard Bayesian</span>
<span id="cb6-1243"><a href="#cb6-1243"></a>optimization.</span>
<span id="cb6-1244"><a href="#cb6-1244"></a></span>
<span id="cb6-1245"><a href="#cb6-1245"></a>$V_n$ involves a nested stochastic optimization task, while qEUBO is a</span>
<span id="cb6-1246"><a href="#cb6-1246"></a>much simpler policy. When human responses are noise-free, we are able to</span>
<span id="cb6-1247"><a href="#cb6-1247"></a>use qEUBO as a sufficient policy due to the following theorem:</span>
<span id="cb6-1248"><a href="#cb6-1248"></a></span>
<span id="cb6-1249"><a href="#cb6-1249"></a>::: theorem</span>
<span id="cb6-1250"><a href="#cb6-1250"></a>$$\underset{X \in \mathbb{X}^q}{\operatorname{argmax}} \mathrm{qEUBO}_n(X) \subseteq \underset{X \in \mathbb{X}^q}{\operatorname{argmax}} V_n(X) .$$</span>
<span id="cb6-1251"><a href="#cb6-1251"></a>:::</span>
<span id="cb6-1252"><a href="#cb6-1252"></a></span>
<span id="cb6-1253"><a href="#cb6-1253"></a>::: proof</span>
<span id="cb6-1254"><a href="#cb6-1254"></a>*Proof.* For a query $X \in \mathcal{X}^q$, let</span>
<span id="cb6-1255"><a href="#cb6-1255"></a>$x^{+}(X, i) \in \operatorname{argmax}_{x \in \mathbb{X}} \mathbb{E}_n<span class="co">[</span><span class="ot">g(x) \mid(X, i)</span><span class="co">]</span>$</span>
<span id="cb6-1256"><a href="#cb6-1256"></a>and define $X^{+}(X)=$ $\left(x^{+}(X, 1), \ldots, x^{+}(X, q)\right)$.</span>
<span id="cb6-1257"><a href="#cb6-1257"></a></span>
<span id="cb6-1258"><a href="#cb6-1258"></a>**Claim 1** $V_n(X) \leq \mathrm{qEUBO}_n\left(X^{+}(X)\right) .$ We see</span>
<span id="cb6-1259"><a href="#cb6-1259"></a>that $$\begin{aligned}</span>
<span id="cb6-1260"><a href="#cb6-1260"></a>V_n(X) &amp; =\sum_{i=1}^q \mathbf{P}_n(r(X)=i) \mathbb{E}_n<span class="co">[</span><span class="ot">g\left(x^{+}(X, i)\right) </span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-1261"><a href="#cb6-1261"></a>&amp; \leq \sum_{i=1}^q \mathbf{P}_n(r(X)=i) \mathbb{E}_n<span class="co">[</span><span class="ot">\max _{i=1, \ldots, q} g(x^{+}(X, i))</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-1262"><a href="#cb6-1262"></a>&amp; =\mathbb{E}_n\left<span class="co">[</span><span class="ot">\max _{i=1, \ldots, q} g\left(x^{+}(X, i)\right)\right</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb6-1263"><a href="#cb6-1263"></a>&amp; =\mathrm{qEUBO}_n\left(X^{+}(X)\right),</span>
<span id="cb6-1264"><a href="#cb6-1264"></a>\end{aligned}$$ as claimed.</span>
<span id="cb6-1265"><a href="#cb6-1265"></a></span>
<span id="cb6-1266"><a href="#cb6-1266"></a>**Claim 2** $\mathrm{qEUBO}_n(X) \leq V_n(X) .$ For any given</span>
<span id="cb6-1267"><a href="#cb6-1267"></a>$X \in \mathbb{X}^q$ we have</span>
<span id="cb6-1268"><a href="#cb6-1268"></a>$$\mathbb{E}_n\left[f\left(x_{r(X)}\right) \mid(X, r(X))\right] \leq \max _{x \in \mathbb{X}} \mathbb{E}_n<span class="co">[</span><span class="ot">f(x) \mid(X, r(X))</span><span class="co">]</span> .$$</span>
<span id="cb6-1269"><a href="#cb6-1269"></a>Since $f\left(x_{r(X)}\right)=\max _{i=1, \ldots, q} f\left(x_i\right)$,</span>
<span id="cb6-1270"><a href="#cb6-1270"></a>taking expectations over $r(X)$ on both sides obtains the required</span>
<span id="cb6-1271"><a href="#cb6-1271"></a>result.</span>
<span id="cb6-1272"><a href="#cb6-1272"></a></span>
<span id="cb6-1273"><a href="#cb6-1273"></a>Now building on the arguments above, let</span>
<span id="cb6-1274"><a href="#cb6-1274"></a>$X^* \in \operatorname{argmax}_{X \in \mathbb{X}^q} \mathrm{qEUBO}_n(X)$</span>
<span id="cb6-1275"><a href="#cb6-1275"></a>and suppose for contradiction that</span>
<span id="cb6-1276"><a href="#cb6-1276"></a>$X^* \notin \operatorname{argmax}_{X \in \mathbb{X}^q} V_n(X)$. Then,</span>
<span id="cb6-1277"><a href="#cb6-1277"></a>there exists $\widetilde{X} \in \mathbb{X}^q$ such that</span>
<span id="cb6-1278"><a href="#cb6-1278"></a>$V_n(\widetilde{X})&gt;V_n\left(X^*\right)$. We have $$\begin{aligned}</span>
<span id="cb6-1279"><a href="#cb6-1279"></a>\operatorname{qEUBO}_n\left(X^{+}(\tilde{X})\right) &amp; \geq V_n(\tilde{X}) <span class="sc">\\</span></span>
<span id="cb6-1280"><a href="#cb6-1280"></a>&amp; &gt;V_n\left(X^*\right) <span class="sc">\\</span></span>
<span id="cb6-1281"><a href="#cb6-1281"></a>&amp; \geq \operatorname{qEUBO}_n\left(X^*\right) <span class="sc">\\</span></span>
<span id="cb6-1282"><a href="#cb6-1282"></a>&amp; \geq \operatorname{qEUBO}_n\left(X^{+}(\tilde{X})\right) .</span>
<span id="cb6-1283"><a href="#cb6-1283"></a>\end{aligned}$$</span>
<span id="cb6-1284"><a href="#cb6-1284"></a></span>
<span id="cb6-1285"><a href="#cb6-1285"></a>The first inequality follows from (1). The second inequality is due to</span>
<span id="cb6-1286"><a href="#cb6-1286"></a>our supposition for contradiction. The third inequality is due to (2).</span>
<span id="cb6-1287"><a href="#cb6-1287"></a>Finally, the fourth inequality holds since</span>
<span id="cb6-1288"><a href="#cb6-1288"></a>$X^* \in \operatorname{argmax}_{X \in \mathbb{X}^q} \mathrm{qEUBO}_n(X)$.</span>
<span id="cb6-1289"><a href="#cb6-1289"></a>This contradiction concludes the proof. ◻</span>
<span id="cb6-1290"><a href="#cb6-1290"></a>:::</span>
<span id="cb6-1291"><a href="#cb6-1291"></a></span>
<span id="cb6-1292"><a href="#cb6-1292"></a>Therefore a sufficient condition for following one-step Bayes optimality</span>
<span id="cb6-1293"><a href="#cb6-1293"></a>is by maximizing $\text{qEUBO}_n$.</span>
<span id="cb6-1294"><a href="#cb6-1294"></a></span>
<span id="cb6-1295"><a href="#cb6-1295"></a>In experiments that were ran comparing qEUBO to other state-of-the-art</span>
<span id="cb6-1296"><a href="#cb6-1296"></a>acquisition functions, qEUBO consistently outperformed on most problems</span>
<span id="cb6-1297"><a href="#cb6-1297"></a>and was closely followed by qEI and qTS. These results also extended to</span>
<span id="cb6-1298"><a href="#cb6-1298"></a>experiments with multiple options when $q&gt;2$. In fact, there is faster</span>
<span id="cb6-1299"><a href="#cb6-1299"></a>convergence in regret when using more options in human queries. <span class="sc">\[</span>Prove</span>
<span id="cb6-1300"><a href="#cb6-1300"></a>Theorem 3: Regret analysis<span class="sc">\]</span></span>
<span id="cb6-1301"><a href="#cb6-1301"></a></span>
<span id="cb6-1302"><a href="#cb6-1302"></a><span class="fu">#### qEI: Batch Expected Improvement {#qei-batch-expected-improvement .unnumbered}</span></span>
<span id="cb6-1303"><a href="#cb6-1303"></a></span>
<span id="cb6-1304"><a href="#cb6-1304"></a>$$\begin{aligned}</span>
<span id="cb6-1305"><a href="#cb6-1305"></a>\mathrm{qEI}= &amp; \mathbb{E}_{\mathbf{y}}\left[\left(\max _{i \in[1, \ldots, q]}\left(\mu_{\min }-y_i\right)\right)_{+}\right] <span class="sc">\\</span></span>
<span id="cb6-1306"><a href="#cb6-1306"></a>= &amp; \sum_{i=1}^q \mathbb{E}_{\mathbf{y}}\left(\mu_{\min }-y_i \mid y_i \leq \mu_{\min }, y_i \leq y_j \forall j \neq i\right) <span class="sc">\\</span></span>
<span id="cb6-1307"><a href="#cb6-1307"></a>&amp; p\left(y_i \leq \mu_{\min }, y_i \leq y_j \forall j \neq i\right) .</span>
<span id="cb6-1308"><a href="#cb6-1308"></a>\end{aligned}$$</span>
<span id="cb6-1309"><a href="#cb6-1309"></a></span>
<span id="cb6-1310"><a href="#cb6-1310"></a><span class="fu">#### qTS: Batch Thompson Sampling {#qts-batch-thompson-sampling .unnumbered}</span></span>
<span id="cb6-1311"><a href="#cb6-1311"></a></span>
<span id="cb6-1312"><a href="#cb6-1312"></a>:::: algorithm</span>
<span id="cb6-1313"><a href="#cb6-1313"></a>::: algorithmic</span>
<span id="cb6-1314"><a href="#cb6-1314"></a>Initial data</span>
<span id="cb6-1315"><a href="#cb6-1315"></a>$\mathcal{D}_{\mathcal{I}(1)}=\{(\mathbf{x}_i, y_i)\}_{i \in \mathcal{I}(1)}$</span>
<span id="cb6-1316"><a href="#cb6-1316"></a>Compute current posterior</span>
<span id="cb6-1317"><a href="#cb6-1317"></a>$p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})$ Sample</span>
<span id="cb6-1318"><a href="#cb6-1318"></a>$\boldsymbol{\theta}$ from</span>
<span id="cb6-1319"><a href="#cb6-1319"></a>$p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})$ Select</span>
<span id="cb6-1320"><a href="#cb6-1320"></a>$k \leftarrow \arg \max_{j \notin \mathcal{I}(t)} \mathbb{E}<span class="co">[</span><span class="ot">y_j \mid \mathbf{x}_j, \boldsymbol{\theta}</span><span class="co">]</span>$</span>
<span id="cb6-1321"><a href="#cb6-1321"></a>Collect $y_k$ by evaluating $f$ at $\mathbf{x}_k$</span>
<span id="cb6-1322"><a href="#cb6-1322"></a>$\mathcal{D}_{\mathcal{I}(t+1)} \leftarrow \mathcal{D}_{\mathcal{I}(t)} \cup <span class="sc">\{</span>(\mathbf{x}_k, y_k)<span class="sc">\}</span>$</span>
<span id="cb6-1323"><a href="#cb6-1323"></a>:::</span>
<span id="cb6-1324"><a href="#cb6-1324"></a>::::</span>
<span id="cb6-1325"><a href="#cb6-1325"></a></span>
<span id="cb6-1326"><a href="#cb6-1326"></a>:::: algorithm</span>
<span id="cb6-1327"><a href="#cb6-1327"></a>::: algorithmic</span>
<span id="cb6-1328"><a href="#cb6-1328"></a>Initial data</span>
<span id="cb6-1329"><a href="#cb6-1329"></a>$\mathcal{D}_{\mathcal{I}(1)}=\{\mathbf{x}_i, y_i\}_{i \in \mathcal{I}(1)}$,</span>
<span id="cb6-1330"><a href="#cb6-1330"></a>batch size $S$ Compute current posterior</span>
<span id="cb6-1331"><a href="#cb6-1331"></a>$p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})$ Sample</span>
<span id="cb6-1332"><a href="#cb6-1332"></a>$\boldsymbol{\theta}$ from</span>
<span id="cb6-1333"><a href="#cb6-1333"></a>$p(\boldsymbol{\theta} \mid \mathcal{D}_{\mathcal{I}(t)})$ Select</span>
<span id="cb6-1334"><a href="#cb6-1334"></a>$k(s) \leftarrow \arg \max_{j \notin \mathcal{I}(t)} \mathbb{E}<span class="co">[</span><span class="ot">y_j \mid \mathbf{x}_j, \boldsymbol{\theta}</span><span class="co">]</span>$</span>
<span id="cb6-1335"><a href="#cb6-1335"></a>$\mathcal{D}_{\mathcal{I}(t+1)} = \mathcal{D}_{\mathcal{I}(t)} \cup \{\mathbf{x}_{k(s)}, y_{k(s)}\}_{s=1}^S$</span>
<span id="cb6-1336"><a href="#cb6-1336"></a>:::</span>
<span id="cb6-1337"><a href="#cb6-1337"></a>::::</span>
<span id="cb6-1338"><a href="#cb6-1338"></a></span>
<span id="cb6-1339"><a href="#cb6-1339"></a><span class="fu">### Regret Analysis</span></span>
<span id="cb6-1340"><a href="#cb6-1340"></a></span>
<span id="cb6-1341"><a href="#cb6-1341"></a><span class="fu">#### qEUBO Regret {#qeubo-regret .unnumbered}</span></span>
<span id="cb6-1342"><a href="#cb6-1342"></a></span>
<span id="cb6-1343"><a href="#cb6-1343"></a>With the definition of Bayesian simple regret, we have that qEUBO</span>
<span id="cb6-1344"><a href="#cb6-1344"></a>converges to zero at a rate of $o(1/n)$, i.e.</span>
<span id="cb6-1345"><a href="#cb6-1345"></a></span>
<span id="cb6-1346"><a href="#cb6-1346"></a>::: theorem</span>
<span id="cb6-1347"><a href="#cb6-1347"></a>$$\label{th:quebo_regret}</span>
<span id="cb6-1348"><a href="#cb6-1348"></a>\mathbb{E}\left<span class="co">[</span><span class="ot">f\left(x^*\right)-f\left(\widehat{x}_n^*\right)\right</span><span class="co">]</span>=o(1 / n)$$</span>
<span id="cb6-1349"><a href="#cb6-1349"></a>:::</span>
<span id="cb6-1350"><a href="#cb6-1350"></a></span>
<span id="cb6-1351"><a href="#cb6-1351"></a>where $x^*=\operatorname{argmax}_{x \in \mathrm{X}} f(x)$ and</span>
<span id="cb6-1352"><a href="#cb6-1352"></a>$\widehat{x}_n^* \in \operatorname{argmax}_{x \in \mathrm{X}} \mathbb{E}_n<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>$.</span>
<span id="cb6-1353"><a href="#cb6-1353"></a></span>
<span id="cb6-1354"><a href="#cb6-1354"></a>This theorem holds under the following assumptions:</span>
<span id="cb6-1355"><a href="#cb6-1355"></a></span>
<span id="cb6-1356"><a href="#cb6-1356"></a><span class="ss">1.  </span>**$f$ is injective** $\mathbf{P}(f(x)=f(y))=0$ for any</span>
<span id="cb6-1357"><a href="#cb6-1357"></a>    $x, y \in \mathbb{X}$ with $x \neq y$.</span>
<span id="cb6-1358"><a href="#cb6-1358"></a></span>
<span id="cb6-1359"><a href="#cb6-1359"></a><span class="ss">2.  </span>**$f$ represents the preferred option** $\exists a&gt;1 / 2$ s.t.</span>
<span id="cb6-1360"><a href="#cb6-1360"></a>    $\mathbf{P}\left(r(X) \in \operatorname{argmax}_{i=1, \ldots, 2} f\left(x_i\right) \mid f(X)\right) \geq a \forall$</span>
<span id="cb6-1361"><a href="#cb6-1361"></a>    $X=\left(x_1, x_2\right) \in \mathbb{X}^2$ with $x_1 \neq x_2$</span>
<span id="cb6-1362"><a href="#cb6-1362"></a>    almost surely under the prior on $f$.</span>
<span id="cb6-1363"><a href="#cb6-1363"></a></span>
<span id="cb6-1364"><a href="#cb6-1364"></a><span class="ss">3.  </span>**Expected difference in utility is proportional to probability of</span>
<span id="cb6-1365"><a href="#cb6-1365"></a>    greater utility** $\exists \Delta \geq \delta&gt;0$ s.t.</span>
<span id="cb6-1366"><a href="#cb6-1366"></a>    $\forall \mathcal{D}^{(n)} \text{and} \forall x, y \in \mathbb{X}$</span>
<span id="cb6-1367"><a href="#cb6-1367"></a>    (potentially depending on $\mathcal{D}^{(n)}$),</span>
<span id="cb6-1368"><a href="#cb6-1368"></a>    $$\delta \mathbf{P}^{(n)}(f(x)&gt;f(y)) \leq \mathbb{E}^{(n)}\left<span class="co">[</span><span class="ot">\{f(x)-f(y)\}^{+}\right</span><span class="co">]</span> \leq \Delta \mathbf{P}^{(n)}(f(x)&gt;f(y))$$</span>
<span id="cb6-1369"><a href="#cb6-1369"></a>    almost surely under the prior on $f$.</span>
<span id="cb6-1370"><a href="#cb6-1370"></a></span>
<span id="cb6-1371"><a href="#cb6-1371"></a>Further lemmas leading to a proof of Theorem</span>
<span id="cb6-1372"><a href="#cb6-1372"></a><span class="co">[</span><span class="ot">\[th:quebo_regret\]</span><span class="co">](#th:quebo_regret)</span>{reference-type="ref"</span>
<span id="cb6-1373"><a href="#cb6-1373"></a>reference="th:quebo_regret"} is given in</span>
<span id="cb6-1374"><a href="#cb6-1374"></a><span class="co">[</span><span class="ot">@astudillo2023qeubodecisiontheoreticacquisitionfunction</span><span class="co">]</span> Section B.</span>
<span id="cb6-1375"><a href="#cb6-1375"></a></span>
<span id="cb6-1376"><a href="#cb6-1376"></a><span class="fu">#### qEI Regret {#qei-regret .unnumbered}</span></span>
<span id="cb6-1377"><a href="#cb6-1377"></a></span>
<span id="cb6-1378"><a href="#cb6-1378"></a>The following theorem shows that, under the same assumptions used for</span>
<span id="cb6-1379"><a href="#cb6-1379"></a>qEUBO regret, simple regret of qEI can fail to converge to 0.</span>
<span id="cb6-1380"><a href="#cb6-1380"></a></span>
<span id="cb6-1381"><a href="#cb6-1381"></a>::: theorem</span>
<span id="cb6-1382"><a href="#cb6-1382"></a>There exists a problem instance (i.e., $\mathbb{X}$ and Bayesian prior</span>
<span id="cb6-1383"><a href="#cb6-1383"></a>distribution over f) satisfying the assumptions described in Theorem</span>
<span id="cb6-1384"><a href="#cb6-1384"></a><span class="co">[</span><span class="ot">\[th:quebo_regret\]</span><span class="co">](#th:quebo_regret)</span>{reference-type="ref"</span>
<span id="cb6-1385"><a href="#cb6-1385"></a>reference="th:quebo_regret"} such that if the sequence of queries is</span>
<span id="cb6-1386"><a href="#cb6-1386"></a>chosen by maximizing qEI, then</span>
<span id="cb6-1387"><a href="#cb6-1387"></a>$\mathbb{E}\left[f\left(x^*\right)-\right.$</span>
<span id="cb6-1388"><a href="#cb6-1388"></a>$\left.f\left(\widehat{x}_n^*\right)\right] \geq R$ for all $n$, for a</span>
<span id="cb6-1389"><a href="#cb6-1389"></a>constant $R&gt;0$.</span>
<span id="cb6-1390"><a href="#cb6-1390"></a>:::</span>
<span id="cb6-1391"><a href="#cb6-1391"></a></span>
<span id="cb6-1392"><a href="#cb6-1392"></a>::: proof</span>
<span id="cb6-1393"><a href="#cb6-1393"></a>*Proof.* Let $X = <span class="sc">\{</span>1, 2, 3, 4<span class="sc">\}</span>$ and consider the functions</span>
<span id="cb6-1394"><a href="#cb6-1394"></a>$f_i:X \rightarrow R$, for $i=1,2,3,4$, given by $f_i(1) = -1$ and</span>
<span id="cb6-1395"><a href="#cb6-1395"></a>$f_i(2) = 0$ for all $i$, and $$\begin{aligned}</span>
<span id="cb6-1396"><a href="#cb6-1396"></a>    f_1(x) = \begin{cases}</span>
<span id="cb6-1397"><a href="#cb6-1397"></a>    1, &amp;\ x=3<span class="sc">\\</span></span>
<span id="cb6-1398"><a href="#cb6-1398"></a>    \frac{1}{2}, &amp;\ x=4</span>
<span id="cb6-1399"><a href="#cb6-1399"></a>    \end{cases},</span>
<span id="cb6-1400"><a href="#cb6-1400"></a>\hspace{0.5cm}</span>
<span id="cb6-1401"><a href="#cb6-1401"></a>f_2(x) = \begin{cases}</span>
<span id="cb6-1402"><a href="#cb6-1402"></a>    \frac{1}{2}, &amp;\ x=3<span class="sc">\\</span></span>
<span id="cb6-1403"><a href="#cb6-1403"></a>    1, &amp;\ x=4</span>
<span id="cb6-1404"><a href="#cb6-1404"></a>    \end{cases},</span>
<span id="cb6-1405"><a href="#cb6-1405"></a>\hspace{0.5cm}</span>
<span id="cb6-1406"><a href="#cb6-1406"></a>f_3(x) = \begin{cases}</span>
<span id="cb6-1407"><a href="#cb6-1407"></a>    -\frac{1}{2}, &amp;\ x=3<span class="sc">\\</span></span>
<span id="cb6-1408"><a href="#cb6-1408"></a>    -1, &amp;\ x=4</span>
<span id="cb6-1409"><a href="#cb6-1409"></a>    \end{cases},</span>
<span id="cb6-1410"><a href="#cb6-1410"></a>\hspace{0.5cm}</span>
<span id="cb6-1411"><a href="#cb6-1411"></a>f_4(x) = \begin{cases}</span>
<span id="cb6-1412"><a href="#cb6-1412"></a>    -1, &amp;\ x=3<span class="sc">\\</span></span>
<span id="cb6-1413"><a href="#cb6-1413"></a>    -\frac{1}{2}, &amp;\ x=4</span>
<span id="cb6-1414"><a href="#cb6-1414"></a>    \end{cases}.</span>
<span id="cb6-1415"><a href="#cb6-1415"></a>\end{aligned}$$</span>
<span id="cb6-1416"><a href="#cb6-1416"></a></span>
<span id="cb6-1417"><a href="#cb6-1417"></a>Let $p$ be a number with $0 &lt; p &lt; 1/3$ and set $q=1-p$. We consider a</span>
<span id="cb6-1418"><a href="#cb6-1418"></a>prior distribution on $f$ with support $<span class="sc">\{</span>f_i<span class="sc">\}</span>_{i=1}^4$ such that</span>
<span id="cb6-1419"><a href="#cb6-1419"></a>$$\begin{aligned}</span>
<span id="cb6-1420"><a href="#cb6-1420"></a>p_i = Pr(f=f_i) = </span>
<span id="cb6-1421"><a href="#cb6-1421"></a>    \begin{cases}</span>
<span id="cb6-1422"><a href="#cb6-1422"></a>        p/2, i =1,2,<span class="sc">\\</span></span>
<span id="cb6-1423"><a href="#cb6-1423"></a>        q/2, i=3,4.</span>
<span id="cb6-1424"><a href="#cb6-1424"></a>    \end{cases}</span>
<span id="cb6-1425"><a href="#cb6-1425"></a>\end{aligned}$$ We also assume the user's response likelihood is given</span>
<span id="cb6-1426"><a href="#cb6-1426"></a>by $Pr(r(X)=1\mid f(x_1) &gt; f(x_2)) = a$ for some $a$ such that</span>
<span id="cb6-1427"><a href="#cb6-1427"></a>$1/2 &lt; a &lt; 1$,</span>
<span id="cb6-1428"><a href="#cb6-1428"></a></span>
<span id="cb6-1429"><a href="#cb6-1429"></a>Let $D^{(n)}$ denote the set of observations up to time $n$ and let</span>
<span id="cb6-1430"><a href="#cb6-1430"></a>$p_i^{(n)} = Pr(f=f_i \mid \mathbb{E}^{(n)})$ for $i=1,2,3,4$. We let the</span>
<span id="cb6-1431"><a href="#cb6-1431"></a>initial data set be $\mathcal{D}^{(0)} = <span class="sc">\{</span>(X^{(0)}, r^{(0)})<span class="sc">\}</span>$, where</span>
<span id="cb6-1432"><a href="#cb6-1432"></a>$X^{(0)}= (1,2)$. We will prove that the following statements are true</span>
<span id="cb6-1433"><a href="#cb6-1433"></a>for all $n\geq 0$.</span>
<span id="cb6-1434"><a href="#cb6-1434"></a></span>
<span id="cb6-1435"><a href="#cb6-1435"></a><span class="ss">1.  </span>$p_i^{(n)} &gt; 0$ for $i=1,2,3,4$.</span>
<span id="cb6-1436"><a href="#cb6-1436"></a></span>
<span id="cb6-1437"><a href="#cb6-1437"></a><span class="ss">2.  </span>$p_1^{(n)} &lt; \frac{1}{2}p_3^{(n)}$ and</span>
<span id="cb6-1438"><a href="#cb6-1438"></a>    $p_2^{(n)} &lt; \frac{1}{2}p_4^{(n)}$.</span>
<span id="cb6-1439"><a href="#cb6-1439"></a></span>
<span id="cb6-1440"><a href="#cb6-1440"></a><span class="ss">3.  </span>$\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>=<span class="sc">\{</span>2<span class="sc">\}</span>$.</span>
<span id="cb6-1441"><a href="#cb6-1441"></a></span>
<span id="cb6-1442"><a href="#cb6-1442"></a><span class="ss">4.  </span>$\arg \max_{X\in\mathcal{X}^2}\text{qEI}^{(n)}(X) = <span class="sc">\{</span>(3, 4)<span class="sc">\}</span>$.</span>
<span id="cb6-1443"><a href="#cb6-1443"></a></span>
<span id="cb6-1444"><a href="#cb6-1444"></a>We prove this by induction over $n$. We begin by proving this for $n=0$.</span>
<span id="cb6-1445"><a href="#cb6-1445"></a>Since $f_i(1) &lt; f_i(2)$ for all $i$, the posterior distribution on $f$</span>
<span id="cb6-1446"><a href="#cb6-1446"></a>given $\mathcal{D}^{(0)}$ remains the same as the prior; i.e., $p_i^{(0)} = p_i$</span>
<span id="cb6-1447"><a href="#cb6-1447"></a>for $i=1,2,3,4$. Using this, statements 1 and 2 can be easily verified.</span>
<span id="cb6-1448"><a href="#cb6-1448"></a>Now note that $\mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(1)</span><span class="co">]</span>=-1$, $\mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(2)</span><span class="co">]</span>=0$, and</span>
<span id="cb6-1449"><a href="#cb6-1449"></a>$\mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(3)</span><span class="co">]</span> = \mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(4)</span><span class="co">]</span> = \frac{3}{2}(p - q)$. Since $p &lt; q$,</span>
<span id="cb6-1450"><a href="#cb6-1450"></a>it follows that $\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>=<span class="sc">\{</span>2<span class="sc">\}</span>$; i.e., statement</span>
<span id="cb6-1451"><a href="#cb6-1451"></a>3 holds. Finally, since $\max_{x\in<span class="sc">\{</span>1,2<span class="sc">\}</span>}\mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span> = 0$, the qEI</span>
<span id="cb6-1452"><a href="#cb6-1452"></a>acquisition function at time $n=0$ is given by</span>
<span id="cb6-1453"><a href="#cb6-1453"></a>$\text{qEI}^{(0)}(X) = \mathbb{E}^{(0)}<span class="co">[</span><span class="ot">\{\max\{f(x_1), f(x_2)\}\}^+</span><span class="co">]</span>$. A direct</span>
<span id="cb6-1454"><a href="#cb6-1454"></a>calculation can now be performed to verify that statement 4 holds. This</span>
<span id="cb6-1455"><a href="#cb6-1455"></a>completes the base case.</span>
<span id="cb6-1456"><a href="#cb6-1456"></a></span>
<span id="cb6-1457"><a href="#cb6-1457"></a>Now suppose statements 1-4 hold for some $n\geq 0$. Since</span>
<span id="cb6-1458"><a href="#cb6-1458"></a>$X^{(n+1)} = (3, 4)$, the posterior distribution on $f$ given</span>
<span id="cb6-1459"><a href="#cb6-1459"></a>$D^{(n+1)}$ is given by $$\begin{aligned}</span>
<span id="cb6-1460"><a href="#cb6-1460"></a>p_i^{(n+1)} \propto \begin{cases}</span>
<span id="cb6-1461"><a href="#cb6-1461"></a>                        p_i^{(n)}\ell, \ i=1,3,<span class="sc">\\</span></span>
<span id="cb6-1462"><a href="#cb6-1462"></a>                         p_i^{(n)} (1 - \ell), \ i=2,4,</span>
<span id="cb6-1463"><a href="#cb6-1463"></a>                        \end{cases}</span>
<span id="cb6-1464"><a href="#cb6-1464"></a>\end{aligned}$$ where</span>
<span id="cb6-1465"><a href="#cb6-1465"></a>$$\ell = a I<span class="sc">\{</span>r^{(n+1)} = 1<span class="sc">\}</span> + (1-a)I<span class="sc">\{</span>r^{(n+1)} = 2<span class="sc">\}</span>.$$ Observe that</span>
<span id="cb6-1466"><a href="#cb6-1466"></a>$0&lt; \ell &lt; 1$ since $0 &lt; a &lt; 1$. Thus, $\ell &gt; 0$ and $1-\ell &gt; 0$.</span>
<span id="cb6-1467"><a href="#cb6-1467"></a>Since $p_i^{(n)} &gt; 0$ by the induction hypothesis, it follows from this</span>
<span id="cb6-1468"><a href="#cb6-1468"></a>that $p_i^{(n+1)} &gt; 0$ for $i=1,2,3,4$. Moreover, since</span>
<span id="cb6-1469"><a href="#cb6-1469"></a>$p_i^{(n+1)} \propto p_i^{(n)}\ell$ for $i=1,3$ and</span>
<span id="cb6-1470"><a href="#cb6-1470"></a>$p_1^{(n)} &lt; \frac{1}{2}p_3^{(n)}$ by the induction hypothesis, it</span>
<span id="cb6-1471"><a href="#cb6-1471"></a>follows that $p_1^{(n+1)} &lt; \frac{1}{2}p_3^{(n+1)}$. Similarly,</span>
<span id="cb6-1472"><a href="#cb6-1472"></a>$p_2^{(n+1)} &lt; \frac{1}{2}p_4^{(n+1)}$. Thus, statements 1 and 2 hold at</span>
<span id="cb6-1473"><a href="#cb6-1473"></a>time $n+1$.</span>
<span id="cb6-1474"><a href="#cb6-1474"></a></span>
<span id="cb6-1475"><a href="#cb6-1475"></a>Now observe that $$\begin{aligned}</span>
<span id="cb6-1476"><a href="#cb6-1476"></a>    \mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">f(3)</span><span class="co">]</span> &amp;= p_1^{(n+1)} + \frac{1}{2}p_2^{(n+1)} - \frac{1}{2}p_3^{(n+1)} - p_4^{(n+1)}<span class="sc">\\</span></span>
<span id="cb6-1477"><a href="#cb6-1477"></a>    &amp;= \left(p_1^{(n+1)} - \frac{1}{2}p_3^{(n+1)}\right) + \left(\frac{1}{2}p_2^{(n+1)} - p_4^{(n+1)}\right)<span class="sc">\\</span></span>
<span id="cb6-1478"><a href="#cb6-1478"></a>    &amp;\leq \left(p_1^{(n+1)} - \frac{1}{2}p_3^{(n+1)}\right) + \left(p_2^{(n+1)} - \frac{1}{2}p_4^{(n+1)}\right)<span class="sc">\\</span></span>
<span id="cb6-1479"><a href="#cb6-1479"></a>    &amp;\leq 0,</span>
<span id="cb6-1480"><a href="#cb6-1480"></a>\end{aligned}$$ where the last inequality holds since</span>
<span id="cb6-1481"><a href="#cb6-1481"></a>$p_1^{(n+1)} &lt; \frac{1}{2}p_3^{(n+1)}$ and</span>
<span id="cb6-1482"><a href="#cb6-1482"></a>$p_2^{(n+1)} &lt; \frac{1}{2}p_4^{(n+1)}$. Similarly, we see that</span>
<span id="cb6-1483"><a href="#cb6-1483"></a>$\mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">f(4)</span><span class="co">]</span> \leq 0$. Since $\mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">f(1)</span><span class="co">]</span>=-1$ and</span>
<span id="cb6-1484"><a href="#cb6-1484"></a>$\mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">f(2)</span><span class="co">]</span>=0$, it follows that</span>
<span id="cb6-1485"><a href="#cb6-1485"></a>$\arg \max_{x\in\mathcal{X}}\mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>=<span class="sc">\{</span>2<span class="sc">\}</span>$; i.e., statement 3 holds at</span>
<span id="cb6-1486"><a href="#cb6-1486"></a>time $n+1$.</span>
<span id="cb6-1487"><a href="#cb6-1487"></a></span>
<span id="cb6-1488"><a href="#cb6-1488"></a>Since $\max_{x\in\mathcal{X}}\mathbb{E}^{(0)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span> = 0$, the qEI acquisition function at</span>
<span id="cb6-1489"><a href="#cb6-1489"></a>time $n+1$ is given by</span>
<span id="cb6-1490"><a href="#cb6-1490"></a>$\text{qEI}^{(n+1)}(X) = \mathbb{E}^{(n+1)}<span class="co">[</span><span class="ot">\{\max\{f(x_1), f(x_2)\}\}^+</span><span class="co">]</span>$. Since</span>
<span id="cb6-1491"><a href="#cb6-1491"></a>$f(1) \leq f(x)$ almost surely under the prior for all $x\in\mathcal{X}$, there</span>
<span id="cb6-1492"><a href="#cb6-1492"></a>is always a maximizer of qEI that does not contain $1$. Thus, to find</span>
<span id="cb6-1493"><a href="#cb6-1493"></a>the maximizer of qEI, it suffices to analyse its value at the pairs</span>
<span id="cb6-1494"><a href="#cb6-1494"></a>$(2, 3)$, $(3,4)$ and $(4,2)$. We have</span>
<span id="cb6-1495"><a href="#cb6-1495"></a>$$\text{qEI}^{(n+1)}(2, 3) = p_1^{(n+1)} + 1/2 p_2^{(n+1)},$$</span>
<span id="cb6-1496"><a href="#cb6-1496"></a>$$\operatorname{qEI}^{(n+1)}(3, 4) = p_1^{(n+1)} + p_2^{(n+1)}$$ and</span>
<span id="cb6-1497"><a href="#cb6-1497"></a>$$\operatorname{qEI}^{(n+1)}(4, 2) = 1/2p_1^{(n+1)} + p_2^{(n+1)}.$$</span>
<span id="cb6-1498"><a href="#cb6-1498"></a>Since $p_1^{(n+1)} &gt; 0$ and $p_2^{(n+1)} &gt; 0$, it follows that</span>
<span id="cb6-1499"><a href="#cb6-1499"></a>$\arg \max_{X \in X^2}\text{qEI}^{(n+1)}(X) = <span class="sc">\{</span>(3, 4)<span class="sc">\}</span>$, which concludes</span>
<span id="cb6-1500"><a href="#cb6-1500"></a>the proof by induction.</span>
<span id="cb6-1501"><a href="#cb6-1501"></a></span>
<span id="cb6-1502"><a href="#cb6-1502"></a>Finally, since $\arg \max_{x\in X}\mathbb{E}^{(n)}<span class="co">[</span><span class="ot">f(x)</span><span class="co">]</span>=<span class="sc">\{</span>2<span class="sc">\}</span>$ for all $n$, the</span>
<span id="cb6-1503"><a href="#cb6-1503"></a>Bayesian simple regret of qEI is given by $$\begin{aligned}</span>
<span id="cb6-1504"><a href="#cb6-1504"></a>    \mathbb{E}\left<span class="co">[</span><span class="ot">f(x^*) - f(2)\right</span><span class="co">]</span> &amp;= \sum_{i=1}p_i\left(\max_{x\in X}f_i(x) - f_i(2)\right)<span class="sc">\\</span></span>
<span id="cb6-1505"><a href="#cb6-1505"></a>    &amp;= p</span>
<span id="cb6-1506"><a href="#cb6-1506"></a>\end{aligned}$$ for all $n$. ◻</span>
<span id="cb6-1507"><a href="#cb6-1507"></a>:::</span>
<span id="cb6-1508"><a href="#cb6-1508"></a></span>
<span id="cb6-1509"><a href="#cb6-1509"></a><span class="fu">#### POP-BO Regret {#pop-bo-regret .unnumbered}</span></span>
<span id="cb6-1510"><a href="#cb6-1510"></a></span>
<span id="cb6-1511"><a href="#cb6-1511"></a>Commonly used kernel functions within the RKHS are:</span>
<span id="cb6-1512"><a href="#cb6-1512"></a></span>
<span id="cb6-1513"><a href="#cb6-1513"></a><span class="ss">1.  </span>Linear: $$k(x, \bar{x})=x^{\top} \bar{x} .$$</span>
<span id="cb6-1514"><a href="#cb6-1514"></a></span>
<span id="cb6-1515"><a href="#cb6-1515"></a><span class="ss">2.  </span>Squared Exponential (SE):</span>
<span id="cb6-1516"><a href="#cb6-1516"></a>    $$k(x, \bar{x})=\sigma_{\mathrm{SE}}^2 \exp \left<span class="sc">\{</span>-\frac{\|x-\bar{x}\|^2}{l^2}\right<span class="sc">\}</span>,$$</span>
<span id="cb6-1517"><a href="#cb6-1517"></a>    where $\sigma_{\mathrm{SE}}^2$ is the variance parameter and $l$ is</span>
<span id="cb6-1518"><a href="#cb6-1518"></a>    the lengthscale parameter.</span>
<span id="cb6-1519"><a href="#cb6-1519"></a></span>
<span id="cb6-1520"><a href="#cb6-1520"></a><span class="ss">3.  </span>Matérn:</span>
<span id="cb6-1521"><a href="#cb6-1521"></a>    $$k(x, \bar{x})=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\sqrt{2 \nu} \frac{\|x-\bar{x}\|}{\rho}\right)^\nu K_\nu\left(\sqrt{2 \nu} \frac{\|x-\bar{x}\|}{\rho}\right),$$</span>
<span id="cb6-1522"><a href="#cb6-1522"></a>    where $\rho$ and $\nu$ are the two positive parameters of the kernel</span>
<span id="cb6-1523"><a href="#cb6-1523"></a>    function, $\Gamma$ is the gamma function, and $K_\nu$ is the</span>
<span id="cb6-1524"><a href="#cb6-1524"></a>    modified Bessel function of the second kind. $\nu$ captures the</span>
<span id="cb6-1525"><a href="#cb6-1525"></a>    smoothness of the kernel function.</span>
<span id="cb6-1526"><a href="#cb6-1526"></a></span>
<span id="cb6-1527"><a href="#cb6-1527"></a>With the definition of Bayesian simple regret, we have the following</span>
<span id="cb6-1528"><a href="#cb6-1528"></a>theorem defining the regret bound:</span>
<span id="cb6-1529"><a href="#cb6-1529"></a></span>
<span id="cb6-1530"><a href="#cb6-1530"></a>::: theorem</span>
<span id="cb6-1531"><a href="#cb6-1531"></a>With probability at least $1-\delta$, the cumulative regret of POP-BO</span>
<span id="cb6-1532"><a href="#cb6-1532"></a>satisfies,</span>
<span id="cb6-1533"><a href="#cb6-1533"></a>$$R_T=\mathcal{O}\left(\sqrt{\beta_T \gamma_T^{f f^{\prime}} T}\right),$$</span>
<span id="cb6-1534"><a href="#cb6-1534"></a>where</span>
<span id="cb6-1535"><a href="#cb6-1535"></a>$$\beta_T=\beta(1 / T, \delta, T)=\mathcal{O}\left(\sqrt{T \log \frac{T \mathcal{N}\left(\mathcal{B}_f, 1 / T,\|\cdot\|_{\infty}\right)}{\delta}}\right).$$</span>
<span id="cb6-1536"><a href="#cb6-1536"></a>:::</span>
<span id="cb6-1537"><a href="#cb6-1537"></a></span>
<span id="cb6-1538"><a href="#cb6-1538"></a>The guaranteed convergence rate is characterised as:</span>
<span id="cb6-1539"><a href="#cb6-1539"></a></span>
<span id="cb6-1540"><a href="#cb6-1540"></a>::: theorem</span>
<span id="cb6-1541"><a href="#cb6-1541"></a>[]{#th: popbo_converge label="th: popbo_converge"} Let $t^{\star}$ be</span>
<span id="cb6-1542"><a href="#cb6-1542"></a>defined as in Eq. (19). With probability at least $1-\delta$,</span>
<span id="cb6-1543"><a href="#cb6-1543"></a>$$f\left(x^{\star}\right)-f\left(x_{t^{\star}}\right) \leq \mathcal{O}\left(\frac{\sqrt{\beta_T \gamma_T^{f f^{\prime}}}}{\sqrt{T}}\right)$$</span>
<span id="cb6-1544"><a href="#cb6-1544"></a>:::</span>
<span id="cb6-1545"><a href="#cb6-1545"></a></span>
<span id="cb6-1546"><a href="#cb6-1546"></a>Theorem</span>
<span id="cb6-1547"><a href="#cb6-1547"></a><span class="co">[</span><span class="ot">\[th: popbo_converge\]</span><span class="co">](#th: popbo_converge)</span>{reference-type="ref"</span>
<span id="cb6-1548"><a href="#cb6-1548"></a>reference="th: popbo_converge"} highlights that by minimizing the known</span>
<span id="cb6-1549"><a href="#cb6-1549"></a>term</span>
<span id="cb6-1550"><a href="#cb6-1550"></a>$2\left(2 B+\lambda^{-1 / 2} \sqrt{\beta\left(\epsilon, \frac{\delta}{2}, t\right)}\right) \sigma_t^{f f^{\prime}}\left(\left(x_t, x_t^{\prime}\right)\right)$,</span>
<span id="cb6-1551"><a href="#cb6-1551"></a>the reported final solution $x_{t^{\star}}$ has a guaranteed convergence</span>
<span id="cb6-1552"><a href="#cb6-1552"></a>rate.</span>
<span id="cb6-1553"><a href="#cb6-1553"></a></span>
<span id="cb6-1554"><a href="#cb6-1554"></a>Further kernel-specific regret bounds for POP-BO are calculated as</span>
<span id="cb6-1555"><a href="#cb6-1555"></a>follows:</span>
<span id="cb6-1556"><a href="#cb6-1556"></a></span>
<span id="cb6-1557"><a href="#cb6-1557"></a>::: theorem</span>
<span id="cb6-1558"><a href="#cb6-1558"></a>Setting $\epsilon=1 / T$ and running our POP-BO algorithm in Alg. 1,</span>
<span id="cb6-1559"><a href="#cb6-1559"></a></span>
<span id="cb6-1560"><a href="#cb6-1560"></a><span class="ss">1.  </span>If $k(x, y)=\langle x, y\rangle$, we have,</span>
<span id="cb6-1561"><a href="#cb6-1561"></a>    $$R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4}\right) .$$</span>
<span id="cb6-1562"><a href="#cb6-1562"></a></span>
<span id="cb6-1563"><a href="#cb6-1563"></a><span class="ss">2.  </span>If $k(x, y)$ is a squared exponential kernel, we have,</span>
<span id="cb6-1564"><a href="#cb6-1564"></a>    $$R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4(d+1)}\right) .$$</span>
<span id="cb6-1565"><a href="#cb6-1565"></a></span>
<span id="cb6-1566"><a href="#cb6-1566"></a><span class="ss">3.  </span>If $k(x, y)$ is a Matérn kernel, we have,</span>
<span id="cb6-1567"><a href="#cb6-1567"></a>    $$\left.R_T=\mathcal{O}\left(T^{3 / 4}(\log T)^{3 / 4} T^{\frac{d}{\nu}\left(\frac{1}{4}+\frac{d+1}{4+2(d+1)^d / \nu}\right.}\right)\right).$$</span>
<span id="cb6-1568"><a href="#cb6-1568"></a>:::</span>
<span id="cb6-1569"><a href="#cb6-1569"></a></span>
<span id="cb6-1570"><a href="#cb6-1570"></a>{{&lt; include psets/pset3.qmd &gt;}}</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/004-optim.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>