<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Model-Based Preference Optimization – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/004-optim.html" rel="next">
<link href="../src/002-reward_model.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a0aefded8822f1bee14b20ac4fd2b1d6.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-5c897cb370a42f0721f6bac59365aff2.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/003-measure.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#active-preference-learning" id="toc-active-preference-learning" class="nav-link active" data-scroll-target="#active-preference-learning"><span class="header-section-number">3.1</span> Active Preference Learning</a>
  <ul class="collapse">
  <li><a href="#introduction-to-active-learning" id="toc-introduction-to-active-learning" class="nav-link" data-scroll-target="#introduction-to-active-learning"><span class="header-section-number">3.1.1</span> Introduction to Active Learning</a></li>
  <li><a href="#introduction-to-active-preference-learning" id="toc-introduction-to-active-preference-learning" class="nav-link" data-scroll-target="#introduction-to-active-preference-learning"><span class="header-section-number">3.1.2</span> Introduction to Active Preference Learning</a></li>
  <li><a href="#uncertainty-qualification" id="toc-uncertainty-qualification" class="nav-link" data-scroll-target="#uncertainty-qualification"><span class="header-section-number">3.1.3</span> Uncertainty Qualification</a></li>
  <li><a href="#acquisition-function" id="toc-acquisition-function" class="nav-link" data-scroll-target="#acquisition-function"><span class="header-section-number">3.1.4</span> Acquisition Function</a></li>
  <li><a href="#active-learning-by-variance-reduction" id="toc-active-learning-by-variance-reduction" class="nav-link" data-scroll-target="#active-learning-by-variance-reduction"><span class="header-section-number">3.1.5</span> Active Learning by Variance Reduction</a></li>
  <li><a href="#active-learning-in-ranking-and-comparison" id="toc-active-learning-in-ranking-and-comparison" class="nav-link" data-scroll-target="#active-learning-in-ranking-and-comparison"><span class="header-section-number">3.1.6</span> Active Learning in Ranking and Comparison</a></li>
  <li><a href="#active-preference-based-learning-of-reward-functions" id="toc-active-preference-based-learning-of-reward-functions" class="nav-link" data-scroll-target="#active-preference-based-learning-of-reward-functions"><span class="header-section-number">3.1.7</span> Active Preference-Based Learning of Reward Functions</a></li>
  <li><a href="#application-foundation-models-for-robotics" id="toc-application-foundation-models-for-robotics" class="nav-link" data-scroll-target="#application-foundation-models-for-robotics"><span class="header-section-number">3.1.8</span> Application: Foundation Models for Robotics</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">3.1.9</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#sec-metric-elicitation" id="toc-sec-metric-elicitation" class="nav-link" data-scroll-target="#sec-metric-elicitation"><span class="header-section-number">3.2</span> Metric Elicitation</a>
  <ul class="collapse">
  <li><a href="#introduction-to-performance-metric-elicitation" id="toc-introduction-to-performance-metric-elicitation" class="nav-link" data-scroll-target="#introduction-to-performance-metric-elicitation"><span class="header-section-number">3.2.1</span> Introduction to Performance Metric Elicitation</a></li>
  <li><a href="#sec-me-preliminaries" id="toc-sec-me-preliminaries" class="nav-link" data-scroll-target="#sec-me-preliminaries"><span class="header-section-number">3.2.2</span> Preliminaries</a></li>
  <li><a href="#sec-metric-elicitation-setup" id="toc-sec-metric-elicitation-setup" class="nav-link" data-scroll-target="#sec-metric-elicitation-setup"><span class="header-section-number">3.2.3</span> Problem Setup</a></li>
  <li><a href="#sec-orgb6dac4e" id="toc-sec-orgb6dac4e" class="nav-link" data-scroll-target="#sec-orgb6dac4e"><span class="header-section-number">3.2.4</span> Linear Performance Metric Elicitation</a></li>
  <li><a href="#sec-lfpm-elicitation" id="toc-sec-lfpm-elicitation" class="nav-link" data-scroll-target="#sec-lfpm-elicitation"><span class="header-section-number">3.2.5</span> Linear-Fractional Performance Metric Elicitation</a></li>
  <li><a href="#multiclass-performance-metric-elicitation" id="toc-multiclass-performance-metric-elicitation" class="nav-link" data-scroll-target="#multiclass-performance-metric-elicitation"><span class="header-section-number">3.2.6</span> Multiclass Performance Metric Elicitation</a></li>
  <li><a href="#linear-reward-estimation" id="toc-linear-reward-estimation" class="nav-link" data-scroll-target="#linear-reward-estimation"><span class="header-section-number">3.2.7</span> Linear Reward Estimation</a></li>
  <li><a href="#truthful-preference-elicitation-with-adversary" id="toc-truthful-preference-elicitation-with-adversary" class="nav-link" data-scroll-target="#truthful-preference-elicitation-with-adversary"><span class="header-section-number">3.2.8</span> Truthful Preference Elicitation with Adversary</a></li>
  <li><a href="#application-guiding-human-demonstrations-in-robotics" id="toc-application-guiding-human-demonstrations-in-robotics" class="nav-link" data-scroll-target="#application-guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.2.9</span> Application: Guiding Human Demonstrations in Robotics</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">3.2.10</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">3.3</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#sec-question-1-uncertainty-quantification-in-preference-learning-40-points" id="toc-sec-question-1-uncertainty-quantification-in-preference-learning-40-points" class="nav-link" data-scroll-target="#sec-question-1-uncertainty-quantification-in-preference-learning-40-points">Question 1: Uncertainty Quantification in Preference Learning (40 points)</a></li>
  <li><a href="#sec-question-2-active-learning-for-preference-learning-40-points" id="toc-sec-question-2-active-learning-for-preference-learning-40-points" class="nav-link" data-scroll-target="#sec-question-2-active-learning-for-preference-learning-40-points">Question 2: Active Learning for Preference Learning (40 points)</a></li>
  <li><a href="#sec-question-3-linear-performance-metric-elicitation-30-points" id="toc-sec-question-3-linear-performance-metric-elicitation-30-points" class="nav-link" data-scroll-target="#sec-question-3-linear-performance-metric-elicitation-30-points">Question 3: Linear Performance Metric Elicitation (30 points)</a></li>
  <li><a href="#sec-question-4-d-optimal-design-with-logistic-model-30-points" id="toc-sec-question-4-d-optimal-design-with-logistic-model-30-points" class="nav-link" data-scroll-target="#sec-question-4-d-optimal-design-with-logistic-model-30-points">Question 4: D-optimal Design with Logistic Model (30 points)</a></li>
  <li><a href="#sec-question-5-nonparametric-metric-elicitation-30-points" id="toc-sec-question-5-nonparametric-metric-elicitation-30-points" class="nav-link" data-scroll-target="#sec-question-5-nonparametric-metric-elicitation-30-points">Question 5: Nonparametric Metric Elicitation (30 points)</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/003-measure.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="ch-model-based" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/" class="btn btn-outline-primary" role="button">Fullscreen - AL</a> <a href="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/" class="btn btn-outline-primary" role="button">Fullscreen - ME</a></p>
<section id="active-preference-learning" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="active-preference-learning"><span class="header-section-number">3.1</span> Active Preference Learning</h2>
<section id="introduction-to-active-learning" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="introduction-to-active-learning"><span class="header-section-number">3.1.1</span> Introduction to Active Learning</h3>
<p>In real-world scenarios, data is often scarce, and acquiring labeled data can be expensive. Active learning is a machine learning paradigm that aims to reduce the amount of labeled data required to train a model to achieve high accuracy. Active learning algorithms iteratively select an input datapoint for an oracle (e.g., a human annotator) to label such that when the label is observed, the model improves the most. The goal of AL algorithms is to minimize the number of labels required to achieve a desired level of performance. This technique is particularly useful in situations where labeling data is expensive, time-consuming, or requires domain expertise.</p>
<p>There are two primary setups in active learning:</p>
<ul>
<li><p><strong>Pool-based:</strong> The model selects samples from a large unlabeled pool of data. For example, a model for text classification selects the most uncertain texts from a large pool to ask a human annotator to label.</p></li>
<li><p><strong>Stream-based:</strong> The model receives samples sequentially (one sample at a time) and decides whether to label them. The data is gone if the decision maker decides not to label it. For example, a system monitoring sensor data decides on-the-fly whether new sensor readings are valuable enough to label.</p></li>
</ul>
<p>A common AL process is shown in <a href="#fig-schema" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>:</p>
<ul>
<li><p>Current model trained on current dataset <span class="math inline">\(\mathcal{D}\)</span>, potential points <span class="math inline">\(\tilde{x}_1 \dots \tilde{x}_m\)</span> are being investigated. AL will choose one of them to add to the dataset.</p></li>
<li><p>Relative to the model, a proxy highlights the relative value of each point to model improvement <span class="math inline">\((v(\tilde{x}_1) \dots v(\tilde{x}_m) )\)</span>. A naive proxy is the model’s uncertainty about the point.</p></li>
<li><p>The cycle repeats until we collect enough data or the model is good enough.</p></li>
</ul>
<div id="fig-schema" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-schema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/active_learning_schema.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-schema-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: The current model is trained on the current data set <span class="math inline">\(\mathcal{D}\)</span>. Potential points <span class="math inline">\(\tilde{x}_1, \ldots, \tilde{x}_m\)</span> are being investigated, and one of them will be chosen and added to the data set. A proxy highlights the relative value of each point in terms of improving the model, denoted by <span class="math inline">\(v(\tilde{x}_1), \ldots, v(\tilde{x}_m)\)</span>. The point with the highest value is selected and added to <span class="math inline">\(\mathcal{D}\)</span>. This cycle repeats until enough data has been collected or the model is good enough.
</figcaption>
</figure>
</div>
<p>Active learning has been successfully applied to various domains to enhance real-world systems, including computer vision, natural language processing, and recommender systems. For example, active learning can improve the computer vision models used in autonomous vehicles <span class="citation" data-cites="AL_app_autonomous">(<a href="#ref-AL_app_autonomous" role="doc-biblioref">Jarl et al. 2021</a>)</span>, here driving scenes can take infinitely many forms, making it impossible to gather an exhaustive dataset. Instead, probing a model to understand what type of data it would benefit from is more practical. In robotics, autonomous agents may query humans when unsure how to act or when facing new situations <span class="citation" data-cites="AL_app_robotics">(<a href="#ref-AL_app_robotics" role="doc-biblioref">Taylor, Berrueta, and Murphey 2021</a>)</span>. In this field, collecting data often incurs significant financial and time costs: the robot must act in real-time in the real world, and while parallelization is possible, being strategic about which examples to collect can best benefit the model. In meteorology, active learning can help decide where to place additional sensors for weather predictions <span class="citation" data-cites="AL_app_sensors">(<a href="#ref-AL_app_sensors" role="doc-biblioref">Singh, Nowak, and Ramanathan 2006</a>)</span>. Sensor placement involves deploying teams to remote locations and expensive construction for an extra data point. Choosing these locations and allocating resources wisely is of interest to governments and businesses. Active learning could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream tasks <span class="citation" data-cites="AL_app_LLMs">(<a href="#ref-AL_app_LLMs" role="doc-biblioref">Margatina et al. 2023</a>)</span>. In this context, it might be difficult to fully describe an NLP task one might want an LLM to solve. Often, instead of defining a task via a dataset of examples, it may be easier for a human to interact with the LLM for a specific use case, identify gaps in the model, and address those using active learning.</p>
</section>
<section id="introduction-to-active-preference-learning" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="introduction-to-active-preference-learning"><span class="header-section-number">3.1.2</span> Introduction to Active Preference Learning</h3>
<p>Consider the scenario where a robot is being trained to assist individuals with feeding. How can such a robot be effectively taught to perform necessary tasks, such as determining the appropriate distance to reach, detecting the location of a person’s mouth, and, most importantly, understanding human preferences? Typically, robots learn by observing human demonstrations, replicating the ways a person performs the task. However, this method poses significant challenges. Expert demonstrations are often limited, and training a supervised learning model would require vast amounts of demonstration data, which is difficult to obtain at scale. Moreover, demonstrations tend to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task.</p>
<p>Active learning algorithms can be employed in preference learning tasks, such as the previously mentioned example, where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive labeled data or reducing the high cost of annotations. This chapter will explore the theoretical foundations of pairwise comparisons and active preference learning, along with extensions to these methods that address known limitations. Practical examples where these approaches prove beneficial will also be discussed. Additionally, we will examine the role of LLMs in assisting robots through corrective feedback and highlight the applications of these techniques.</p>
</section>
<section id="uncertainty-qualification" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="uncertainty-qualification"><span class="header-section-number">3.1.3</span> Uncertainty Qualification</h3>
<p><strong>Problem Setup</strong>: In this section, we consider a binary classification problem. The model is trained on a small labeled dataset <span class="math inline">\(\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}\)</span>, where <span class="math inline">\(x_i\)</span> represents the input data and <span class="math inline">\(y_i\)</span> is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while maximizing the model’s performance.</p>
<p>Uncertainty quantification (UQ) is a critical aspect of active learning that allows models to evaluate the informativeness of new data points. In machine learning (ML), two primary types of uncertainty are often considered: epistemic and aleatoric uncertainty. <strong>Epistemic uncertainty</strong>, or model uncertainty, arises from a lack of knowledge and can be reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence due to insufficient or incomplete information in its training set. On the other hand, <strong>aleatoric uncertainty</strong>, or data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real data-generating process. Several approaches exist to quantify uncertainty in active learning, each with its strengths and limitations.</p>
<p><strong>Bayesian methods</strong>, such as Bayesian Neural Networks (BNNs) and Gaussian Processes (GPs), offer a principled way of estimating uncertainty by incorporating prior knowledge into the model. These approaches can generate meaningful uncertainty estimates that aid in choosing informative samples for labeling. However, they can become computationally prohibitive, especially for large and complex models, limiting their applicability in some practical scenarios.</p>
<p>Another common technique for uncertainty quantification is the use of <strong>ensemble methods</strong>, such as Random Forests or Gradient Boosting Machines. These methods involve training multiple models and combining their predictions to provide an estimate of uncertainty. Ensemble methods are relatively easy to implement and can give valuable insights into model uncertainty. However, they can be computationally expensive and may not always produce well-calibrated uncertainty estimates. Moreover, they do not integrate prior knowledge, which can be a disadvantage in certain applications.</p>
<p><strong>Conformal prediction methods</strong> also provide a framework for estimating uncertainty by offering a measure of confidence in predictions based on the conformity of a given instance with the training data. While these methods are useful in some contexts, this book focuses primarily on the Bayesian approach due to its theoretical robustness and capacity to quantify uncertainty in a more comprehensive manner.</p>
</section>
<section id="acquisition-function" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="acquisition-function"><span class="header-section-number">3.1.4</span> Acquisition Function</h3>
<p>Uncertainty quantification plays a vital role in <strong>acquisition functions</strong>, which are central to active learning strategies. These functions determine which samples are most valuable to label by evaluating their utility based on the model’s current uncertainty estimates. Common acquisition functions include <strong>uncertainty sampling</strong> <span class="citation" data-cites="AL_uncertainty">(<a href="#ref-AL_uncertainty" role="doc-biblioref">Zhu et al. 2010</a>)</span>, which selects samples the model is least confident about, <strong>query-by-committee</strong> <span class="citation" data-cites="AL_committee">(<a href="#ref-AL_committee" role="doc-biblioref">Beluch et al. 2018</a>)</span>, which utilizes a set of models to choose the most uncertain samples, and <strong>Bayesian Active Learning by Disagreement (BALD)</strong> <span class="citation" data-cites="AL_BALD">(<a href="#ref-AL_BALD" role="doc-biblioref">Houlsby et al. 2011</a>)</span>, which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, acquisition functions guide the active learning process, improving the model’s efficiency in learning from limited data. Other acquisition functions that can be employed include:</p>
<ul>
<li><p><strong>Active Thompson Sampling</strong> <span class="citation" data-cites="AL_exploreexploit">(<a href="#ref-AL_exploreexploit" role="doc-biblioref">Bouneffouf et al. 2014</a>)</span>: This method leverages the Thompson Sampling algorithm to select a posterior sample from the model’s distribution and compute the expected utility of labeling using that sample. By doing so, the algorithm balances exploration and exploitation, leading to effective active learning.</p></li>
<li><p><strong>Expected model change</strong> <span class="citation" data-cites="AL_expmodelchange">(<a href="#ref-AL_expmodelchange" role="doc-biblioref">Cai, Zhang, and Zhou 2013</a>)</span>: This approach focuses on labeling points that would have the most impact on changing the current model parameters.</p></li>
<li><p><strong>Expected error reduction</strong> <span class="citation" data-cites="AL_experrorredn">(<a href="#ref-AL_experrorredn" role="doc-biblioref">Mussmann et al. 2022</a>)</span>: Points that would most effectively reduce the model’s generalization error are labeled using this strategy.</p></li>
<li><p><strong>Variance reduction</strong> <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span>: This approach labels points that would minimize output variance, which is one component of error. By selecting points that reduce variability in the model’s predictions, it aims to improve overall performance.</p></li>
<li><p><strong>User Centered Labeling Strategies</strong> <span class="citation" data-cites="AL_usercentered">(<a href="#ref-AL_usercentered" role="doc-biblioref">Bernard et al. 2018</a>)</span>: This approach involves actively involving the user in the labeling process by visualizing data through dimensionality reduction techniques. The user then provides labels for the compiled data based on their domain expertise and preferences. This strategy leverages user input to improve the quality and relevance of the labeled data.</p></li>
<li><p><strong>Querying from diverse subspaces or partitions</strong> <span class="citation" data-cites="AL_partition">(<a href="#ref-AL_partition" role="doc-biblioref">Ma et al. 2022</a>)</span>: When using a forest of trees as the underlying model, the leaf nodes can represent overlapping partitions of the feature space. This strategy selects instances from non-overlapping or minimally overlapping partitions for labeling.</p></li>
<li><p><strong>Conformal prediction</strong> <span class="citation" data-cites="AL_conformal">(<a href="#ref-AL_conformal" role="doc-biblioref">Makili, Sánchez, and Dormido-Canto 2012</a>)</span>: This method predicts that a new data point will have a label similar to old data points in some specified way. The degree of similarity within the old examples is used to estimate the confidence in the prediction.</p></li>
<li><p><strong>Mismatch-first farthest-traversal</strong> <span class="citation" data-cites="AL_mismatch">(<a href="#ref-AL_mismatch" role="doc-biblioref">Zhao, Heittola, and Virtanen 2020</a>)</span>: This strategy first prioritizes data points that are wrongly predicted by the current model compared to the nearest-neighbor prediction. The second criterion is the distance to previously selected data, with preference given to those that are farthest away. The goal is to optimize both the correction of mispredictions and the diversity of the selected data.</p></li>
</ul>
<section id="uncertainty-sampling" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="uncertainty-sampling">Uncertainty Sampling</h4>
<p>Uncertainty sampling <span class="citation" data-cites="AL_uncertainty">(<a href="#ref-AL_uncertainty" role="doc-biblioref">Zhu et al. 2010</a>)</span> is a widely used acquisition function in active learning that selects data points for which the model exhibits the greatest uncertainty. This method aims to improve model performance by focusing labeling efforts on ambiguous samples, where additional information is likely to yield the greatest benefit. Let <span class="math inline">\(x\)</span> represent the input, and <span class="math inline">\(p(y|x)\)</span> the probability distribution of the output <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>. Several acquisition strategies fall under uncertainty sampling, including <strong>entropy sampling</strong>, <strong>margin sampling</strong>, and <strong>least confidence sampling</strong>, each providing a unique measure of uncertainty.</p>
<ul>
<li><p><strong>Entropy sampling</strong> measures uncertainty by calculating the entropy of the predicted probability distribution. The acquisition function is given by <span class="math inline">\(\alpha(x) = - \sum_{y} p(y|x) \log p(y|x)\)</span>, with higher entropy values indicating higher uncertainty.</p></li>
<li><p><strong>Margin sampling</strong> focuses on the difference between the two highest predicted probabilities for a sample. The acquisition function is given by <span class="math inline">\(\alpha(x) = p(y_1|x) - p(y_2|x)\)</span>, where <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are two most likely classes. Smaller margins signify greater uncertainty.</p></li>
<li><p><strong>Least confidence sampling</strong> measures uncertainty by identifying the sample with the lowest predicted probability for its most likely class. The acquisition function is <span class="math inline">\(\alpha(x) = 1 - p(y_{\text{max}}|x)\)</span>, where <span class="math inline">\(y_{\text{max}}\)</span> is the class with the highest probability.</p></li>
</ul>
<p><strong>Example:</strong> Consider a binary classification problem with two classes <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. We have three samples <span class="math inline">\(x_1, x_2, x_3\)</span> and the corresponding predictive distributions are as follows: <span id="eq-eq3.1"><span class="math display">\[\begin{aligned}
p(y_1|x_1) &amp;= 0.6, \quad p(y_2|x_1) = 0.4\\
p(y_1|x_2) &amp;= 0.3, \quad p(y_2|x_2) = 0.7\\
p(y_1|x_3) &amp;= 0.8, \quad p(y_2|x_3) = 0.2
\end{aligned} \tag{3.1}\]</span></span></p>
<ul>
<li><strong>Entropy Sampling</strong>
<ul>
<li><span class="math inline">\(\alpha(x_1) = -0.6 \log (0.6) - 0.4 \log (0.4) = 0.29\)</span></li>
<li><span class="math inline">\(\alpha(x_2) = -0.3 \log (0.3) - 0.7 \log (0.7) = 0.27\)</span></li>
<li><span class="math inline">\(\alpha(x_3) = -0.8 \log (0.8) - 0.2 \log (0.2) = 0.22\)</span></li>
</ul></li>
</ul>
<p>We would select <span class="math inline">\(x_1\)</span> for labeling as it has the highest entropy, indicating the model is most uncertain about its prediction at <span class="math inline">\(x_1\)</span>.</p>
<ul>
<li><strong>Margin Sampling</strong>
<ul>
<li><span class="math inline">\(\alpha(x_1) = 0.6 - 0.4 = 0.2\)</span></li>
<li><span class="math inline">\(\alpha(x_2) = 0.7 - 0.3 = 0.4\)</span></li>
<li><span class="math inline">\(\alpha(x_3) = 0.8 - 0.2 = 0.6\)</span></li>
</ul></li>
</ul>
<p>We would select <span class="math inline">\(x_1\)</span> for labeling as it has the smallest margin, indicating the model is most uncertain about the prediction at <span class="math inline">\(x_1\)</span>.</p>
<ul>
<li><strong>Least Confidence Sampling</strong>
<ul>
<li><span class="math inline">\(\alpha(x_1) = 1 - 0.6 = 0.4\)</span></li>
<li><span class="math inline">\(\alpha(x_2) = 1 - 0.7 = 0.3\)</span></li>
<li><span class="math inline">\(\alpha(x_3) = 1 - 0.8 = 0.2\)</span></li>
</ul></li>
</ul>
<p>We would select <span class="math inline">\(x_1\)</span> for labeling as it has the lowest confidence, indicating the model is most uncertain about the prediction at <span class="math inline">\(x_1\)</span>.</p>
<p>In summary, uncertainty sampling methods, whether based on entropy, margin, or least confidence, help prioritize data points that the model struggles with the most. By focusing on these uncertain samples, the model can more efficiently improve its performance, making uncertainty sampling a key tool in active learning.</p>
</section>
<section id="query-by-committee" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="query-by-committee">Query-by-Committee</h4>
<p>Query-by-Committee <span class="citation" data-cites="AL_committee">(<a href="#ref-AL_committee" role="doc-biblioref">Beluch et al. 2018</a>)</span> is an active learning strategy where a committee of models selects samples for labeling based on the level of disagreement among the committee members. Several acquisition functions can be employed under this framework to quantify the disagreement:</p>
<ul>
<li><p><strong>Vote Entropy:</strong> The vote entropy measures the uncertainty based on how often the committee members vote for each class. The acquisition function is defined as <span class="math inline">\(\alpha(x) = \mathbb{H}\left[\frac{V(y)}{C}\right]\)</span>, where <span class="math inline">\(V(y)\)</span> is the number of votes for class <span class="math inline">\(y\)</span> and <span class="math inline">\(C\)</span> is the number of committee members.</p></li>
<li><p><strong>Consensus Entropy:</strong> This acquisition function measures the entropy of the average probability distribution across committee members. It is given by <span class="math inline">\(\alpha(x) = \mathbb{H}[P_C(y|x)]\)</span>, where <span class="math inline">\(P_C(y|x)\)</span> is the average probability distribution for sample <span class="math inline">\(x\)</span> across all committee members.</p></li>
<li><p><strong>KL Divergence:</strong> The KL divergence quantifies the disagreement by comparing the probability distribution of each committee member to the average distribution. The acquisition function is given by <span class="math inline">\(\alpha(x) = \frac{1}{C} \sum_{c=1}^{C} D_{KL}[P_c(y|x) || P_C(y|x)]\)</span>, where <span class="math inline">\(P_c(y|x)\)</span> is the probability distribution of committee member <span class="math inline">\(c\)</span> and <span class="math inline">\(P_C(y|x)\)</span> is the average distribution across the committee.</p></li>
</ul>
<p><strong>Example:</strong> Consider a binary classification problem with two classes <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. We have three committee members and three samples: <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span>. The predictive distributions for each committee member are given below:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(p_1(y_1 \vert \cdot)\)</span></th>
<th><span class="math inline">\(p_1(y_2 \vert \cdot)\)</span></th>
<th><span class="math inline">\(p_2(y_1 \vert \cdot)\)</span></th>
<th><span class="math inline">\(p_2(y_2 \vert \cdot)\)</span></th>
<th><span class="math inline">\(p_3(y_1 \vert \cdot)\)</span></th>
<th><span class="math inline">\(p_3(y_2 \vert \cdot)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td>0.6</td>
<td>0.4</td>
<td>0.7</td>
<td>0.3</td>
<td>0.3</td>
<td>0.7</td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_2\)</span></td>
<td>0.3</td>
<td>0.7</td>
<td>0.4</td>
<td>0.6</td>
<td>0.4</td>
<td>0.6</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x_3\)</span></td>
<td>0.8</td>
<td>0.2</td>
<td>0.9</td>
<td>0.1</td>
<td>0.7</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p><strong>Query-by-Committee: Vote Entropy</strong></p>
<ul>
<li>For sample <span class="math inline">\(x_1\)</span>, the votes for <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> are <span class="math inline">\(V(y_1) = 2\)</span> and <span class="math inline">\(V(y_2) = 1\)</span>. The vote entropy is <span class="math inline">\(\alpha(x_1) = - \frac{2}{3} \log (\frac{2}{3}) - \frac{1}{3} \log (\frac{1}{3}) = 0.28\)</span>.</li>
<li>For sample <span class="math inline">\(x_2\)</span>, the votes are <span class="math inline">\(V(y_1) = 0\)</span> and <span class="math inline">\(V(y_2) = 3\)</span>, resulting in vote entropy <span class="math inline">\(\alpha(x_2) = 0\)</span>.</li>
<li>For sample <span class="math inline">\(x_3\)</span>, the votes are <span class="math inline">\(V(y_1) = 3\)</span> and <span class="math inline">\(V(y_2) = 0\)</span>, resulting in vote entropy <span class="math inline">\(\alpha(x_3) = 0\)</span>.</li>
</ul>
<p>Thus, sample <span class="math inline">\(x_1\)</span> would be selected for labeling as it has the highest vote entropy, indicating the greatest disagreement among the committee members.</p>
<p><strong>Query-by-Committee: Consensus Entropy</strong></p>
<p>The first step is to compute the consensus probability of each class for each sample:</p>
<ul>
<li>For <span class="math inline">\(x_1\)</span>, <span class="math inline">\(p_c(y_1|x_1) = \frac{0.6 + 0.7 + 0.3}{3} = 0.53\)</span> and <span class="math inline">\(p_c(y_2|x_1) = \frac{0.4 + 0.3 + 0.7}{3} = 0.47\)</span>.</li>
<li>For <span class="math inline">\(x_2\)</span>, <span class="math inline">\(p_c(y_1|x_2) = \frac{0.3 + 0.4 + 0.4}{3} = 0.37\)</span> and <span class="math inline">\(p_c(y_2|x_2) = \frac{0.7 + 0.6 + 0.6}{3} = 0.63\)</span>.</li>
<li>For <span class="math inline">\(x_3\)</span>, <span class="math inline">\(p_c(y_1|x_3) = \frac{0.8 + 0.9 + 0.7}{3} = 0.8\)</span> and <span class="math inline">\(p_c(y_2|x_3) = \frac{0.2 + 0.1 + 0.3}{3} = 0.2\)</span>.</li>
</ul>
<p>Next, we compute the entropy of these consensus probabilities:</p>
<ul>
<li>For <span class="math inline">\(x_1\)</span>, <span class="math inline">\(\mathbb{H}[p_c(y|x_1)] = -0.53 \log (0.53) - 0.47 \log (0.47) = 0.30\)</span>.</li>
<li>For <span class="math inline">\(x_2\)</span>, <span class="math inline">\(\mathbb{H}[p_c(y|x_2)] = -0.37 \log (0.37) - 0.63 \log (0.63) = 0.29\)</span>.</li>
<li>For <span class="math inline">\(x_3\)</span>, <span class="math inline">\(\mathbb{H}[p_c(y|x_3)] = -0.8 \log (0.8) - 0.2 \log (0.2) = 0.22\)</span>.</li>
</ul>
<p>Thus, <span class="math inline">\(x_1\)</span> would be selected for labeling as it has the highest consensus entropy, indicating the highest level of disagreement among the committee members.</p>
</section>
<section id="bayesian-active-learning-by-disagreement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bayesian-active-learning-by-disagreement">Bayesian Active Learning by Disagreement</h4>
<p>Bayesian Active Learning by Disagreement (BALD) <span class="citation" data-cites="AL_BALD">(<a href="#ref-AL_BALD" role="doc-biblioref">Houlsby et al. 2011</a>)</span> selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:</p>
<p><span id="eq-eq3.2"><span class="math display">\[
\mathbb{I}(\theta; y|x, \mathcal{D}) = \mathbb{H}[p(y|x, \mathcal{D})] - \mathbb{E}_{p(\theta | \mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]]
\tag{3.2}\]</span></span></p>
<p>where <span class="math inline">\(\mathbb{H}[\cdot]\)</span> denotes entropy. When there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized.</p>
<ul>
<li>To compute the first term, we can derive the following expression:</li>
</ul>
<p><span id="eq-eq3.3"><span class="math display">\[\begin{aligned}
\mathbb{H}[p(y|x, \mathcal{D})] &amp;= \mathbb{H}\left[\int_{\theta} p(y|x, \theta, \mathcal{D}) p(\theta | \mathcal{D}) d\theta\right]\\
&amp;\approx \mathbb{H}\left[\frac{1}{N}\sum_{i=1}^{N} p(y|x, \theta_i, \mathcal{D})\right]\\
&amp;= \mathbb{H}\left[\overline{p}(y|x, \mathcal{D})\right]
\end{aligned} \tag{3.3}\]</span></span></p>
<ul>
<li>To compute the second term, we can derive the following expression:</li>
</ul>
<p><span id="eq-eq3.4"><span class="math display">\[\begin{aligned}
\mathbb{E}_{p(\theta|\mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]] &amp;= \mathbb{E}_{p(\theta|\mathcal{D})} \left[ - \sum_{y} p(y|x, \theta, \mathcal{D}) \log p(y|x, \theta, \mathcal{D}) \right] \\
&amp;\approx - \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{y} p(y|x, \theta_i, \mathcal{D}) \log p(y|x, \theta_i, \mathcal{D}) \right)
\end{aligned} \tag{3.4}\]</span></span></p>
<p><strong>Example:</strong> Consider a binary classification problem with two classes, <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>. We have two samples, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, and the model’s predictive distributions are as follows:</p>
<ul>
<li><p><strong>First-time inference</strong> (with <span class="math inline">\(\theta_1 \sim p(\theta | \mathcal{D})\)</span>): <span id="eq-eq3.5"><span class="math display">\[
p(y_1|x_1, \theta_1, \mathcal{D}) = 0.6, \quad p(y_2|x_1, \theta_1, \mathcal{D}) = 0.4
\tag{3.5}\]</span></span> <span id="eq-eq3.6"><span class="math display">\[
p(y_1|x_2, \theta_1, \mathcal{D}) = 0.4, \quad p(y_2|x_2, \theta_1, \mathcal{D}) = 0.6
\tag{3.6}\]</span></span></p></li>
<li><p><strong>Second-time inference</strong> (with <span class="math inline">\(\theta_2 \sim p(\theta | \mathcal{D})\)</span>): <span id="eq-eq3.7"><span class="math display">\[
p(y_1|x_1, \theta_2, \mathcal{D}) = 0.8, \quad p(y_2|x_1, \theta_2, \mathcal{D}) = 0.2
\tag{3.7}\]</span></span> <span id="eq-eq3.8"><span class="math display">\[
p(y_1|x_2, \theta_2, \mathcal{D}) = 0.5, \quad p(y_2|x_2, \theta_2, \mathcal{D}) = 0.5
\tag{3.8}\]</span></span></p></li>
</ul>
<p><strong>Solution:</strong></p>
<p><strong>Step 1:</strong> Compute the entropy of the model’s predictive distribution for each sample:</p>
<ul>
<li><span class="math inline">\(\overline{p}_{\theta}(y_1|x_1, \theta, \mathcal{D}) = 0.7\)</span></li>
<li><span class="math inline">\(\overline{p}_{\theta}(y_2|x_1, \theta, \mathcal{D}) = 0.3\)</span></li>
<li><span class="math inline">\(\overline{p}_{\theta}(y_1|x_2, \theta, \mathcal{D}) = 0.45\)</span></li>
<li><span class="math inline">\(\overline{p}_{\theta}(y_2|x_2, \theta, \mathcal{D}) = 0.55\)</span></li>
</ul>
<p>Now, we compute the entropy for each sample using the formula:</p>
<p><span id="eq-eq3.9"><span class="math display">\[
\mathbb{H}[p(y|x, \mathcal{D})] = - p(y_1|x, \mathcal{D}) \log(p(y_1|x, \mathcal{D})) - p(y_2|x, \mathcal{D}) \log(p(y_2|x, \mathcal{D}))
\tag{3.9}\]</span></span></p>
<p>For <span class="math inline">\(x_1\)</span>:</p>
<p><span id="eq-eq3.10"><span class="math display">\[
\mathbb{H}[p(y|x_1, \mathcal{D})] = - 0.7 \log(0.7) - 0.3 \log(0.3) = 0.27
\tag{3.10}\]</span></span></p>
<p>For <span class="math inline">\(x_2\)</span>:</p>
<p><span id="eq-eq3.11"><span class="math display">\[
\mathbb{H}[p(y|x_2, \mathcal{D})] = - 0.45 \log(0.45) - 0.55 \log(0.55) = 0.30
\tag{3.11}\]</span></span></p>
<p><strong>Step 2:</strong> Compute the expected entropy of the model’s predictive distribution for each sample.</p>
<p>For <span class="math inline">\(x_1\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathbb{H}_{\theta_1}[p(y|x_1, \theta, \mathcal{D})] = -0.6 \log(0.6) - 0.4 \log(0.4) = 0.29\)</span></li>
<li><span class="math inline">\(\mathbb{H}_{\theta_2}[p(y|x_1, \theta, \mathcal{D})] = -0.8 \log(0.8) - 0.2 \log(0.2) = 0.22\)</span></li>
</ul>
<p>Average the results:</p>
<p><span id="eq-eq3.12"><span class="math display">\[
\mathbb{E}_{p(\theta|\mathcal{D})}[\mathbb{H}[p(y|x_1, \theta, \mathcal{D})]] \approx \frac{0.29 + 0.22}{2} = 0.255
\tag{3.12}\]</span></span></p>
<p>For <span class="math inline">\(x_2\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathbb{H}_{\theta_1}[p(y|x_2, \theta, \mathcal{D})] = -0.4 \log(0.4) - 0.6 \log(0.6) = 0.29\)</span></li>
<li><span class="math inline">\(\mathbb{H}_{\theta_2}[p(y|x_2, \theta, \mathcal{D})] = -0.5 \log(0.5) - 0.5 \log(0.5) = 0.30\)</span></li>
</ul>
<p>Average the results:</p>
<p><span id="eq-eq3.13"><span class="math display">\[
\mathbb{E}_{p(\theta|\mathcal{D})}[\mathbb{H}[p(y|x_2, \theta, \mathcal{D})]] \approx \frac{0.29 + 0.30}{2} = 0.295
\tag{3.13}\]</span></span></p>
<p><strong>Step 3:</strong> Compute the BALD score for each sample.</p>
<p>The BALD score <span class="math inline">\(\alpha(x)\)</span> is the difference between the predictive entropy and the expected entropy:</p>
<p>For <span class="math inline">\(x_1\)</span>:</p>
<p><span id="eq-eq3.14"><span class="math display">\[
\alpha(x_1) = \mathbb{H}[p(y|x_1, \mathcal{D})] - \mathbb{E}_{p(\theta|\mathcal{D})}[\mathbb{H}[p(y|x_1, \theta, \mathcal{D})]] = 0.27 - 0.255 = 0.015
\tag{3.14}\]</span></span></p>
<p>For <span class="math inline">\(x_2\)</span>:</p>
<p><span id="eq-eq3.15"><span class="math display">\[
\alpha(x_2) = \mathbb{H}[p(y|x_2, \mathcal{D})] - \mathbb{E}_{p(\theta|\mathcal{D})}[\mathbb{H}[p(y|x_2, \theta, \mathcal{D})]] = 0.30 - 0.295 = 0.005
\tag{3.15}\]</span></span></p>
<p>We would select <span class="math inline">\(x_1\)</span> for labeling since it has the highest BALD score, indicating that labeling <span class="math inline">\(x_1\)</span> will provide the most information gain for the model.</p>
</section>
</section>
<section id="active-learning-by-variance-reduction" class="level3" data-number="3.1.5">
<h3 data-number="3.1.5" class="anchored" data-anchor-id="active-learning-by-variance-reduction"><span class="header-section-number">3.1.5</span> Active Learning by Variance Reduction</h3>
<p>Active Learning by Variance Reduction <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span> is an algorithm designed to select the next data point for labeling based on the anticipated reduction in the model’s variance. The objective is to identify the point <span class="math inline">\(\tilde{x} \sim p(x)\)</span> that, when labeled (<span class="math inline">\(y(\tilde{x})\)</span>), will most effectively decrease the model’s variance. To quantify the expected error at a given input <span class="math inline">\(x\)</span>, we can mathematically express it as follows:</p>
<p><span id="eq-eq3.16"><span class="math display">\[
\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2
\tag{3.16}\]</span></span></p>
<p>In <a href="#eq-eq3.16" class="quarto-xref">Equation&nbsp;<span>3.16</span></a>, <span class="math inline">\(\hat{y}\)</span> represents the model’s prediction, while <span class="math inline">\(y\)</span> denotes the true label corresponding to the input <span class="math inline">\(x\)</span>. This formulation captures the average squared difference between the predicted and actual values, providing a measure of the model’s accuracy. Utilizing concepts from bias-variance decomposition as outlined in the literature <span class="citation" data-cites="bias_variance_orig_paper">(<a href="#ref-bias_variance_orig_paper" role="doc-biblioref">Geman, Bienenstock, and Doursat 1992</a>)</span>, we can expand the expected error term. The expansion is given by:</p>
<p><span id="eq-eq3.17"><span class="math display">\[\begin{aligned}
\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2 &amp;= \mathbb{E}_{\hat{y}, y}[(\hat{y} - \mathbb{E}[y|x]) + (\mathbb{E}[y|x] - y)]^2 \\
&amp;= \mathbb{E}_{\hat{y}, y} [(y - \mathbb{E}[y|x])^2]\\
&amp;+ 2\mathbb{E}_{\hat{y}, y} [(\hat{y} - \mathbb{E}[y|x])(\mathbb{E}[y|x] - y)]\\
&amp;+ \mathbb{E}_{\hat{y}, y}(\hat{y} - \mathbb{E}[y|x])^2
\end{aligned} \tag{3.17}\]</span></span></p>
<p>In <a href="#eq-eq3.17" class="quarto-xref">Equation&nbsp;<span>3.17</span></a>, the first term represents the variance of the true label <span class="math inline">\(y\)</span>, the second term evaluates to zero, and the third term accounts for the variance of the model’s prediction <span class="math inline">\(\hat{y}\)</span>. To clarify why the second term is zero, we note that:</p>
<p><span id="eq-eq3.18"><span class="math display">\[
\mathbb{E}_{\hat{y}, y}[\mathbb{E}[y|x] - y] = 0
\tag{3.18}\]</span></span></p>
<p>This indicates that the expected deviation of the true label from its conditional mean is null, as <span class="math inline">\(\mathbb{E}[y|x]\)</span> is, by definition, the average of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>. Focusing on the third term, we derive it as follows:</p>
<p><span id="eq-eq3.19"><span class="math display">\[\begin{aligned}
\mathbb{E}_{\hat{y}, y}(\hat{y} - \mathbb{E}[y|x])^2 &amp;= \mathbb{E}_{\hat{y}, y}[(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}] + \mathbb{E}_{\hat{y}}[\hat{y}] - \mathbb{E}[y|x])^2] \\
&amp;= \mathbb{E}_{\hat{y}, y}[(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2] + (\mathbb{E}_{\hat{y}}[\hat{y}] - \mathbb{E}[y|x])^2
\end{aligned} \tag{3.19}\]</span></span></p>
<p>Here, <span class="math inline">\(\mathbb{E}_{\hat{y}}[\hat{y}]\)</span> represents the expected model prediction conditioned on the data <span class="math inline">\(\mathcal{D}\)</span> and input <span class="math inline">\(x\)</span>. Combining the results of our analysis, we arrive at the total expected error as:</p>
<p><span id="eq-eq3.20"><span class="math display">\[
\mathbb{E}_{y} [(y - \mathbb{E}[y|x])^2] + (\mathbb{E}_{\hat{y}} [\hat{y} - \mathbb{E}[y|x]] )^2 + \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2]
\tag{3.20}\]</span></span></p>
<p>In this equation, the first term signifies the variance of the true label, which remains constant for a given <span class="math inline">\(x\)</span>. The second term captures the bias of the model, reflecting how much the average model prediction deviates from the expected true label. The third term quantifies the model’s uncertainty concerning the selected input <span class="math inline">\(x\)</span>.</p>
<p>Referring to <span class="citation" data-cites="AL_variance">(<a href="#ref-AL_variance" role="doc-biblioref">Cohn, Ghahramani, and Jordan 1996</a>)</span>, we can denote the uncertainty term as:</p>
<p><span id="eq-eq3.21"><span class="math display">\[
\sigma^2_{\hat{y}} (x | \mathcal{D}) = \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2]
\tag{3.21}\]</span></span></p>
<p>This term explicitly represents the variance of the model predictions at the input <span class="math inline">\(x\)</span> given the dataset <span class="math inline">\(\mathcal{D}\)</span>. More explicitly, it can be expressed as:</p>
<p><span id="eq-eq3.22"><span class="math display">\[
\sigma^2_{\hat{y}} (x | \mathcal{D}) =  \mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x)} [(\hat{y} - \mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x)}[\hat{y}])^2]
\tag{3.22}\]</span></span></p>
<p>This formulation emphasizes the variability of the model’s predictions around their mean, providing insights into the model’s reliability in its estimations. The active learning by variance reduction algorithm can be summarized as follows:</p>
<ol type="1">
<li><strong>Sampling Candidates</strong>: Sample candidate points <span class="math inline">\(\tilde{x}_1, \dots, \tilde{x}_m\)</span> from <span class="math inline">\(p(x)\)</span>.</li>
<li><strong>Compute Expected Variance Reduction</strong>: For each candidate <span class="math inline">\(\tilde{x}_i\)</span>, compute: <span id="eq-eq3.23"><span class="math display">\[
\mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]
\tag{3.23}\]</span></span></li>
<li><strong>Select the Best Candidate</strong>: Choose the point that minimizes expected variance reduction: <span id="eq-eq3.24"><span class="math display">\[
   \tilde{x}^* = \arg\min_{\tilde{x}_i} \mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]
\tag{3.24}\]</span></span></li>
<li><strong>Update Model</strong>: Incorporate the newly labeled data and repeat the process.</li>
</ol>
<p>While there is no general recipe for the number of iterations to perform, one could imagine relying on some empirical measure like a loss on left-out labelled data to gauge model improvement (as seen in <a href="#fig-empirical:gauss" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>, <a href="#fig-empirical:regress" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>). Intuitively, the size of the data set and its relationship to the loss is intimately tied to the model complexity which impacts its data-thirstiness.</p>
<p>We note to the reader that <span class="math inline">\(P(X=x)\)</span> is a distribution with potentially-infinite support and the authors do not compute this integral exactly. Instead, the computational estimate of that integral consists of sampling several points <span class="math inline">\(x \sim P(X=x)\)</span> and averaging the quantity inside the integral over these points until convergence using Monte-Carlo sampling approaches (see <span class="citation" data-cites="monte-carlo">(<a href="#ref-monte-carlo" role="doc-biblioref">Ghojogh et al. 2020</a>)</span>).</p>
<div id="fig-two_models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-two_models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/1_two_models.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-two_models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Two models were empirically explored. These two models lead to closed-form, accurately and efficiently-computed expected learner variance which can be plugged into the algorithm.
</figcaption>
</figure>
</div>
<p>Arm2D (<a href="#fig-arm2D" class="quarto-xref">Figure&nbsp;<span>3.3</span></a>) is a kinematics problem where learner has to predict the tip position of a robotic arm given a set of joint angles <span class="math inline">\(\mathbf{\theta_1}, \mathbf{\theta_2}\)</span>. In this analysis, the two models seen in <a href="#fig-two_models" class="quarto-xref">Figure&nbsp;<span>3.2</span></a>, namely the Gaussian mixture model and locally-weighted regression (LOESS).</p>
<div id="fig-arm2D" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-arm2D-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/1_experiment_setup.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-arm2D-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: The arm kinematics problem. The learner attempts to predict tip position given a set of joint angles <span class="math inline">\(\mathbf{\theta_1}, \mathbf{\theta_2}\)</span>
</figcaption>
</figure>
</div>
<div id="fig-empirical:gauss" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-empirical:gauss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/1_experiment_results_gaussian.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-empirical:gauss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Arm2D domain. Dotted lines denote standard error for average of 10 runs, each started with one initial random example.
</figcaption>
</figure>
</div>
<p>The results shown in <a href="#fig-empirical:gauss" class="quarto-xref">Figure&nbsp;<span>3.4</span></a>, <a href="#fig-empirical:regress" class="quarto-xref">Figure&nbsp;<span>3.5</span></a> are intriguing. As expected, the variance of the learner decreases because the authors selected points to minimize expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both models as the dataset size increases. This is a notable outcome because the expected learner variance for these models can be computed accurately and efficiently relative to a new point. When integrated into the general active learning loop (<a href="#fig-schema" class="quarto-xref">Figure&nbsp;<span>3.1</span></a>), this significantly enhances model performance.</p>
<p>In the case of the locally-weighted regression model (<a href="#fig-empirical:regress" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>), it is surprising that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. However, when active learning by variance reduction is applied, using expected learner variance as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.</p>
<div id="fig-empirical:regress" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-empirical:regress-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/1_experiment_results_regression.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-empirical:regress-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Variance and MSE learning curves for LOESS model trained on the Arm2D domain. Dotted lines denote standard error for average of 60 runs, each started with a single initial random example.
</figcaption>
</figure>
</div>
</section>
<section id="active-learning-in-ranking-and-comparison" class="level3" data-number="3.1.6">
<h3 data-number="3.1.6" class="anchored" data-anchor-id="active-learning-in-ranking-and-comparison"><span class="header-section-number">3.1.6</span> Active Learning in Ranking and Comparison</h3>
<p>Many researchers have shown that making comparisons is easier and more convenient for users than assigning a specific score to each item. Individual comparisons yield a complete ranking over a set of <span class="math inline">\(n\)</span> objects <span class="math inline">\(\Theta = (\theta_1, \theta_2, \cdots, \theta_n)\)</span>. This ranking is defined as a mapping <span class="math inline">\(\sigma : \{1, \cdots, n\} \rightarrow \{1,\cdots, n\}\)</span> that orders the set of objects <span class="math inline">\(\Theta\)</span>. Specifically, for a single <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\sigma(\Theta) = \theta_{\sigma(1)} &lt; \theta_{\sigma(2)} &lt; \cdots &lt; \theta_{\sigma(n-1)} &lt; \theta_{\sigma(n)}\)</span>, where <span class="math inline">\(\theta_{i} &lt; \theta_{j}\)</span> means that <span class="math inline">\(\theta_{i}\)</span> is rated lower than <span class="math inline">\(\theta_{j}\)</span>.</p>
<p>For any <span class="math inline">\(n\)</span> elements to be ranked, there are <span class="math inline">\(n!\)</span> possible orderings that can result in the correct complete ranking. Given that a lower bound on sorting is <span class="math inline">\(n\log n\)</span>, obtaining a guaranteed true rating over <span class="math inline">\(n\)</span> objects requires <span class="math inline">\(n\log n\)</span> pairwise comparisons if those comparisons are chosen at random. This number can be quite high and costly in many applications, especially since most ranking information comes from humans. The more comparisons they have to make, the more money and time is spent. This process can also be inefficient, as some comparisons provide more value to the learning process than others, making some comparisons a waste. This inefficiency can be detrimental in fields like psychology and market research, where comparisons are heavily utilized, and a faster process could offer significant benefits.</p>
<p>The reason the lower bound on the number of comparisons is <span class="math inline">\(n\log n\)</span> is that it assumes no prior information about the underlying space and field, so comparisons are chosen at random. However, leveraging the structures within the comparison space can provide more information about which comparisons are most valuable. For example, <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> discusses how eye doctors have a wide range of options when assigning prescriptions for glasses, yet patients do not see them making many comparisons before deciding on the best option. This is because eye doctors incorporate domain knowledge into the process and only ask clients for comparisons when necessary. Applying similar knowledge in the ranking field leads to an active learning approach that selects data based on the relevance of a comparison query toward finding the final <span class="math inline">\(\sigma(\Theta)\)</span>.</p>
<section id="geometric-approach-to-comparisons" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="geometric-approach-to-comparisons">Geometric Approach to Comparisons</h4>
<p>In this section, we will review the paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span>, which explores active learning within data that can be embedded in a multi-dimensional space. In this context, comparisons between two different objects divide the space into halves, with one object being superior in each half. By leveraging such spatial information, the paper develops a geometric approach to ranking and active learning. This spatial information serves as the domain knowledge that informs which comparisons to perform to achieve the ranking.</p>
<p>For this application, the following terms are defined:</p>
<ol type="1">
<li><p><strong><span class="math inline">\(R^d\)</span></strong>: The space in which objects can be embedded.</p></li>
<li><p><strong><span class="math inline">\(\theta_1, \cdots,\theta_n\)</span></strong>: The objects, now representing their locations in <span class="math inline">\(R^d\)</span>.</p></li>
<li><p>For each ranking <span class="math inline">\(\sigma\)</span>, there is a reference point <span class="math inline">\(r_{\sigma} \in R^d\)</span>, such that if, according to ranking <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(\theta_{i} &lt; \theta_{j}\)</span> (object <span class="math inline">\(i\)</span> is worse than <span class="math inline">\(j\)</span>), then <span class="math inline">\(||\theta_i - r_{\sigma}|| &lt; ||\theta_j - r_{\sigma}||\)</span>. In other words, object <span class="math inline">\(i\)</span> is closer to the reference point <span class="math inline">\(r_{\sigma}\)</span> of the ranking than object <span class="math inline">\(j\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\Sigma_{n,d}\)</span></strong>: The set of all possible rankings of the <span class="math inline">\(n\)</span> objects that satisfy the embedding distances in the space <span class="math inline">\(R^d\)</span> as defined above. Note that not all possible rankings will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions.</p></li>
<li><p>For every ranking <span class="math inline">\(\sigma\)</span>, there is <span class="math inline">\(M_n(\sigma)\)</span>, the number of pairwise comparisons needed to identify the ranking. When comparisons are done at random, <span class="math inline">\(E[M_n(\sigma)] = n\log n\)</span>. The paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> examines this quantity to demonstrate that it can be reduced by incorporating spatial knowledge.</p></li>
<li><p><strong><span class="math inline">\(q_{i,j}\)</span></strong>: The query of comparison between objects <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>.</p></li>
</ol>
</section>
<section id="embedding-space" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="embedding-space">Embedding Space</h4>
<div id="fig-dim-space" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dim-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/SPACE.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dim-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Objects <span class="math inline">\(\theta_1, \theta_2, \theta_3\)</span> and queries in <span class="math inline">\(R^2\)</span>. The <span class="math inline">\(r_\theta\)</span> lies in the shaded region which represents <span class="math inline">\(\Sigma_{n,2}\)</span>(consistent with the labels of <span class="math inline">\(q_{1,2}, q_{1,3}, q_{2,3}\)</span>). The dotted (dashed) lines represent new queries whose labels are (are not) ambiguous.
</figcaption>
</figure>
</div>
<p>To properly understand how to select the most valuable queries, it is essential to examine the space where the objects exist and how the queries divide that space to determine the proper rankings. For this example, in <a href="#fig-dim-space" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>, the paper <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> operates in <span class="math inline">\(R^2\)</span> space with three objects: <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>. There are pairwise queries <span class="math inline">\(q_{1,3}\)</span>, <span class="math inline">\(q_{2,3}\)</span>, and <span class="math inline">\(q_{1,2}\)</span> between them, denoted by solid lines equidistant from the two objects they compare. These lines split the <span class="math inline">\(R^2\)</span> space into halves, with each half closer to one of the two objects. The paper colors the side of the worse object for each query in dark grey and takes the intersection of these halves, resulting in the dark grey region in the image. This region indicates <span class="math inline">\(\Sigma_{n,2}\)</span> since all points follow the embedding conditions. Specifically, for every point <span class="math inline">\(r\)</span> in the dark grey area, <span class="math inline">\(||\theta_3 - r|| &lt; ||\theta_2 - r|| &lt; ||\theta_1 - r||\)</span>, meaning <span class="math inline">\(\theta_3 &lt; \theta_2 &lt; \theta_1\)</span>. Thus, every point <span class="math inline">\(r\)</span> is one of the <span class="math inline">\(r_\sigma\)</span> representing their respective rankings <span class="math inline">\(\sigma \in \Sigma_{n,2}\)</span>. In other words, the paper aims to have the reference points and dark grey region closest to the worst object and furthest from the best object.</p>
<p>The authors also denote the label for each query <span class="math inline">\(q_{i,j}\)</span>, such as label <span class="math inline">\(y_{i,j} = 1\{q_{i,j}\}\)</span> (for example, <span class="math inline">\(y_{1,2} = 0, y_{3,2} = 1\)</span>). This allows for deciding how to label new queries represented by dashed and dotted lines, depending on which objects each query compares. Focusing on the dotted line, called <span class="math inline">\(q_{i,4}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span>, and considering potential locations of <span class="math inline">\(\theta_4\)</span>, the line must be equidistant from one of the three objects in the picture and <span class="math inline">\(\theta_4\)</span>, meaning <span class="math inline">\(\theta_4\)</span> can be placed in three different locations. If the query performed is <span class="math inline">\(q_{2,4}\)</span>, then <span class="math inline">\(\theta_4\)</span> will be closer to the dark grey area than <span class="math inline">\(\theta_2\)</span>, thus <span class="math inline">\(y_{2,4} = 0\)</span>. However, if <span class="math inline">\(q_{1,4}\)</span> or <span class="math inline">\(q_{3,4}\)</span> are performed, <span class="math inline">\(\theta_4\)</span> will be further from the dark grey area than <span class="math inline">\(\theta_1\)</span> or <span class="math inline">\(\theta_3\)</span>, meaning <span class="math inline">\(y_{1,4} = y_{3,4} = 1\)</span>. In this case, the labels are contradictory and depend on which object they are compared with, making such a query <span class="math inline">\(q_{i,4}\)</span> ambiguous.</p>
<p>In contrast, the authors analyze the dashed line, called <span class="math inline">\(q_{i,5}\)</span>, where <span class="math inline">\(i={1,2,3}\)</span>, and consider potential locations of <span class="math inline">\(\theta_5\)</span>. Since the line must be equidistant from one of the three objects in the picture and <span class="math inline">\(\theta_5\)</span>, it can be placed in three different locations. If one of the three potential queries is performed, <span class="math inline">\(\theta_5\)</span> will be closer to the dark grey area than <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, and <span class="math inline">\(\theta_3\)</span>, meaning <span class="math inline">\(y_{1,5} = y_{2,5} = y_{3,5} = 0\)</span>. In this case, all labels are the same regardless of which object is used, meaning such a query will not be contradictory, as all agree on the label.</p>
<p>The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease the total <span class="math inline">\(M_n(\sigma)\)</span>. Intuitively, if there is contradictory information about a query, it needs to be erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain space agree on the query’s label, that information can be used without asking a human, incorporating the knowledge of the embedding distances.</p>
<p>Lastly, to consider the general case of the <span class="math inline">\(R^d\)</span> space, rather than discussing halves of the image, it is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of <span class="math inline">\(1\)</span> to the query and the half-space assigning a label of <span class="math inline">\(0\)</span>. If both half-spaces exist, they have conflicting information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.</p>
<section id="algorithms-for-ambiguous-query-selection" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="algorithms-for-ambiguous-query-selection">Algorithms for Ambiguous Query Selection</h5>
<div id="alg-qsa" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="1">
<div class="pseudocode">
\begin{algorithm} \caption{Query Selection Algorithm} \begin{algorithmic} \State \textbf{input:} $n$ objects in $\mathbb{R}^d$ \State \textbf{initialize:} objects $\theta_1, \dots, \theta_n$ in uniformly random order \For{$j=2, \dots, n$} \For{$i=1, \dots, j-1$} \If{$q_{i,j}$ is ambiguous} \State request $q_{i,j}$'s label from reference \Else \State impute $q_{i,j}$'s label from previously labeled queries \EndIf \EndFor \EndFor \State \textbf{output:} ranking of $n$ objects \end{algorithmic} \end{algorithm}
</div>
</div>
<p>The standard algorithm in <a href="#alg-qsa" class="quarto-xref">Algorithm 1</a> requests labels for <span class="math inline">\(q_{i,j}\)</span> if those queries are ambiguous; otherwise, it infers the information from prior comparisons and their labels.</p>
<p>It is important to demonstrate that the number of comparisons decreases. Specifically, <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> shows that this algorithm has <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, where <span class="math inline">\(d\)</span> is the dimension of the space and <span class="math inline">\(d &lt; n\)</span>, which improves on the <span class="math inline">\(O(n\log n)\)</span> baseline. The proof can be studied in detail in the paper itself, but at a high level, it starts by reasoning about the probability of a query being ambiguous and a comparison being requested from a human, thus representing <span class="math inline">\(M_n = \Sigma_{k=1}^{n-1}\Sigma_{i=1}^k 1\{Requestq_{i,k+1}\}\)</span>. For that, the authors define <span class="math inline">\(Q(i,j)\)</span>, which represents the number of different rankings that exist for <span class="math inline">\(i\)</span> elements in <span class="math inline">\(j\)</span>-dimensional space (e.g., <span class="math inline">\(Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!\)</span>). In that case, <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d)\)</span>. Further, using recurrence relations for <span class="math inline">\(Q(i,j)\)</span>, the authors derive that <span class="math inline">\(|\Sigma_{n,d}| = Q(n,d) = O(n^{2d})\)</span>, which is omitted here. Analogously, the authors define <span class="math inline">\(P(i,j)\)</span>, which represents the number of rankings in <span class="math inline">\(\Sigma_{n,d}\)</span> that will still be possible with the addition of a new element <span class="math inline">\(i+1\)</span> to the ranking objects. Referring back to <a href="#fig-dim-space" class="quarto-xref">Figure&nbsp;<span>3.6</span></a>, <span class="math inline">\(P(i,j)\)</span> estimates how much of the dark grey area will still exist after making a query for <span class="math inline">\(i+1\)</span>. As indicated there, the dotted line ambiguous query did not change the dark grey a rea at all (<span class="math inline">\(P(n,d) = Q(n,d)\)</span>), whereas the dashed non-ambiguous query would cut a piece from it (<span class="math inline">\(P(n,d) &lt; Q(n,d)\)</span>). Thus, <span class="math inline">\(Request q_{i,k+1} = P(k,d) / Q(k,d)\)</span>, so a higher value indicates more possible rankings and an ambiguous query that needs to be requested to obtain more useful information. With this in mind, the authors derive that <span class="math inline">\(E[M_n(\sigma)] = O(d\log n)\)</span>, showing that fewer queries are needed for effective ranking.</p>
<p>The issue with this algorithm is that only one human provides the answers to the requested queries, which means it does not account for their biases. An alternative approach is a Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span>, which uses majority voting for every query to indicate the ground truth of the query’s label. However, the authors consider that a group of people can still give incorrect or divided responses. If the votes for each answer are almost equal in number, the authors push that query to the end of the algorithm to see if it can become a non-ambiguous query with more information learned. If it does not, an odd number of voters is used to determine the final ranking.</p>
</section>
<section id="sec-QSA" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="sec-QSA">Performance Analysis</h5>
<div id="fig-rand_n" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rand_n-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Dim:query_graph.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rand_n-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Mean and standard deviation of requested queries (solid) in the noiseless case for <span class="math inline">\(n = 100\)</span>; <span class="math inline">\(\log_2|\Sigma_{n,d}|\)</span> is a lower bound (dashed).
</figcaption>
</figure>
</div>
<div id="tbl-geo_acc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-geo_acc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Statistics for the Robust Query Selection Algorithm (RQSA) <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> discussed at the end of <span class="quarto-unresolved-ref">?sec-QSA</span> and the baseline of conducting all comparisons. <span class="math inline">\(y\)</span> serves as a noisy ground truth, <span class="math inline">\(\tilde{y}\)</span> is the result of all comparisons, and <span class="math inline">\(\hat{y}\)</span> is the output of the RQSA.
</figcaption>
<div aria-describedby="tbl-geo_acc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th></th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">% of queries</td>
<td>mean</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">18.5</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td>std</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average error</td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\(d(\bar{y}, y)\)</span></td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.29</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-rand_n" class="quarto-xref">Figure&nbsp;<span>3.7</span></a> shows that the number of comparisons fits within the expected bounds, as <span class="math inline">\(\log|\Sigma_{n,d}| = \log(n^d) = d\log n\)</span>. To derive that graph, authors <span class="citation" data-cites="geo_paper">(<a href="#ref-geo_paper" role="doc-biblioref">G. and Nowak 2011</a>)</span> sampled 100 random data points in a <span class="math inline">\(R^d\)</span> space, where <span class="math inline">\(d\)</span> took on 10 different values as indicated on the graph. Each dimension’s experiments were repeated 25 times for consistency.</p>
<p>With regard to the accuracy and performance of the method, the authors did a ranking experiment on 100 different audio signals, results of which can be seen in <a href="#tbl-geo_acc" class="quarto-xref">Table&nbsp;<span>3.1</span></a>. The ground truth labels came from humans, indicated by <span class="math inline">\(y\)</span> in the table. That resulted in the existence of noise and potential errors in the ground truth, which could influence the performance of both the baseline algorithm that does all comparisons (<span class="math inline">\(\tilde{y}\)</span>) and the Robust Query Selection Algorithm (RQSA) proposed in <span class="quarto-unresolved-ref">?sec-QSA</span> (<span class="math inline">\(\hat{y}\)</span>). As can be seen in both 2 and 3-dimensional spaces RQSA performed worse by <span class="math inline">\(8\%\)</span> compared to the baseline, which indicates that active learning that uses the domain information can still be erroneous due to the inference of certain comparisons that sometimes may not be entirely correct. However, as can be seen by the upper part of <a href="#tbl-geo_acc" class="quarto-xref">Table&nbsp;<span>3.1</span></a>, significantly less queries were requested compared to the baseline, which means that the approach can have a significant benefit at a cost of slight loss in accuracy.</p>
</section>
</section>
<section id="sec-geo_app" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sec-geo_app">User Information as Domain Knowledge for Active Learning</h4>
<p>An alternative source of domain knowledge could be users themselves, who can indicate their uncertainty when it comes to comparing two objects. Prior studies have shown <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> that when presented with only two options when selecting which object is better, but not being able to properly decide, users would get frustrated and tend to respond more faultyly, creating noise and incorrect responses in the data. Through feedback and other studies <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span> it was determined that presenting users with an option of indifference between the two objects can remove those problems. Moreover, in connection to active learning, the authors show that such an option helps to select more informative queries since it provides more domain knowledge that can be used, resulting in a decrease in the number of queries required.</p>
<p>For this problem, the following terms are defined:</p>
<ol type="1">
<li><p><span class="math inline">\(c\)</span> - a cost function that represents user preferences, and the result the model has to determine at the end of training. The preferred items will have lower costs, and less preferred ones will have higher costs. The goal is to determine this function with the fewest possible number of queries using active learning.</p></li>
<li><p><span class="math inline">\(H\)</span> - a set of hypotheses over the possible cost functions, where for each <span class="math inline">\(h \in H\)</span> there is a cost function <span class="math inline">\(c_h\)</span> associated with it.</p></li>
<li><p><span class="math inline">\(h^*\)</span> - a true hypothesis that the model needs to determine, which has cost <span class="math inline">\(c_{h^*}\)</span> associated with it</p></li>
<li><p><span class="math inline">\(t(x,y)\)</span> - a test performed to compare items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (the user is being asked to provide a response to which item is better). Those tests result in changes and adjustments to <span class="math inline">\(H\)</span> as more information is learned.</p></li>
<li><p><span class="math inline">\(o(x,y)\)</span> - observation or result of <span class="math inline">\(t(x,y)\)</span>, where <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y\}\)</span></p></li>
<li><p><span class="math inline">\(S = \{(t_1, o_1), (t_2, o_2),...,(t_m, o_m)\}\)</span> - a sequence of <span class="math inline">\(m\)</span> pairs of tests and observations</p></li>
<li><p><span class="math inline">\(w(H|S)\)</span> - probability mass of all hypotheses that are still consistent with the observations (similar to the dark grey area from <a href="#fig-dim-space" class="quarto-xref">Figure&nbsp;<span>3.6</span></a> and <span class="math inline">\(Q(i,j)\)</span> discussed in <span class="quarto-unresolved-ref">?sec-QSA</span>. This means that if <span class="math inline">\(h \in H\)</span> is inconsistent with user responses received, it is removed from <span class="math inline">\(H\)</span>.</p></li>
</ol>
<p>With the key terms defined, let’s consider the noiseless base setting where users only have two options for response. Those components will also later be translated to the setting with the third option so the true cost function can be determined there. <span class="math inline">\(w(H|S)\)</span> is the sum of the weights of all hypotheses that are still consistent with the evidence. <span id="eq-eq3.25"><span class="math display">\[\begin{aligned}
    w(H|S) = \sum_{h \in H} w(h | S)\\
\end{aligned} \tag{3.25}\]</span></span> Each <span class="math inline">\(w(h|S)\)</span> is a probability of the evidence’s existence given such hypothesis: <span id="eq-eq3.26"><span class="math display">\[\begin{aligned}
    w(h|S) = p(S|h)
\end{aligned} \tag{3.26}\]</span></span> Such probability comes from the test-observation pairs since they compose the set <span class="math inline">\(S\)</span>. Moreover, each test is independent of other tests, which gives: <span id="eq-eq3.27"><span class="math display">\[\begin{aligned}
    p(S|h) = \prod_{(t,o) \in S} p((t,o) | h)
\end{aligned} \tag{3.27}\]</span></span> In the noiseless setting, users will select an option that minimizes their cost function (selecting more preferred items), mathematically defined as: <span id="eq-prob_base"><span class="math display">\[\begin{aligned}
    p((t, o = x) | h) =
    \begin{cases}
        1 &amp; c_h(x) &lt; c_h(y)\\
        0 &amp; else
    \end{cases}
\end{aligned} \tag{3.28}\]</span></span></p>
<p><strong>6.3.3.1 User Noise Modeling</strong></p>
<p>As has been discussed, users are not perfect evaluators and even get frustrated if unable to select the better option. Prior work <span class="citation" data-cites="unnoisy_humans">(<a href="#ref-unnoisy_humans" role="doc-biblioref">Amershi et al. 2014</a>)</span> has shown that treating users as perfect can lead to poor performance. That gave rise to accounting for noise in users’ responses, but a majority of such work applies the same noise to all queries and all responses. While those led to great performance results <span class="citation" data-cites="noisy_humans">(<a href="#ref-noisy_humans" role="doc-biblioref">Guillory and Bilmes 2011</a>)</span>, they don’t accurately reflect the real world, which gave rise to the idea of creating query-based noise.</p>
<p>Effectively, for some of the queries it is important to incorporate the fact that the user is unsure and noisy, but for others, if the user is confident, noise in the response is not needed at all. For comparison-based learning, this means that the noise is related to the costs of the two items compared. Specifically for items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, if <span class="math inline">\(c_{h^*}(x) \simeq c_{h^*}(y)\)</span> then the items are hard to distinguish for the user, so here it is preferred to incorporate user uncertainty and noise. But if <span class="math inline">\(c_{h^*}(x) &gt;&gt; c_{h^*}(y)\)</span>, the user will certainly select <span class="math inline">\(y\)</span> and the other way around, which is where the noise is not needed.</p>
<p>Query-dependent noise is also supported in the psychology literature, which means that such an approach is more related to the real world. In particular, psychologists talk about the Luce-Sheppard Choice rule <span class="citation" data-cites="lus-shep">(<a href="#ref-lus-shep" role="doc-biblioref">Shepard 1957</a>)</span> when talking about comparisons. This rule previously gave rise to a logistic model based on the noise <span class="citation" data-cites="lus-log">(<a href="#ref-lus-log" role="doc-biblioref">Viappiani and Boutilier 2010</a>)</span> where the probability of observation for a given test is: <span id="eq-noise_model"><span class="math display">\[\begin{aligned}
    p((t, o = x) | h) \propto exp(-\gamma * c_h(x))
\end{aligned} \tag{3.29}\]</span></span></p>
<div id="fig-noiseless_1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noiseless_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Noiseless probs.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noiseless_1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: User response model in the noiseless setting
</figcaption>
</figure>
</div>
<div id="fig-noiseless_2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-noiseless_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Noise probs.png" class="img-fluid figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-noiseless_2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: User response with Luce Sheppard noise model
</figcaption>
</figure>
</div>
<p><a href="#fig-noiseless_1" class="quarto-xref">Figure&nbsp;<span>3.8</span></a>, <a href="#fig-noiseless_2" class="quarto-xref">Figure&nbsp;<span>3.9</span></a> demonstrate the difference between the noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS is the baseline model with only 2 response options, and CLAUS is the model with the uncertainty option added. The figures show how incorporating such noise influences and smoothes the probability distribution of the user’s response.</p>
<p><strong>6.3.3.2 User Uncertainty</strong></p>
<p>We will now discuss the functionality of CLAUS, which is an algorithm designed by <span class="citation" data-cites="claus">(<a href="#ref-claus" role="doc-biblioref">Holladay et al. 2016</a>)</span> that allows users to select an uncertain response about the two options that they need to rank. The authors model such uncertainty as <span class="math inline">\(\epsilon\)</span> and it is associated with each <span class="math inline">\(c_h\)</span>, so now every hypothesis <span class="math inline">\(h\)</span> is defined over a pair of <span class="math inline">\((c_h, \epsilon_h)\)</span>. It is important to note that the goal is to still learn and maintain our objective on <span class="math inline">\(c\)</span>, <span class="math inline">\(\epsilon\)</span> is only necessary to model the users’ responses. The uncertainty relates to the cost function in the following way: <span id="eq-eq3.30"><span class="math display">\[\begin{aligned}
    |c_h(x) - c_h(y)| &lt; \epsilon_h
\end{aligned} \tag{3.30}\]</span></span> this means that the user is uncertain between items <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and their cost difference is negligible such that the user is not able to select which item is better. This in turn gives more information about the real value of the two items, as a binary response would indicate the user’s preference towards one item, which will not be real and will skew the cost functions.</p>
<p>This causes modifications of the problem set-up:</p>
<ol type="1">
<li><p>For test <span class="math inline">\(t(x,y)\)</span> the observation will be <span class="math inline">\(o(x,y) \in \{x&lt;y, x&gt;y, \tilde{xy}\}\)</span>, where <span class="math inline">\(\tilde{xy}\)</span> is the uncertain response.</p></li>
<li><p>The probability distribution over the user’s response (<a href="#eq-prob_base" class="quarto-xref">Equation&nbsp;<span>3.28</span></a>) will now be defined as: <span id="eq-eq3.31"><span class="math display">\[\begin{aligned}
p((t, o = x) | h) =
\begin{cases}
    1 &amp; c_h(x) &lt; c_h(y) - \epsilon_h\\
    0 &amp; else
\end{cases}
\end{aligned} \tag{3.31}\]</span></span></p></li>
</ol>
<p><span id="eq-eq3.32"><span class="math display">\[\begin{aligned}
    p((t, o = \tilde{xy}) | h) =
    \begin{cases}
        1 &amp; |c_h(x) - c_h(y)|^2 &lt; \epsilon_h^2\\
        0 &amp; else
    \end{cases}
\end{aligned} \tag{3.32}\]</span></span></p>
<p>This means the user confidently selects <span class="math inline">\(x\)</span> when it is better than <span class="math inline">\(y\)</span> by more than <span class="math inline">\(\epsilon\)</span>, but if the squared difference of the cost functions of two items is negligible by <span class="math inline">\(\epsilon\)</span> user will choose the indifferent option.</p>
<ol start="3" type="1">
<li>Finally this also updates the noise model (<a href="#eq-noise_model" class="quarto-xref">Equation&nbsp;<span>3.29</span></a>): <span id="eq-eq3.33"><span class="math display">\[\begin{aligned}
p((t, o = x) | h) \propto \exp(-\gamma * [c_h(x) - c_h(y)])
\end{aligned} \tag{3.33}\]</span></span></li>
</ol>
<p><span id="eq-eq3.34"><span class="math display">\[\begin{aligned}
    p((t, o = \tilde{xy}) | h) \propto exp(-1/\epsilon_h^2 * [c_h(x) - c_h(y)]^2)
\end{aligned} \tag{3.34}\]</span></span></p>
<p><strong>6.3.3.3 Performance Analysis</strong></p>
<div id="fig-equiv_c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-equiv_c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/equiv.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-equiv_c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: CLAUS using equivalence classes. Each cost function <span class="math inline">\(c\)</span> corresponds to an equivalence class (blue ellipse). Hypotheses (black dots) are <span class="math inline">\(\{c_h,\epsilon_h\}\)</span> pairs. Hypotheses sharing a cost <span class="math inline">\(c\)</span> are said to be inside the equivalence class of <span class="math inline">\(c\)</span>. After performing a test and receiving an observation, the evidence results in downweighting connections among some of the hypotheses.
</figcaption>
</figure>
</div>
<p>Before diving deeper into the comparisons of performance, it is important to indicate that rather than predicting a specific pair <span class="math inline">\((c_h, \epsilon_h)\)</span>, the algorithm focuses on predicting a group of pairs that are similar to one another, otherwise called equivalence class (<a href="#fig-equiv_c" class="quarto-xref">Figure&nbsp;<span>3.10</span></a>), which indicates not essentially different hypothesis for the cost function and uncertainty. That information is learned through each new test, as the algorithm updates the information about <span class="math inline">\(c\)</span> and <span class="math inline">\(\epsilon\)</span> that distinguishes between the distinct <span class="math inline">\(h\)</span>, finding the equivalence groups among them. Moreover, the authors tweaked the parameter responsible for the size of the equivalence class (how many hypotheses can be grouped together at a time).</p>
<div id="fig-claus_num" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-claus_num-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/GBS:CLAUS.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-claus_num-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: Performance of GBS and its variants
</figcaption>
</figure>
</div>
<div id="tbl-claus_tab" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-claus_tab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Performance of GBS and CLAUS with different labels for the uncertainty
</figcaption>
<div aria-describedby="tbl-claus_tab-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Category</strong></th>
<th style="text-align: center;"><strong>Accuracy</strong></th>
<th style="text-align: center;"><strong>Query Count</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">GBS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(94.15 \pm 0.52\)</span></td>
<td style="text-align: center;"><span class="math inline">\(36.02 \pm 0.03\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">GBS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(\textbf{94.66} \pm \textbf{0.55}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(35.95 \pm 0.04\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CLAUS - About Equal</td>
<td style="text-align: center;"><span class="math inline">\(91.56 \pm 0.84\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\textbf{25.93} \pm \textbf{0.41}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">CLAUS - Not Sure</td>
<td style="text-align: center;"><span class="math inline">\(90.86 \pm 0.74\)</span></td>
<td style="text-align: center;"><span class="math inline">\(26.98 \pm 0.47\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The first performance evaluation is done on the number of queries and confirms that it decreases in <a href="#fig-claus_num" class="quarto-xref">Figure&nbsp;<span>3.11</span></a>. The GBS model serves as the baseline, as it will do all of the comparison queries using the binary response options. The CLAUS model is measured over different values of <span class="math inline">\(\epsilon\)</span> on the x-axis and over different sizes of the equivalence sets indicated by different shades of blue. Figure shows that all variants of CLAUS use approximately 10 fewer queries on average compared to GBS. Moreover, using bigger-sized equivalence classes can further decrease the number of needed queries. The most optimal <span class="math inline">\(\epsilon \simeq 0.07\)</span>, after which higher <span class="math inline">\(\epsilon\)</span> does not provide any benefit.</p>
<p>Lastly, the authors considered the performance difference, which is indicated in <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a>. For that authors used two different labels for the uncertainty button in CLAUS, it was either labeled as "About Equal" or "Not Sure" as those can provoke different responses and feelings in users. Moreover, GBS and CLAUS-type responses were mixed in the same set of questions to the user, which splits the metrics for both in two as can be seen in <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a>. The performance of CLAUS is lower by <span class="math inline">\(3\%\)</span> on average, indicating similar results to <span class="quarto-unresolved-ref">?sec-geo_app</span>, showing that a smaller number of queries can still lead to a performance loss. However, the second column of <a href="#tbl-claus_tab" class="quarto-xref">Table&nbsp;<span>3.2</span></a> supports the information in <a href="#fig-claus_num" class="quarto-xref">Figure&nbsp;<span>3.11</span></a>, as it also shows that 10 fewer queries were conducted on average.</p>
</section>
</section>
<section id="active-preference-based-learning-of-reward-functions" class="level3" data-number="3.1.7">
<h3 data-number="3.1.7" class="anchored" data-anchor-id="active-preference-based-learning-of-reward-functions"><span class="header-section-number">3.1.7</span> Active Preference-Based Learning of Reward Functions</h3>
<p>Active learning can be essential in learning within dynamic systems and environments. Say we have an agent in an environment, and we want it to conform to a certain behavior as set by a human. How exactly do we go about doing this? In a traditional RL setting, this is solved by a class of algorithms under Inverse Reinforcement Learning. Techniques such as VICE and GAIL attempt to learn a reward function that can distinguish between states visited by the agent and states desired to be visited as defined by a human. In effect, a human will demonstrate what it would like the agent to do in the environment, and from there, learning is done. However, what if humans do not precisely know how an agent should optimally behave in an environment but still have some opinion on what trajectories would be better than others? This is where a paper like Active Preference-Based Learning of Reward Functions comes into the picture. The paper aims to use human preferences to aid an agent’s learning within a dynamic system.</p>
<p>A dynamic system contains human input, robotic input, and an environment state. The transitions between states is defined by <span class="math inline">\(f_{HR}\)</span>, so that we have: <span id="eq-eq3.35"><span class="math display">\[x^{t+1} = f_{HR}(x^t, u_R, u_H) \tag{3.35}\]</span></span> At a given time step <span class="math inline">\(t\)</span>, we have <span class="math inline">\(x_t\)</span>, <span class="math inline">\(u_R^t\)</span>, and <span class="math inline">\(u_H^t\)</span>. This can be encapsulated into a single <span class="math inline">\(d\)</span> dimensional feature vector that the authors denote as <span class="math inline">\(\phi\)</span>. The paper then assumes that the underlying reward model we are trying to learn can be represented linearly. If we have our human reward preference function defined as <span class="math inline">\(r_H\)</span>, this means we can write <span class="math inline">\(r_H\)</span> as: <span id="eq-eq3.35"><span class="math display">\[r_H(x^t, u_R^t, u_H^t) = w^{\intercal}\phi(x^t, u_R^t, u_H^t) \tag{3.36}\]</span></span> Because the reward function is linear, we can take the weight vector out of the summation if we want to calculate the reward over an entire trajectory: <span id="eq-eq3.36"><span class="math display">\[\begin{aligned}
R_{H}(x^0, u_R, u_H) &amp;= \sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t)\\
\Phi &amp;= \sum \phi(x^t, u_R^t, u_H^t)\\
R_H(traj) &amp;= w\cdot\Phi(traj)\end{aligned} \tag{3.37}\]</span></span></p>
<section id="properties-of-w" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="properties-of-w">Properties of <span class="math inline">\(W\)</span></h4>
<p>First, the scale of <span class="math inline">\(w\)</span> does not matter because we only care about the relative rewards produced with <span class="math inline">\(w\)</span> (given two different trajectories, we want to answer the question of which trajectory a human would prefer, i.e.&nbsp;which one has a higher preference reward). This means we can constrain <span class="math inline">\(||w|| &lt;= 1\)</span>, so the initial prior is uniform over a unit ball. From here, we can determine a probabilistic expression to assess whether we should prefer trajectory A or B (because it can be noisy with human input). Let <span class="math inline">\(I_t = +1\)</span> if the human prefers trajectory <span class="math inline">\(A\)</span> and let <span class="math inline">\(I_t = -1\)</span> if the human prefers trajectory <span class="math inline">\(B\)</span>. We get the following for <span class="math inline">\(p(I_t | w)\)</span>.</p>
<p><span id="eq-eq3.37"><span class="math display">\[\begin{aligned}
p(I_t = +1|w) &amp;= \frac{exp(R_H(traj_A))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}\\
p(I_t = -1|w) &amp;= \frac{exp(R_H(traj_B))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}
\end{aligned} \tag{3.38}\]</span></span></p>
<p>We can re-write this expression to make it cleaner, using the following substitution: <span id="eq-eq3.38"><span class="math display">\[\psi = \Phi(traj_a) - \Phi(traj_b) \tag{3.39}\]</span></span> <span id="eq-eq3.39"><span class="math display">\[f_{\psi} (w) = p(I_t|w) = \frac{1}{1 + exp(-I_tw^{\intercal}\psi)} \tag{3.40}\]</span></span></p>
<p>The idea now is that we can update <span class="math inline">\(p(w)\)</span> everytime we get a result from a human preference query using Bayes:</p>
<p><span id="eq-eq3.40"><span class="math display">\[p(w|I_t) &lt;- p(w) \cdot p(I_t|w) \tag{3.41}\]</span></span></p>
<p>We do not need to know <span class="math inline">\(p(I_t)\)</span> because we can use an algorithm like the Metropolis algorithm to actually sample.</p>
</section>
<section id="generating-queries" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="generating-queries">Generating Queries</h4>
<p>This is where the interesting part of the paper comes into play. How do we actually generate queries for the user to pick between? This paper synthetically generates queries through an optimization process and then presents them to a human to pick between. The idea is that we want to generate a query that maximizes the conditional entropy <span class="math inline">\(H(I|w)\)</span>. There are a few ways to think about this – intuitively we want to pick a query that we are most uncertain about given our current weights (thus having the highest conditional entropy given the weights). The way the authors of the paper frame this originally in the paper is that "we want to find the next query such that it will help us remove as much volume (the integral of the unnormalized pdf over w) as possible from the space of possible rewards." Mathematically this can be written as:</p>
<p><span id="eq-eq3.41"><span class="math display">\[max_{x^0, u_R, u_H^A, u_H^B} min\{E[1-f_{\psi}(w)], E[1 - f_{-\psi}(w)]\} \tag{3.42}\]</span></span></p>
<p>But how exactly do we optimize this expression mathematically? After all, we need to use this expression to generate synthetic queries. The answer is to sample <span class="math inline">\(w_1, ... w_m\)</span> from <span class="math inline">\(p(w)\)</span>. We can assume we are sampling points from a point cloud, thus approximating the distribution <span class="math inline">\(p(w)\)</span> as</p>
<p><span id="eq-eq3.42"><span class="math display">\[p(w) = \frac{1}{M} \sum \delta (w_i). \tag{3.43}\]</span></span> We can now approximate the expectation expression like so: <span id="eq-eq3.43"><span class="math display">\[E[1 - f_{\psi}(w)] = \frac{1}{M} (\sum 1 - f_{\psi}(w_i)) \tag{3.44}\]</span></span></p>
<p>and now we can optimize the expression to generate a synthetic query! Altogether, the algorithm looks like the following:</p>
<div id="alg-design" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="2">
<div class="pseudocode">
\begin{algorithm} \caption{Preference-Based Learning of Reward Functions} \begin{algorithmic} \State \textbf{input:} features $\phi$, horizon $N$, dynamics $f$, $iter$ \State \textbf{initialize:} $p(w) \sim Uniform(B)$, for a unit ball $B$ \While{$t &lt; iter$} \State $W \gets M$ samples from $AdaptiveMetropolis(p(w))$ \State $(x^0, u_R, u^A_H, u^B_H) \gets SynthExps(W,f)$ \State $I_t \gets QueryHuman(x^0, u_R, u^A_H, u^B_H)$ \State $\varphi = \Phi(x^0, u_R, u^A_H) - \Phi(x^0, u_R, u^B_H)$ \State $f_\varphi(w) = \min(1, I_t\exp(w^\top \varphi))$ \State $p(w) \gets p(w) \cdot f_\varphi(w)$ \State $t \gets t+1$ \EndWhile \State \textbf{output:} distribution of $w: p(w)$ \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="batching-queries" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="batching-queries">Batching Queries</h4>
<p>The algorithm itself works well, however there ends up being a bottle neck that each query needs to be synthesized before being sent to the human – one at a time. In other words, the human gives their feedback, waits for a query to be synthesized, and then gives another data point of feedback. There is no room for parallelization and so the authors proposed a second algorithm in a separate paper that allows for the batching of queries. Simply put, we change the mathematical expression to the following:</p>
<p><span id="eq-eq3.44"><span class="math display">\[max_{\xi_{ib+1_A}, \xi_{ib+1_B}, ... , \xi_{ib+b_A}, \xi_{ib+b_B} H(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)} \tag{3.45}\]</span></span></p>
<p>Naively, we could consider optimizing this in the greedy fashion. This would mean just synthetically generating <span class="math inline">\(b\)</span> independent queries. The obvious drawback of this method would be that the queries would likely be very similar to each other. The authors propose a few other heuristics that would help guide the algorithm away from generating very similar queries. As an example, the authors propose Medioid Selection where we have to cluster <span class="math inline">\(B\)</span> greedy vectors into <span class="math inline">\(b &lt; B\)</span> groups and pick one vector from each group (the medioid). The authors also propose two other methods rooted in providing different queries: boundary medioids selection and successive elimination. They are best visually depicted as:</p>
<div id="fig-selection-strategy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-selection-strategy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/greedy.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-selection-strategy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.12: Different selection strategies
</figcaption>
</figure>
</div>
</section>
<section id="results" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="results">Results</h4>
<p>The authors test both the non-batched and variety of batched learning algorithms on multiple environments:</p>
<div id="fig-batch-nonbatch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-batch-nonbatch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/activeresults.png" class="img-fluid figure-img" style="width:75.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-batch-nonbatch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.13: Comparison between batched and non-batched algorithms
</figcaption>
</figure>
</div>
<p>What is interesting to note is that when graphed over <span class="math inline">\(N\)</span> the non-batched active learning approach does in the same ball-park of performance as the batched approaches. However, if you graph it over time, we see that learning is a much slower process when not-batched.</p>
</section>
</section>
<section id="application-foundation-models-for-robotics" class="level3" data-number="3.1.8">
<h3 data-number="3.1.8" class="anchored" data-anchor-id="application-foundation-models-for-robotics"><span class="header-section-number">3.1.8</span> Application: Foundation Models for Robotics</h3>
<p>Modern foundation models have been ubiquitous in discussions of powerful, general purpose AI systems that can accomplish myriad tasks across many disciplines such as programming, medicine, law, open question-answering and much more, with rapidly increasing capabilities <span class="citation" data-cites="bommasani2022opportunities">(<a href="#ref-bommasani2022opportunities" role="doc-biblioref">Bommasani et al. 2022</a>)</span>. However, despite successes from large labs in controlled environments <span class="citation" data-cites="brohan2023rt2">(<a href="#ref-brohan2023rt2" role="doc-biblioref">Brohan et al. 2023</a>)</span> foundation models have not seen ubiquitous use in robotics due to shifting robot morphology, lack of data, and the sim to real gap in robotics <span class="citation" data-cites="walke2023bridgedata">(<a href="#ref-walke2023bridgedata" role="doc-biblioref">Walke et al. 2023</a>)</span>. For this subsection we explore two promising approaches known as R3M and Voltron which are the first to leverage pre-training on vast amounts of data towards performance improvement on downstream robotic tasks despite the aforementioned issues <span class="citation" data-cites="nair2022r3m karamcheti2023languagedriven">(<a href="#ref-nair2022r3m" role="doc-biblioref">Nair et al. 2022</a>; <a href="#ref-karamcheti2023languagedriven" role="doc-biblioref">Karamcheti et al. 2023</a>)</span>.</p>
<section id="r3m-universal-visual-representation-for-robotics" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="r3m-universal-visual-representation-for-robotics">R3M: Universal Visual Representation for Robotics</h4>
<div id="fig-r3m-pipline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-r3m-pipline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/r3m.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-r3m-pipline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.14: R3M pipeline
</figcaption>
</figure>
</div>
<p>R3M represents a significant advancement in the field of robotic manipulation and learning. This model diverges from traditional approaches that rely on training from scratch within the same domain on the same robot data as instead it leverags pretraining on large datasets, akin to the practices in computer vision and natural language processing (NLP) where models are trained on diverse, large-scale datasets to create reusable, general-purpose representations.</p>
<p>The core principle behind R3M is its training methodology. It is pre-trained on a wide array of human videos, encompassing various activities and interactions. This diverse dataset enables the model to capture a broad spectrum of physical interactions and dynamics, which are crucial for effective robotic manipulation known as EGO4D <span class="citation" data-cites="grauman2022ego4d">(<a href="#ref-grauman2022ego4d" role="doc-biblioref">Grauman et al. 2022</a>)</span>. However, prior papers could not fit this dataset well, and R3M leveraged. The training utilizes a unique objective that combines time contrastive learning, video-language alignment, and a sparsity penalty. This objective ensures that R3M not only understands the temporal dynamics of scenes (i.e., how states transition over time) but also focuses on semantically relevant features, such as objects and their interrelations, while maintaining a compact and efficient representation.</p>
<p>What sets R3M apart in the realm of robotics is its efficiency and effectiveness in learning from a limited amount of data. The model demonstrates remarkable performance in learning tasks in the real world with minimal human supervision – typically less than 10 minutes. This is a stark contrast to traditional models that require extensive and often prohibitively large datasets for training. Furthermore, R3M’s pre-trained nature allows for its application across a variety of tasks and environments without the need for retraining from scratch, making it a versatile tool in robotic manipulation. The empirical results from using R3M are compelling, leading to a 10% improvement over training from a pretrained image-net model, self-supervised approaches such as MoCo or even CLIP <span class="citation" data-cites="deng2009imagenet he2020momentum radford2021learning">(<a href="#ref-deng2009imagenet" role="doc-biblioref">Deng et al. 2009</a>; <a href="#ref-he2020momentum" role="doc-biblioref">He et al. 2020</a>; <a href="#ref-radford2021learning" role="doc-biblioref">Radford et al. 2021</a>)</span>. Note however, that R3m does <strong>not</strong> use any language data which leaves quite a bit of supervision to be desired.</p>
</section>
<section id="voltron-language-driven-representation-learning-for-robotics" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="voltron-language-driven-representation-learning-for-robotics">Voltron: Language Driven Representation Learning for Robotics</h4>
<p>Building off the success of R3M, Voltron proposes a further extension of leveraging self-supervision and advancements in foundation models, and multi-modality. Voltron takes on an intuitive and simple dual use objective, where the trained model alternates between predicting the task in an image through natural language and classifying images based on a natural text label. This forces a nuanced understanding of both modalities <span class="citation" data-cites="radford2021learning">(<a href="#ref-radford2021learning" role="doc-biblioref">Radford et al. 2021</a>)</span>.</p>
<p>Voltron’s approach is distinguished by its versatility and depth of learning. It is adept at handling a wide range of robotic tasks, from low-level spatial feature recognition to high-level semantic understanding required in language-conditioned imitation and intent scoring. This flexibility makes it suitable for various applications in robotic manipulation, from grasping objects based on descriptive language to performing complex sequences of actions in response to verbal instructions.</p>
<div id="fig-voltron-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-voltron-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/voltron.png" class="img-fluid figure-img" style="width:95.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-voltron-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.15: Voltron pipeline
</figcaption>
</figure>
</div>
<p>The authors rigorously test Voltron in scenarios such as dense segmentation for grasp affordance prediction, object detection in cluttered scenes, and learning multi-task language-conditioned policies for real-world manipulation with up to 15% improvement over baselines. In each of these domains, Voltron has shown a remarkable ability to outperform existing models like MVP and R3M, showcasing its superior adaptability and learning capabilities <span class="citation" data-cites="xiao2022masked">(<a href="#ref-xiao2022masked" role="doc-biblioref">Xiao et al. 2022</a>)</span>.</p>
<p>Moreover, Voltron’s framework allows for a balance between encoding low-level and high-level features, which is critical in the context of robotics. This balance enables the model to excel in both control tasks and those requiring deeper semantic understanding, offering a comprehensive solution in the realm of robotic vision and manipulation.</p>
<p>Voltron stands as a groundbreaking approach in the field of robotics, offering a language-driven, versatile, and efficient approach to learning and manipulation. Its ability to seamlessly integrate visual and linguistic data makes it a potent tool in the ever-evolving landscape of robotic technology, with potential applications that extend far beyond current capabilities. Interesting the authors show Voltron does not beat R3M off the shelf but only when trained on similar amounts of data. Nevertheless, Voltron’s success in diverse tasks and environments heralds a new era in robotic manipulation, where language and vision coalesce to create more intelligent, adaptable, and capable robotic systems.</p>
</section>
</section>
<section id="conclusion" class="level3" data-number="3.1.9">
<h3 data-number="3.1.9" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">3.1.9</span> Conclusion</h3>
<p>On the note of applying active learning to RL and environment settings, there have been many recent papers that have attempted to extend this to more modern RL environments. For example, the paper "When to Ask for Help" <span class="citation" data-cites="ask_help">(<a href="#ref-ask_help" role="doc-biblioref">Xie et al. 2022</a>)</span> examines the intersection of autonomous and active learning. Instead of just expecting an RL agent to autonomously solve a task, making the assumption that an agent could get stuck and need human input to get "unstuck" is a key insight of the paper. In general, there has been an emphasis in recent literature in robotics on not just blindly using demonstration data as a form of human input, but rather actively querying a human and using this to better synthesize correct actions.</p>
<p>Active learning holds promise for enhancing AI models in real-world scenarios, yet several challenges persist. This discussion aims to provide an overview of these challenges.</p>
<p><strong>Task-Specific Considerations:</strong></p>
<p>For certain tasks, the input space of a model may have some rare yet extremely important pockets which may never be discovered by active learning and may cause severe blindspots in the model. In medical imaging for instance, there can be rare yet critical diseases. Designing AL strategies for medical image analysis must prioritize rare classes, such as various forms of cancers. Oftentimes, collecting data around those rare classes is not a recommendation of the active learning process because these examples constitute heavy distribution drifts from the input distribution a model has seen.</p>
<p><strong>Complex Task Adaptation:</strong></p>
<p>AL has predominantly been adopted for simple classification tasks, leaving more other types of tasks (generative ones for instance), less explored. In Natural Language Processing, tasks like natural language inference, question-answering pose additional complexities that affect the direct application of the active learning process. While machine translation has seen AL applications, generation tasks in NLP require more thorough exploration. Challenges arise in obtaining unlabeled data, particularly for tasks with intricate inputs.</p>
<p><strong>Unsupervised and Semi-Supervised Approaches:</strong></p>
<p>In the presence of large datasets without sufficient labels, unsupervised and semi-supervised approaches become crucial. These methods offer a means to extract information without relying on labeled data for every data point, potentially revolutionizing fields like medical image analysis. There is an ongoing need for methods that combine self/semi-supervised learning with active learning.</p>
<p><strong>Algorithm Scalability:</strong></p>
<p>Scalability is a critical concern for online AL algorithms, particularly when dealing with large datasets and high-velocity data streams. The computational demands of AL can become prohibitive as data volume increases, posing challenges for practical deployment. Issues of catastrophic forgetting and model plasticity further complicate scalability, requiring careful consideration in algorithm design.</p>
<p><strong>Labeling Quality Assurance:</strong></p>
<p>The effectiveness of most online AL strategies hinges on the quality of labeled data. Ensuring labeling accuracy in real-world scenarios is challenging, with human annotators prone to errors, biases, and diverse interpretations. Addressing imperfections in labeling through considerations of oracle imperfections becomes essential in real-life AL applications. Solutions for cleaning up data and verifying its quality need to be more aggressively pursued.</p>
<p><strong>Data Drift Challenges:</strong></p>
<p>Real-world settings introduce data drift, where distributions shift over time, challenging models to adapt for accurate predictions. These shifts can impact the quality of labeled data acquired in the AL process. For example, the criterion or proxy used for selecting informative instances may be thrown off when the distribution a model is trained on, and the distribution we want it to perform well on, are too far away from one another.</p>
<p><strong>Evaluation in Real-Life Scenarios</strong>:</p>
<p>While AL methods are often evaluated assuming access to ground-truth labels, the real motivation for AL lies in label scarcity. Assessing the effectiveness of AL strategies becomes challenging in real-life scenarios where ground-truth labels may be limited. In other words, one may verify the goodness of an AL algorithm within the lab, but once the algorithm is deployed for improving all sorts of models on all sorts of data distributions, verifying whether AL is actually improving a model is tricky, especially when collecting and labeling data from the target distribution is expensive and defeats the purpose of using AL in the first place.</p>
<p>By systematically addressing these challenges, the field of active learning in AI can progress towards more effective and practical applications.</p>
<p>In summary, active learning is a promising modern tool to model training that presents potential benefits. As was mentioned at the start, there are numerous approaches that can be employed by active learning, starting from reducing error of model’s prediction, reducing variance, to more conformal predictions. The flavor of active learning heavily depends on the applications, which include robotics, LLM, autonomous vehicles, and more. We discussed in more detail how to perform active learning for variance reduction in the case of predicting kinematics of the robotic arms, which showed decrease in MSE as well as more stable reduction in it. Next we talked about using active learning for reducing the number of comparisons required to create a ranking of objects, and the examples discussed were able to achieve that but with some loss in the prediction accuracy. Finally, we discussed how active learning can be used for modeling of reward functions within a dynamical system, which demonstrated improvements in performance and time required to achieve it. For a more hands-on experience with active learning and demonstrated example, we encourage the readers to explore a blogpost by Max Halford <span class="citation" data-cites="max_halford">(<a href="#ref-max_halford" role="doc-biblioref">Halford 2023</a>)</span>.</p>
</section>
</section>
<section id="sec-metric-elicitation" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-metric-elicitation"><span class="header-section-number">3.2</span> Metric Elicitation</h2>
<section id="introduction-to-performance-metric-elicitation" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="introduction-to-performance-metric-elicitation"><span class="header-section-number">3.2.1</span> Introduction to Performance Metric Elicitation</h3>
<p>In binary classification problems, selecting an appropriate performance metric that aligns with the real-world task is crucial. The problem of <em>metric elicitation</em> aims to characterize and discover the performance metric of a practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive).</p>
<p>Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation considers the process of devising a metric that best matches the preferences of practitioners or users. This is achieved by querying an “oracle” who provides feedback on proposed potential metrics through pairwise comparisons. Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.</p>
<p><strong>Note:</strong> Contents in this section are derived from “Performance Metric Elicitation from Pairwise Classifier Comparisons” by <span class="citation" data-cites="pmlr-v89-hiranandani19a">(<a href="#ref-pmlr-v89-hiranandani19a" role="doc-biblioref">Hiranandani et al. 2019a</a>)</span>, which introduced the problem of metric elicitation and the framework for binary-class metric elicitation from pairwise comparisons. This section aims to present their work expository while providing additional motivation and intuitive explanations to supplement their work.</p>
<p>The motivation for the pairwise comparison aspect of metric elicitation stems from a rich history of literature in psychology, economics, and computer science <span class="citation" data-cites="pref1 pref2 pref3 pref4 ab">(<a href="#ref-pref1" role="doc-biblioref">Samuelson 1938</a>; <a href="#ref-pref2" role="doc-biblioref">Mas-Colell 1977</a>; <a href="#ref-pref3" role="doc-biblioref">Varian 2006</a>; <a href="#ref-pref4" role="doc-biblioref">Braziunas and Boutilier 2012</a>; <a href="#ref-ab" role="doc-biblioref">Tamburrelli and Margara 2014</a>)</span>, demonstrating that humans are often ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately capture binary metrics such as accuracy, <span class="math inline">\(F_\beta\)</span>, and Jaccard similarity by recording the number of false positives, true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to introduce two binary-search procedures that can approximate the oracle’s performance metric for two types of metrics (linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary for a binary classification problem.</p>
<p>First, we introduce some relevant notation that will later be used to formalize notions of oracle queries, classifiers, and metrics. In this context, <span class="math inline">\(X \in \mathcal{X}\)</span> represents an input random variable, while <span class="math inline">\(Y \in \{0, 1\}\)</span> denotes the output random variable. We learn from a dataset of size <span class="math inline">\(n\)</span>, denoted by <span class="math inline">\(\{(x, y)_i\}^n_{i=1}\)</span>, which is generated independently and identically distributed (i.i.d.) from some distribution <span class="math inline">\(\mathbb{P}(X, Y)\)</span>. The conditional probability of the positive class, given some sample <span class="math inline">\(x\)</span>, is denoted by <span class="math inline">\(\eta(\vec{x}) = \mathbb{P}(Y=1 | X=x)\)</span>. The marginal probability of the positive class is represented by <span class="math inline">\(\zeta = \mathbb{P}(Y=1)\)</span>.</p>
<p>The set of all potential classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \{0,1\}\}\)</span>. The confusion matrix for a classifier <span class="math inline">\(h\)</span> is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{2 \times 2}\)</span>, where <span class="math inline">\(C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j)\)</span> for <span class="math inline">\(i, j \in \{0,1\}\)</span>. These entries represent the false positives, true positives, false negatives, and true negatives, ensuring that <span class="math inline">\(\sum_{i,j}C_{ij}=1\)</span>. The set of all confusion matrices is denoted by <span class="math inline">\(\mathcal{C}\)</span>. Since <span class="math inline">\(FN(h, \mathbb{P}) = \zeta - TP(h, \mathbb{P})\)</span> and <span class="math inline">\(FP(h, \mathbb{P}) = 1 - \zeta - TN(h, \mathbb{P})\)</span>, <span class="math inline">\(\mathcal{C}\)</span> is actually a 2-dimensional space, not a 4-dimensional space.</p>
<p>Any hyperplane in the <span class="math inline">\((tp, tn)\)</span> space is given by <span class="math inline">\(\ell := a \cdot tp + b \cdot tn = c\)</span>, where <span class="math inline">\(a, b, c \in \mathbb{R}\)</span>. Given a classifier <span class="math inline">\(h\)</span>, we define a performance metric <span class="math inline">\(\phi : [0, 1]^{2 \times 2} \rightarrow \mathbb{R}\)</span>. The value <span class="math inline">\(\phi(C(h))\)</span>, which represents the performance of a classifier with respect to a certain metric, is referred to as the <em>utility</em> of the classifier <span class="math inline">\(h\)</span>. We assume, without loss of generality, that a higher value of <span class="math inline">\(\phi\)</span> indicates a better performance metric for <span class="math inline">\(h\)</span>. Our focus is to recover some metric <span class="math inline">\(\phi\)</span> using comparisons between confusion matrices <span class="math inline">\(C(h)\)</span>, determined by classifiers <span class="math inline">\(h\)</span>, which approximates the oracle’s “ground-truth” metric <span class="math inline">\(\phi^*\)</span>.</p>
<p>Next, we introduce two classes of performance metrics—<em>Linear Performance Metrics (LPM)</em> and <em>Linear-Fractional Performance Metrics (LFPM)</em>—for which we will present two elicitation algorithms.</p>
<p>An <em>LPM</em>, given constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}\} \in \mathbb{R}^{4}\)</span>, is defined as:</p>
<p><span id="eq-eqlpm"><span class="math display">\[\begin{aligned}
\phi(C) &amp;= a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN\\
&amp;= m_{11} TP + m_{00} TN + m_{0},
\end{aligned} \tag{3.46}\]</span></span></p>
<p>where <span class="math inline">\(m_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(m_{00} = (a_{00} - a_{01})\)</span>, and <span class="math inline">\(m_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>. This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. One example of an LPM is <em>weighted accuracy</em>, defined as <span class="math inline">\(WA = w_1TP + w_2TN\)</span>, where adjusting <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> controls the relative importance of different types of misclassification.</p>
<p>An <em>LFPM</em>, defined by constants <span class="math inline">\(\{a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}\} \in \mathbb{R}^{8}\)</span>, is given by:</p>
<p><span id="eq-eqlfpm"><span class="math display">\[\begin{aligned}
\phi(C) &amp;= \frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN}\\
&amp;= \frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},
\end{aligned} \tag{3.47}\]</span></span></p>
<p>where <span class="math inline">\(p_{11} = (a_{11} - a_{10})\)</span>, <span class="math inline">\(p_{00} = (a_{00} - a_{01})\)</span>, <span class="math inline">\(q_{11} = (b_{11} - b_{10})\)</span>, <span class="math inline">\(q_{00} = (b_{00} - b_{01})\)</span>, <span class="math inline">\(p_{0} = a_{10} \zeta + a_{01} (1 - \zeta)\)</span>, and <span class="math inline">\(q_{0} = b_{10} \zeta + b_{01} (1 - \zeta)\)</span>. This parametrization also simplifies the elicitation process by reducing the number of variables. Common LFPMs include the <span class="math inline">\(F_\beta\)</span> score and Jaccard similarity, defined as:</p>
<p><span id="eq-lfpm_metrics"><span class="math display">\[F_{\beta} = \frac{TP}{\frac{TP}{1+\beta^{2}} - \frac{TN}{1+\beta^{2}} + \frac{\beta^{2} \zeta + 1 - \zeta}{1+\beta^{2}}}, \quad JAC = \frac{TP}{1 - TN}. \tag{3.48}\]</span></span></p>
<p>Setting <span class="math inline">\(\beta = 1\)</span> gives the F1 score, which is widely used as a classification metric in machine learning.</p>
</section>
<section id="sec-me-preliminaries" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-me-preliminaries"><span class="header-section-number">3.2.2</span> Preliminaries</h3>
<section id="sec-confusion-matrices" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sec-confusion-matrices">Confusion Matrices</h4>
<p>Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions about <span class="math inline">\(\mathcal{C}\)</span>. Particularly, we will assume that <span class="math inline">\(g(t) = \mathbb{P}[\eta(X) \geq t]\)</span> is continuous and strictly decreasing for <span class="math inline">\(t \in [0, 1]\)</span>; essentially, <span class="math inline">\(\eta\)</span> has positive density and zero probability.</p>
<p>Additionally, <span class="math inline">\(\mathcal{C}\)</span> is convex, closed, and contained within the rectangle <span class="math inline">\([0, \zeta] \times [0, 1-\zeta]\)</span>, and is rotationally symmetric around its center, <span class="math inline">\((\frac{\zeta}{2}, \frac{1-\zeta}{2})\)</span>, where the axes represent the proportion of true positives and negatives. The only vertices of <span class="math inline">\(\mathcal{C}\)</span> are <span class="math inline">\((0, 1-\zeta)\)</span> and <span class="math inline">\((\zeta, 0)\)</span>, corresponding to predicting all <span class="math inline">\(0\)</span>’s or all <span class="math inline">\(1\)</span>’s on a given dataset. Therefore, <span class="math inline">\(\mathcal{C}\)</span> is strictly convex, and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix; these properties can be visually observed in <a href="#fig-c" class="quarto-xref">Figure&nbsp;<span>3.16</span></a>.</p>
<div id="fig-c" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Screenshot 2023-11-13 at 6.56.44 PM.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-c-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.16: Visual representation of <span class="math inline">\(\mathcal{C}\)</span>
</figcaption>
</figure>
</div>
<p>Next, recall that an LPM is represented in terms of three parameters (<span class="math inline">\(\phi = m_{11}TP + m_{00}TN + m_0\)</span>). We have just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of <span class="math inline">\(\mathcal{C}\)</span>. We first note that this point is independent of <span class="math inline">\(m_0\)</span>. Additionally, we only care about the relative weightings of <span class="math inline">\(m_{11}\)</span> and <span class="math inline">\(m_{00}\)</span>, not their actual values—they are scale invariant. Therefore, we can parametrize the space of LPMs as <span class="math inline">\(\varphi_{LPM} = \{\mathbf{m} = (\cos \theta, \sin \theta) : \theta \in [0, 2\pi]\}\)</span>, where <span class="math inline">\(\cos \theta\)</span> corresponds to <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(\sin \theta\)</span> corresponds to <span class="math inline">\(m_{11}\)</span>. As we already know, we can recover the Bayes classifier given <span class="math inline">\(\mathbf{m}\)</span>, and it is unique, corresponding to one point on the boundary of <span class="math inline">\(\mathcal{C}\)</span> due to its convexity. The supporting hyperplane at this point is defined as</p>
<p><span id="eq-eq3.47"><span class="math display">\[\bar{\ell}_{\mathbf{m}} := m_{11} \cdot tp + m_{00} \cdot tn = m_{11} \overline{TP}_{\mathbf{m}} + m_{00} \overline{TN}_{\mathbf{m}} \tag{3.49}\]</span></span></p>
<p>We note that if <span class="math inline">\(m_{00}\)</span> and <span class="math inline">\(m_{11}\)</span> have opposite signs, then <span class="math inline">\(\bar{h}_m\)</span> is the trivial classifier predicting all 1’s or all 0’s, since either predicting true positives or true negatives results in negative reward. This corresponds to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices.</p>
<p>Additionally, the boundary <span class="math inline">\(\partial \mathcal{C}\)</span> can be split into upper and lower boundaries (<span class="math inline">\(\partial \mathcal{C}_{+}, \partial \mathcal{C}_{-}\)</span>), corresponding to <span class="math inline">\(\theta \in (0, \pi/2)\)</span> and <span class="math inline">\(\theta \in (\pi, 3\pi/2)\)</span> respectively (and whether <span class="math inline">\(m_{00}, m_{11}\)</span> are positive or negative).</p>
</section>
<section id="bayes-optimal-and-inverse-optimal-classifiers" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="bayes-optimal-and-inverse-optimal-classifiers">Bayes Optimal and Inverse-Optimal Classifiers</h4>
<p>We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric <span class="math inline">\(\phi\)</span>, we define:</p>
<ul>
<li>The <em>Bayes utility</em> as <span class="math inline">\(\bar{\tau} := \sup_{h \in \mathcal{H}} \phi(C(h)) = \sup_{C \in \mathcal{C}} \phi(C)\)</span>; this is the highest achievable utility (using the metric <span class="math inline">\(\phi\)</span>) over all classifiers $h <span class="math inline">\(\mathcal{H}\)</span> for a given problem.</li>
<li>The <em>Bayes classifier</em> as <span class="math inline">\(\bar{h} := \arg \max_{h \in \mathcal{H}} \phi(C(h))\)</span>; this is the classifier <span class="math inline">\(h\)</span> corresponding to the Bayes utility.</li>
<li>The <em>Bayes confusion matrix</em> as <span class="math inline">\(\bar{C} := \arg \max_{C \in \mathcal{C}} \phi(C)\)</span>; this is the confusion matrix corresponding to the Bayes utility and classifier.</li>
</ul>
<p>Similarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing “<span class="math inline">\(\sup\)</span>” with “<span class="math inline">\(\inf\)</span>”; they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem.</p>
<p>We also have the following useful proposition:</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="prp-prp3.1" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.1</strong></span> Let <span class="math inline">\(\phi \in \varphi_{LPM}\)</span>. Then</p>
<p><span id="eq-eq3.45"><span class="math display">\[\bar{h}(x) = \left\{\begin{array}{lr}
\unicode{x1D7D9} \left[\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right], &amp; m_{11} + m_{00} \geq 0 \\
\unicode{x1D7D9} \left[\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right], &amp; \text { o.w. }
\end{array}\right\} \tag{3.50}\]</span></span></p>
<p>is a Bayes optimal classifier with respect to <span class="math inline">\(\phi\)</span>. The inverse Bayes classifier is given by <span class="math inline">\(\underline{h} = 1 - \bar{h}\)</span>.</p>
</div>
</div>
</div>
</div>
<p>This is a simple derivation based on the fact that we only get rewards from true positives and true negatives. Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.</p>
</section>
</section>
<section id="sec-metric-elicitation-setup" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-metric-elicitation-setup"><span class="header-section-number">3.2.3</span> Problem Setup</h3>
<p>We will now formalize the problem of metric elicitation. Given two classifiers <span class="math inline">\(h\)</span> and <span class="math inline">\(h'\)</span> (or equivalently, two confusion matrices <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span>), we define an <em>oracle query</em> as the function:</p>
<p><span id="eq-oracle"><span class="math display">\[\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\unicode{x1D7D9}\left[\phi(C)&gt;\phi\left(C^{\prime}\right)\right]=: \unicode{x1D7D9} \left[C \succ C^{\prime}\right], \tag{3.51}\]</span></span></p>
<p>which represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:</p>
<div class="callout callout-style-default callout-note callout-titled" title="definition">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
definition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="def-def3.1" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1</strong></span> Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The goal is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(C, C^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi - \hat{\phi}\|_{--} &lt; \kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
</div>
</div>
</div>
<p>In practice, we do not have access to the true probability distribution or the population, which would provide the true values of <span class="math inline">\(C\)</span> and <span class="math inline">\(C'\)</span>. However, we can subtly alter this problem description to use <span class="math inline">\(\hat{C}\)</span> and <span class="math inline">\(\hat{C}^{\prime}\)</span>, which are derived from our dataset of <span class="math inline">\(n\)</span> samples:</p>
<div class="callout callout-style-default callout-note callout-titled" title="definition">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
definition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="def-def3.2" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2</strong></span> Suppose the true (oracle) performance metric is <span class="math inline">\(\phi\)</span>. The aim is to recover a metric <span class="math inline">\(\hat{\phi}\)</span> by querying the oracle for as few pairwise comparisons of the form <span class="math inline">\(\Omega\left(\hat{C}, \hat{C}^{\prime}\right)\)</span> so that <span class="math inline">\(\|\phi - \hat{\phi}\|_{--} &lt; \kappa\)</span> for a sufficiently small <span class="math inline">\(\kappa &gt; 0\)</span> and for any suitable norm <span class="math inline">\(\|\cdot\|_{--}\)</span>.</p>
</div>
</div>
</div>
</div>
<p>As is common in theoretical ML research, we solve the population problem and then consider ways to extend this to practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the confusion matrices from a portion of the dataset we have access to.</p>
</section>
<section id="sec-orgb6dac4e" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="sec-orgb6dac4e"><span class="header-section-number">3.2.4</span> Linear Performance Metric Elicitation</h3>
<p>For LPM elicitation, we need one more proposition.</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="prp-prp3.2" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.2</strong></span> For a metric <span class="math inline">\(\psi\)</span> (quasiconvex and monotone increasing in TP/TN) or <span class="math inline">\(\phi\)</span> (quasiconcave and monotone increasing), and parametrization <span class="math inline">\(\rho^+\)</span>/<span class="math inline">\(\rho^-\)</span> of upper/lower boundary, composition <span class="math inline">\(\psi \circ \rho^-\)</span> is quasiconvex and unimodal on [0, 1], and <span class="math inline">\(\phi \circ \rho^+\)</span> is quasiconcave and unimodal on [0, 1].</p>
</div>
</div>
</div>
</div>
<p>Quasiconcavity and quasiconvexity are slightly more general variations on concavity and convexity. Their main useful property in our setting is that they are unimodal (they have a singular extremum), so we can devise a binary-search-style algorithm for eliciting the Bayes optimal and inverse-optimal confusion matrices for a given setting, as well as the corresponding <span class="math inline">\(\phi\)</span>’s.</p>
<p>We first note that to maximize a quasiconcave metric, in which <span class="math inline">\(\phi\)</span> is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, we note that the resulting maximizer (and supporting hyperplane) will occur on the upper boundary of <span class="math inline">\(\mathcal{C}\)</span>. We thus set our initial search range to be <span class="math inline">\([0, \pi/2]\)</span> and repeatedly divide it into four regions. Then, we calculate the resulting confusion matrix on the 5 resulting boundaries of these regions and query the oracle <span class="math inline">\(4\)</span> times. We repeat this in each iteration of the binary search until a maximizer is found.</p>
<div class="callout callout-style-default callout-note callout-titled" title="remark">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
remark
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="rem-explaination_binary_search" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em>. </span>In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical binary search must be used. To illustrate this, consider the two distributions in <a href="#fig-bsearch" class="quarto-xref">Figure&nbsp;<span>3.17</span></a>:</p>
<div id="fig-bsearch" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bsearch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bsearch" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-normal-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-normal-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/normaldistribution.png" class="img-fluid figure-img" style="width:100.0%" data-ref-parent="fig-bsearch">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-normal-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) First distribution
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-bsearch" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-normal-distribution-copy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-normal-distribution-copy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/normaldistribution copy.png" class="img-fluid figure-img" style="width:94.0%" data-ref-parent="fig-bsearch">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-normal-distribution-copy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Second distribution
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-bsearch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.17
</figcaption>
</figure>
</div>
<p>For both the symmetric and skewed distributions, if we were to divide the search range into two portions and compare <span class="math inline">\(A\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(E\)</span>, we would find that <span class="math inline">\(C &gt; A\)</span> and <span class="math inline">\(C &gt; E\)</span>. In both cases, this does not help us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), or at <span class="math inline">\(C\)</span> itself (as in the first case). Therefore, we must make comparisons between all five points <span class="math inline">\(A, B, C, D, and E\)</span>. This allows us to correctly restrict our search range to <span class="math inline">\([B, D]\)</span> in the first case and <span class="math inline">\([C, E]\)</span> in the second. These extra search requirements are due to the quasiconcavity of the search space we are considering, in which there exists a maximum but we need to make several comparisons at various points throughout the search space to be able to reduce its size in each iteration.</p>
</div>
</div>
</div>
</div>
<div id="alg-lpm" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="3">
<div class="pseudocode">
\begin{algorithm} \caption{Quasiconcave Metric Maximization} \begin{algorithmic} \State \textbf{input:} $\epsilon &gt; 0$ and oracle $\Omega$ \State \textbf{initialize:} $\theta_a = 0, \theta_b = \frac{\pi}{2}$ \While{$|\theta_b - \theta_a| &gt; \epsilon$} \State set $\theta_c = \frac{3\theta_a+\theta_b}{4}$, $\theta_d = \frac{\theta_a+\theta_b}{2}$, and $\theta_e = \frac{\theta_a+3\theta_b}{4}$ \State obtain $h\theta_a, h\theta_c, h\theta_d, h\theta_e, h\theta_b$ using Proposition 1 \State Compute $C\theta_a, C\theta_c, C\theta_d, C\theta_e, C\theta_b$ using (1) \State Query $\Omega(C\theta_c, C\theta_a), \Omega(C\theta_d, C\theta_c), \Omega(C\theta_e, C\theta_d)$, and $\Omega(C\theta_b, C\theta_e)$ \If{$q_{i,j}$ is ambiguous} \State request $q_{i,j}$'s label from reference \Else \State impute $q_{i,j}$'s label from previously labeled queries \EndIf \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$} \State assume the default order $C\theta \prec C\theta' \prec C\theta''$ \EndIf \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$} \State assume the default order $C\theta \prec C\theta' \prec C\theta''$ \EndIf \If{$C\theta_a \succ C\theta_c$} \State Set $\theta_b = \theta_d$ \ElsIf{$C\theta_a \prec C\theta_c \succ C\theta_d$} \State Set $\theta_b = \theta_d$ \ElsIf{$C\theta_c \prec C\theta_d \succ C\theta_e$} \State Set $\theta_a = \theta_c$ \State Set $\theta_b = \theta_e$ \ElsIf{$C\theta_d \prec C\theta_e \succ C\theta_b$} \State Set $\theta_a = \theta_d$ \Else \State Set $\theta_a = \theta_d$ \EndIf \EndWhile \State \textbf{output:} $\vec{m}, C$, and $\vec{l}$, where $\vec{m} = m_l(\theta_d), C = C\theta_d$, and $\vec{l} := (\vec{m}, (tp, tn)) = (\vec{m}, C)$ \end{algorithmic} \end{algorithm}
</div>
</div>
<p>To elicit LPMs, we run <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a>, querying the oracle in each iteration, and set the elicited metric <span class="math inline">\(\hat{m}\)</span> (which is the maximizer on <span class="math inline">\(\mathcal{C}\)</span>) to be the slope of the resulting hyperplane, since the metric is linear.</p>
<div class="callout callout-style-default callout-note callout-titled" title="remark">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
remark
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="rem-explaination_lpm" class="proof remark">
<p><span class="proof-title"><em>Remark 3.2</em>. </span>To find the minimum of a quasiconvex metric, we flip all instances of <span class="math inline">\(\prec\)</span> and <span class="math inline">\(\succ\)</span>, and use an initial search range of <span class="math inline">\([\pi, 3\pi/2]\)</span>; we use this algorithm, which we refer to as <a href="#alg-lfpm" class="quarto-xref">Algorithm 4</a>, in our elicitation of LFPMs.</p>
</div>
</div>
</div>
</div>
<p>Next, we provide a Python implementation of <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="4d20b435" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> get_m(theta):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">    Inputs: </span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">    - theta: the value that parametrizes m</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">    Outputs:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">    - m_0 and m_1 for the LPM</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">    """</span></span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="cf">return</span> (math.cos(theta), math.sin(theta))</span>
<span id="cb1-10"><a href="#cb1-10"></a></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="kw">def</span> lpm_elicitation(epsilon, oracle):</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co">"""</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">    Inputs:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="co">    Outputs:</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="co">    - estimate for m, which is used to compute the LPM as described above</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="co">    """</span></span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a>    a <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-22"><a href="#cb1-22"></a>    b <span class="op">=</span> math.pi<span class="op">/</span><span class="dv">2</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>    <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb1-24"><a href="#cb1-24"></a>        c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>        d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>        e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a>        m_a, m_b, m_c, m_d, m_e <span class="op">=</span> (get_m(x) <span class="cf">for</span> x <span class="kw">in</span> [a,b,c,d,e]) <span class="co"># using definition of m</span></span>
<span id="cb1-29"><a href="#cb1-29"></a>        c_a, c_b, c_c, c_d, c_e <span class="op">=</span> (get_c(x) <span class="cf">for</span> x <span class="kw">in</span> [m_a, m_b, m_c, m_d, m_e]) <span class="co"># compute classifier from m's then calculate confusion matrices</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>        </span>
<span id="cb1-31"><a href="#cb1-31"></a>        response_ac <span class="op">=</span> oracle(c_a, c_c)</span>
<span id="cb1-32"><a href="#cb1-32"></a>        response_cd <span class="op">=</span> oracle(c_c, c_d)</span>
<span id="cb1-33"><a href="#cb1-33"></a>        response_de <span class="op">=</span> oracle(c_d, c_e)</span>
<span id="cb1-34"><a href="#cb1-34"></a>        response_eb <span class="op">=</span> oracle(c_e, c_b)</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>        <span class="co"># update ranges to keep the peak</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        <span class="cf">if</span> response_ac:</span>
<span id="cb1-38"><a href="#cb1-38"></a>            b <span class="op">=</span> d</span>
<span id="cb1-39"><a href="#cb1-39"></a>        <span class="cf">elif</span> response_cd:</span>
<span id="cb1-40"><a href="#cb1-40"></a>            b <span class="op">=</span> d</span>
<span id="cb1-41"><a href="#cb1-41"></a>        <span class="cf">elif</span> response_de:</span>
<span id="cb1-42"><a href="#cb1-42"></a>            a <span class="op">=</span> c</span>
<span id="cb1-43"><a href="#cb1-43"></a>            b <span class="op">=</span> e</span>
<span id="cb1-44"><a href="#cb1-44"></a>        <span class="cf">elif</span> response_eb:</span>
<span id="cb1-45"><a href="#cb1-45"></a>            a <span class="op">=</span> d</span>
<span id="cb1-46"><a href="#cb1-46"></a>        <span class="cf">else</span>:</span>
<span id="cb1-47"><a href="#cb1-47"></a>            a <span class="op">=</span> d</span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="cf">return</span> get_m(d), get_c(d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-lfpm-elicitation" class="level3" data-number="3.2.5">
<h3 data-number="3.2.5" class="anchored" data-anchor-id="sec-lfpm-elicitation"><span class="header-section-number">3.2.5</span> Linear-Fractional Performance Metric Elicitation</h3>
<p>Now, we present the next main result, which is an algorithm to elicit linear-fractional performance metrics. For this task, we will need the following assumption:</p>
<p>Let <span class="math inline">\(\phi \in \varphi_{L F P M}\)</span>. We assume <span class="math inline">\(p_{11}, p_{00} \geq 0, p_{11} \geq q_{11}, p_{00} \geq q_{00},\)</span> <span class="math inline">\(p_{0}=0, q_{0}=\)</span> <span class="math inline">\(\left(p_{11}-q_{11}\right) \zeta+\left(p_{00}-q_{00}\right)(1-\zeta)\)</span>, and <span class="math inline">\(p_{11}+p_{00}=1\)</span>.</p>
<p>These assumptions guarantee that the LFPM <span class="math inline">\(\phi\)</span> which we are trying to elicit is monotonically increasing in <span class="math inline">\(TP\)</span> and <span class="math inline">\(TN\)</span>, just as in the LPM elicitation case.</p>
<p>We first provide motivation and an overview of the approach for LFPM elicitation and then present pseudocode for the algorithm.</p>
<p>The general idea of the algorithm is to use <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a> to obtain a maximizer and a minimizer for the given dataset; these result in two systems of equations involving the true LFPM <span class="math inline">\(\phi^*\)</span> with 1 degree of freedom. Then, we run a grid search that is independent of oracle queries to find the point where solutions to the systems match pointwise on the resulting confusion matrices; this occurs close to where the true metric lies.</p>
<p>More formally, suppose that the true metric is <span id="eq-eq3.48"><span class="math display">\[\phi^{*}(C)=\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}. \tag{3.52}\]</span></span> Then, let <span class="math inline">\(\bar{\tau}\)</span> and <span class="math inline">\(\underline{\tau}\)</span> represent the maximizer and minimizer of <span class="math inline">\(\phi\)</span> over <span class="math inline">\(\mathcal{C}\)</span>, respectively. There exists a hyperplane <span id="eq-eq3.49"><span class="math display">\[\begin{aligned}
\bar{\ell}_{f}^{*}:=\left(p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}\right) t n=\bar{\tau}^{*} q_{0}^{*},
\end{aligned} \tag{3.53}\]</span></span> which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\overline{T P}^{*}, \overline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{+}\)</span>.</p>
<p>Correspondingly, there also exists a hyperplane <span id="eq-eq3.50"><span class="math display">\[\begin{aligned}
\underline{\ell}_{f}^{*}:=\left(p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}\right) \operatorname{tn}=\underline{\tau}^{*} q_{0}^{*},
\end{aligned} \tag{3.54}\]</span></span> which touches <span class="math inline">\(\mathcal{C}\)</span> at <span class="math inline">\(\left(\underline{TP}^{*}, \underline{T N}^{*}\right)\)</span> on <span class="math inline">\(\partial \mathcal{C}_{-}\)</span>. <a href="#fig-minmax" class="quarto-xref">Figure&nbsp;<span>3.18</span></a> illustrates this visually on <span class="math inline">\(\mathcal{C}\)</span>.</p>
<div id="fig-minmax" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-minmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/Screenshot 2023-11-13 at 6.56.52 PM.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-minmax-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.18: Visual representation of the minimizer and maximizer on <span class="math inline">\(\mathcal{C}\)</span>
</figcaption>
</figure>
</div>
<p>While we are unable to obtain <a href="#eq-eq3.48" class="quarto-xref">Equation&nbsp;<span>3.52</span></a> and <a href="#eq-eq3.49" class="quarto-xref">Equation&nbsp;<span>3.53</span></a> directly, we can use <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a> to get a hyperplane <span id="eq-eq3.51"><span class="math display">\[\bar{\ell}:=\bar{m}_{11} t p+\bar{m}_{00} t n= \bar{m}_{11} \overline{T P}^{*}+\bar{m}_{00} \overline{T N}^{*} = \bar{C}_{0}, \tag{3.55}\]</span></span> which is equivalent to <span class="math inline">\(\bar{\ell}_{f}^{*}\)</span> (<a href="#eq-eq3.48" class="quarto-xref">Equation&nbsp;<span>3.52</span></a>) up to a constant multiple. From here, we can obtain the system of equations</p>
<p><span id="eq-eq3.52"><span class="math display">\[p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}=\alpha \bar{m}_{11}, p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}=\alpha \bar{m}_{00}, \bar{\tau}^{*} q_{0}^{*}=\alpha \bar{C}_{0}, \tag{3.56}\]</span></span> where <span class="math inline">\(\alpha &gt; 0\)</span> (we know it is <span class="math inline">\(\geq0\)</span> due to our assumptions earlier and because <span class="math inline">\(\bar{m}\)</span> is positive, but if it is equal to <span class="math inline">\(0\)</span> then <span class="math inline">\(\phi^*\)</span> would be constant. So, our resulting system of equations is <span id="eq-eq3.53"><span class="math display">\[\begin{aligned}
    p_{11}^{\prime}-\bar{\tau}^{*} q_{11}^{\prime}=\bar{m}_{11}, p_{00}^{\prime}-\bar{\tau}^{*} q_{00}^{\prime}=\bar{m}_{00}, \bar{\tau}^{*} q_{0}^{\prime}=\bar{C}_{0}.
\end{aligned} \tag{3.57}\]</span></span></p>
<p>Now, similarly, we can approximate <a href="#eq-eq3.49" class="quarto-xref">Equation&nbsp;<span>3.53</span></a> using the algorithm we defined for quasiconvex metrics (<a href="#alg-lfpm" class="quarto-xref">Algorithm 4</a>), where we altered the search range and comparisons. After finding the minimizer, we obtain the hyperplane <span id="eq-eq3.54"><span class="math display">\[\underline{\ell}:=\underline{m}_{11} t p+\underline{m}_{00} t n=\underline{m}_{11} \underline{TP}^{*}+\underline{m}_{00} \underline{TN}^{*} = \underline{C}_{0}, \tag{3.58}\]</span></span> which is equivalent to <span class="math inline">\(\underline{\ell}_{f}^{*}\)</span> (<a href="#eq-eq3.49" class="quarto-xref">Equation&nbsp;<span>3.53</span></a>) up to a constant multiple. So then, our system of equations is <span id="eq-eq3.55"><span class="math display">\[p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}=\gamma \underline{m}_{11}, p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}=\gamma \underline{m}_{00}, \underline{\tau}^{*} q_{0}^{*}=\gamma \underline{C}_{0}, \tag{3.59}\]</span></span> where <span class="math inline">\(\gamma &lt;0\)</span> (for a reason analogous to why we have <span class="math inline">\(\alpha &gt;0\)</span>), meaning our resulting system of equations is <span id="eq-eq3.56"><span class="math display">\[\begin{aligned}
    p_{11}^{\prime \prime}-\underline{\tau}^{*} q_{11}^{\prime \prime}=\underline{m}_{11}, p_{00}^{\prime \prime}-\underline{\tau}^{*} q_{00}^{\prime \prime}=\underline{m}_{00}, \underline{\tau}^{*} q_{0}^{\prime \prime}=\underline{C}_{0}.
\end{aligned} \tag{3.60}\]</span></span></p>
<p><a href="#eq-eq3.55" class="quarto-xref">Equation&nbsp;<span>3.59</span></a> and <a href="#eq-eq3.56" class="quarto-xref">Equation&nbsp;<span>3.60</span></a> form the two systems of equations mentioned in our overview of the algorithm. Next, we demonstrate that they have only one degree of freedom. Note that if we know <span class="math inline">\(p_{11}'\)</span>, we could solve both systems of equations as follows: <span id="eq-eq3.57"><span class="math display">\[\begin{aligned}
    p_{00}^{\prime}  &amp;=1-p_{11}^{\prime}, q_{0}^{\prime}=\bar{C}_{0} \frac{P^{\prime}}{Q^{\prime}}\\
    q_{11}^{\prime}  &amp;=\left(p_{11}^{\prime}-\bar{m}_{11}\right) \frac{P^{\prime}}{Q^{\prime}} \\
    q_{00}^{\prime}&amp;=\left(p_{00}^{\prime}-\bar{m}_{00}\right) \frac{P^{\prime}}{Q^{\prime}},
\end{aligned} \tag{3.61}\]</span></span> where <span class="math inline">\(P^{\prime}=p_{11}^{\prime} \zeta+p_{00}^{\prime}(1-\zeta)\)</span> and <span class="math inline">\(Q^{\prime}=P^{\prime}+\bar{C}_{0}-\)</span> <span class="math inline">\(\bar{m}_{11} \zeta-\bar{m}_{00}(1-\zeta).\)</span></p>
<p>Now, suppose we know <span class="math inline">\(p_{11}'\)</span>. We could use this value to solve both systems <a href="#eq-eq3.55" class="quarto-xref">Equation&nbsp;<span>3.59</span></a> and <a href="#eq-eq3.56" class="quarto-xref">Equation&nbsp;<span>3.60</span></a>, yielding two metrics, <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>, from the maximizer and minimizer, respectively. Importantly, when <span id="eq-eq3.58"><span class="math display">\[p_{11}^{*} / p_{00}^{*}=p_{11}^{\prime} / p_{00}^{\prime}=p_{11}^{\prime \prime} / p_{00}^{\prime \prime}, \tag{3.62}\]</span></span> then <span class="math inline">\(\phi^{*}(C)=\phi^{\prime}(C) / \alpha=-\phi^{\prime \prime}(C) / \gamma\)</span>. Essentially, when we find a value of <span class="math inline">\(p_{11}'\)</span> that results in <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span> h aving constant ratios at all points on the boundary of <span class="math inline">\(\mathcal{C}\)</span>, we can obtain <span class="math inline">\(\phi^*\)</span>, as it is derivable from <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\alpha\)</span> (or, alternatively, <span class="math inline">\(\phi''\)</span> and <span class="math inline">\(\gamma\)</span>).</p>
<p>We will perform a grid search for <span class="math inline">\(p_{11}'\)</span> on <span class="math inline">\([0,1]\)</span>. For each point in our search, we will compute <span class="math inline">\(\phi'\)</span> and <span class="math inline">\(\phi''\)</span>. Then, we will generate several confusion matrices on the boundaries and calculate the ratio $’’ / <span class="math inline">\(\phi'\)</span> for each. We will select the value of <span class="math inline">\(p_{11}'\)</span> for which the ratio <span class="math inline">\(\phi'' / \phi'\)</span> is closest to constant and use it to compute the elicited metric <span class="math inline">\(\hat{\phi}\)</span>. The pseudocode for LFPM elicitation is given in <a href="#alg-lfpm" class="quarto-xref">Algorithm 4</a>.</p>
<div id="alg-lfpm" class="pseudocode-container quarto-float" data-caption-prefix="Algorithm" data-pseudocode-number="4">
<div class="pseudocode">
\begin{algorithm} \caption{Grid Search for Best Ratio} \begin{algorithmic} \State \textbf{Input:} $k, \Delta$. \State \textbf{Initialize:} $\sigma_{\text{opt}} = \infty, p'_{11,\text{opt}} = 0$. \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3). \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3). \For{$p'_{11} = 0; \; p'_{11} \leq 1; \; p'_{11} = p'_{11} + \Delta$} \State Compute $\phi'$, $\phi''$ using Proposition 4. \State Compute array $r = \left[ \frac{\phi'(C_1)}{\phi''(C_1)}, \dots, \frac{\phi'(C_k)}{\phi''(C_k)} \right]$. \State Set $\sigma = \text{std}(r)$. \If{$\sigma &lt; \sigma_{\text{opt}}$} \State Set $\sigma_{\text{opt}} = \sigma$ and $p'_{11,\text{opt}} = p'_{11}$. \EndIf \EndFor \State \textbf{Output:} $p'_{11,\text{opt}}$. \end{algorithmic} \end{algorithm}
</div>
</div>
<p>We provide a Python implementation as below.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="0590caa5" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> lfpm_elicitation(k, delta):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">    Inputs:</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">    - k: the number of confusion matrices to evaluate on</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">    - delta: the spacing for the grid search</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co">    Outputs:</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co">    - p_11', which will allow us to compute the elicited LFPM</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">    """</span></span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a>    sigma_opt <span class="op">=</span> np.inf</span>
<span id="cb2-11"><a href="#cb2-11"></a>    p11_opt <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-12"><a href="#cb2-12"></a>    C <span class="op">=</span> compute_confusion_matrices(k) <span class="co"># generates k confusion matrices to evaluate on</span></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">int</span>(<span class="dv">1</span><span class="op">/</span>delta)):</span>
<span id="cb2-15"><a href="#cb2-15"></a>        p11 <span class="op">=</span> i <span class="op">*</span> delta</span>
<span id="cb2-16"><a href="#cb2-16"></a>        phi1 <span class="op">=</span> compute_upper_metric(p11) <span class="co"># solves the first system of equations with p11 </span></span>
<span id="cb2-17"><a href="#cb2-17"></a>        phi2 <span class="op">=</span> compute_lower_metric(p11) <span class="co"># solves the second system of equations with p11 </span></span>
<span id="cb2-18"><a href="#cb2-18"></a>        utility_1 <span class="op">=</span> [phi1(c) <span class="cf">for</span> c <span class="kw">in</span> C] <span class="co">#calculate phi for both systems of equations</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>        utility_2 <span class="op">=</span> [phi2(c) <span class="cf">for</span> c <span class="kw">in</span> C]</span>
<span id="cb2-20"><a href="#cb2-20"></a></span>
<span id="cb2-21"><a href="#cb2-21"></a>        r <span class="op">=</span> []</span>
<span id="cb2-22"><a href="#cb2-22"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb2-23"><a href="#cb2-23"></a>            r.append(utility_1[i] <span class="op">/</span> utility_2[i])</span>
<span id="cb2-24"><a href="#cb2-24"></a>        sigma <span class="op">=</span> np.std(r)</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a>        <span class="cf">if</span>(sigma <span class="op">&lt;</span> sigma_opt):</span>
<span id="cb2-27"><a href="#cb2-27"></a>            sigma_opt <span class="op">=</span> sigma</span>
<span id="cb2-28"><a href="#cb2-28"></a>            p11_opt <span class="op">=</span> p11</span>
<span id="cb2-29"><a href="#cb2-29"></a>    <span class="cf">return</span> p11_opt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>In summary, to elicit LFPMs, we utilize a special property of the LPM minimizer and maximizer on <span class="math inline">\(\mathcal{C}\)</span>–namely, that we can use the corresponding supporting hyperplanes to form a system of equations that can be used to approximate <span class="math inline">\(\phi^*\)</span> if one parameter (<span class="math inline">\(p_{11}'\)</span>) is found, and that this parameter can be found using an oracle-independent grid search.</p>
<section id="guarantees" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="guarantees">Guarantees</h4>
<p>Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide formal statement and intuitive interpretation of these guarantees here, with their proofs available in the appendix of the original paper.</p>
<p>First, we define the oracle noise <span class="math inline">\(\epsilon_{\Omega}\)</span>, which arises from the oracle potentially flipping the comparison output on two confusion matrices that are close enough in utility.</p>
<div class="callout callout-style-default callout-note callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="thm-thm1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1</strong></span> Given <span class="math inline">\(\epsilon, \epsilon_{\Omega} \geq 0\)</span> and a metric <span class="math inline">\(\phi\)</span> satisfying our assumptions, <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a> or <a href="#alg-lfpm" class="quarto-xref">Algorithm 4</a> finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of <span class="math inline">\(\phi\)</span> at that point is within <span class="math inline">\(O\left(\sqrt{\epsilon_{\Omega}} + \epsilon\right)\)</span> of the optimum, and the number of queries is <span class="math inline">\(O\left(\log \frac{1}{\epsilon}\right)\)</span>.</p>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="thm-thm2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2</strong></span> Let <span class="math inline">\(\mathbf{m}^{*}\)</span> be the true performance metric. Given <span class="math inline">\(\epsilon &gt; 0\)</span>, LPM elicitation outputs a performance metric <span class="math inline">\(\hat{\mathbf{m}}\)</span>, such that <span class="math inline">\(\left\|\mathbf{m}^{*} - \hat{\mathbf{m}}\right\|_{\infty} \leq \sqrt{2} \epsilon + \frac{2}{k_{0}} \sqrt{2 k_{1} \epsilon_{\Omega}}\)</span>.</p>
</div>
</div>
</div>
</div>
<p>These two theorems ensure that <a href="#alg-lpm" class="quarto-xref">Algorithm 3</a> and <a href="#alg-lfpm" class="quarto-xref">Algorithm 4</a> find an appropriate maximizer and minimizer in the search space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of queries. Both of these statements are guaranteed by the binary search approach.</p>
<div class="callout callout-style-default callout-note callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="thm-thm3" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3</strong></span> Let <span class="math inline">\(h_{\theta}\)</span> and <span class="math inline">\(\hat{h}_{\theta}\)</span> be two classifiers estimated using <span class="math inline">\(\eta\)</span> and <span class="math inline">\(\hat{\eta}\)</span>, respectively. Further, let <span class="math inline">\(\bar{\theta}\)</span> be such that <span class="math inline">\(h_{\bar{\theta}} = \arg \max _{\theta} \phi\left(h_{\theta}\right)\)</span>. Then <span class="math inline">\(\|C(\hat{h}_{\bar{\theta}}) - C\left(h_{\bar{\theta}}\right)\|_{\infty} = O\left(\left\|\hat{\eta}_{n} - \eta\right\|_{\infty}\right)\)</span>.</p>
</div>
</div>
</div>
</div>
<p>This theorem indicates that the drop in elicited metric quality caused by using a dataset of samples rather than population confusion matrices is bounded by the drop in performance of the decision boundary <span class="math inline">\(\eta\)</span>. These three guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the standard machine learning paradigm of training a decision boundary and using a pre-established metric.</p>
<p>For further interesting exploration of the types of problems that can be solved using the framework of metric elicitation, we refer the reader to <span class="citation" data-cites="nips">(<a href="#ref-nips" role="doc-biblioref">Hiranandani, Narasimhan, and Koyejo 2020</a>)</span>, which performs metric elicitation to determine the oracle’s ideal tradeoff between the classifier’s overall performance and the discrepancy between its performance on certain protected groups.</p>
</section>
</section>
<section id="multiclass-performance-metric-elicitation" class="level3" data-number="3.2.6">
<h3 data-number="3.2.6" class="anchored" data-anchor-id="multiclass-performance-metric-elicitation"><span class="header-section-number">3.2.6</span> Multiclass Performance Metric Elicitation</h3>
<p>Although the previous section only described metric elicitation for binary classification problems, the general framework can still be applied to multiclass classification problems, as described in “Multiclass Performance Metric Elicitation” by <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>.</p>
<p>Consider the case of classifying subtypes of leukemia <span class="citation" data-cites="YangNaiman+2014+477+496">(<a href="#ref-YangNaiman+2014+477+496" role="doc-biblioref">Yang and Naiman 2014</a>)</span>. We can train a neural network to predict conditional probability of a certain leukemia subtype given certain gene expressions. However, it may not be appropriate to classify the subtype purely based on whichever one has the highest confidence. For instance, a treatment for leukemia subtype C1 may be perfect for cases of C1, but it may be ineffective or harmful for certain other subtypes. Therefore, the final response from the classifier may not be as simple as as choosing the class with the highest conditional probability, just like how the threshold for binary classification may not always be 50%.</p>
<p>With multiclass metric elicitation, we can show confusion matrices to an oracle (like the doctor in the leukemia example) to determine which classifier has the best tradeoffs. In <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>, the authors focus on eliciting linear performance metrics, which is what we will describe in this chapter.</p>
<section id="preliminaries" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="preliminaries">Preliminaries</h4>
<p>Most of the notation from Binary Metric Elicitation still persists, just modified to provide categorical responses:</p>
<ul>
<li><p><span class="math inline">\(X \in \mathcal{X}\)</span> is the input random variable.</p></li>
<li><p><span class="math inline">\(Y \in [k]\)</span> is the output random variable, where <span class="math inline">\([k]\)</span> is the index set <span class="math inline">\(\{1, 2, \dots, k\}\)</span>.</p></li>
<li><p>The dataset of size <span class="math inline">\(n\)</span> is denoted by <span class="math inline">\(\{(\vec{x}, y)\}_{i=1}^n\)</span> generated independently and identically from <span class="math inline">\(\mathbb{P}(X, Y)\)</span>.</p></li>
<li><p><span class="math inline">\(\eta_i(\vec{x}) = \mathbb{P}(Y=i | X=\vec{x})\)</span> gives the conditional probability of class <span class="math inline">\(i \in [k]\)</span> given an observation.</p></li>
<li><p><span class="math inline">\(\xi_i = \mathbb{P}(Y=i)\)</span> is the marginal probability of class <span class="math inline">\(i \in [k]\)</span>.</p></li>
<li><p>The set of all classifiers is <span class="math inline">\(\mathcal{H} = \{h : \mathcal{X} \rightarrow \Delta_k\}\)</span>, where <span class="math inline">\(\Delta_k\)</span> is (k-1) dimensional simplex. In this case, the outputs of classifiers are 1-hot vectors of size <span class="math inline">\(k\)</span> where the only index with value 1 is the predicted class and all other positions have a value of 0.</p></li>
<li><p>The confusion matrix for a classifier, <span class="math inline">\(h\)</span>, is <span class="math inline">\(C(h, \mathbb{P}) \in \mathbb{R}^{k \times k}\)</span>, where: <span id="eq-eq3.59"><span class="math display">\[C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j) \text{\qquad for } i, j \in [k] \tag{3.63}\]</span></span></p></li>
</ul>
<p>Note that the confusion matrices are <span class="math inline">\(k\times k\)</span> and store the joint probabilities of each type of classification for each possible class. This means that the sum of row <span class="math inline">\(i\)</span> in the confusion matrix equals <span class="math inline">\(\xi_i\)</span>, because this is equivalent to adding over all possible classifications. Since we know the sums of each row, all diagonal elements can be reconstructed from just the off-diagonal elements, so a confusion matrix <span class="math inline">\(C(h, \mathbb{P})\)</span> can be expressed as a vector of off-diagonal elements, <span class="math inline">\(\vec{c}(h, \mathbb{P}) = \textit{off-diag}(C(h, \mathbb{P}))\)</span>, and <span class="math inline">\(\vec{c} \in \mathbb{R}^q\)</span> where <span class="math inline">\(q := k^2 - k\)</span>. The vector <span class="math inline">\(\vec{c}\)</span> is called the vector of <em>‘off-diagonal confusions.’</em> The space of off-diagonal confusions is <span class="math inline">\(\mathcal{C} = \{\vec{c}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>In cases where the oracle would care about the exact type of misclassification (i.e.&nbsp;misclassifying and object from class 1 as class 2), this off-diagonal confusion matrix is necessary. However, there are many cases where the performance of a classifier is determined by just the probability of correct prediction for each class, which just requires the diagonal elements. In these cases, we can define the vector of <em>‘diagonal confusions’</em> as <span class="math inline">\(\vec{d}(h, \mathbb{P}) = \textit{diag}(C(h, \mathbb{P})) \in \mathbb{R}^k\)</span>. The space of diagonal confusions is <span class="math inline">\(\mathcal{D} = \{\vec{d}(h, \mathbb{P}) : h \in \mathcal{H}\}\)</span>.</p>
<p>Finally, the setup for metric elicitation is identical to the one examined in the previous chapter. We still assume access to an oracle that can choose between two classifiers or confusion matrices, using notation <span class="math inline">\(\Gamma\)</span> for comparing two classifiers and <span class="math inline">\(\Omega\)</span> for comparing confusion matrices, which returns 1 if the first classifier is better and 0 otherwise. We still assume that the oracle behaves according to some unknown performance metric, and we wish to recover this metric up to some small error tolerance (based on a suitable norm).</p>
<p>The two different types of confusion vectors result in different algorithms for metric elicitation, which we will explore in later sections.</p>
</section>
<section id="introduction-to-diagonal-linear-performance-metric-elicitation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="introduction-to-diagonal-linear-performance-metric-elicitation">Introduction to Diagonal Linear Performance Metric Elicitation</h4>
<p>A Diagonal Linear Performance Metric (DLPM) is a performance metric that only considers the diagonal elements in the confusion matrix. The metric is defined as <span class="math inline">\(\psi(\vec{d}) = \langle \vec{a}, \vec{d} \rangle\)</span>, where <span class="math inline">\(\vec{a} \in \mathbb{R}^k\)</span> such that <span class="math inline">\(||\vec{a}||_1 = 1\)</span>. It is also called weighted accuracy <span class="citation" data-cites="pmlr-v37-narasimhanb15">(<a href="#ref-pmlr-v37-narasimhanb15" role="doc-biblioref">Narasimhan et al. 2015</a>)</span>.</p>
<p>The family of DLPMs is denoted as <span class="math inline">\(\varphi_{DLPM}\)</span>. Since these only consider the diagonal elements, which we want to maximize, we can focus on only eliciting monotonically increasing DLPMs, meaning that all elements in <span class="math inline">\(\vec{a}\)</span> are non-negative.</p>
</section>
<section id="geometry-of-space-of-diagonal-confusions-mathcald" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="geometry-of-space-of-diagonal-confusions-mathcald">Geometry of Space of Diagonal Confusions <span class="math inline">\(\mathcal{D}\)</span></h4>
<p>Consider the trivial classifiers that only predict a single class at all times. The diagonal confusions when only predicting class <span class="math inline">\(i\)</span> are <span class="math inline">\(\vec{v}_i \in \mathbb{R}^k\)</span> with <span class="math inline">\(\xi_i\)</span> at index <span class="math inline">\(i\)</span> and zero elsewhere. Note that this is the maximum possible value in index <span class="math inline">\(i\)</span>, because this represents perfectly classifying all points that have a true class of <span class="math inline">\(i\)</span>.</p>
<p>We can consider the space of diagonal confusions, visualized in <a href="#fig-diag_geom" class="quarto-xref">Figure&nbsp;<span>3.19</span></a> (taken from <span class="citation" data-cites="NEURIPS2019_1fd09c5f">(<a href="#ref-NEURIPS2019_1fd09c5f" role="doc-biblioref">Hiranandani et al. 2019b</a>)</span>). The space of <span class="math inline">\(\mathcal{D}\)</span> is strictly convex, closed, and contained in the box <span class="math inline">\([0, \xi_1] \times \dots \times [0, \xi_k]\)</span>. We also know that the only vertices are <span class="math inline">\(\vec{v}_i\)</span> for each <span class="math inline">\(i \in [k]^{(k-1)}\)</span>.</p>
<div id="fig-diag_geom" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-diag_geom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/diag_geometry.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-diag_geom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.19: (a) Geometry of space of diagonal confusions for <span class="math inline">\(k=3\)</span>. This is a convex region with three flat areas representing confusions when restricted to only two classes. (b) Geometry of diagonal confusions when restricted to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>. Notice how this is identical to the space of confusion matrices examined in the previous chapter.
</figcaption>
</figure>
</div>
<p>We know that this is strictly convex under the assumption that an object from any class can be misclassified as any other class. Mathematically, the assumption is that <span class="math inline">\(g_{ij}(r) = \mathbb{P} \left[\frac{\eta_i(X)}{\eta_j(X)} \geq r \right]\)</span> <span class="math inline">\(\forall i, j \in [k]\)</span> are continuous and strictly decreasing for <span class="math inline">\(r \in [0, \infty)\)</span>.</p>
<p>We can also define the space of binary classification confusion matrices confined to classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, which is the 2-D <span class="math inline">\((k_1, k_2)\)</span> axis-aligned face of <span class="math inline">\(\mathcal{D}\)</span>, denoted as <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>. Note that this is strictly convex, since <span class="math inline">\(\mathcal{D}\)</span> itself is strictly convex, and it has the same geometry as the space of binary confusion matrices examined in the previous chapter. Therefore, we can construct a Restricted Bayes Optimal (RBO) classifier for <span class="math inline">\(\psi \in \varphi_{DLPM}\)</span>, parameterized by <span class="math inline">\(\vec{a}\)</span>, as follows: <span id="eq-rbo_eq"><span class="math display">\[\begin{aligned}
\bar{h}_{k_1, k_2}(\vec{x})= \left\{
\begin{array}{ll}
      k_1, \text{ if } a_{k_1} \eta_{k_1}(\vec{x}) \geq a_{k_2} \eta_{k_2}(\vec{x})\\
k_2, \text{ o.w.}
\end{array}
\right\}.
\end{aligned} \tag{3.64}\]</span></span></p>
<p>We can parameterize the upper boundary of <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span>, denoted as <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>, using a single parameter <span class="math inline">\(m \in [0, 1]\)</span>. Specifically, we can construct a DLPM by setting <span class="math inline">\(a_{k_1} = m\)</span>, <span class="math inline">\(a_{k_2} = 1 - m\)</span>, and all others to 0. Using <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.64</span></a>, we can get the diagonal confusions, so varying <span class="math inline">\(m\)</span> parameterizes <span class="math inline">\(\partial \mathcal{D}^{+}_{k_1, k_2}\)</span>. The parameterization is denoted as <span class="math inline">\(\nu(m; k_1, k_2)\)</span>.</p>
</section>
<section id="diagonal-linear-performance-metric-elicitation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="diagonal-linear-performance-metric-elicitation">Diagonal Linear Performance Metric Elicitation</h4>
<p>Suppose the oracle follows a true metric, <span class="math inline">\(\psi\)</span>, that is linear and monotone increasing across all axes. If we consider the composition <span class="math inline">\(\psi \circ \nu(m; k_1, k_2): [0, 1] \rightarrow \mathbb{R}\)</span>, we know it must be concave and unimodal, because <span class="math inline">\(\mathcal{D}_{k_1, k_2}\)</span> is a convex set. Therefore, we can find the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> for any given <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> using a binary search procedure.</p>
<p>Since the RBO classifier for classes <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> only rely on the relative weights of the classes in the DLPM (see <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.64</span></a>), finding the value of <span class="math inline">\(m\)</span> that maximizes <span class="math inline">\(\psi \circ \nu(m; k_1, k_2)\)</span> gives us the true relative ratio between <span class="math inline">\(a_{k_1}\)</span> and <span class="math inline">\(a_{k_2}\)</span>. Specifically, from the definition of <span class="math inline">\(\nu\)</span>, we know that <span class="math inline">\(\frac{a_{k_2}}{a_{k_1}} = \frac{1-m}{m}\)</span>. We can therefore simply calculate the ratio between <span class="math inline">\(a_1\)</span> and all other weights to reconstruct an estimate for the true metric. A python implementation of this algorithm is provided below.</p>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="6d5c7f21" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="kw">def</span> rbo_dlpm(m, k1, k2, k):</span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="co">"""</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">    This constructs DLPM weights for the upper boundary of the</span></span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co">    restricted diagonal confusions, given a parameter m.</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">    This is equivalent to </span><span class="ch">\n</span><span class="co">u(m; k1, k2)</span></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="co">    </span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">    Inputs:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">    - m: parameter (between 0 and 1) for the upper boundary</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">    - k1: first axis for this  face</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    - k2: second axis for this face</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    - k: number of classes</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">    Outputs:</span></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co">    - DLPM weights for this point on the upper boundary</span></span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="co">    """</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    new_a <span class="op">=</span> np.zeros(k)</span>
<span id="cb3-18"><a href="#cb3-18"></a>    new_a[k1] <span class="op">=</span> m</span>
<span id="cb3-19"><a href="#cb3-19"></a>    new_a[k2] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> m</span>
<span id="cb3-20"><a href="#cb3-20"></a>    <span class="cf">return</span> new_a</span>
<span id="cb3-21"><a href="#cb3-21"></a></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="kw">def</span> dlpm_elicitation(epsilon, oracle, get_d, k):</span>
<span id="cb3-23"><a href="#cb3-23"></a>    <span class="co">"""</span></span>
<span id="cb3-24"><a href="#cb3-24"></a><span class="co">    Inputs:</span></span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb3-28"><a href="#cb3-28"></a><span class="co">    - get_d: some function that accepts dlpm weights and returns </span></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co">        diagonal confusions</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">    - k: number of classes</span></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">    Outputs:</span></span>
<span id="cb3-32"><a href="#cb3-32"></a><span class="co">    - estimate for true DLPM weights</span></span>
<span id="cb3-33"><a href="#cb3-33"></a><span class="co">    """</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>    a_hat <span class="op">=</span> np.zeros(k)</span>
<span id="cb3-35"><a href="#cb3-35"></a>    a_hat[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k):</span>
<span id="cb3-37"><a href="#cb3-37"></a>        <span class="co"># iterate over each axis to find appropriate ratio</span></span>
<span id="cb3-38"><a href="#cb3-38"></a>        a <span class="op">=</span> <span class="dv">0</span>  <span class="co"># lower bound of binary search</span></span>
<span id="cb3-39"><a href="#cb3-39"></a>        b <span class="op">=</span> <span class="dv">1</span>  <span class="co"># upper bound of binary search</span></span>
<span id="cb3-40"><a href="#cb3-40"></a></span>
<span id="cb3-41"><a href="#cb3-41"></a>        <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb3-42"><a href="#cb3-42"></a>            c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb3-43"><a href="#cb3-43"></a>            d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-44"><a href="#cb3-44"></a>            e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb3-45"><a href="#cb3-45"></a></span>
<span id="cb3-46"><a href="#cb3-46"></a>            <span class="co"># get diagonal confusions for each point</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>            d_a, d_c, d_d, d_e, d_b <span class="op">=</span> (get_d(rbo_dlpm(x, <span class="dv">0</span>, i, k)) </span>
<span id="cb3-48"><a href="#cb3-48"></a>                <span class="cf">for</span> x <span class="kw">in</span> [a, c, d, e, b])</span>
<span id="cb3-49"><a href="#cb3-49"></a></span>
<span id="cb3-50"><a href="#cb3-50"></a>            <span class="co"># query oracle for each pair</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>            response_ac <span class="op">=</span> oracle(d_a, d_c)</span>
<span id="cb3-52"><a href="#cb3-52"></a>            response_cd <span class="op">=</span> oracle(d_c, d_d)</span>
<span id="cb3-53"><a href="#cb3-53"></a>            response_de <span class="op">=</span> oracle(d_d, d_e)</span>
<span id="cb3-54"><a href="#cb3-54"></a>            response_eb <span class="op">=</span> oracle(d_e, d_b)</span>
<span id="cb3-55"><a href="#cb3-55"></a></span>
<span id="cb3-56"><a href="#cb3-56"></a>            <span class="co"># update ranges to keep the peak</span></span>
<span id="cb3-57"><a href="#cb3-57"></a>            <span class="cf">if</span> response_ac:</span>
<span id="cb3-58"><a href="#cb3-58"></a>                b <span class="op">=</span> d</span>
<span id="cb3-59"><a href="#cb3-59"></a>            <span class="cf">elif</span> response_cd:</span>
<span id="cb3-60"><a href="#cb3-60"></a>                b <span class="op">=</span> d</span>
<span id="cb3-61"><a href="#cb3-61"></a>            <span class="cf">elif</span> response_de:</span>
<span id="cb3-62"><a href="#cb3-62"></a>                a <span class="op">=</span> c</span>
<span id="cb3-63"><a href="#cb3-63"></a>                b <span class="op">=</span> e</span>
<span id="cb3-64"><a href="#cb3-64"></a>            <span class="cf">elif</span> response_eb:</span>
<span id="cb3-65"><a href="#cb3-65"></a>                a <span class="op">=</span> d</span>
<span id="cb3-66"><a href="#cb3-66"></a>            <span class="cf">else</span>:</span>
<span id="cb3-67"><a href="#cb3-67"></a>                a <span class="op">=</span> d</span>
<span id="cb3-68"><a href="#cb3-68"></a></span>
<span id="cb3-69"><a href="#cb3-69"></a>        midpt <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb3-70"><a href="#cb3-70"></a>        a_hat[i] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> midpt) <span class="op">/</span> midpt</span>
<span id="cb3-71"><a href="#cb3-71"></a>    <span class="cf">return</span> a_hat <span class="op">/</span> np.<span class="bu">sum</span>(a_hat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>To use this algorithm for metric elicitation on a real dataset, we need to supply the “oracle” and “get_d” functions. The oracle function is an interface to an expert who judges which of two confusion matrices is better. The get_d function will need to construct a classifier given the DLPM weights, following the principles of the RBO classifier from <a href="#eq-rbo_eq" class="quarto-xref">Equation&nbsp;<span>3.64</span></a>, and calculate the confusion matrix from a validation set.</p>
</section>
<section id="guarantees-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="guarantees-1">Guarantees</h4>
<p>Using the same oracle feedback noise model from the binary metric elicitation, we can make the following guarantees:</p>
<div class="callout callout-style-default callout-note callout-titled" title="proposition">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="prop-prop_dlpm">
<p>Given <span class="math inline">\(\epsilon, \epsilon_\Omega \geq 0\)</span>, and a 1-Lipschitz DLPM <span class="math inline">\(\varphi^*\)</span> parameterized by <span class="math inline">\(\vec{a}^*\)</span>. Then the output <span class="math inline">\(\hat{a}\)</span> of the DLPM elicitation algorithm after <span class="math inline">\(O((k-1)\log\frac{1}{\epsilon})\)</span> queries to the oracle satisfies <span class="math inline">\(||\vec{a}^* - \hat{a}||_\infty \leq O(\epsilon + \sqrt{\epsilon_\Omega})\)</span>, which is equivalent to <span class="math inline">\(||\vec{a}^* - \hat{a}||_2 \leq O(\sqrt{k}(\epsilon + \sqrt{\epsilon_\Omega}))\)</span>.</p>
</div>
</div>
</div>
</div>
<p>In other words, the maximum difference between the estimate and true value along any component (indicated by the L-infinity norm) is linearly bounded by the sum of the epsilon specified by the algorithm and the square root of the oracle’s correctness guarantee (<span class="math inline">\(\epsilon_\Omega\)</span>).</p>
</section>
</section>
<section id="linear-reward-estimation" class="level3" data-number="3.2.7">
<h3 data-number="3.2.7" class="anchored" data-anchor-id="linear-reward-estimation"><span class="header-section-number">3.2.7</span> Linear Reward Estimation</h3>
<p>How exactly do robots learn human preferences from just the pairwise comparisons, if they need to learn how to act in the environment itself? The comparisons in turn help robots learn the reward function of the human, which allows them to further take actions in real settings.</p>
<section id="geometry-of-pairwise-comparisons" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="geometry-of-pairwise-comparisons">Geometry of Pairwise Comparisons</h4>
<p>Let’s say there are two trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> that might be taken as the next course of action in any context, like choosing the next turn, or choosing the next chatGPT response. The robot is offering both to a human for comparison. To answer which of them is better, the human would ask themselves if <span class="math inline">\(R(\xi_A)\)</span> or <span class="math inline">\(R(\xi_B)\)</span> is bigger, with <span class="math inline">\(R(\xi) = w * \phi(\xi)\)</span> being the reward function. In this equation <span class="math inline">\(w\)</span> and <span class="math inline">\(\phi(\xi)\)</span> are vectors of weights and features of the trajectory, so alternatively, we can express this as:</p>
<p><span id="eq-reward_eq"><span class="math display">\[R(\xi) = \begin{bmatrix} w_1 \\ w_2 \\ ... \\ w_N \end{bmatrix} \cdot \begin{bmatrix} \phi_1(\xi) \\ \phi_2(\xi) \\ ... \\ \phi_N(\xi) \end{bmatrix} \tag{3.65}\]</span></span></p>
<p>If one says that they preferred <span class="math inline">\(\xi_2\)</span> less than <span class="math inline">\(\xi_1\)</span> then it means <span class="math inline">\(\xi_2 &lt; \xi_1 \implies R(\xi_2) &lt; R(\xi_1) \implies w * \phi(\xi_2) &lt; w * \phi(\xi_1) \implies 0 &lt; w * (\phi(\xi_1) - \phi(\xi_2)) \implies 0 &lt; w * \Phi\)</span>. Alternatively, if one preferred <span class="math inline">\(\xi_2\)</span> more than <span class="math inline">\(\xi_1\)</span>, the signs would be flipped, resulting in <span class="math inline">\(0 &gt; w * \Phi\)</span>. The two results can be represented in the N-dimensional space, where when it is split by the decision boundary, it creates half-spaces indicating preferences for each of the sides. For example in <a href="#fig-2dcomp" class="quarto-xref">Figure&nbsp;<span>3.20</span></a> we can see how a query between two objects can split the plain into two halves, indicating preference towards one of the objects. Such an image can be extended into bigger dimensions, where a line would become a separating hyperplane like in <a href="#fig-2dcomp" class="quarto-xref">Figure&nbsp;<span>3.20</span></a>.</p>
<div id="fig-2dcomp" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2dcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/2D-comp.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>A single query for a comparison between the two objects splits 2D space into two halves, each of which prefers one of the objects based on feature weights <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Figures/3D-comp.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Extension into 3D space</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2dcomp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.20: Comparison in 2D and 3D
</figcaption>
</figure>
</div>
<p>If one is to truly believe the answers of one person, they would remove everything from the other side of the hyperplane that does not agree with the received human preference. But since humans are noisy, that approach is not optimal, thus most applications up-weight the indicated side of the plane to emphasize that points on that side are better, and down-weight the other side as they do not agree with the provided comparison.</p>
<p>How should someone choose which queries to conduct, otherwise, what is the most informative query sequence? After completing one query, the next query should be orthogonal to the previous one so that the potential space consistent with the preferences decreases in half. The intuition behind that is the potential space has all of the reward functions that agree with the provided answers, so to find a specific reward function for a human, decreasing the space narrows down the possible options. For example, orthogonal query to the query in <a href="#fig-2dcomp" class="quarto-xref">Figure&nbsp;<span>3.20</span></a> is shown in <a href="#fig-2dspace" class="quarto-xref">Figure&nbsp;<span>3.21</span></a>. The original query created the blue space, and a new one created a red space, resulting in a purple intersection of the two which is still consistent with both of the queries’s results. The image shows that the purple portion is exactly half of the blue portion.</p>
<div id="fig-2dspace" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2dspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/2D-space.jpg" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2dspace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.21: Creating further comparisons limits the space that agrees with answers to all of them. The blue area demonstrates a preference for object 1 over object 2. The red area demonstrates a preference for object 3 over object 4. Combination (purple area) shows the space that is consistent with both of those preferences.
</figcaption>
</figure>
</div>
<p>Mathematically, from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span> this can be expressed as set <span class="math inline">\(F\)</span> of potential queries <span class="math inline">\(\phi\)</span>, where <span class="math inline">\(F = \{\phi: \phi = \Phi(\xi_A) - \Phi(\xi_B), \xi_A, \xi_B \in \Xi\}\)</span> (defining that a query is the difference between the features of two trajectories). Using that, the authors define a human update function <span class="math inline">\(f_{\phi}(w) = \min(1, \exp(I^T\phi))\)</span> that accounts for how much of the space will still be consistent with the preferences. Finally, for a specific query, they define the minimum volume removed as <span class="math inline">\(\min\{\mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span> (expected size of the two sides of the remaining space after it is split by a query - purple area in <a href="#fig-2dspace" class="quarto-xref">Figure&nbsp;<span>3.21</span></a>), and the final goal is to maximize that amount over all possible queries since it is optimal to get rid of as much space as possible to narrow down the options for the reward function: <span class="math inline">\(\max_{\phi} \min\{ \mathbb{E}[1 - f_{\phi}(w)], \mathbb{E}[1 - f_{-\phi}(w)]\}\)</span>. Effectively this is finding such <span class="math inline">\(\phi\)</span> that maximizes the information one can get by asking the next comparison query. While this approach uses minimum volume removed, there can be other metrics inside the <span class="math inline">\(\max\)</span> function. Some applications like movie recommendations do not require extra constraints, however in robotics one might want to add more constraints that satisfy certain rules, so that the resulting query follows the dynamics of the physical world.</p>
</section>
<section id="driving-simulator-example" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="driving-simulator-example">Driving Simulator Example</h4>
<p>The first real example of learning reward functions from pairwise comparisons is a 2D driving simulator from <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span>. In <a href="#fig-car_direct" class="quarto-xref">Figure&nbsp;<span>3.22</span></a> you can see the setting of a 3-lane road with the orange car being controlled by the computer.</p>
<div id="fig-car_direct" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-car_direct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/car_dir.png" class="img-fluid figure-img" style="width:40.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-car_direct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.22: The choices presented to a human for feedback are represented by green and red trajectories. White trajectory demonstrates the lane change of another vehicle in the space. <span class="citation" data-cites="pmlr-v87-biyik18a">(<a href="#ref-pmlr-v87-biyik18a" role="doc-biblioref">Biyik and Sadigh 2018</a>)</span>
</figcaption>
</figure>
</div>
<p>The queries conducted for this problem are two different trajectories presented to the human, and they are asked to evaluate which one of them is better. For the features that contribute to the reward function, it is important to consider that robots might not find some of the information as informative for the learning process as a human would. For this example, the underlying features included the distance between lane boundaries, distance to other cars, and the heading and speed of the controlled car. The weights toward the last feature were weighted the highest according to the authors, since it takes a lot of effort for the car to change or correct its direction.</p>
<p>At the start of the learning process, the car had no direction learned and was moving all over the road. In the middle of learning after 30 queries, the simulator learned to follow the direction of the road and go straight but still experienced collisions. After 70 queries, the simulator learned to avoid collisions, as well as keep the car within the lane without swerving.</p>
</section>
<section id="active-learning-for-pairwise-comparisons" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="active-learning-for-pairwise-comparisons">Active Learning for Pairwise Comparisons</h4>
<p>We have discussed that pairwise comparisons should be selected to maximize the minimum volume of remaining options removed. The question that can come out of the driving example is does it really matter to follow that goal or does random choice of queries performs as well? It turns out that indeed most active learning algorithms (purposefully selecting queries) over time converge with the performance of the random query selection, so in long term the performance is similar. However, what is different is that active learning achieves better performance earlier, which in time-sensitive tasks can be a critical factor.</p>
<p>One example of such a setting can be exoskeletons for humans as part of the rehabilitation after surgery <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span>. Different people have significantly different walking patterns as well as rehabilitation requirements, so the exoskeleton needs to adapt to the human as soon as possible for a more successful rehabilitation. Figure <a href="#fig-robotics" class="quarto-xref">Figure&nbsp;<span>3.23</span></a> demonstrates the difference in the time needed between the two approaches. In general, in robotics, the time differences that might seem small to a human might be detrimental to the final performance.</p>
<div id="fig-robotics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robotics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robo_graph.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robotics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.23: Performance of active learning and random query selection algorithms in the task of exoskeleton learning with human preferences. <span class="citation" data-cites="Li_2021">(<a href="#ref-Li_2021" role="doc-biblioref">Li et al. 2021</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="multi-modal-reward-functions-for-pairwise-comparisons" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="multi-modal-reward-functions-for-pairwise-comparisons">Multi-Modal Reward Functions for Pairwise Comparisons</h4>
<p>What if one is working with multiple people and their responses to the queries for comparisons? It will be impossible to recover the different personalities based on the answers, and it might be necessary to conduct a full ranking before it is clear which responses belonged to which person, but the underlying theory for the number of comparisons is non-trivial. For that, the researchers <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> have used multi-modal models for reward function learning, which allows to account for different types of valid behaviours and trajectories that can come from different humans.</p>
<div id="fig-negotiation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-negotiation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/negotiation.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-negotiation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.24: The negotiation setting with two people and three shared items. Each person has a desired number of items indicated in their utility box. Alice is the controlled agent that has many different response options that are illustrated by the approaches different models might take. <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>
</figcaption>
</figure>
</div>
<p>An example setting for such type of problem is negotiations <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>. Let’s say there are some shared items and two people with different utilities and desires for items, where each person only knows their utility. In a specific case of <a href="#fig-negotiation" class="quarto-xref">Figure&nbsp;<span>3.24</span></a>, Bob as a proposing agent and Alice as a controlled agent who has many different ways of responding to Bob’s proposals. Different methods can be used to design Alice as an AI agent. The first idea is reinforcement learning, where multiple rounds of negotiations are done, the model simulates game theory and sees how Bob reacts. Authors of this setting <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span> show that over time the model learns to ask for the same thing over and over again, as Alice is not trained to be human-like or negotiable, and just tries to maximize Alice’s utility. The second approach is supervised learning, where the model can be trained on some dataset, learning the history of negotiations. This results in Alice being very agreeable, which demonstrates two polar results of the two approaches, and it would be ideal to find a middle ground and combine both of them. The authors proposed the Targeted acquisition approach, which is based on active learning ideas. The model asks diverse questions at different cases and stages of negotiations like humans, determining which questions are more valuable to be asked throughout learning. Such an approach ended up in more fair and optimal results than supervised or reinforcement learning <span class="citation" data-cites="kwon2021targeted">(<a href="#ref-kwon2021targeted" role="doc-biblioref">Kwon et al. 2021</a>)</span>.</p>
<p>In conclusion, pairwise comparisons show to be a great way of learning linear reward functions, but at times present challenges or incapabilities that can be further improved with additional incorporations of approaches like Active Learning. That improves many applications in terms of time spent getting to the result in case of exoskeleton adjustments, as well as getting to a middle ground between polar behaviors in applications like negotiations.</p>
</section>
</section>
<section id="truthful-preference-elicitation-with-adversary" class="level3" data-number="3.2.8">
<h3 data-number="3.2.8" class="anchored" data-anchor-id="truthful-preference-elicitation-with-adversary"><span class="header-section-number">3.2.8</span> Truthful Preference Elicitation with Adversary</h3>
<p>In our study of social choice models in Chapter <a href="#2human-decision-making-choice-models" data-reference-type="ref" data-reference="2model">[2model]</a>, we study how axiomatic properties are implemented to prevent strategic manipulation of a population. This brings us onto the field of <strong>mechanism design</strong>. At its core, mechanism design is the science of making rules. The intent in this field is to design systems so that the strategic behaviour of individuals leads to desirable outcomes. Just thinking about services on the Internet – file sharing, reputation systems, web search, web advertising, email, Internet auctions, congestion control – all have to be set up so that an individual’s selfish behavior leads to better outcomes for the entire community. A more specific example of this is the phenomenon of “bid-sniping” that was present on eBay in the early 2000s. When people could bid on E-bay, the rule was that the highest bidder by the end of some specified time period would get the item. As a result, people would just wait until the very last minute to bid in order to not raise the price of the item too early. On the other hand, when Amazon still allowed bidding, they had a rule that any time a bid was placed it would extend the time of the bid by ten minutes. This simple difference had drastic effects on bidding prices over time. Mechanism design develops the theoretical framework for learning social choices and eliciting truthful preference.</p>
<p>We will cover frameworks that model several scenarios that mechanism design is usefully applied to: recommendation systems (where users will selfishly try to stick to their preferences while a planner encourages exploration); auctions (where bidders will try to maximise their reward compared to others); and peer grading (where truthful reporting is not necessarily an incentive for students).</p>
<section id="auction-theory" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="auction-theory">Auction Theory</h4>
<section id="single-item-auctions" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="single-item-auctions">Single-Item Auctions</h5>
<p>The first problem within auction theory we will consider is the <em>single-item auction</em>. The premise of this problem is that there is a single item to sell, <span class="math inline">\(n\)</span> bidders (with unknown private valuations of the item <span class="math inline">\(v_1\)</span>, ..., <span class="math inline">\(v_n\)</span>). The bidder’s individual objective is to maximize utility: the value <span class="math inline">\(v_i\)</span> of the item subtracted by the price paid for the item. The auction procedure is standard in the sense that bids are solicited and the highest bid will win the auction. While the objective of the individual bidder is clear, there could be a plethora of different objectives for the auction as a whole. One option could be to maximize social surplus, meaning the goal is to maximize the value of the winner. Another objective could be to maximize seller profit which is the payment of the winner. For simplicity, we can focus on the first objective where the goal is to maximize social surplus. If we want to maximize social surplus it turns out that a great way to do this is the “second-price auction”.</p>
<section id="maximizing-social-surplus" class="level6 unnumbered">
<h6 class="unnumbered anchored" data-anchor-id="maximizing-social-surplus">Maximizing Social Surplus</h6>
<p>In the second-price auction, we will operate under slightly different conditions. In the second-price auction we 1) solicit sealed bids, 2) have the winner be the highest bidder, and 3) charger winner the second-highest bid price. As an example, if the solicited bids are <span class="math inline">\(b = (2, 6, 4, 1)\)</span> the winner will be that who bid <span class="math inline">\(6\)</span>, but will pay a price of <span class="math inline">\(4\)</span>. From here, we can do some equilibrium analysis to try and learn what the optimal bidding strategy is for each bidder. Let the amount bidder <span class="math inline">\(i\)</span> bids to be <span class="math inline">\(b_i\)</span>, so we have bids <span class="math inline">\(b_1, b_2, ..., b_n\)</span>. How much should bidder <span class="math inline">\(i\)</span> bid? To analyze this, let us define <span class="math inline">\(t_i = max_{j \neq i} b_j\)</span> which represents the max of the bids that is not from bidder <span class="math inline">\(i\)</span>. There are now two cases to consider: if <span class="math inline">\(b_i\)</span> &gt; <span class="math inline">\(t_i\)</span> and if <span class="math inline">\(b_i\)</span> &lt; <span class="math inline">\(t_i\)</span>. In the first case the bidder <span class="math inline">\(i\)</span> wins, and if the bidder bid <span class="math inline">\(b_i = v_i\)</span>, they are guaranteed to have a positive return on bid. In the other case, they lose the bid and the net loss is 0 because they don’t have to pay. From this we can conclude that bidder <span class="math inline">\(i\)</span>’s dominant strategy is to just bid <span class="math inline">\(b_i = v_i\)</span>. Rigorously proving this is a little bit trickier, but it was shown from Vickrey in 1961 [cite] that truthful bidding is the dominant strategy in second-price auctions. A corollary of this is that we are maximizing social surplus since bids are values and the winner is the bidder with highest valuation.</p>
</section>
<section id="maximize-seller-profit" class="level6 unnumbered">
<h6 class="unnumbered anchored" data-anchor-id="maximize-seller-profit">Maximize Seller Profit</h6>
<p>If we want to look at things from the perspective of a seller trying to maximize their profit we need to treat the bidder’s bids as uniform random variables. Consider the example scenario where we have two bidders each bidding uniformly between 0 and 1. What is the seller’s expected profit? (in this case profit and revenue for the seller are the same because we assume the seller throws away the item if it doesn’t sell/has no valuation for it).</p>
<p>From there the question now becomes, can we get more expected profit from the seller’s perspective? It turns out there is a design where we can add a reserve price of <span class="math inline">\(r\)</span> to the second-price auction. The way this works is we can 1) Insert seller-bid at <span class="math inline">\(r\)</span>, 2) solicit bids, 3) pick the highest bidder, and 3) charge the 2nd-highest bid. In effect, this is just the second-price auction but with a bid from the seller as well, at a price of <span class="math inline">\(r\)</span>. A lemma, that we won’t prove here, is that the second-price auction with reserve price <span class="math inline">\(r\)</span> still has a dominant strategy of just being truthful.</p>
<p>Let’s now consider what the profit of a second-price auction would be with two bidders that uniformly bid between 0 and 1 – but this time we have a reserve price of <span class="math inline">\(1/2\)</span>. To calculate the expected profit we break down the situation into 3 cases:</p>
<ul>
<li><p>Case 1: <span class="math inline">\(1/2 &gt; v_1 &gt; v_2 \rightarrow 1/4 \text{ probability} \rightarrow  E[\text{profit}] = 0\)</span></p></li>
<li><p>Case 2: <span class="math inline">\(v_1 &gt; v_2 &gt; 1/2 \rightarrow 1/4 \text{probability} \rightarrow E[v2 | case 2] = 2/3\)</span></p></li>
<li><p>Case 3: <span class="math inline">\(v_1 &gt; 1/2 &gt; v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2\)</span></p></li>
</ul>
<p>Why is <span class="math inline">\(E[v2 | case 2] = 2/3\)</span>? If <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are greater than <span class="math inline">\(1/2\)</span>, they are evenly spread across the interval, meaning the expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get <span class="math inline">\(E[profit] = 5/12\)</span>. It turns out that second-price auctions with reserve actually maximize profit in general (for symmetric bidders)!</p>
<p>In the previous section we conclude that second-price auctions with reserve maximize profit for the seller. In order to prove this, we now move to the more general topic of asking how should a monopolist divide good across separate markets. We can make the assumption that the demand model is a concave revenue <span class="math inline">\(R(q)\)</span> in quantity <span class="math inline">\(q\)</span>. Under this assumption, we can just divide supply into <span class="math inline">\(q = q_a + q_b\)</span> such that <span class="math inline">\(R'_a(q_a) = R'_b(q_b)\)</span>. The idea from here is a theorem from Myerson in 1981 that states an optimal action maximizes "marginal revenue". Consider an example where we have two bidders bidding a uniform value between 0 and 1. Our revenue curve can now be derived from the offering price <span class="math inline">\(V(q) = 1 - q\)</span> like so: <span class="math inline">\(R(q) = qV(q) = q - q^2\)</span>. Taking the derivative gives us the marginal revenue <span class="math inline">\(R'(q) = 1-2q\)</span>. This means two things: 1) we want to sell to bidder <span class="math inline">\(i\)</span> with the highest <span class="math inline">\(R'(q_i)\)</span> and 2) we want to sell to bidder <span class="math inline">\(i\)</span> with value at least <span class="math inline">\(1/2\)</span> (if we want a positive <span class="math inline">\(R'(q_i)\)</span>. But this is just a second-price auction with reserve <span class="math inline">\(1/2\)</span>! This means that for symmetric bidders, a second price with reserve is the optimal auction.</p>
</section>
<section id="what-good-are-auctions" class="level6 unnumbered">
<h6 class="unnumbered anchored" data-anchor-id="what-good-are-auctions">What good are auctions?</h6>
<p>An interesting topic to discuss is what benefits auctions bring to the table as opposed to just standard pricing. Online auctions used to be a lot more popular in the early 2000s and have been completely replaced by standard online pricing, even on sites like e-bay. While auctions are slower and have added inherent complexities, they are actually optimal on paper. Standard pricing on the other is non-optimal; although it is fast and simpler for buyers. There is actually a way to quantify this: for pricing <span class="math inline">\(k\)</span> units, the loss is at most <span class="math inline">\(1 / \sqrt{2\pi k}\)</span> of optimal profit.</p>
<p>Let’s consider applications in duopoly platform design. We know that the optimal auction is second-price with reserve, but what happens when we introduce competition between two auction platforms? Some important details related to the revenue of a second-price auction is that a second-price auction with no reserve and n bidders leads to larger revenue having an optimal reserve and n - 1 bidders <span class="citation" data-cites="bulow-klemperer1996">(<a href="#ref-bulow-klemperer1996" role="doc-biblioref">Bulow and Klemperer 1996</a>)</span>. Additionally, with an entry cost, no reserve is the optimal strategy for maximizing revenue <span class="citation" data-cites="mcafee-87">(<a href="#ref-mcafee-87" role="doc-biblioref">McAfee and McMillan 1987</a>)</span>. Let’s consider an example of a competing auction system which is Google ads vs Bing ads. How should an advertiser divide the budget between Google and Bing? They should give the same budget to both companies. What happens if Bing raises their prices? Then, the advertising company moves more of its budget to Google from Bing.</p>
</section>
</section>
</section>
<section id="prior-independent-auctions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="prior-independent-auctions">Prior-Independent Auctions</h4>
<p>The Bulow-Klemperer theorem demonstrates that increased competition can be more valuable than perfect knowledge of bidders’ valuation distributions. This result provides insight into the potential of simple, prior-independent auctions to approach the performance of optimal auctions. The theorem states that for a single-item auction with bidders’ valuations drawn independently from a regular distribution <span class="math inline">\(F\)</span>:</p>
<div class="callout callout-style-default callout-note callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-14-contents" aria-controls="callout-14" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-14" class="callout-14-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="thm-bulow-klemperer" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.4</strong></span> Let <span class="math inline">\(F\)</span> be a regular distribution and <span class="math inline">\(n\)</span> a positive integer. Then: <span id="eq-eq3.64"><span class="math display">\[E_{v_1,\ldots,v_{n+1} \sim F}[\text{Rev(VA)}(n+1 \text{ bidders})] \geq E_{v_1,\ldots,v_n \sim F}[\text{Rev(OPT}_F)(n \text{ bidders})] \tag{3.66}\]</span></span> where VA denotes the Vickrey auction and <span class="math inline">\(\text{OPT}_F\)</span> denotes the optimal auction for <span class="math inline">\(F\)</span>.</p>
</div>
</div>
</div>
</div>
<p>This shows that running a simple Vickrey auction with one extra bidder outperforms the revenue-optimal auction that requires precise knowledge of the distribution. It suggests that in practice, effort spent on recruiting additional bidders may be more fruitful than fine-tuning auction parameters.</p>
<section id="the-vcg-mechanism" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="the-vcg-mechanism">The VCG Mechanism</h5>
<p>The VCG mechanism is a cornerstone of mechanism design theory, providing a general solution for welfare maximization in multi-parameter environments. The key result is:</p>
<div class="callout callout-style-default callout-note callout-titled" title="theorem">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-15-contents" aria-controls="callout-15" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
theorem
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-15" class="callout-15-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<div id="thm-VCG" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.5</strong></span> <span id="thm:VCG" data-label="thm:VCG"></span> In every general mechanism design environment, there is a dominant-strategy incentive-compatible (DSIC) welfare-maximizing mechanism.</p>
</div>
</div>
</div>
</div>
<p>The VCG mechanism operates as follows:</p>
<ol type="1">
<li><p>Given bids <span class="math inline">\(b_1, \ldots, b_n\)</span>, where each <span class="math inline">\(b_i\)</span> is indexed by the outcome set <span class="math inline">\(\Omega\)</span>, the allocation rule is:</p>
<p><span id="eq-eq3.65"><span class="math display">\[x(b) = \arg \max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega) \tag{3.67}\]</span></span></p></li>
<li><p>The payment rule for each agent <span class="math inline">\(i\)</span> is:</p>
<p><span id="eq-eq3.66"><span class="math display">\[p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*) \tag{3.68}\]</span></span></p>
<p>where <span class="math inline">\(\omega^* = x(b)\)</span> is the chosen outcome.</p></li>
</ol>
<p>The key insight is to charge each agent its “externality” - the welfare loss inflicted on other agents by its presence. This payment rule, coupled with the welfare-maximizing allocation rule, yields a DSIC mechanism.</p>
<p>The VCG mechanism can be interpreted as having each agent pay its bid minus a "rebate" equal to the increase in welfare attributable to its presence:</p>
<p><span id="eq-eq3.67"><span class="math display">\[p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right] \tag{3.69}\]</span></span></p>
<p>While the VCG mechanism provides a theoretical solution for DSIC welfare-maximization in general environments, it can be challenging to implement in practice due to computational and communication complexities.</p>
</section>
<section id="combinatorial-auctions" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="combinatorial-auctions">Combinatorial Auctions</h5>
<p>Combinatorial auctions are an important class of multi-parameter mechanism design problems, with applications ranging from spectrum auctions to airport slot allocation. In a combinatorial auction:</p>
<ul>
<li><p>There are <span class="math inline">\(n\)</span> bidders and a set <span class="math inline">\(M\)</span> of <span class="math inline">\(m\)</span> items.</p></li>
<li><p>The outcome set <span class="math inline">\(\Omega\)</span> consists of allocations <span class="math inline">\((S_1, \ldots, S_n)\)</span>, where <span class="math inline">\(S_i\)</span> is the bundle allocated to bidder <span class="math inline">\(i\)</span>.</p></li>
<li><p>Each bidder <span class="math inline">\(i\)</span> has a private valuation <span class="math inline">\(v_i(S)\)</span> for each bundle <span class="math inline">\(S \subseteq M\)</span>.</p></li>
</ul>
<p>While the VCG mechanism theoretically solves the welfare-maximization problem, combinatorial auctions face several major challenges in practice:</p>
<ol type="1">
<li><p>Preference Elicitation: Each bidder has <span class="math inline">\(2^m - 1\)</span> private parameters, making direct revelation infeasible for even moderate numbers of items. This necessitates the use of indirect mechanisms that elicit information on a "need-to-know" basis.</p></li>
<li><p>Computational Complexity: Even when preference elicitation is not an issue, welfare-maximization can be an intractable problem. In practice, approximations are often used, hoping to achieve reasonably good welfare.</p></li>
<li><p>VCG Limitations: The VCG mechanism can exhibit bad revenue and incentive properties in combinatorial settings. For example, adding bidders can sometimes decrease revenue to zero, and the mechanism can be vulnerable to collusion and false-name bids.</p></li>
<li><p>Strategic Behavior in Iterative Auctions: Most practical combinatorial auctions are iterative, comprising multiple rounds. This introduces new opportunities for strategic behavior, such as using bids to signal intentions to other bidders.</p></li>
</ol>
<p>These challenges make combinatorial auctions a rich and complex area of study, requiring careful design to balance theoretical guarantees with practical considerations.</p>
</section>
<section id="spectrum-auctions" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="spectrum-auctions">Spectrum Auctions</h5>
<p>Spectrum auctions represent a complex application of combinatorial auction theory. With n bidders and m non-identical items, each bidder has a private valuation for every possible bundle of items, making it impractical to directly elicit all preferences. This necessitates the use of indirect, iterative mechanisms that query bidders for valuation information on a “need-to-know” basis, sacrificing some of the desirable properties of direct mechanisms like dominant strategy incentive compatibility (DSIC) and full welfare maximization.</p>
<p>The fundamental challenge in spectrum auctions lies in the nature of the items being sold. There is a dichotomy between items that are substitutes (where <span class="math inline">\(v(AB) \leq v(A) + v(B))\)</span> and those that are complements (where <span class="math inline">\(v(AB) &gt; v(A) + v(B))\)</span>. Substitute items, such as licenses for the same area with equal-sized frequency ranges, are generally easier to handle. When items are substitutes, welfare maximization is computationally tractable, and the VCG mechanism avoids many undesirable properties. However, complementary items, which arise naturally in spectrum auctions when bidders want adjacent licenses, present significant challenges.</p>
<p>Early attempts at spectrum auctions revealed the pitfalls of naive approaches. Sequential auctions, where items are sold one after another, proved problematic as demonstrated by a Swiss auction in 2000. Bidders struggled to bid intelligently without knowing future prices, leading to unpredictable outcomes and potential revenue loss. Similarly, simultaneous sealed-bid auctions, as used in New Zealand in 1990, created difficulties for bidders in coordinating their bids across multiple items, resulting in severely suboptimal outcomes.</p>
<p>The Simultaneous Ascending Auction (SAA) emerged as a solution to these issues and has formed the basis of most spectrum auctions over the past two decades. In an SAA, multiple items are auctioned simultaneously in rounds, with bidders placing bids on any subset of items subject to an activity rule. This format facilitates price discovery, allowing bidders to adjust their strategies as they learn about others’ valuations. It also allows bidders to determine valuations on a need-to-know basis, reducing the cognitive burden compared to direct-revelation auctions.</p>
<p>Despite its advantages, the SAA is not without vulnerabilities. Demand reduction, where bidders strategically reduce their demand to lower prices, can lead to inefficient outcomes even when items are substitutes. The exposure problem arises with complementary items, where bidders risk winning only a subset of desired items at unfavorable prices. These issues highlight the ongoing challenges in designing effective spectrum auctions, balancing theoretical guarantees with practical considerations.</p>
</section>
<section id="case-study-classroom-peer-grading" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="case-study-classroom-peer-grading">Case study: Classroom Peer Grading</h5>
<p>This chapter discusses work by Jason Hartline, Yingkai Li, Liren Shan, and Yifan Wu at Northwestern University, where researchers examined mechanism design for the classroom, specifically in terms of the optimization of scoring rules. They explored peer grading in the classroom and how to construct a peer grading system that optimizes the objectives for each stakeholder in the system, including those being graded, the peer graders, the TAs of the class, and the professor.</p>
<p>Firstly, let’s think of the classroom like a computer. We can think of students as local optimizers; their incentive is to minimize the amount of work they need to do and maximize the grades that they receive. The graders are imprecise operators, which means that there is some uncertainty in their ability to grade the work completed by the students. The syllabus can be thought of as the rules that map the actions of the students to the grade they end up receiving in the class. Our overall goals for this classroom based on these definitions is to minimize work, maximize learning, and fairly assess the students for the work that they do <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>.</p>
<p>One basic question that we can examine, is what is the best syllabus that maximizes our objectives for our classroom design. Some components of this could include grading randomized exams, grading with partial credit, group projects, and finally, peer grading, which is the component that we will be taking a deeper dive into.</p>
<p>The general situation of the peer grading problem is that proper scoring rules make peer grades horrible <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>. So we want to be able to optimize scoring rules and make sure that we are optimizing each component of the peer grading pipeline.</p>
<p>The main algorithms focused on in this peer grading design paper were matching peers and TAs to submissions and the grading of those submissions from the TAs and the peer reviews <span class="citation" data-cites="jasonH2020">(<a href="#ref-jasonH2020" role="doc-biblioref">Hartline et al. 2020</a>)</span>. There are quite a number of advantages to peer grading including that peers are able to learn from reviewing other people’s work, it reduces the work for the teacher, and improves the turnaround time for assignment feedback (which are all part of our overarching goals for our mechanism design for the classroom). But, it is also important to acknowledge the potential disadvantages of the peer grading system: it is possible that the peer graders present inaccurate grades and there is student unrest. This presents us with a challenge: being able to incentivize accurate peer reviews.</p>
<p>One problem that we run into, when we use the proper scoring rule to score peer reviews, if the peer graders use the lazy peer strategy, which means that they always report 80<span class="math inline">\(\%\)</span> for their peer reviews, they get graded very well using the proper scoring rule algorithm. In fact, the proper scoring rule says that their peer review is 96<span class="math inline">\(\%\)</span> accurate <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>. So how do we incentivize effort in reviews from peer graders? We use a scoring rule that maximizes the difference in score between effort or no effort reviews as indicated by the peer reviewers <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>. So overall, the analysis of datasets leads to decision optimizations and, eventually, payoff from those decisions.</p>
<p>To conclude our mechanism design in the classroom discussion, we have two key takeaways: scoring rules are essential in being able to understand and analyze data thoroughly, and optimal scoring rules for binary effort allow us to understand the setting independent of the dataset <span class="citation" data-cites="jasonH2023">(<a href="#ref-jasonH2023" role="doc-biblioref">Hartline et al. 2023</a>)</span>.</p>
</section>
</section>
<section id="mutual-information-paradigm" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="mutual-information-paradigm">Mutual Information Paradigm</h4>
<p>In this section we discuss an influential new framework for designing peer prediction mechanisms, the Mutual Information Paradigm (MIP) introduced by Kong and Schoenebeck <span class="citation" data-cites="kongschoenebeck2019">(<a href="#ref-kongschoenebeck2019" role="doc-biblioref">Kong and Schoenebeck 2019</a>)</span>. Traditional peer prediction approaches typically rely on scoring rules and correlation between agents’ signals. However, these methods often struggle with issues like uninformed equilibria, where agents can coordinate on uninformative strategies that yield higher payoffs than truth-telling. The core idea is to reward agents based on the mutual information between their report and the reports of other agents.</p>
<p>We consider a setting with <span class="math inline">\(n\)</span> agents, each possessing a private signal <span class="math inline">\(\Psi_i\)</span> drawn from some set <span class="math inline">\(\Sigma\)</span>. The mechanism asks each agent to report their signal, which we denote as <span class="math inline">\(\hat{\Psi}_i\)</span>. For each agent <span class="math inline">\(i\)</span>, the mechanism randomly selects a reference agent <span class="math inline">\(j \neq i\)</span>. Agent <span class="math inline">\(i\)</span>’s payment is then calculated as: <span id="eq-eq3.68"><span class="math display">\[MI(\hat{\Psi}_i; \hat{\Psi}_j) \tag{3.70}\]</span></span> where <span class="math inline">\(MI\)</span> is an information-monotone mutual information measure. An information-monotone <span class="math inline">\(MI\)</span> measure must satisfy the following properties:</p>
<ul>
<li><p><strong>Symmetry</strong>: <span class="math inline">\(MI(X; Y) = MI(Y; X)\)</span>.</p></li>
<li><p><strong>Non-negativity</strong>: <span class="math inline">\(MI(X; Y) \geq 0\)</span>, with equality if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</p></li>
<li><p><strong>Data processing inequality</strong>: For any transition probability <span class="math inline">\(M\)</span>, if <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(M(X)\)</span> conditioned on <span class="math inline">\(X\)</span>, then <span class="math inline">\(MI(M(X); Y) \leq MI(X; Y)\)</span>.</p></li>
</ul>
<p>Two important families of mutual information measures that satisfy these properties are <span class="math inline">\(f\)</span>-mutual information and Bregman mutual information. The <span class="math inline">\(f\)</span>-mutual information is defined as: <span id="eq-eq3.69"><span class="math display">\[MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y}) \tag{3.71}\]</span></span> where <span class="math inline">\(D_f\)</span> is an <span class="math inline">\(f\)</span>-divergence, <span class="math inline">\(U_{X,Y}\)</span> is the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(V_{X,Y}\)</span> is the product of their marginal distributions. The Bregman mutual information is defined as: <span id="eq-eq3.70"><span class="math display">\[BMI_{PS}(X; Y) = \mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)] \tag{3.72}\]</span></span> where <span class="math inline">\(D_{PS}\)</span> is a Bregman divergence based on a proper scoring rule <span class="math inline">\(PS\)</span>, <span class="math inline">\(U_{Y|X}\)</span> is the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and <span class="math inline">\(U_Y\)</span> is the marginal distribution of <span class="math inline">\(Y\)</span>.</p>
<p>The MIP framework can be applied in both single-question and multi-question settings. In the multi-question setting, the mechanism can estimate the mutual information empirically from multiple questions. In the single-question setting, additional techniques like asking for predictions about other agents’ reports are used to estimate the mutual information.</p>
<p>A key theoretical result of the MIP framework is that when the chosen mutual information measure is strictly information-monotone with respect to agents’ priors, the resulting mechanism is both dominantly truthful and strongly truthful. This means that truth-telling is a dominant strategy for each agent and that the truth-telling equilibrium yields strictly higher payoffs than any other non-permutation strategy profile.</p>
<p>As research continues to address practical implementation challenges of designing truthful mechanisms, MIP-based approaches have significant potential to improve preference elicitation and aggregation in real-world applications lacking verifiable ground truth.</p>
</section>
<section id="auction-theory-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="auction-theory-2">Auction Theory 2</h4>
<section id="single-item-auctions-1" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="single-item-auctions-1">Single-Item Auctions</h5>
<p>The first problem within auction we will consider is the <em>single-item auction</em>. In this problem setup, there is a single item to sell and <span class="math inline">\(n\)</span> bidders each with unknown private valuations of the item <span class="math inline">\(v_1, \ldots, v_n\)</span>,</p>
</section>
</section>
</section>
<section id="application-guiding-human-demonstrations-in-robotics" class="level3" data-number="3.2.9">
<h3 data-number="3.2.9" class="anchored" data-anchor-id="application-guiding-human-demonstrations-in-robotics"><span class="header-section-number">3.2.9</span> Application: Guiding Human Demonstrations in Robotics</h3>
<p>A strong approach to learning policies for robotic manipulation is imitation learning, the technique of learning behaviors from human demonstrations. In particular, interactive imitation learning allows a group of humans to contribute their own demonstrations for a task, allowing for scalable learning. However, not all groups of demonstrators are equally helpful for interactive imitation learning.</p>
<p>The ideal set of demonstrations for imitation learning would follow a single, optimal method for performing the task, which a robot could learn to mimic. Conversely, <em>multimodality</em>, the presence of multiple optimal methods in the demonstration set, is challenging for imitation learning since it has to learn from contradicting information for how to accomplish a task.</p>
<p>A common reason for multimodality is the fact that different people often subconsciously choose different paths for execution, as illustrated in <a href="#fig-multimodalexecution" class="quarto-xref">Figure&nbsp;<span>3.25</span></a>.</p>
<div id="fig-multimodalexecution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multimodalexecution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/multimodal_peg.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multimodalexecution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.25: Examples of two different ways to insert a nut onto a round peg. The orange demonstration picks up the nut from the hole while the blue demonstration picks up the nut from the side <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Gandhi et al. <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span> identifies whether demonstrations are compatible with one another and offer an active elicitation interface to guide humans to provide better demonstrations in interactive imitation learning. Their key motivation is to allow multiple users to contribute demonstrations over the course of data collection by guiding users towards compatible demonstrations.</p>
<p>To identify whether a demonstration is “compatible” with a base policy trained with prior demonstrations, the researchers measure the <em>likelihood</em> of demonstrated actions under the base policy, and the <em>novelty</em> of the visited states. Intuitively, low likelihood and low novelty demonstrations should be excluded since they represent conflicting modes of behavior on states that the robot can already handle, and are therefore incompatible. This concept of compatibility is used for filtering a new set of demonstrations and actively eliciting compatible demonstrations.</p>
<p>In the following subsections, we describe the process of estimating compatibility and active elicitation in more detal.</p>
<section id="estimating-compatiblity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="estimating-compatiblity">Estimating Compatiblity</h4>
<p>We want to define a compatibility measure <span class="math inline">\(\mathcal{M}\)</span>, that estimates the performance of policy <span class="math inline">\(\pi_{base}\)</span> that is retrained on a union of <span class="math inline">\(\mathcal{D}_{base}\)</span>, the known base dataset, and <span class="math inline">\(\mathcal{D}_{new}\)</span>, the newly collected dataset. To define this compatibility measure in a way that is easy to compute, we can use two interpretable metrics: likelihood and novelty.</p>
<p>The likelihood of actions <span class="math inline">\(a_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is measured as the negative mean squared error between actions predicted by the base policy and this proposed action:</p>
<p><span id="eq-eq3.61"><span class="math display">\[\begin{aligned}
    likelihood(s_{new}, a_{new}) = -\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2].
\end{aligned} \tag{3.73}\]</span></span></p>
<p>The novelty of the state <span class="math inline">\(s_{new}\)</span> in <span class="math inline">\(\mathcal{D}_{new}\)</span> is the standard deviation in the predicted actions under base policy:</p>
<p><span id="eq-eq3.62"><span class="math display">\[\begin{aligned}
    novelty(s_{new}) = \mathrm{Var}[\pi_{base}(s_{new})].
\end{aligned} \tag{3.74}\]</span></span></p>
<p>We can plot likelihood and novelty on a 2D plane, as shown in <a href="#fig-likelihood_novelty" class="quarto-xref">Figure&nbsp;<span>3.26</span></a>, and identify thresholds on likelihood and novelty, denoted as <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> respectively. Intuitively, demonstrations with low likelihood in low novelty states should be excluded, because this indicates that there is a conflict between the base behavior and the new demonstration due to multimodality. Note that in high novelty states, the likelihood should be disregarded because the base policy does not have a concrete idea for how to handle these states anyways so more data is needed.</p>
<div id="fig-likelihood_novelty" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-likelihood_novelty-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/likelihood_novelty.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-likelihood_novelty-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.26: Examples of plots of likelihood and novelty for compatible and incompatible operators <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
<p>The final compatibility metric, parameterized by the likelihood and novelty thresholds <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span>, is <span class="math inline">\(\mathcal{M}(\mathcal{D}_{base}, (s_{new}, a_{new})) \in [0, 1]\)</span>, defined as:</p>
<p><span id="eq-eq3.63"><span class="math display">\[\begin{aligned}
    \mathcal{M} = \begin{cases}
        1 - \min(\frac{\mathbb{E}[|| \pi_{base}(s_{new}) - a_{new} ||^2_2]}{\lambda}, 1) &amp; \text{ if } \text{novelty}(s_{new}) &lt; \eta \\
        1 &amp; \text{ otherwise }
       \end{cases}.
\end{aligned} \tag{3.75}\]</span></span></p>
<p>Note that <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> need to be specified by hand. This is accomplished by assuming the ability to collect <em>a priori incompatible</em> demonstrations to identify reasonable thresholds that remove the most datapoints in the incompatible demonstrations while keeping the most datapoints in the compatible demonstrations.</p>
</section>
<section id="case-studies-with-fixed-sets" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="case-studies-with-fixed-sets">Case Studies with Fixed Sets</h4>
<p>The researchers evaluate the utility of the compatibility metric on three tasks: placing a square nut on a square peg, placing a round nut on a round peg, and opening a drawer and placing a hammer inside. For each task, they train a base policy using a “proficient” operator’s demonstration while sampling trajectories from other operators for the new set.</p>
<p>The naive baseline is to use all datapoints while the <span class="math inline">\(\mathcal{M}\)</span>-Filtered demonstrations use the compatibility metric to filter out incompatible demonstrations. The results are presented in <a href="#tbl-m_filter_table" class="quarto-xref">Table&nbsp;<span>3.3</span></a>. As you can see, M-filtering results in equal or greater performance despite using less data than the naive baseline, demonstrating the effectiveness of compatibility-based filtering.</p>
<div id="tbl-m_filter_table" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-m_filter_table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.3: Success rates (mean/std across 3 training runs) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive) or filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered) <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
<div aria-describedby="tbl-m_filter_table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;"><strong>Square Nut</strong></td>
<td></td>
<td style="text-align: center;"><strong>Round Nut</strong></td>
<td></td>
<td style="text-align: center;"><strong>Hammer Placement</strong></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Operator</strong></td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
<td style="text-align: center;">Naive</td>
<td><span class="math inline">\(\mathcal{M}\)</span>-Filtered</td>
</tr>
<tr class="odd">
<td>Base Operator</td>
<td style="text-align: center;">38.7 (2.1)</td>
<td>-</td>
<td style="text-align: center;">13.3 (2.3)</td>
<td>-</td>
<td style="text-align: center;">24.7 (6.1)</td>
<td>-</td>
</tr>
<tr class="even">
<td>Operator 1</td>
<td style="text-align: center;">54.3 (1.5)</td>
<td>61.0 (4.4)</td>
<td style="text-align: center;">26.7 (11.7)</td>
<td>32.0 (12.2)</td>
<td style="text-align: center;">38.0 (2.0)</td>
<td>39.7 (4.6)</td>
</tr>
<tr class="odd">
<td>Operator 2</td>
<td style="text-align: center;">40.3 (5.1)</td>
<td>42.0 (2.0)</td>
<td style="text-align: center;">22.0 (7.2)</td>
<td>26.7 (5.0)</td>
<td style="text-align: center;">33.3 (3.1)</td>
<td>32.7 (6.4)</td>
</tr>
<tr class="even">
<td>Operator 3</td>
<td style="text-align: center;">37.3 (2.1)</td>
<td>42.7 (0.6)</td>
<td style="text-align: center;">17.3 (4.6)</td>
<td>18.0 (13.9)</td>
<td style="text-align: center;">8.0 (0.0)</td>
<td>12.0 (0.0)</td>
</tr>
<tr class="odd">
<td>Operator 4</td>
<td style="text-align: center;">27.3 (3.5)</td>
<td>37.3 (2.1)</td>
<td style="text-align: center;">7.3 (4.6)</td>
<td>13.3 (1.2)</td>
<td style="text-align: center;">4.0 (0.0)</td>
<td>4.0 (0.0)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-active_elicitation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-active_elicitation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/active_elicitation.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-active_elicitation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.27: The phases of the active elicitation interface: (a) initial prompting, (b) demonstrations with live feedback, and (c) corrective feedback <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="actively-eliciting-compatible-demonstrations" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="actively-eliciting-compatible-demonstrations">Actively Eliciting Compatible Demonstrations</h4>
<p>In the previous section, we assume access to a dataset that has already been collected, and we see how filtering out incompatible demonstrations helps improve performance. However, when collecting a new dataset, it would be better to ensure that operators collect compatible demonstrations from the start, allowing us to retain as much data as possible for training.</p>
<p>To actively elicit compatible demonstrations, the researchers set up a pipeline for live feedback and examples. At the start, operators are given a task specification and some episodes to practice using the robot. Then, the active elicitation process begins, as shown in <a href="#fig-active_elicitation" class="quarto-xref">Figure&nbsp;<span>3.27</span></a>. Each operator is shown some rollouts of the base policy to understand the style of the base operator. Next, the operator provides a demonstration similar to the ones they were shown. As they record their demonstrations, the interface provides online feedback, with green indicating compatible actions and red indicating incompatible actions. If the number of incompatible state-action pairs (ones where <span class="math inline">\(\mathcal{M}\)</span> is zero) exceeds 5% of the demonstration length, the demonstration is rejected. However, to provide corrective feedback, the interface shows the areas of the demonstration with the highest average incompatibility and also provides an expert demo that shows what should actually be done. Demonstrators can use this feedback to provide more compatible demonstrations moving forward.</p>
<p>This process helps improve the demonstration quality in both simulation and real experiments, as show in <a href="#tbl-active_elicitation_results" class="quarto-xref">Table&nbsp;<span>3.4</span></a>. Specifically, on the real results, active elicitation outperformed the base policy by 25% and naive data collection by 55%. Overall, active elicitation is a powerful tool to ensure that data collected for imitation learning improves the quality of the learned policy.</p>
<div id="tbl-active_elicitation_results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-active_elicitation_results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.4: Success rates (mean/std across users) for policies trained on <span class="math inline">\(\mathcal{D}_{new}\)</span> by using all the data (Naive), filtering by compatibility (<span class="math inline">\(\mathcal{M}\)</span>-Filtered), or using informed demonstration collection <span class="citation" data-cites="gandhi2022eliciting">(<a href="#ref-gandhi2022eliciting" role="doc-biblioref">Gandhi et al. 2022</a>)</span>
</figcaption>
<div aria-describedby="tbl-active_elicitation_results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Task</strong></th>
<th style="text-align: center;"><strong>Base</strong></th>
<th style="text-align: center;"><strong>Naive</strong></th>
<th style="text-align: center;"><strong>Naive + Filtered</strong></th>
<th style="text-align: center;"><strong>Informed</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Round Nut</strong></td>
<td style="text-align: center;">13.3 (2.3)</td>
<td style="text-align: center;">9.6 (4.6)</td>
<td style="text-align: center;">9.7 (4.2)</td>
<td style="text-align: center;">15.7 (6.0)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hammer Placement</strong></td>
<td style="text-align: center;">24.7 (6.1)</td>
<td style="text-align: center;">20.8 (15.7)</td>
<td style="text-align: center;">22.0 (15.5)</td>
<td style="text-align: center;">31.8 (16.3)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><span class="math inline">\(\left[ \textup{Real} \right]\)</span> Food Plating</strong></td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">30.0 (17.3)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.0 (9.6)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="limitations-and-future-work-for-active-elicitation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="limitations-and-future-work-for-active-elicitation">Limitations and Future Work for Active Elicitation</h4>
<p>A fundamental limitation of eliciting compatible demonstrations is the fact that the “base” demonstrator is considered the ground truth. When the base demonstrator specifies a preference, all other demonstrators must abide by it, even if they have strong preferences against it. For instance, when pouring milk and cereal into a bowl, different people have different preferences for what is the correct order, but active elicitation forces all demonstrators to follow the initial preference of the base operator. The researchers hope that future work can enable users to override the default demonstration set and follow a base behavior that better aligns with their preferences. This could enable multiple modes of behavior to be collected in data while only following a user’s specified preference instead of attempting to collapse all modes into a single policy.</p>
<p>Looking forward, active elicitation provides a foundation for allowing robots to query humans about the type of data needed, enabling more efficient data collection through transparency.</p>
</section>
</section>
<section id="conclusion-1" class="level3" data-number="3.2.10">
<h3 data-number="3.2.10" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">3.2.10</span> Conclusion</h3>
<p>In summary, this chapter has explored the complexities and innovations in interactive learning as applied to large models within robotics. It begins by investigating pairwise comparisons and their role in efficiently learning linear reward functions from large datasets, overcoming limitations in supervised learning. When combined with active learning techniques, these comparisons supply timely, targeted, and context-appropriate feedback, enhancing performance in time-critical applications like exoskeleton adjustments during rehabilitation.</p>
<p>We then shift to imitation learning or inverse reward learning from demonstrations, emphasizing the difficulties introduced by multimodal demonstration sets. active elicitation approaches to compile compatible demonstrations, streamlining the learning process by guiding users to provide more valuable, steady examples are incredibly promising, however, to tackling this issue. This method shows promise in refining the interactive imitation learning data collection pipeline, enabling more capable and effective robotic training.</p>
<p>Additionally, the chapter examines the integration of foundation models into robotics, highlighting the transformative innovations of R3M and Voltron. R3M’s pre-training on diverse human activities dramatically improves robotic manipulation with minimal supervision. Meanwhile, Voltron builds on these capabilities by incorporating language-driven representation learning for remarkably adaptable and nuanced robotic task performance. These models represent significant leaps in robotics while opening new frontiers for future research and applications.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="exercises"><span class="header-section-number">3.3</span> Exercises</h2>
<section id="sec-question-1-uncertainty-quantification-in-preference-learning-40-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-1-uncertainty-quantification-in-preference-learning-40-points">Question 1: Uncertainty Quantification in Preference Learning (40 points)</h3>
<p>In this question, we will explore Bayesian approaches to logistic regression in the context of preference learning using the Bradley-Terry model. We will compare different models and inference methods, including parametric linear models estimated using Metropolis-Hastings, parametric neural network models estimated using Hamiltonian Monte Carlo, and non-parametric models with Gaussian Processes. Finally, we will assess the uncertainty quantification in these models using the Expected Calibration Error (ECE).</p>
<p>Assume we have a dataset of pairwise preferences <span class="math inline">\(\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N\)</span>, where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> represents the feature difference between two items (i.e., <span class="math inline">\(x_i = e^{(i)}_1 - e^{(i)}_2\)</span> for embeddings <span class="math inline">\(e^{(i)}_1\)</span> and <span class="math inline">\(e^{(i)}_2\)</span>), and <span class="math inline">\(y_i \in \{0, 1\}\)</span> indicates the preference (<span class="math inline">\(y_i = 1\)</span> if item 1 is preferred over item 2 in the <span class="math inline">\(i\)</span>-th pair).</p>
<p>The likelihood of observing <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span> and model parameters <span class="math inline">\(\theta\)</span> is given by the logistic function:</p>
<p><span class="math display">\[P(y_i = 1 | x_i, \theta) = \sigma(x_i^\top \theta) = \frac{1}{1 + e^{-x_i^\top \theta}}.\]</span></p>
<p>We will adopt a Bayesian approach by placing priors on the model parameters and using Markov Chain Monte Carlo (MCMC) methods to estimate the posterior distributions.</p>
<ol type="a">
<li><p><strong>Uncertainty Quantification and Expected Calibration Error (11 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 2 point)</strong>. Spend some time reading <a href="https://tinyurl.com/m77mk9c" class="uri">https://tinyurl.com/m77mk9c</a>. Explain what the Expected Calibration Error (ECE) measures and why it is important for assessing uncertainty quantification in probabilistic models.</p></li>
<li><p><strong>(Coding, 6 points)</strong>. In <code>uncertainty_quantification/ece.py</code>, implement the ECE using the formula <span class="math display">\[\text{ECE} = \sum_{k=1}^K \frac{n_k}{N} \left| \text{acc}(B_k) - \text{conf}(B_k) \right|,\]</span> where <span class="math inline">\(n_k\)</span> is the number of samples in bin <span class="math inline">\(B_k\)</span>, <span class="math inline">\(N\)</span> is the total number of samples, <span class="math inline">\(\text{acc}(B_k)\)</span> is the accuracy in bin <span class="math inline">\(B_k\)</span>, and <span class="math inline">\(\text{conf}(B_k)\)</span> is the average confidence in bin <span class="math inline">\(B_k\)</span>.</p></li>
<li><p><strong>(Written, 3 point)</strong>. After doing parts (b), (c), and (d), compare the ECE scores and reliability diagrams of the 3 models. Which model(s) provide the best uncertainty quantification? Discuss possible reasons for the observed differences.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="f214f558" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="kw">def</span> expected_calibration_error(probs, labels, model_name, n_bins<span class="op">=</span><span class="dv">20</span>, n_ticks<span class="op">=</span><span class="dv">10</span>, plot<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-5"><a href="#cb4-5"></a>    <span class="co">"""</span></span>
<span id="cb4-6"><a href="#cb4-6"></a><span class="co">    Computes the Expected Calibration Error (ECE) for a model and plots a refined reliability diagram</span></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="co">    with confidence histogram and additional calibration statistics.</span></span>
<span id="cb4-8"><a href="#cb4-8"></a><span class="co">    </span></span>
<span id="cb4-9"><a href="#cb4-9"></a><span class="co">    Args:</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="co">    - probs (np.array): Array of predicted probabilities for the positive class (for binary classification).</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">    - labels (np.array): Array of true labels (0 or 1).</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="co">    - model_name (str): Name of the model for labeling the plot.</span></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="co">    - n_bins (int): Number of bins to divide the probability interval [0,1] into.</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co">    - n_ticks (int): Number of ticks to show along the x-axis.</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">    - plot (bool): If True, generates the reliability plot; otherwise, only computes ECE.</span></span>
<span id="cb4-16"><a href="#cb4-16"></a></span>
<span id="cb4-17"><a href="#cb4-17"></a><span class="co">    Returns:</span></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">    - float: Computed ECE value.</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">    """</span></span>
<span id="cb4-20"><a href="#cb4-20"></a>    </span>
<span id="cb4-21"><a href="#cb4-21"></a>    <span class="co"># Ensure probabilities are in the range [0, 1]</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    <span class="cf">assert</span> np.<span class="bu">all</span>((probs <span class="op">&gt;=</span> <span class="dv">0</span>) <span class="op">&amp;</span> (probs <span class="op">&lt;=</span> <span class="dv">1</span>)), <span class="st">"Probabilities must be in the range [0, 1]"</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>    </span>
<span id="cb4-24"><a href="#cb4-24"></a>    <span class="co"># Initialize bin edges, centers, and storage for accuracy, confidence, and counts</span></span>
<span id="cb4-25"><a href="#cb4-25"></a>    bin_edges <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n_bins <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-26"><a href="#cb4-26"></a>    bin_centers <span class="op">=</span> (bin_edges[:<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> bin_edges[<span class="dv">1</span>:]) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-27"><a href="#cb4-27"></a>    bar_width <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> n_bins</span>
<span id="cb4-28"><a href="#cb4-28"></a></span>
<span id="cb4-29"><a href="#cb4-29"></a>    accs <span class="op">=</span> np.zeros(n_bins)</span>
<span id="cb4-30"><a href="#cb4-30"></a>    confs <span class="op">=</span> np.zeros(n_bins)</span>
<span id="cb4-31"><a href="#cb4-31"></a>    bin_counts <span class="op">=</span> np.zeros(n_bins)</span>
<span id="cb4-32"><a href="#cb4-32"></a></span>
<span id="cb4-33"><a href="#cb4-33"></a>    <span class="co"># Populate bin statistics: accuracy, confidence, and count</span></span>
<span id="cb4-34"><a href="#cb4-34"></a>    <span class="co"># YOUR CODE HERE (~7 lines)</span></span>
<span id="cb4-35"><a href="#cb4-35"></a>    <span class="co"># Loop over each bin and:</span></span>
<span id="cb4-36"><a href="#cb4-36"></a>    <span class="co"># - Find indices of probabilities that fall within the bin.</span></span>
<span id="cb4-37"><a href="#cb4-37"></a>    <span class="co"># - Count the number of items in the bin.</span></span>
<span id="cb4-38"><a href="#cb4-38"></a>    <span class="co"># - Calculate the accuracy (average of true labels) within the bin.</span></span>
<span id="cb4-39"><a href="#cb4-39"></a>    <span class="co"># - Calculate the confidence (average of predicted probabilities) within the bin.</span></span>
<span id="cb4-40"><a href="#cb4-40"></a>    <span class="cf">pass</span> </span>
<span id="cb4-41"><a href="#cb4-41"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-42"><a href="#cb4-42"></a>    </span>
<span id="cb4-43"><a href="#cb4-43"></a>    <span class="co"># Compute ECE: weighted average of |accuracy - confidence| across bins</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>    <span class="co"># YOUR CODE HERE (1 line)</span></span>
<span id="cb4-45"><a href="#cb4-45"></a>    <span class="co"># - Use the bin counts to calculate a weighted average of the differences between accuracy and confidence.</span></span>
<span id="cb4-46"><a href="#cb4-46"></a>    ece_value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-48"><a href="#cb4-48"></a>    </span>
<span id="cb4-49"><a href="#cb4-49"></a>    <span class="co"># Return only ECE if plot is not required</span></span>
<span id="cb4-50"><a href="#cb4-50"></a>    <span class="cf">if</span> <span class="kw">not</span> plot:</span>
<span id="cb4-51"><a href="#cb4-51"></a>        <span class="cf">return</span> ece_value</span>
<span id="cb4-52"><a href="#cb4-52"></a></span>
<span id="cb4-53"><a href="#cb4-53"></a>    <span class="co"># Compute average confidence and accuracy for reference lines</span></span>
<span id="cb4-54"><a href="#cb4-54"></a>    avg_confidence <span class="op">=</span> np.mean(probs)</span>
<span id="cb4-55"><a href="#cb4-55"></a>    avg_accuracy <span class="op">=</span> np.mean(labels)</span>
<span id="cb4-56"><a href="#cb4-56"></a>    </span>
<span id="cb4-57"><a href="#cb4-57"></a>    <span class="co"># Create reliability diagram and histogram</span></span>
<span id="cb4-58"><a href="#cb4-58"></a>    fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, gridspec_kw<span class="op">=</span>{<span class="st">'height_ratios'</span>: [<span class="dv">3</span>, <span class="dv">1</span>]}, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">10</span>))</span>
<span id="cb4-59"><a href="#cb4-59"></a>    </span>
<span id="cb4-60"><a href="#cb4-60"></a>    <span class="co"># Reliability diagram (top plot)</span></span>
<span id="cb4-61"><a href="#cb4-61"></a>    ax1.plot([<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">0</span>, <span class="dv">1</span>], <span class="st">'k--'</span>, label<span class="op">=</span><span class="st">'Perfect Calibration'</span>)</span>
<span id="cb4-62"><a href="#cb4-62"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_bins):</span>
<span id="cb4-63"><a href="#cb4-63"></a>        <span class="co"># Draw the gap bar starting from the diagonal line (perfect calibration)</span></span>
<span id="cb4-64"><a href="#cb4-64"></a>        ax1.bar(bin_centers[i], <span class="bu">abs</span>(accs[i] <span class="op">-</span> confs[i]), width<span class="op">=</span>bar_width, bottom<span class="op">=</span><span class="bu">min</span>(accs[i], confs[i]), </span>
<span id="cb4-65"><a href="#cb4-65"></a>                color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, label<span class="op">=</span><span class="st">'Accuracy-Confidence Gap'</span> <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb4-66"><a href="#cb4-66"></a>        <span class="co"># Draw the accuracy bar as a small black line on top of the gap bar</span></span>
<span id="cb4-67"><a href="#cb4-67"></a>        ax1.plot([bin_centers[i] <span class="op">-</span> bar_width <span class="op">/</span> <span class="dv">2</span>, bin_centers[i] <span class="op">+</span> bar_width <span class="op">/</span> <span class="dv">2</span>], </span>
<span id="cb4-68"><a href="#cb4-68"></a>                 [accs[i], accs[i]], color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-69"><a href="#cb4-69"></a></span>
<span id="cb4-70"><a href="#cb4-70"></a>    <span class="co"># Add a black line as a sample for accuracy in the legend</span></span>
<span id="cb4-71"><a href="#cb4-71"></a>    ax1.plot([], [], color<span class="op">=</span><span class="st">'black'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Accuracy Marker'</span>)</span>
<span id="cb4-72"><a href="#cb4-72"></a></span>
<span id="cb4-73"><a href="#cb4-73"></a>    ax1.set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-74"><a href="#cb4-74"></a>    ax1.set_ylim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-75"><a href="#cb4-75"></a>    ax1.set_ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb4-76"><a href="#cb4-76"></a>    ax1.set_title(<span class="ss">f'</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ch">\n</span><span class="ss">ECE=</span><span class="sc">{</span>ece_value<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb4-77"><a href="#cb4-77"></a>    ax1.legend()</span>
<span id="cb4-78"><a href="#cb4-78"></a></span>
<span id="cb4-79"><a href="#cb4-79"></a>    <span class="co"># Set tick marks based on `n_ticks` evenly spaced along the x-axis</span></span>
<span id="cb4-80"><a href="#cb4-80"></a>    tick_positions <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, n_ticks <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-81"><a href="#cb4-81"></a>    ax1.set_xticks(tick_positions)</span>
<span id="cb4-82"><a href="#cb4-82"></a>    ax2.set_xticks(tick_positions)</span>
<span id="cb4-83"><a href="#cb4-83"></a>    ax1.set_xticklabels([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.2f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> tick_positions])</span>
<span id="cb4-84"><a href="#cb4-84"></a>    ax2.set_xticklabels([<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.2f}</span><span class="ss">'</span> <span class="cf">for</span> x <span class="kw">in</span> tick_positions])</span>
<span id="cb4-85"><a href="#cb4-85"></a></span>
<span id="cb4-86"><a href="#cb4-86"></a>    <span class="co"># Confidence histogram with average markers</span></span>
<span id="cb4-87"><a href="#cb4-87"></a>    ax2.bar(bin_centers, bin_counts, width<span class="op">=</span>bar_width, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb4-88"><a href="#cb4-88"></a>    ax2.axvline(x<span class="op">=</span>avg_confidence, color<span class="op">=</span><span class="st">'gray'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Avg. confidence'</span>)</span>
<span id="cb4-89"><a href="#cb4-89"></a>    ax2.axvline(x<span class="op">=</span>avg_accuracy, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Avg. accuracy'</span>)</span>
<span id="cb4-90"><a href="#cb4-90"></a>    ax2.set_xlim(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-91"><a href="#cb4-91"></a>    ax2.set_xlabel(<span class="st">'Confidence'</span>)</span>
<span id="cb4-92"><a href="#cb4-92"></a>    ax2.set_ylabel(<span class="st">'Count'</span>)</span>
<span id="cb4-93"><a href="#cb4-93"></a>    ax2.legend()</span>
<span id="cb4-94"><a href="#cb4-94"></a></span>
<span id="cb4-95"><a href="#cb4-95"></a>    plt.tight_layout()</span>
<span id="cb4-96"><a href="#cb4-96"></a>    plt.show()</span>
<span id="cb4-97"><a href="#cb4-97"></a>    </span>
<span id="cb4-98"><a href="#cb4-98"></a>    <span class="cf">return</span> ece_value</span>
<span id="cb4-99"><a href="#cb4-99"></a></span>
<span id="cb4-100"><a href="#cb4-100"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-101"><a href="#cb4-101"></a>    <span class="co"># Test with random probabilities and labels</span></span>
<span id="cb4-102"><a href="#cb4-102"></a>    probs <span class="op">=</span> np.random.rand(<span class="dv">10000</span>)  <span class="co"># Random probabilities between 0 and 1</span></span>
<span id="cb4-103"><a href="#cb4-103"></a>    labels <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, (probs <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb4-104"><a href="#cb4-104"></a></span>
<span id="cb4-105"><a href="#cb4-105"></a>    <span class="co"># Run the function and display the result</span></span>
<span id="cb4-106"><a href="#cb4-106"></a>    ece_value <span class="op">=</span> expected_calibration_error(probs, labels, <span class="st">"Test Model"</span>, plot<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-107"><a href="#cb4-107"></a>    <span class="bu">print</span>(<span class="ss">f"ECE Value: </span><span class="sc">{</span>ece_value<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<ol start="2" type="a">
<li><p><strong>Parametric Linear Model Estimated Using Metropolis-Hastings (11 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 3 points)</strong>. Assume a prior on <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(\theta \sim \mathcal{N}(0, \sigma^2 I)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the variance and <span class="math inline">\(I\)</span> is the identity matrix. Derive the expression for the posterior distribution <span class="math inline">\(P(\theta | \mathcal{D})\)</span> up to a normalization constant.</p></li>
<li><p><strong>(Coding, 6 points)</strong>. Implement the Metropolis-Hastings algorithm to sample from the posterior distribution of <span class="math inline">\(\theta\)</span> in <code>uncertainty_quantification/metropolis.py</code>.</p></li>
<li><p><strong>(Written, 2 points)</strong>. Discuss how you chose the proposal variance <span class="math inline">\(\tau^2\)</span> and the number of iterations <span class="math inline">\(T\)</span> and <span class="math inline">\(T_{\text{burn-in}}\)</span>. How did these choices affect the convergence and mixing of your MCMC chain?</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="fd873c81" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">from</span> ece <span class="im">import</span> expected_calibration_error</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="co"># Load training and testing data</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>x_train <span class="op">=</span> torch.tensor(np.load(<span class="st">'../data/differences_train.npy'</span>))</span>
<span id="cb5-9"><a href="#cb5-9"></a>x_test <span class="op">=</span> torch.tensor(np.load(<span class="st">'../data/differences_test.npy'</span>))</span>
<span id="cb5-10"><a href="#cb5-10"></a>y_train <span class="op">=</span> torch.tensor(np.load(<span class="st">'../data/labels_train.npy'</span>))</span>
<span id="cb5-11"><a href="#cb5-11"></a>y_test <span class="op">=</span> torch.tensor(np.load(<span class="st">'../data/labels_test.npy'</span>))</span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co"># Likelihood function for logistic regression (per data point)</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="kw">def</span> likelihood(theta, x, y):</span>
<span id="cb5-15"><a href="#cb5-15"></a>    <span class="co">"""</span></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co">    Computes the likelihood of the data given the logistic regression parameters.</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">    </span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">    Args:</span></span>
<span id="cb5-19"><a href="#cb5-19"></a><span class="co">    - theta (torch.Tensor): Model parameters.</span></span>
<span id="cb5-20"><a href="#cb5-20"></a><span class="co">    - x (torch.Tensor): Input data.</span></span>
<span id="cb5-21"><a href="#cb5-21"></a><span class="co">    - y (torch.Tensor): True labels.</span></span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a><span class="co">    Returns:</span></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="co">    - torch.Tensor: Likelihood values for each data point.</span></span>
<span id="cb5-25"><a href="#cb5-25"></a><span class="co">    """</span></span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span class="co"># YOUR CODE HERE (~3 lines)</span></span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="co"># Calculate logits as the linear combination of inputs and parameters.</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>    <span class="co"># Use the sigmoid function to compute the probability of the positive class.</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>    <span class="cf">pass</span></span>
<span id="cb5-30"><a href="#cb5-30"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb5-31"><a href="#cb5-31"></a></span>
<span id="cb5-32"><a href="#cb5-32"></a><span class="co"># Prior probability (theta ~ N(0, I)) - only depends on theta, not per sample</span></span>
<span id="cb5-33"><a href="#cb5-33"></a><span class="kw">def</span> prior(theta, sigma):</span>
<span id="cb5-34"><a href="#cb5-34"></a>    <span class="co">"""</span></span>
<span id="cb5-35"><a href="#cb5-35"></a><span class="co">    Computes the prior probability of theta under a Gaussian distribution with variance sigma^2.</span></span>
<span id="cb5-36"><a href="#cb5-36"></a></span>
<span id="cb5-37"><a href="#cb5-37"></a><span class="co">    Args:</span></span>
<span id="cb5-38"><a href="#cb5-38"></a><span class="co">    - theta (torch.Tensor): Model parameters.</span></span>
<span id="cb5-39"><a href="#cb5-39"></a><span class="co">    - sigma (float): Standard deviation of the prior distribution.</span></span>
<span id="cb5-40"><a href="#cb5-40"></a></span>
<span id="cb5-41"><a href="#cb5-41"></a><span class="co">    Returns:</span></span>
<span id="cb5-42"><a href="#cb5-42"></a><span class="co">    - torch.Tensor: Prior probability value.</span></span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="co">    """</span></span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb5-45"><a href="#cb5-45"></a>    <span class="co"># Implement Gaussian prior with zero mean and identity covariance.</span></span>
<span id="cb5-46"><a href="#cb5-46"></a>    <span class="co"># Note that the normalization constant is not needed for Metropolis-Hastings.</span></span>
<span id="cb5-47"><a href="#cb5-47"></a>    <span class="cf">pass</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb5-49"><a href="#cb5-49"></a></span>
<span id="cb5-50"><a href="#cb5-50"></a><span class="co"># Metropolis-Hastings sampler</span></span>
<span id="cb5-51"><a href="#cb5-51"></a><span class="kw">def</span> metropolis_hastings(x, y, num_samples, burn_in, tau, sigma):</span>
<span id="cb5-52"><a href="#cb5-52"></a>    <span class="co">"""</span></span>
<span id="cb5-53"><a href="#cb5-53"></a><span class="co">    Runs the Metropolis-Hastings algorithm to sample from the posterior distribution.</span></span>
<span id="cb5-54"><a href="#cb5-54"></a></span>
<span id="cb5-55"><a href="#cb5-55"></a><span class="co">    Args:</span></span>
<span id="cb5-56"><a href="#cb5-56"></a><span class="co">    - x (torch.Tensor): Input data.</span></span>
<span id="cb5-57"><a href="#cb5-57"></a><span class="co">    - y (torch.Tensor): True labels.</span></span>
<span id="cb5-58"><a href="#cb5-58"></a><span class="co">    - num_samples (int): Total number of samples to draw.</span></span>
<span id="cb5-59"><a href="#cb5-59"></a><span class="co">    - burn_in (int): Number of initial samples to discard.</span></span>
<span id="cb5-60"><a href="#cb5-60"></a><span class="co">    - tau (float): Proposal standard deviation.</span></span>
<span id="cb5-61"><a href="#cb5-61"></a><span class="co">    - sigma (float): Prior standard deviation.</span></span>
<span id="cb5-62"><a href="#cb5-62"></a></span>
<span id="cb5-63"><a href="#cb5-63"></a><span class="co">    Returns:</span></span>
<span id="cb5-64"><a href="#cb5-64"></a><span class="co">    - torch.Tensor: Collected samples post burn-in.</span></span>
<span id="cb5-65"><a href="#cb5-65"></a><span class="co">    - float: Acceptance ratio.</span></span>
<span id="cb5-66"><a href="#cb5-66"></a><span class="co">    """</span></span>
<span id="cb5-67"><a href="#cb5-67"></a>    <span class="co"># Initialize theta (starting point of the chain) and containers for samples and acceptance count</span></span>
<span id="cb5-68"><a href="#cb5-68"></a>    theta <span class="op">=</span> torch.zeros(x.shape[<span class="dv">1</span>])</span>
<span id="cb5-69"><a href="#cb5-69"></a>    samples <span class="op">=</span> []</span>
<span id="cb5-70"><a href="#cb5-70"></a>    acceptances <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-71"><a href="#cb5-71"></a>    </span>
<span id="cb5-72"><a href="#cb5-72"></a>    <span class="co"># Run the Metropolis-Hastings algorithm</span></span>
<span id="cb5-73"><a href="#cb5-73"></a>    <span class="cf">for</span> t <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_samples), desc<span class="op">=</span><span class="st">"MCMC Iteration"</span>):</span>
<span id="cb5-74"><a href="#cb5-74"></a>        <span class="co"># YOUR CODE HERE (~12-16 lines)</span></span>
<span id="cb5-75"><a href="#cb5-75"></a>        <span class="co"># 1. Propose new theta from the proposal distribution (e.g., Gaussian around current theta).</span></span>
<span id="cb5-76"><a href="#cb5-76"></a>        <span class="co"># 2. Compute prior and likelihood for current and proposed theta</span></span>
<span id="cb5-77"><a href="#cb5-77"></a>        <span class="co"># 3. Calculate the acceptance ratio as the product of likelihood and prior ratios.</span></span>
<span id="cb5-78"><a href="#cb5-78"></a>        <span class="co"># 4. Accept or reject the proposal based on the acceptance probability.</span></span>
<span id="cb5-79"><a href="#cb5-79"></a>        <span class="co"># 5. Store the sample after the burn-in period</span></span>
<span id="cb5-80"><a href="#cb5-80"></a>        <span class="cf">pass</span></span>
<span id="cb5-81"><a href="#cb5-81"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb5-82"><a href="#cb5-82"></a>    </span>
<span id="cb5-83"><a href="#cb5-83"></a>    <span class="cf">return</span> torch.stack(samples), acceptances <span class="op">/</span> num_samples</span>
<span id="cb5-84"><a href="#cb5-84"></a></span>
<span id="cb5-85"><a href="#cb5-85"></a><span class="co"># Run Metropolis-Hastings on training data</span></span>
<span id="cb5-86"><a href="#cb5-86"></a>num_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb5-87"><a href="#cb5-87"></a>burn_in <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-88"><a href="#cb5-88"></a>tau <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># Proposal variance (tune this for convergence)</span></span>
<span id="cb5-89"><a href="#cb5-89"></a>sigma <span class="op">=</span> <span class="fl">2.0</span>  <span class="co"># Prior variance</span></span>
<span id="cb5-90"><a href="#cb5-90"></a></span>
<span id="cb5-91"><a href="#cb5-91"></a><span class="co"># Collect samples and compute acceptance ratio</span></span>
<span id="cb5-92"><a href="#cb5-92"></a>samples, acceptance_ratio <span class="op">=</span> metropolis_hastings(x_train, y_train, num_samples<span class="op">=</span>num_samples, burn_in<span class="op">=</span>burn_in, tau<span class="op">=</span>tau, sigma<span class="op">=</span>sigma)</span>
<span id="cb5-93"><a href="#cb5-93"></a>averaged_weights <span class="op">=</span> samples.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-94"><a href="#cb5-94"></a><span class="bu">print</span>(<span class="ss">f'Predicted weights: </span><span class="sc">{</span>averaged_weights<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-95"><a href="#cb5-95"></a><span class="bu">print</span>(<span class="ss">f'Acceptance Ratio: </span><span class="sc">{</span>acceptance_ratio<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-96"><a href="#cb5-96"></a></span>
<span id="cb5-97"><a href="#cb5-97"></a><span class="co"># Evaluate accuracy on training set</span></span>
<span id="cb5-98"><a href="#cb5-98"></a>train_predictions <span class="op">=</span> (x_train <span class="op">@</span> averaged_weights <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb5-99"><a href="#cb5-99"></a>train_acc <span class="op">=</span> (train_predictions <span class="op">==</span> y_train).<span class="bu">float</span>().mean()</span>
<span id="cb5-100"><a href="#cb5-100"></a><span class="bu">print</span>(<span class="ss">f'Train Accuracy: </span><span class="sc">{</span>train_acc<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-101"><a href="#cb5-101"></a></span>
<span id="cb5-102"><a href="#cb5-102"></a><span class="co"># Evaluate accuracy on testing set</span></span>
<span id="cb5-103"><a href="#cb5-103"></a>test_predictions <span class="op">=</span> (x_test <span class="op">@</span> averaged_weights <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb5-104"><a href="#cb5-104"></a>acc <span class="op">=</span> (test_predictions <span class="op">==</span> y_test).<span class="bu">float</span>().mean()</span>
<span id="cb5-105"><a href="#cb5-105"></a><span class="bu">print</span>(<span class="ss">f'Test Accuracy: </span><span class="sc">{</span>acc<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-106"><a href="#cb5-106"></a></span>
<span id="cb5-107"><a href="#cb5-107"></a><span class="co"># Compute expected calibration error on testing set</span></span>
<span id="cb5-108"><a href="#cb5-108"></a>expected_calibration_error(torch.sigmoid(x_test <span class="op">@</span> averaged_weights).numpy(), y_test.numpy(), model_name<span class="op">=</span><span class="st">"Metropolis-Hastings"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<ol start="3" type="a">
<li><p><strong>Parametric Neural Network Model Estimated Using Hamiltonian Monte Carlo (11 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 2 points)</strong>. Explain why Hamiltonian Monte Carlo (HMC) is suitable for sampling from the posterior distribution of neural network parameters compared to Metropolis-Hastings.</p></li>
<li><p><strong>(Coding, 7 points)</strong>. Implement HMC to sample from the posterior distribution of the parameters <span class="math inline">\(\theta\)</span> of a neural network <span class="math inline">\(f(x; \theta)\)</span> used for preference prediction in <code>uncertainty_quantification/hmc_nn.py</code>. This will require a GPU and take around 5 minutes on it!</p></li>
<li><p><strong>(Written, 2 points)</strong>. Briefly describe the performance of the HMC and Metropolis-Hastings models and provide the accuracy numbers.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="f4b12037" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Use a GPU when running this file! JAX should automatically default to GPU.</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">import</span> jax.numpy <span class="im">as</span> np</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">import</span> numpyro</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="im">import</span> numpyro.distributions <span class="im">as</span> dist</span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="im">from</span> numpyro.infer <span class="im">import</span> MCMC, NUTS</span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="im">from</span> ece <span class="im">import</span> expected_calibration_error</span>
<span id="cb6-8"><a href="#cb6-8"></a></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co"># DO NOT CHANGE! This function can be ignored.</span></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="kw">def</span> set_numpyro(new_sampler):</span>
<span id="cb6-11"><a href="#cb6-11"></a>    numpyro.sample <span class="op">=</span> new_sampler</span>
<span id="cb6-12"><a href="#cb6-12"></a></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co"># Define the neural network model with one hidden layer</span></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="kw">def</span> nn_model(x_data, y_data, hidden_dim<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb6-15"><a href="#cb6-15"></a>    <span class="co">"""</span></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co">    Defines a Bayesian neural network with one hidden layer.</span></span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a><span class="co">    Args:</span></span>
<span id="cb6-19"><a href="#cb6-19"></a><span class="co">    - x_data (np.array): Input data.</span></span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="co">    - y_data (np.array): Target labels.</span></span>
<span id="cb6-21"><a href="#cb6-21"></a><span class="co">    - hidden_dim (int): Number of units in the hidden layer.</span></span>
<span id="cb6-22"><a href="#cb6-22"></a></span>
<span id="cb6-23"><a href="#cb6-23"></a><span class="co">    Returns:</span></span>
<span id="cb6-24"><a href="#cb6-24"></a><span class="co">    - hidden_activations: Activations from the hidden layer.</span></span>
<span id="cb6-25"><a href="#cb6-25"></a><span class="co">    - logits: Logits for the output layer.</span></span>
<span id="cb6-26"><a href="#cb6-26"></a><span class="co">    """</span></span>
<span id="cb6-27"><a href="#cb6-27"></a>    input_dim <span class="op">=</span> x_data.shape[<span class="dv">1</span>]</span>
<span id="cb6-28"><a href="#cb6-28"></a>    </span>
<span id="cb6-29"><a href="#cb6-29"></a>    <span class="co"># Prior over the weights and biases for the hidden layer</span></span>
<span id="cb6-30"><a href="#cb6-30"></a>    w_hidden <span class="op">=</span> numpyro.sample(<span class="st">'w_hidden'</span>, dist.Normal(np.zeros((input_dim, hidden_dim)), np.ones((input_dim, hidden_dim))))</span>
<span id="cb6-31"><a href="#cb6-31"></a>    b_hidden <span class="op">=</span> numpyro.sample(<span class="st">'b_hidden'</span>, dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))</span>
<span id="cb6-32"><a href="#cb6-32"></a>    </span>
<span id="cb6-33"><a href="#cb6-33"></a>    <span class="co"># Compute the hidden layer activations using ReLU</span></span>
<span id="cb6-34"><a href="#cb6-34"></a>    <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb6-35"><a href="#cb6-35"></a>    <span class="co"># Implement the hidden layer computation, applying a ReLU activation.</span></span>
<span id="cb6-36"><a href="#cb6-36"></a>    <span class="cf">pass</span></span>
<span id="cb6-37"><a href="#cb6-37"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb6-38"><a href="#cb6-38"></a>    </span>
<span id="cb6-39"><a href="#cb6-39"></a>    <span class="co"># Prior over the weights and biases for the output layer</span></span>
<span id="cb6-40"><a href="#cb6-40"></a>    w_output <span class="op">=</span> numpyro.sample(<span class="st">'w_output'</span>, dist.Normal(np.zeros(hidden_dim), np.ones(hidden_dim)))</span>
<span id="cb6-41"><a href="#cb6-41"></a>    b_output <span class="op">=</span> numpyro.sample(<span class="st">'b_output'</span>, dist.Normal(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb6-42"><a href="#cb6-42"></a>    </span>
<span id="cb6-43"><a href="#cb6-43"></a>    <span class="co"># Compute the logits for the output layer</span></span>
<span id="cb6-44"><a href="#cb6-44"></a>    <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb6-45"><a href="#cb6-45"></a>    <span class="co"># Calculate the logits as the linear combination of hidden activations and output layer weights.</span></span>
<span id="cb6-46"><a href="#cb6-46"></a>    <span class="cf">pass</span></span>
<span id="cb6-47"><a href="#cb6-47"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a>    <span class="co"># Likelihood (Bernoulli likelihood with logits)</span></span>
<span id="cb6-50"><a href="#cb6-50"></a>    numpyro.sample(<span class="st">'obs'</span>, dist.Bernoulli(logits<span class="op">=</span>logits), obs<span class="op">=</span>y_data)</span>
<span id="cb6-51"><a href="#cb6-51"></a>    <span class="cf">return</span> hidden_activations, logits</span>
<span id="cb6-52"><a href="#cb6-52"></a></span>
<span id="cb6-53"><a href="#cb6-53"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb6-54"><a href="#cb6-54"></a>    <span class="co">"""Helper function to compute the sigmoid of x."""</span></span>
<span id="cb6-55"><a href="#cb6-55"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb6-58"><a href="#cb6-58"></a>    <span class="co"># Load training and testing data</span></span>
<span id="cb6-59"><a href="#cb6-59"></a>    x_train <span class="op">=</span> np.load(<span class="st">'../data/differences_train.npy'</span>)</span>
<span id="cb6-60"><a href="#cb6-60"></a>    x_test <span class="op">=</span> np.load(<span class="st">'../data/differences_test.npy'</span>)</span>
<span id="cb6-61"><a href="#cb6-61"></a>    y_train <span class="op">=</span> np.load(<span class="st">'../data/labels_train.npy'</span>)</span>
<span id="cb6-62"><a href="#cb6-62"></a>    y_test <span class="op">=</span> np.load(<span class="st">'../data/labels_test.npy'</span>)</span>
<span id="cb6-63"><a href="#cb6-63"></a></span>
<span id="cb6-64"><a href="#cb6-64"></a>    <span class="co"># HMC Sampler Configuration</span></span>
<span id="cb6-65"><a href="#cb6-65"></a>    hmc_kernel <span class="op">=</span> NUTS(nn_model)</span>
<span id="cb6-66"><a href="#cb6-66"></a></span>
<span id="cb6-67"><a href="#cb6-67"></a>    <span class="co"># Running HMC with the MCMC interface in NumPyro</span></span>
<span id="cb6-68"><a href="#cb6-68"></a>    num_samples <span class="op">=</span> <span class="dv">200</span>  <span class="co"># Number of samples</span></span>
<span id="cb6-69"><a href="#cb6-69"></a>    warmup_steps <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Number of burn-in steps</span></span>
<span id="cb6-70"><a href="#cb6-70"></a>    rng_key <span class="op">=</span> random.PRNGKey(<span class="dv">0</span>)  <span class="co"># Random seed</span></span>
<span id="cb6-71"><a href="#cb6-71"></a></span>
<span id="cb6-72"><a href="#cb6-72"></a>    <span class="co"># MCMC object with HMC kernel</span></span>
<span id="cb6-73"><a href="#cb6-73"></a>    mcmc <span class="op">=</span> MCMC(hmc_kernel, num_samples<span class="op">=</span>num_samples, num_warmup<span class="op">=</span>warmup_steps)</span>
<span id="cb6-74"><a href="#cb6-74"></a>    mcmc.run(rng_key, x_train, y_train)</span>
<span id="cb6-75"><a href="#cb6-75"></a></span>
<span id="cb6-76"><a href="#cb6-76"></a>    <span class="co"># Get the sampled weights (theta samples)</span></span>
<span id="cb6-77"><a href="#cb6-77"></a>    samples <span class="op">=</span> mcmc.get_samples()</span>
<span id="cb6-78"><a href="#cb6-78"></a></span>
<span id="cb6-79"><a href="#cb6-79"></a>    <span class="co"># Extract the weight samples</span></span>
<span id="cb6-80"><a href="#cb6-80"></a>    w_hidden_samples <span class="op">=</span> samples[<span class="st">'w_hidden'</span>]</span>
<span id="cb6-81"><a href="#cb6-81"></a>    b_hidden_samples <span class="op">=</span> samples[<span class="st">'b_hidden'</span>]</span>
<span id="cb6-82"><a href="#cb6-82"></a>    w_output_samples <span class="op">=</span> samples[<span class="st">'w_output'</span>]</span>
<span id="cb6-83"><a href="#cb6-83"></a>    b_output_samples <span class="op">=</span> samples[<span class="st">'b_output'</span>]</span>
<span id="cb6-84"><a href="#cb6-84"></a></span>
<span id="cb6-85"><a href="#cb6-85"></a>    <span class="co"># Compute the averaged weights and biases</span></span>
<span id="cb6-86"><a href="#cb6-86"></a>    w_hidden_mean <span class="op">=</span> np.mean(w_hidden_samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-87"><a href="#cb6-87"></a>    b_hidden_mean <span class="op">=</span> np.mean(b_hidden_samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-88"><a href="#cb6-88"></a>    w_output_mean <span class="op">=</span> np.mean(w_output_samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-89"><a href="#cb6-89"></a>    b_output_mean <span class="op">=</span> np.mean(b_output_samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-90"><a href="#cb6-90"></a></span>
<span id="cb6-91"><a href="#cb6-91"></a>    <span class="co"># Forward pass through the network for testing set</span></span>
<span id="cb6-92"><a href="#cb6-92"></a>    <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb6-93"><a href="#cb6-93"></a>    <span class="co"># Compute hidden layer activations and logits for the test set using the mean weights and biases.</span></span>
<span id="cb6-94"><a href="#cb6-94"></a>    <span class="cf">pass</span></span>
<span id="cb6-95"><a href="#cb6-95"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-96"><a href="#cb6-96"></a>    test_predictions <span class="op">=</span> test_logits <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb6-97"><a href="#cb6-97"></a>    test_accuracy <span class="op">=</span> np.mean(test_predictions <span class="op">==</span> y_test)</span>
<span id="cb6-98"><a href="#cb6-98"></a>    <span class="bu">print</span>(<span class="ss">f'Test Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-99"><a href="#cb6-99"></a></span>
<span id="cb6-100"><a href="#cb6-100"></a>    <span class="co"># Forward pass through the network for training set</span></span>
<span id="cb6-101"><a href="#cb6-101"></a>    <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb6-102"><a href="#cb6-102"></a>    <span class="co"># Compute hidden layer activations and logits for the training set.</span></span>
<span id="cb6-103"><a href="#cb6-103"></a>    <span class="cf">pass</span></span>
<span id="cb6-104"><a href="#cb6-104"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-105"><a href="#cb6-105"></a>    train_predictions <span class="op">=</span> train_logits <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb6-106"><a href="#cb6-106"></a>    train_accuracy <span class="op">=</span> np.mean(train_predictions <span class="op">==</span> y_train)</span>
<span id="cb6-107"><a href="#cb6-107"></a>    <span class="bu">print</span>(<span class="ss">f'Train Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-108"><a href="#cb6-108"></a></span>
<span id="cb6-109"><a href="#cb6-109"></a>    <span class="co"># Compute expected calibration error on testing set</span></span>
<span id="cb6-110"><a href="#cb6-110"></a>    expected_calibration_error(sigmoid(test_logits), y_test, model_name<span class="op">=</span><span class="st">"HMC"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<ol start="4" type="a">
<li><p><strong>Non-Parametric Model with Gaussian Process (GP) (7 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 2 point)</strong>. Describe how a Gaussian Process can be used for preference learning in this context (i.e., describe how the latent function is used for classification).</p></li>
<li><p><strong>(Coding, 2 points)</strong>. Run the GP classification for preference learning code in<br>
<code>uncertainty_quantification/gaussian_process.py</code> and provide the accuracy numbers. This can only be run on a CPU and may take around 10 minutes to complete.</p></li>
<li><p><strong>(Written, 3 point)</strong>. Discuss the computational complexity of the GP model compared to the parametric models. What are the advantages and disadvantages of using a GP in this setting?</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="8a9810d8" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">from</span> sklearn.gaussian_process <span class="im">import</span> GaussianProcessClassifier</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="im">from</span> sklearn.gaussian_process.kernels <span class="im">import</span> RBF</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="im">from</span> ece <span class="im">import</span> expected_calibration_error</span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a>x_train <span class="op">=</span> np.load(<span class="st">'../data/differences_train.npy'</span>)</span>
<span id="cb7-8"><a href="#cb7-8"></a>x_test <span class="op">=</span> np.load(<span class="st">'../data/differences_test.npy'</span>)</span>
<span id="cb7-9"><a href="#cb7-9"></a>y_train <span class="op">=</span> np.load(<span class="st">'../data/labels_train.npy'</span>)</span>
<span id="cb7-10"><a href="#cb7-10"></a>y_test <span class="op">=</span> np.load(<span class="st">'../data/labels_test.npy'</span>)</span>
<span id="cb7-11"><a href="#cb7-11"></a></span>
<span id="cb7-12"><a href="#cb7-12"></a>kernel <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> RBF(length_scale<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb7-13"><a href="#cb7-13"></a>gp_classifier <span class="op">=</span> GaussianProcessClassifier(kernel<span class="op">=</span>kernel, random_state<span class="op">=</span><span class="dv">42</span>, n_jobs<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb7-14"><a href="#cb7-14"></a>gp_classifier.fit(x_train, y_train)</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>y_pred_probs <span class="op">=</span> gp_classifier.predict_proba(x_test)[:, <span class="dv">1</span>]</span>
<span id="cb7-17"><a href="#cb7-17"></a>y_pred_labels <span class="op">=</span> (y_pred_probs <span class="op">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb7-18"><a href="#cb7-18"></a></span>
<span id="cb7-19"><a href="#cb7-19"></a>train_accuracy <span class="op">=</span> accuracy_score(y_train, gp_classifier.predict(x_train))</span>
<span id="cb7-20"><a href="#cb7-20"></a><span class="bu">print</span>(<span class="ss">f'Train Accuracy: </span><span class="sc">{</span>train_accuracy<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb7-21"><a href="#cb7-21"></a></span>
<span id="cb7-22"><a href="#cb7-22"></a>test_accuracy <span class="op">=</span> accuracy_score(y_test, y_pred_labels)</span>
<span id="cb7-23"><a href="#cb7-23"></a><span class="bu">print</span>(<span class="ss">f'Test Accuracy: </span><span class="sc">{</span>test_accuracy<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb7-24"><a href="#cb7-24"></a></span>
<span id="cb7-25"><a href="#cb7-25"></a>expected_calibration_error(y_pred_probs, y_test, model_name<span class="op">=</span><span class="st">"Gaussian Process Classifier"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-2-active-learning-for-preference-learning-40-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-2-active-learning-for-preference-learning-40-points">Question 2: Active Learning for Preference Learning (40 points)</h3>
<p>In this question, you will explore active learning strategies for preference learning using a linear model. We will use expected information gain as the acquisition function to select the most informative queries, where each query is a pair of items. Assume that we model the preferences using a simple linear model. Given feature vectors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> corresponding to two items, the probability that <span class="math inline">\(x_1\)</span> is preferred over <span class="math inline">\(x_2\)</span> is modeled using a logistic regression model, i.e.,</p>
<p><span class="math display">\[P(x_1 \succ x_2 | \theta) = \sigma(\theta^\top (x_1 - x_2)),\]</span></p>
<p>where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> is the model parameter vector, and <span class="math inline">\(\sigma(z)\)</span> is the sigmoid function <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>. The goal is to sequentially select pairs of items to maximize the information gained about <span class="math inline">\(\theta\)</span> through preference queries.</p>
<ol type="a">
<li><p><strong>Expected Information Gain (15 points)</strong></p>
<ol type="i">
<li><p><strong>Derive the Expected Information Gain (Written, 3 points).</strong> Suppose that after observing a preference between two items <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the posterior distribution over <span class="math inline">\(\theta\)</span> is updated. The information gain from this observation is the reduction in uncertainty about <span class="math inline">\(\theta\)</span> measured using the Kullback-Leibler (KL) divergence between the prior and posterior distributions. Given the current posterior distribution <span class="math inline">\(P(\theta | \mathcal{D})\)</span> and a possible observation <span class="math inline">\(y \in \{0, 1\}\)</span> (where <span class="math inline">\(y = 1\)</span> if <span class="math inline">\(x_1\)</span> is preferred over <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(y = 0\)</span> otherwise), the expected information gain is: <span class="math display">\[\begin{aligned}
\mathbb{E}[\text{IG}(x_1, x_2)] = &amp;P(y=1 | x_1, x_2, \theta) D_{\text{KL}}\left( P(\theta | y = 1, \mathcal{D}) \parallel P(\theta | \mathcal{D}) \right) \\+
&amp;P(y=0 | x_1, x_2, \theta) D_{\text{KL}}\left( P(\theta | y = 0, \mathcal{D}) \parallel P(\theta | \mathcal{D}) \right)
\end{aligned}\]</span></p>
<p>Derive this expression for the expected information gain of selecting the pair <span class="math inline">\((x_1, x_2)\)</span> for a preference query. Start by explaining how the KL divergence measures the information gain, and break down the expectation over the possible outcomes of the query.</p></li>
<li><p><strong>Simplifying the KL Divergence (Written, 4 points).</strong> Assuming the prior and posterior distributions over <span class="math inline">\(\theta\)</span> are Gaussian (i.e., <span class="math inline">\(P(\theta) \sim \mathcal{N}(\mu, \Sigma)\)</span> and <span class="math inline">\(P(\theta | \mathcal{D}) \sim \mathcal{N}(\mu', \Sigma')\)</span>), show that the KL divergence between the Gaussian posterior and prior simplifies to: <span class="math display">\[\begin{aligned}
    D_{\text{KL}}\left( \mathcal{N}(\mu', \Sigma') \parallel \mathcal{N}(\mu, \Sigma) \right) &amp;= \frac{1}{2} \left( \text{tr}(\Sigma^{-1} \Sigma') + (\mu' - \mu)^\top \Sigma^{-1} (\mu' - \mu)\right.\\
    &amp;\left.- d + \log\left( \frac{\det(\Sigma)}{\det(\Sigma')} \right) \right).
    \end{aligned}\]</span></p></li>
<li><p><strong>Approximate Information Gain for a Linear Model (Written, 4 points).</strong> In the case of a linear model with Gaussian priors on <span class="math inline">\(\theta\)</span>, assume that the posterior distribution <span class="math inline">\(P(\theta | \mathcal{D}) \sim \mathcal{N}(\mu, \Sigma)\)</span> is updated using Bayes’ rule after each observation. The likelihood of observing a preference <span class="math inline">\(y\)</span> is logistic, which does not conjugate with the Gaussian prior. However, for the purposes of this question, assume that after each query, the posterior mean <span class="math inline">\(\mu'\)</span> and covariance <span class="math inline">\(\Sigma'\)</span> can be updated using an approximation method such as Laplace’s approximation.</p>
<p>Using these assumptions, compute the expected information gain for a specific query <span class="math inline">\((x_1, x_2)\)</span> in closed form. You may express the information gain in terms of the updated mean <span class="math inline">\(\mu'\)</span> and covariance <span class="math inline">\(\Sigma'\)</span> after observing the preference outcome.</p></li>
<li><p><strong>Laplace Approximation for Posterior (Written, 4 points).</strong> The Laplace approximation for the posterior is given by <span class="math display">\[\begin{aligned}
\mu'=\arg \min_\theta -\log P(\theta | \mathcal{D})\\
\Sigma'^{-1}=\nabla_\theta\nabla_\theta -\log P(\theta|\mathcal{D})|_{\theta=\mu'}
\end{aligned}\]</span> In our scenario with the Bradley-Terry model for likelihood, simplify <span class="math inline">\(-\log P(\theta | \mathcal{D})\)</span> and its Hessian ignoring the normalization constant.</p></li>
</ol></li>
<li><p><strong>Active Learning Algorithm (25 points)</strong> In this section, you will implement an active learning algorithm for selecting the most informative queries using the expected information gain criterion.</p>
<ol type="i">
<li><p><strong>(Coding, 4 points).</strong> Implement <code>kl_divergence_gaussians</code> in <code>active_learning/main.py</code>.</p></li>
<li><p><strong>(Coding, 4 points).</strong> Following your derived Laplace approximation, implement <code>negative_log_posterior</code>.</p></li>
<li><p><strong>(Coding, 4 points).</strong> Implement <code>compute_hessian</code> that is used to obtain the inverse of the covariance matrix.</p></li>
<li><p><strong>(Coding, 3 points).</strong> Implement <code>expected_information_gain</code>.</p></li>
<li><p><strong>(Coding, 4 points).</strong> Finally, implement <code>active_learning</code>.</p></li>
<li><p><strong>(Coding + Written, 6 points).</strong> Plot the <span class="math inline">\(L^2\)</span> norm of the covariance matrix for each loop of the active learning loop. Additionally, on the same plot, implement a random baseline and plot its <span class="math inline">\(L^2\)</span> covariance matrix norm. The random baseline should randomly select a point in the dataset and not use any acquisition function. Interpret your plot and use it to compare the two methods.</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="4cac9419" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> torch</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="kw">class</span> LogisticActiveLearning:</span>
<span id="cb8-9"><a href="#cb8-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, test_size<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb8-10"><a href="#cb8-10"></a>        <span class="co">"""</span></span>
<span id="cb8-11"><a href="#cb8-11"></a><span class="co">        Initializes LogisticActiveLearning model, sets device, and prepares data.</span></span>
<span id="cb8-12"><a href="#cb8-12"></a><span class="co">        </span></span>
<span id="cb8-13"><a href="#cb8-13"></a><span class="co">        Args:</span></span>
<span id="cb8-14"><a href="#cb8-14"></a><span class="co">        - test_size (float): Proportion of the dataset used for validation.</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="co">        """</span></span>
<span id="cb8-16"><a href="#cb8-16"></a>        <span class="co"># Make device customizable</span></span>
<span id="cb8-17"><a href="#cb8-17"></a>        <span class="va">self</span>.device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb8-18"><a href="#cb8-18"></a>        X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">10000</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-19"><a href="#cb8-19"></a></span>
<span id="cb8-20"><a href="#cb8-20"></a>        <span class="co"># Convert data and labels to tensors</span></span>
<span id="cb8-21"><a href="#cb8-21"></a>        x_data <span class="op">=</span> torch.tensor(X, dtype<span class="op">=</span>torch.float32).to(<span class="va">self</span>.device)</span>
<span id="cb8-22"><a href="#cb8-22"></a>        y_data <span class="op">=</span> torch.tensor(y, dtype<span class="op">=</span>torch.float32).to(<span class="va">self</span>.device)</span>
<span id="cb8-23"><a href="#cb8-23"></a>        <span class="va">self</span>.N, <span class="va">self</span>.D <span class="op">=</span> x_data.shape</span>
<span id="cb8-24"><a href="#cb8-24"></a></span>
<span id="cb8-25"><a href="#cb8-25"></a>        <span class="co"># Split into training and validation sets</span></span>
<span id="cb8-26"><a href="#cb8-26"></a>        train_indices, val_indices <span class="op">=</span> train_test_split(<span class="bu">range</span>(<span class="va">self</span>.N), test_size<span class="op">=</span>test_size, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-27"><a href="#cb8-27"></a>        <span class="va">self</span>.x_train <span class="op">=</span> x_data[train_indices]</span>
<span id="cb8-28"><a href="#cb8-28"></a>        <span class="va">self</span>.y_train <span class="op">=</span> y_data[train_indices]</span>
<span id="cb8-29"><a href="#cb8-29"></a>        <span class="va">self</span>.x_val <span class="op">=</span> x_data[val_indices]</span>
<span id="cb8-30"><a href="#cb8-30"></a>        <span class="va">self</span>.y_val <span class="op">=</span> y_data[val_indices]</span>
<span id="cb8-31"><a href="#cb8-31"></a></span>
<span id="cb8-32"><a href="#cb8-32"></a>        <span class="co"># Initialize mean and inverse covariance for the prior</span></span>
<span id="cb8-33"><a href="#cb8-33"></a>        <span class="va">self</span>.weights_mean <span class="op">=</span> torch.zeros(<span class="va">self</span>.D, requires_grad<span class="op">=</span><span class="va">True</span>, device<span class="op">=</span><span class="va">self</span>.device)</span>
<span id="cb8-34"><a href="#cb8-34"></a>        <span class="va">self</span>.weights_inv_cov <span class="op">=</span> torch.eye(<span class="va">self</span>.D).to(<span class="va">self</span>.device)  <span class="co"># Start with identity inverse covariance</span></span>
<span id="cb8-35"><a href="#cb8-35"></a></span>
<span id="cb8-36"><a href="#cb8-36"></a>    <span class="kw">def</span> negative_log_posterior(<span class="va">self</span>, w, x, y):</span>
<span id="cb8-37"><a href="#cb8-37"></a>        <span class="co">"""</span></span>
<span id="cb8-38"><a href="#cb8-38"></a><span class="co">        Computes the negative log-posterior (negative log-prior + log-likelihood).</span></span>
<span id="cb8-39"><a href="#cb8-39"></a><span class="co">        </span></span>
<span id="cb8-40"><a href="#cb8-40"></a><span class="co">        Args:</span></span>
<span id="cb8-41"><a href="#cb8-41"></a><span class="co">        - w (torch.Tensor): Model weights.</span></span>
<span id="cb8-42"><a href="#cb8-42"></a><span class="co">        - x (torch.Tensor): Input data point.</span></span>
<span id="cb8-43"><a href="#cb8-43"></a><span class="co">        - y (torch.Tensor): True label.</span></span>
<span id="cb8-44"><a href="#cb8-44"></a><span class="co">        </span></span>
<span id="cb8-45"><a href="#cb8-45"></a><span class="co">        Returns:</span></span>
<span id="cb8-46"><a href="#cb8-46"></a><span class="co">        - torch.Tensor: Negative log-posterior value.</span></span>
<span id="cb8-47"><a href="#cb8-47"></a><span class="co">        """</span></span>
<span id="cb8-48"><a href="#cb8-48"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb8-49"><a href="#cb8-49"></a>        <span class="co"># Compute log-prior term using inverse covariance</span></span>
<span id="cb8-50"><a href="#cb8-50"></a>        <span class="cf">pass</span></span>
<span id="cb8-51"><a href="#cb8-51"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb8-52"><a href="#cb8-52"></a></span>
<span id="cb8-53"><a href="#cb8-53"></a>    <span class="kw">def</span> optimize_weights(<span class="va">self</span>, w, x, y, num_steps<span class="op">=</span><span class="dv">50</span>, lr<span class="op">=</span><span class="fl">1e-2</span>):</span>
<span id="cb8-54"><a href="#cb8-54"></a>        <span class="co">"""</span></span>
<span id="cb8-55"><a href="#cb8-55"></a><span class="co">        Optimizes weights using Adam optimizer.</span></span>
<span id="cb8-56"><a href="#cb8-56"></a><span class="co">        </span></span>
<span id="cb8-57"><a href="#cb8-57"></a><span class="co">        Args:</span></span>
<span id="cb8-58"><a href="#cb8-58"></a><span class="co">        - w (torch.Tensor): Initial weights.</span></span>
<span id="cb8-59"><a href="#cb8-59"></a><span class="co">        - x (torch.Tensor): Input data point.</span></span>
<span id="cb8-60"><a href="#cb8-60"></a><span class="co">        - y (torch.Tensor): True label.</span></span>
<span id="cb8-61"><a href="#cb8-61"></a><span class="co">        - num_steps (int): Number of optimization steps.</span></span>
<span id="cb8-62"><a href="#cb8-62"></a><span class="co">        - lr (float): Learning rate.</span></span>
<span id="cb8-63"><a href="#cb8-63"></a><span class="co">        </span></span>
<span id="cb8-64"><a href="#cb8-64"></a><span class="co">        Returns:</span></span>
<span id="cb8-65"><a href="#cb8-65"></a><span class="co">        - torch.Tensor: Updated weights.</span></span>
<span id="cb8-66"><a href="#cb8-66"></a><span class="co">        - torch.Tensor: Hessian inverse covariance.</span></span>
<span id="cb8-67"><a href="#cb8-67"></a><span class="co">        """</span></span>
<span id="cb8-68"><a href="#cb8-68"></a>        optimizer <span class="op">=</span> Adam([w], lr<span class="op">=</span>lr)</span>
<span id="cb8-69"><a href="#cb8-69"></a>        </span>
<span id="cb8-70"><a href="#cb8-70"></a>        <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb8-71"><a href="#cb8-71"></a>            optimizer.zero_grad()</span>
<span id="cb8-72"><a href="#cb8-72"></a>            loss <span class="op">=</span> <span class="va">self</span>.negative_log_posterior(w, x, y)</span>
<span id="cb8-73"><a href="#cb8-73"></a>            loss.backward()</span>
<span id="cb8-74"><a href="#cb8-74"></a>            optimizer.step()</span>
<span id="cb8-75"><a href="#cb8-75"></a></span>
<span id="cb8-76"><a href="#cb8-76"></a>        <span class="co"># Compute the Hessian of log-posterior, serving as inverse covariance</span></span>
<span id="cb8-77"><a href="#cb8-77"></a>        inv_cov <span class="op">=</span> <span class="va">self</span>.compute_hessian(w.detach(), x, y)</span>
<span id="cb8-78"><a href="#cb8-78"></a>        <span class="cf">return</span> w.detach().clone(), inv_cov</span>
<span id="cb8-79"><a href="#cb8-79"></a></span>
<span id="cb8-80"><a href="#cb8-80"></a>    <span class="kw">def</span> compute_hessian(<span class="va">self</span>, w, x, y):</span>
<span id="cb8-81"><a href="#cb8-81"></a>        <span class="co">"""</span></span>
<span id="cb8-82"><a href="#cb8-82"></a><span class="co">        Computes the Hessian of the negative log-posterior, used as the inverse covariance.</span></span>
<span id="cb8-83"><a href="#cb8-83"></a><span class="co">        </span></span>
<span id="cb8-84"><a href="#cb8-84"></a><span class="co">        Args:</span></span>
<span id="cb8-85"><a href="#cb8-85"></a><span class="co">        - w (torch.Tensor): Model weights.</span></span>
<span id="cb8-86"><a href="#cb8-86"></a><span class="co">        - x (torch.Tensor): Input data point.</span></span>
<span id="cb8-87"><a href="#cb8-87"></a><span class="co">        - y (torch.Tensor): True label.</span></span>
<span id="cb8-88"><a href="#cb8-88"></a><span class="co">        </span></span>
<span id="cb8-89"><a href="#cb8-89"></a><span class="co">        Returns:</span></span>
<span id="cb8-90"><a href="#cb8-90"></a><span class="co">        - torch.Tensor: Hessian of the negative log-posterior.</span></span>
<span id="cb8-91"><a href="#cb8-91"></a><span class="co">        """</span></span>
<span id="cb8-92"><a href="#cb8-92"></a>        <span class="co"># YOUR CODE HERE (~5-8 lines)</span></span>
<span id="cb8-93"><a href="#cb8-93"></a>        <span class="co"># Hessian of the prior term</span></span>
<span id="cb8-94"><a href="#cb8-94"></a>        <span class="cf">pass</span></span>
<span id="cb8-95"><a href="#cb8-95"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb8-96"><a href="#cb8-96"></a></span>
<span id="cb8-97"><a href="#cb8-97"></a>    <span class="kw">def</span> acquisition_fn(<span class="va">self</span>, x):</span>
<span id="cb8-98"><a href="#cb8-98"></a>        <span class="co">"""</span></span>
<span id="cb8-99"><a href="#cb8-99"></a><span class="co">        Computes posterior means and inverse covariances for y=1 and y=0 without modifying original parameters.</span></span>
<span id="cb8-100"><a href="#cb8-100"></a><span class="co">        </span></span>
<span id="cb8-101"><a href="#cb8-101"></a><span class="co">        Args:</span></span>
<span id="cb8-102"><a href="#cb8-102"></a><span class="co">        - x (torch.Tensor): Input data point.</span></span>
<span id="cb8-103"><a href="#cb8-103"></a><span class="co">        </span></span>
<span id="cb8-104"><a href="#cb8-104"></a><span class="co">        Returns:</span></span>
<span id="cb8-105"><a href="#cb8-105"></a><span class="co">        - dict: Posterior properties for y=1 and y=0 cases.</span></span>
<span id="cb8-106"><a href="#cb8-106"></a><span class="co">        """</span></span>
<span id="cb8-107"><a href="#cb8-107"></a>        weights_y1 <span class="op">=</span> <span class="va">self</span>.weights_mean.clone().detach().requires_grad_(<span class="va">True</span>)</span>
<span id="cb8-108"><a href="#cb8-108"></a>        weights_y0 <span class="op">=</span> <span class="va">self</span>.weights_mean.clone().detach().requires_grad_(<span class="va">True</span>)</span>
<span id="cb8-109"><a href="#cb8-109"></a></span>
<span id="cb8-110"><a href="#cb8-110"></a>        <span class="co"># Optimize weights and get Hessian for both y=1 and y=0 cases</span></span>
<span id="cb8-111"><a href="#cb8-111"></a>        posterior_mean_y1, inv_cov_y1 <span class="op">=</span> <span class="va">self</span>.optimize_weights(weights_y1, x, <span class="dv">1</span>, num_steps<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb8-112"><a href="#cb8-112"></a>        posterior_mean_y0, inv_cov_y0 <span class="op">=</span> <span class="va">self</span>.optimize_weights(weights_y0, x, <span class="dv">0</span>, num_steps<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb8-113"><a href="#cb8-113"></a></span>
<span id="cb8-114"><a href="#cb8-114"></a>        <span class="co"># Calculate probabilities for the acquisition function</span></span>
<span id="cb8-115"><a href="#cb8-115"></a>        prob_y1 <span class="op">=</span> torch.sigmoid(torch.dot(<span class="va">self</span>.weights_mean.detach(), x))</span>
<span id="cb8-116"><a href="#cb8-116"></a>        prob_y0 <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> prob_y1</span>
<span id="cb8-117"><a href="#cb8-117"></a></span>
<span id="cb8-118"><a href="#cb8-118"></a>        <span class="cf">return</span> {</span>
<span id="cb8-119"><a href="#cb8-119"></a>            <span class="st">'prob_y1'</span>: prob_y1,</span>
<span id="cb8-120"><a href="#cb8-120"></a>            <span class="st">'prob_y0'</span>: prob_y0,</span>
<span id="cb8-121"><a href="#cb8-121"></a>            <span class="st">'posterior_mean_y1'</span>: posterior_mean_y1,</span>
<span id="cb8-122"><a href="#cb8-122"></a>            <span class="st">'posterior_inv_cov_y1'</span>: inv_cov_y1,</span>
<span id="cb8-123"><a href="#cb8-123"></a>            <span class="st">'posterior_mean_y0'</span>: posterior_mean_y0,</span>
<span id="cb8-124"><a href="#cb8-124"></a>            <span class="st">'posterior_inv_cov_y0'</span>: inv_cov_y0</span>
<span id="cb8-125"><a href="#cb8-125"></a>        }</span>
<span id="cb8-126"><a href="#cb8-126"></a></span>
<span id="cb8-127"><a href="#cb8-127"></a>    <span class="kw">def</span> expected_information_gain(<span class="va">self</span>, x):</span>
<span id="cb8-128"><a href="#cb8-128"></a>        <span class="co">"""</span></span>
<span id="cb8-129"><a href="#cb8-129"></a><span class="co">        Computes expected information gain for a given point `x`.</span></span>
<span id="cb8-130"><a href="#cb8-130"></a><span class="co">        </span></span>
<span id="cb8-131"><a href="#cb8-131"></a><span class="co">        Args:</span></span>
<span id="cb8-132"><a href="#cb8-132"></a><span class="co">        - x (torch.Tensor): Input data point.</span></span>
<span id="cb8-133"><a href="#cb8-133"></a><span class="co">        </span></span>
<span id="cb8-134"><a href="#cb8-134"></a><span class="co">        Returns:</span></span>
<span id="cb8-135"><a href="#cb8-135"></a><span class="co">        - torch.Tensor: Expected Information Gain (EIG) value.</span></span>
<span id="cb8-136"><a href="#cb8-136"></a><span class="co">        """</span></span>
<span id="cb8-137"><a href="#cb8-137"></a>        acquisition <span class="op">=</span> <span class="va">self</span>.acquisition_fn(x)</span>
<span id="cb8-138"><a href="#cb8-138"></a></span>
<span id="cb8-139"><a href="#cb8-139"></a>        <span class="co"># Compute KL divergences for y=1 and y=0 using inverse covariances</span></span>
<span id="cb8-140"><a href="#cb8-140"></a>        kl_y1 <span class="op">=</span> kl_divergence_gaussians(</span>
<span id="cb8-141"><a href="#cb8-141"></a>            acquisition[<span class="st">'posterior_mean_y1'</span>],</span>
<span id="cb8-142"><a href="#cb8-142"></a>            acquisition[<span class="st">'posterior_inv_cov_y1'</span>],</span>
<span id="cb8-143"><a href="#cb8-143"></a>            <span class="va">self</span>.weights_mean.detach(),</span>
<span id="cb8-144"><a href="#cb8-144"></a>            <span class="va">self</span>.weights_inv_cov</span>
<span id="cb8-145"><a href="#cb8-145"></a>        )</span>
<span id="cb8-146"><a href="#cb8-146"></a></span>
<span id="cb8-147"><a href="#cb8-147"></a>        kl_y0 <span class="op">=</span> kl_divergence_gaussians(</span>
<span id="cb8-148"><a href="#cb8-148"></a>            acquisition[<span class="st">'posterior_mean_y0'</span>],</span>
<span id="cb8-149"><a href="#cb8-149"></a>            acquisition[<span class="st">'posterior_inv_cov_y0'</span>],</span>
<span id="cb8-150"><a href="#cb8-150"></a>            <span class="va">self</span>.weights_mean.detach(),</span>
<span id="cb8-151"><a href="#cb8-151"></a>            <span class="va">self</span>.weights_inv_cov</span>
<span id="cb8-152"><a href="#cb8-152"></a>        )</span>
<span id="cb8-153"><a href="#cb8-153"></a></span>
<span id="cb8-154"><a href="#cb8-154"></a>        <span class="co"># Expected Information Gain (EIG)</span></span>
<span id="cb8-155"><a href="#cb8-155"></a>        eig <span class="op">=</span> <span class="va">None</span> <span class="co"># YOUR CODE HERE (1 line)</span></span>
<span id="cb8-156"><a href="#cb8-156"></a>        <span class="cf">return</span> eig</span>
<span id="cb8-157"><a href="#cb8-157"></a></span>
<span id="cb8-158"><a href="#cb8-158"></a>    <span class="kw">def</span> active_learning(<span class="va">self</span>, selected_indices, subset_size<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb8-159"><a href="#cb8-159"></a>        <span class="co">"""</span></span>
<span id="cb8-160"><a href="#cb8-160"></a><span class="co">        Active learning loop that selects the most informative data point based on EIG.</span></span>
<span id="cb8-161"><a href="#cb8-161"></a><span class="co">        </span></span>
<span id="cb8-162"><a href="#cb8-162"></a><span class="co">        Args:</span></span>
<span id="cb8-163"><a href="#cb8-163"></a><span class="co">        - selected_indices (list): Indices of previously selected samples.</span></span>
<span id="cb8-164"><a href="#cb8-164"></a><span class="co">        - subset_size (int): Number of samples to consider in each subset.</span></span>
<span id="cb8-165"><a href="#cb8-165"></a></span>
<span id="cb8-166"><a href="#cb8-166"></a><span class="co">        Returns:</span></span>
<span id="cb8-167"><a href="#cb8-167"></a><span class="co">        - best_x, best_x_idx, best_acquisition: Selected data point and acquisition details.</span></span>
<span id="cb8-168"><a href="#cb8-168"></a><span class="co">        """</span></span>
<span id="cb8-169"><a href="#cb8-169"></a>        best_eig <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb8-170"><a href="#cb8-170"></a>        best_x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-171"><a href="#cb8-171"></a>        best_x_idx <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb8-172"><a href="#cb8-172"></a>        best_acquisition <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-173"><a href="#cb8-173"></a></span>
<span id="cb8-174"><a href="#cb8-174"></a>        subset_indices <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> torch.randperm(<span class="bu">len</span>(<span class="va">self</span>.x_train)).tolist() <span class="cf">if</span> i <span class="kw">not</span> <span class="kw">in</span> selected_indices][:subset_size]</span>
<span id="cb8-175"><a href="#cb8-175"></a></span>
<span id="cb8-176"><a href="#cb8-176"></a>        <span class="co"># YOUR CODE HERE (~ 10 lines)</span></span>
<span id="cb8-177"><a href="#cb8-177"></a>        <span class="cf">pass</span></span>
<span id="cb8-178"><a href="#cb8-178"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb8-179"><a href="#cb8-179"></a>        <span class="cf">return</span> best_x, best_x_idx, best_acquisition</span>
<span id="cb8-180"><a href="#cb8-180"></a></span>
<span id="cb8-181"><a href="#cb8-181"></a>    <span class="kw">def</span> validate(<span class="va">self</span>):</span>
<span id="cb8-182"><a href="#cb8-182"></a>        <span class="co">"""</span></span>
<span id="cb8-183"><a href="#cb8-183"></a><span class="co">        Computes accuracy on the validation set by predicting labels and comparing to true labels.</span></span>
<span id="cb8-184"><a href="#cb8-184"></a><span class="co">        </span></span>
<span id="cb8-185"><a href="#cb8-185"></a><span class="co">        Returns:</span></span>
<span id="cb8-186"><a href="#cb8-186"></a><span class="co">        - float: Validation accuracy.</span></span>
<span id="cb8-187"><a href="#cb8-187"></a><span class="co">        """</span></span>
<span id="cb8-188"><a href="#cb8-188"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-189"><a href="#cb8-189"></a>            logits <span class="op">=</span> <span class="va">self</span>.x_val <span class="op">@</span> <span class="va">self</span>.weights_mean</span>
<span id="cb8-190"><a href="#cb8-190"></a>            predictions <span class="op">=</span> torch.sigmoid(logits) <span class="op">&gt;=</span> <span class="fl">0.5</span>  <span class="co"># Convert logits to binary predictions</span></span>
<span id="cb8-191"><a href="#cb8-191"></a>            accuracy <span class="op">=</span> (predictions <span class="op">==</span> <span class="va">self</span>.y_val).<span class="bu">float</span>().mean().item()</span>
<span id="cb8-192"><a href="#cb8-192"></a>            <span class="bu">print</span>(<span class="ss">f"Validation accuracy: </span><span class="sc">{</span>accuracy <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb8-193"><a href="#cb8-193"></a>        <span class="cf">return</span> accuracy</span>
<span id="cb8-194"><a href="#cb8-194"></a></span>
<span id="cb8-195"><a href="#cb8-195"></a>    <span class="kw">def</span> train(<span class="va">self</span>, num_iterations<span class="op">=</span><span class="dv">10</span>, subset_size<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb8-196"><a href="#cb8-196"></a>        <span class="co">"""</span></span>
<span id="cb8-197"><a href="#cb8-197"></a><span class="co">        Train the model using active learning with subset sampling.</span></span>
<span id="cb8-198"><a href="#cb8-198"></a><span class="co">        </span></span>
<span id="cb8-199"><a href="#cb8-199"></a><span class="co">        Args:</span></span>
<span id="cb8-200"><a href="#cb8-200"></a><span class="co">        - num_iterations (int): Number of active learning iterations.</span></span>
<span id="cb8-201"><a href="#cb8-201"></a><span class="co">        - subset_size (int): Number of samples to consider in each subset.</span></span>
<span id="cb8-202"><a href="#cb8-202"></a><span class="co">        """</span></span>
<span id="cb8-203"><a href="#cb8-203"></a>        selected_indices <span class="op">=</span> []</span>
<span id="cb8-204"><a href="#cb8-204"></a>        <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb8-205"><a href="#cb8-205"></a>            <span class="bu">print</span>(<span class="ss">f"Iteration </span><span class="sc">{</span>iteration <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_iterations<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-206"><a href="#cb8-206"></a></span>
<span id="cb8-207"><a href="#cb8-207"></a>            <span class="co"># Select the most informative data point from a random subset</span></span>
<span id="cb8-208"><a href="#cb8-208"></a>            best_x, best_x_idx, acquisition <span class="op">=</span> <span class="va">self</span>.active_learning(selected_indices, subset_size<span class="op">=</span>subset_size)</span>
<span id="cb8-209"><a href="#cb8-209"></a>            selected_indices.append(best_x_idx)</span>
<span id="cb8-210"><a href="#cb8-210"></a>            <span class="bu">print</span>(<span class="ss">f"Selected data point with EIG."</span>)</span>
<span id="cb8-211"><a href="#cb8-211"></a></span>
<span id="cb8-212"><a href="#cb8-212"></a>            <span class="co"># Get the true label for the selected data point</span></span>
<span id="cb8-213"><a href="#cb8-213"></a>            y <span class="op">=</span> <span class="va">self</span>.y_train[best_x_idx].item()</span>
<span id="cb8-214"><a href="#cb8-214"></a></span>
<span id="cb8-215"><a href="#cb8-215"></a>            <span class="co"># Update posterior mean and inverse covariance based on true label</span></span>
<span id="cb8-216"><a href="#cb8-216"></a>            <span class="cf">if</span> y <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb8-217"><a href="#cb8-217"></a>                <span class="va">self</span>.weights_mean <span class="op">=</span> acquisition[<span class="st">'posterior_mean_y1'</span>]</span>
<span id="cb8-218"><a href="#cb8-218"></a>                <span class="va">self</span>.weights_inv_cov <span class="op">=</span> acquisition[<span class="st">'posterior_inv_cov_y1'</span>]</span>
<span id="cb8-219"><a href="#cb8-219"></a>            <span class="cf">else</span>:</span>
<span id="cb8-220"><a href="#cb8-220"></a>                <span class="va">self</span>.weights_mean <span class="op">=</span> acquisition[<span class="st">'posterior_mean_y0'</span>]</span>
<span id="cb8-221"><a href="#cb8-221"></a>                <span class="va">self</span>.weights_inv_cov <span class="op">=</span> acquisition[<span class="st">'posterior_inv_cov_y0'</span>]</span>
<span id="cb8-222"><a href="#cb8-222"></a></span>
<span id="cb8-223"><a href="#cb8-223"></a>            <span class="bu">print</span>(<span class="ss">f"Covariance L2: </span><span class="sc">{</span>torch<span class="sc">.</span>inverse(<span class="va">self</span>.weights_inv_cov)<span class="sc">.</span>norm()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-224"><a href="#cb8-224"></a></span>
<span id="cb8-225"><a href="#cb8-225"></a>            <span class="co"># Validate model performance on the validation set</span></span>
<span id="cb8-226"><a href="#cb8-226"></a>            <span class="va">self</span>.validate()</span>
<span id="cb8-227"><a href="#cb8-227"></a></span>
<span id="cb8-228"><a href="#cb8-228"></a><span class="co"># KL divergence between two multivariate normal distributions</span></span>
<span id="cb8-229"><a href="#cb8-229"></a><span class="kw">def</span> kl_divergence_gaussians(mu1, sigma1_inv, mu2, sigma2_inv):</span>
<span id="cb8-230"><a href="#cb8-230"></a>    <span class="co">"""</span></span>
<span id="cb8-231"><a href="#cb8-231"></a><span class="co">    Computes the KL divergence between two multivariate Gaussian distributions.</span></span>
<span id="cb8-232"><a href="#cb8-232"></a><span class="co">    </span></span>
<span id="cb8-233"><a href="#cb8-233"></a><span class="co">    Args:</span></span>
<span id="cb8-234"><a href="#cb8-234"></a><span class="co">    - mu1, mu2 (torch.Tensor): Mean vectors of the distributions.</span></span>
<span id="cb8-235"><a href="#cb8-235"></a><span class="co">    - sigma1_inv, sigma2_inv (torch.Tensor): Inverse covariance matrices of the distributions. PLEASE </span><span class="al">NOTE</span><span class="co"> THE INVERSE!</span></span>
<span id="cb8-236"><a href="#cb8-236"></a><span class="co">    </span></span>
<span id="cb8-237"><a href="#cb8-237"></a><span class="co">    Returns:</span></span>
<span id="cb8-238"><a href="#cb8-238"></a><span class="co">    - torch.Tensor: KL divergence value.</span></span>
<span id="cb8-239"><a href="#cb8-239"></a><span class="co">    """</span></span>
<span id="cb8-240"><a href="#cb8-240"></a>    <span class="co"># YOUR CODE HERE (~ 9-12 lines)</span></span>
<span id="cb8-241"><a href="#cb8-241"></a>    <span class="cf">pass</span></span>
<span id="cb8-242"><a href="#cb8-242"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb8-243"><a href="#cb8-243"></a></span>
<span id="cb8-244"><a href="#cb8-244"></a><span class="co"># Example usage</span></span>
<span id="cb8-245"><a href="#cb8-245"></a>model <span class="op">=</span> LogisticActiveLearning()</span>
<span id="cb8-246"><a href="#cb8-246"></a>model.train(num_iterations<span class="op">=</span><span class="dv">100</span>, subset_size<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-3-linear-performance-metric-elicitation-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-3-linear-performance-metric-elicitation-30-points">Question 3: Linear Performance Metric Elicitation (30 points)</h3>
<ol type="1">
<li><p><strong>(Written, 10 points).</strong> For background on the problem setting, read <a href="https://tinyurl.com/3b92sufm" class="uri">https://tinyurl.com/3b92sufm</a>. Suppose we have a linear performance metric given by <span class="math display">\[p(C) = 1-\alpha (FP)-\beta (FN)\]</span> where <span class="math inline">\(C\)</span> is a confusion matrix and <span class="math inline">\(FP, FN\)</span> denote false positive and false negative rates. We wish to find the optimal classifier w.r.t. <span class="math inline">\(p\)</span>. That is, <span class="math display">\[\phi^* = \arg \max_{\phi\in\Phi} p(C(\phi))\]</span> where <span class="math inline">\(\Phi\)</span> is the space of all probabilistic binary classifiers from <span class="math inline">\(X\to [0, 1]\)</span>. Note that these classifiers return probabilities corresponding to the label <span class="math inline">\(1\)</span>. Show that <span class="math inline">\(\phi^*\)</span> is in fact deterministic and given by <span class="math display">\[\phi(x)=\begin{cases}
    1 &amp; \text{if } p(y|x) &gt; f(\alpha,\beta) \\
    0 &amp; \text{otherwise}.
\end{cases}\]</span> for a threshold function <span class="math inline">\(f\)</span> that you must find. (Hint: For a classifier <span class="math inline">\(\phi\)</span>, <span class="math inline">\(FP=P(\phi=1, y=0)\)</span> and <span class="math inline">\(FN=P(\phi=0, y=1)\)</span>. Marginalize these joint probabilities over <span class="math inline">\(x\)</span> and simplify.)</p></li>
<li><p><strong>(Written + Coding, 5 points).</strong> Implement <code>classifier_metrics</code> in <code>lpme/main.py</code>. After doing so, run <code>plot_confusion_region</code> and attach the plot. What do you notice about the region of possible confusion matrices?</p></li>
<li><p><strong>(Coding, 15 points).</strong> Implement <code>search_theta</code> in order to elicit the metric used by the oracle (which is parametrized by <span class="math inline">\(\theta\)</span>). Play around with the oracle’s theta and run <code>start_search</code> to see how close you can approximate it!</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="fdc7603b" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="kw">class</span> DataDistribution:</span>
<span id="cb9-6"><a href="#cb9-6"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, N: <span class="bu">int</span>):</span>
<span id="cb9-7"><a href="#cb9-7"></a>        <span class="co">"""</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="co">        Initializes the data distribution with a specified number of samples.</span></span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="co">        </span></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">        Args:</span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="co">        - N (int): Number of data points.</span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="co">        """</span></span>
<span id="cb9-13"><a href="#cb9-13"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="fl">0.3356</span>, <span class="op">-</span><span class="fl">1.4104</span>, <span class="fl">0.3144</span>, <span class="op">-</span><span class="fl">0.5591</span>, <span class="fl">1.0426</span>, <span class="fl">0.6036</span>, <span class="op">-</span><span class="fl">0.7549</span>, <span class="op">-</span><span class="fl">1.1909</span>, <span class="fl">1.4779</span>, <span class="op">-</span><span class="fl">0.7513</span>])</span>
<span id="cb9-14"><a href="#cb9-14"></a>        <span class="va">self</span>.D <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.weights)</span>
<span id="cb9-15"><a href="#cb9-15"></a></span>
<span id="cb9-16"><a href="#cb9-16"></a>        gen <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">42</span>)</span>
<span id="cb9-17"><a href="#cb9-17"></a>        <span class="va">self</span>.data <span class="op">=</span> torch.randn(N, <span class="va">self</span>.D, generator<span class="op">=</span>gen)</span>
<span id="cb9-18"><a href="#cb9-18"></a>        <span class="va">self</span>.probs <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.data <span class="op">@</span> <span class="va">self</span>.weights)</span>
<span id="cb9-19"><a href="#cb9-19"></a>    </span>
<span id="cb9-20"><a href="#cb9-20"></a><span class="kw">def</span> classifier_metrics(data_dist, threshold, upper<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-21"><a href="#cb9-21"></a>    <span class="co">"""</span></span>
<span id="cb9-22"><a href="#cb9-22"></a><span class="co">    Computes the True Positive and True Negative rates based on a classifier threshold.</span></span>
<span id="cb9-23"><a href="#cb9-23"></a><span class="co">    </span></span>
<span id="cb9-24"><a href="#cb9-24"></a><span class="co">    Args:</span></span>
<span id="cb9-25"><a href="#cb9-25"></a><span class="co">    - data_dist (DataDistribution): The data distribution instance.</span></span>
<span id="cb9-26"><a href="#cb9-26"></a><span class="co">    - threshold (float): Threshold value for classification.</span></span>
<span id="cb9-27"><a href="#cb9-27"></a><span class="co">    - upper (bool): If True, classifies as positive if above threshold; else, if below.</span></span>
<span id="cb9-28"><a href="#cb9-28"></a><span class="co">    </span></span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="co">    Returns:</span></span>
<span id="cb9-30"><a href="#cb9-30"></a><span class="co">    - tuple (float, float): True Positive Rate (TP) and True Negative Rate (TN) in that order.</span></span>
<span id="cb9-31"><a href="#cb9-31"></a><span class="co">    """</span></span>
<span id="cb9-32"><a href="#cb9-32"></a>    <span class="co"># YOUR CODE HERE (~3-5 lines)</span></span>
<span id="cb9-33"><a href="#cb9-33"></a>    <span class="cf">pass</span></span>
<span id="cb9-34"><a href="#cb9-34"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb9-35"><a href="#cb9-35"></a></span>
<span id="cb9-36"><a href="#cb9-36"></a><span class="kw">def</span> sweep_classifiers(data_dist: DataDistribution):</span>
<span id="cb9-37"><a href="#cb9-37"></a>    <span class="co">"""</span></span>
<span id="cb9-38"><a href="#cb9-38"></a><span class="co">    Sweeps through classifier thresholds and calculates True Positive and True Negative rates.</span></span>
<span id="cb9-39"><a href="#cb9-39"></a><span class="co">    </span></span>
<span id="cb9-40"><a href="#cb9-40"></a><span class="co">    Args:</span></span>
<span id="cb9-41"><a href="#cb9-41"></a><span class="co">    - data_dist (DataDistribution): The data distribution instance.</span></span>
<span id="cb9-42"><a href="#cb9-42"></a><span class="co">    </span></span>
<span id="cb9-43"><a href="#cb9-43"></a><span class="co">    Returns:</span></span>
<span id="cb9-44"><a href="#cb9-44"></a><span class="co">    - tuple: Upper and lower boundary data for True Positive and True Negative rates.</span></span>
<span id="cb9-45"><a href="#cb9-45"></a><span class="co">    """</span></span>
<span id="cb9-46"><a href="#cb9-46"></a>    thresholds <span class="op">=</span> torch.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb9-47"><a href="#cb9-47"></a>    upper_boundary <span class="op">=</span> []</span>
<span id="cb9-48"><a href="#cb9-48"></a>    lower_boundary <span class="op">=</span> []</span>
<span id="cb9-49"><a href="#cb9-49"></a>    </span>
<span id="cb9-50"><a href="#cb9-50"></a>    <span class="cf">for</span> threshold <span class="kw">in</span> tqdm(thresholds, desc<span class="op">=</span><span class="st">"Thresholds"</span>):</span>
<span id="cb9-51"><a href="#cb9-51"></a>        tp_upper, tn_upper <span class="op">=</span> classifier_metrics(data_dist, threshold, upper<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-52"><a href="#cb9-52"></a>        upper_boundary.append((tp_upper, tn_upper))</span>
<span id="cb9-53"><a href="#cb9-53"></a></span>
<span id="cb9-54"><a href="#cb9-54"></a>        tp_lower, tn_lower <span class="op">=</span> classifier_metrics(data_dist, threshold, upper<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-55"><a href="#cb9-55"></a>        lower_boundary.append((tp_lower, tn_lower))</span>
<span id="cb9-56"><a href="#cb9-56"></a></span>
<span id="cb9-57"><a href="#cb9-57"></a>    <span class="cf">return</span> upper_boundary, lower_boundary</span>
<span id="cb9-58"><a href="#cb9-58"></a></span>
<span id="cb9-59"><a href="#cb9-59"></a><span class="kw">class</span> Oracle:</span>
<span id="cb9-60"><a href="#cb9-60"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, theta: <span class="bu">float</span>):</span>
<span id="cb9-61"><a href="#cb9-61"></a>        <span class="co">"""</span></span>
<span id="cb9-62"><a href="#cb9-62"></a><span class="co">        Initializes the oracle with a given theta for preference evaluation.</span></span>
<span id="cb9-63"><a href="#cb9-63"></a><span class="co">        </span></span>
<span id="cb9-64"><a href="#cb9-64"></a><span class="co">        Args:</span></span>
<span id="cb9-65"><a href="#cb9-65"></a><span class="co">        - theta (float): Oracle angle in radians.</span></span>
<span id="cb9-66"><a href="#cb9-66"></a><span class="co">        """</span></span>
<span id="cb9-67"><a href="#cb9-67"></a>        <span class="va">self</span>.theta <span class="op">=</span> torch.tensor(theta)</span>
<span id="cb9-68"><a href="#cb9-68"></a></span>
<span id="cb9-69"><a href="#cb9-69"></a>    <span class="kw">def</span> evaluate_lpm(<span class="va">self</span>, tp, tn):</span>
<span id="cb9-70"><a href="#cb9-70"></a>        <span class="co">"""</span></span>
<span id="cb9-71"><a href="#cb9-71"></a><span class="co">        Computes the linear performance metric (LPM) based on theta.</span></span>
<span id="cb9-72"><a href="#cb9-72"></a><span class="co">        </span></span>
<span id="cb9-73"><a href="#cb9-73"></a><span class="co">        Args:</span></span>
<span id="cb9-74"><a href="#cb9-74"></a><span class="co">        - tp (float): True Positive rate.</span></span>
<span id="cb9-75"><a href="#cb9-75"></a><span class="co">        - tn (float): True Negative rate.</span></span>
<span id="cb9-76"><a href="#cb9-76"></a><span class="co">        </span></span>
<span id="cb9-77"><a href="#cb9-77"></a><span class="co">        Returns:</span></span>
<span id="cb9-78"><a href="#cb9-78"></a><span class="co">        - float: Linear performance metric evaluation.</span></span>
<span id="cb9-79"><a href="#cb9-79"></a><span class="co">        """</span></span>
<span id="cb9-80"><a href="#cb9-80"></a>        <span class="cf">return</span> torch.cos(<span class="va">self</span>.theta) <span class="op">*</span> tp <span class="op">+</span> torch.sin(<span class="va">self</span>.theta) <span class="op">*</span> tn</span>
<span id="cb9-81"><a href="#cb9-81"></a>    </span>
<span id="cb9-82"><a href="#cb9-82"></a>    <span class="kw">def</span> preferred_classifier(<span class="va">self</span>, tp_1, tn_1, tp_2, tn_2):</span>
<span id="cb9-83"><a href="#cb9-83"></a>        <span class="co">"""</span></span>
<span id="cb9-84"><a href="#cb9-84"></a><span class="co">        Determines the preferred classifier based on LPM values.</span></span>
<span id="cb9-85"><a href="#cb9-85"></a><span class="co">        </span></span>
<span id="cb9-86"><a href="#cb9-86"></a><span class="co">        Args:</span></span>
<span id="cb9-87"><a href="#cb9-87"></a><span class="co">        - tp_1, tn_1, tp_2, tn_2 (float): True Positive and True Negative rates for two classifiers.</span></span>
<span id="cb9-88"><a href="#cb9-88"></a><span class="co">        </span></span>
<span id="cb9-89"><a href="#cb9-89"></a><span class="co">        Returns:</span></span>
<span id="cb9-90"><a href="#cb9-90"></a><span class="co">        - bool: True if first classifier is preferred, False otherwise.</span></span>
<span id="cb9-91"><a href="#cb9-91"></a><span class="co">        """</span></span>
<span id="cb9-92"><a href="#cb9-92"></a>        lpm_1 <span class="op">=</span> <span class="va">self</span>.evaluate_lpm(tp_1, tn_1)</span>
<span id="cb9-93"><a href="#cb9-93"></a>        lpm_2 <span class="op">=</span> <span class="va">self</span>.evaluate_lpm(tp_2, tn_2)</span>
<span id="cb9-94"><a href="#cb9-94"></a>        <span class="cf">return</span> (lpm_1 <span class="op">&gt;</span> lpm_2).item()</span>
<span id="cb9-95"><a href="#cb9-95"></a>    </span>
<span id="cb9-96"><a href="#cb9-96"></a><span class="kw">def</span> theta_to_threshold(theta):</span>
<span id="cb9-97"><a href="#cb9-97"></a>    <span class="co">"""Converts theta angle to classification threshold."""</span></span>
<span id="cb9-98"><a href="#cb9-98"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> torch.tan(theta) <span class="op">**</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-99"><a href="#cb9-99"></a></span>
<span id="cb9-100"><a href="#cb9-100"></a><span class="kw">def</span> search_theta(oracle: Oracle, data_dist, lower_bound, upper_bound):</span>
<span id="cb9-101"><a href="#cb9-101"></a>    <span class="co">"""</span></span>
<span id="cb9-102"><a href="#cb9-102"></a><span class="co">    Performs a search over theta values to optimize the classification threshold.</span></span>
<span id="cb9-103"><a href="#cb9-103"></a><span class="co">    </span></span>
<span id="cb9-104"><a href="#cb9-104"></a><span class="co">    Args:</span></span>
<span id="cb9-105"><a href="#cb9-105"></a><span class="co">    - oracle (Oracle): The oracle for LPM evaluation.</span></span>
<span id="cb9-106"><a href="#cb9-106"></a><span class="co">    - data_dist (DataDistribution): The data distribution instance.</span></span>
<span id="cb9-107"><a href="#cb9-107"></a><span class="co">    - lower_bound (float): Lower bound for theta.</span></span>
<span id="cb9-108"><a href="#cb9-108"></a><span class="co">    - upper_bound (float): Upper bound for theta.</span></span>
<span id="cb9-109"><a href="#cb9-109"></a><span class="co">    </span></span>
<span id="cb9-110"><a href="#cb9-110"></a><span class="co">    Returns:</span></span>
<span id="cb9-111"><a href="#cb9-111"></a><span class="co">    - tuple: Updated lower and upper bounds for theta.</span></span>
<span id="cb9-112"><a href="#cb9-112"></a><span class="co">    """</span></span>
<span id="cb9-113"><a href="#cb9-113"></a>    left <span class="op">=</span> <span class="fl">0.75</span> <span class="op">*</span> lower_bound <span class="op">+</span> <span class="fl">0.25</span> <span class="op">*</span> upper_bound</span>
<span id="cb9-114"><a href="#cb9-114"></a>    middle <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> lower_bound <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> upper_bound</span>
<span id="cb9-115"><a href="#cb9-115"></a>    right <span class="op">=</span> <span class="fl">0.25</span> <span class="op">*</span> lower_bound <span class="op">+</span> <span class="fl">0.75</span> <span class="op">*</span> upper_bound</span>
<span id="cb9-116"><a href="#cb9-116"></a></span>
<span id="cb9-117"><a href="#cb9-117"></a>    thetas <span class="op">=</span> [lower_bound, left, middle, right, upper_bound]</span>
<span id="cb9-118"><a href="#cb9-118"></a>    thresholds <span class="op">=</span> theta_to_threshold(torch.tensor(thetas))</span>
<span id="cb9-119"><a href="#cb9-119"></a>    new_lower, new_upper <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb9-120"><a href="#cb9-120"></a></span>
<span id="cb9-121"><a href="#cb9-121"></a>    <span class="co"># YOUR CODE HERE (~18-25 lines)</span></span>
<span id="cb9-122"><a href="#cb9-122"></a>    <span class="co"># 1. Collect metrics for each threshold value.</span></span>
<span id="cb9-123"><a href="#cb9-123"></a>    <span class="co"># 2. Determine if LPM increases as theta increases.</span></span>
<span id="cb9-124"><a href="#cb9-124"></a>    <span class="co"># 3. Check for pattern of increases and decreases in LPM.</span></span>
<span id="cb9-125"><a href="#cb9-125"></a>    <span class="co"># 4. Update bounds based on observed LPM patterns.</span></span>
<span id="cb9-126"><a href="#cb9-126"></a>    <span class="cf">pass</span></span>
<span id="cb9-127"><a href="#cb9-127"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb9-128"><a href="#cb9-128"></a></span>
<span id="cb9-129"><a href="#cb9-129"></a>    <span class="cf">return</span> new_lower, new_upper</span>
<span id="cb9-130"><a href="#cb9-130"></a></span>
<span id="cb9-131"><a href="#cb9-131"></a><span class="co"># Create instance and get upper &amp; lower boundary data</span></span>
<span id="cb9-132"><a href="#cb9-132"></a>data_dist <span class="op">=</span> DataDistribution(N<span class="op">=</span><span class="dv">10000000</span>)</span>
<span id="cb9-133"><a href="#cb9-133"></a>oracle <span class="op">=</span> Oracle(theta<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb9-134"><a href="#cb9-134"></a></span>
<span id="cb9-135"><a href="#cb9-135"></a><span class="kw">def</span> plot_confusion_region():</span>
<span id="cb9-136"><a href="#cb9-136"></a>    <span class="co">"""</span></span>
<span id="cb9-137"><a href="#cb9-137"></a><span class="co">    Plots the True Positive vs. True Negative rates for the upper and lower classifier boundaries.</span></span>
<span id="cb9-138"><a href="#cb9-138"></a><span class="co">    """</span></span>
<span id="cb9-139"><a href="#cb9-139"></a>    upper_boundary, lower_boundary <span class="op">=</span> sweep_classifiers(data_dist)</span>
<span id="cb9-140"><a href="#cb9-140"></a></span>
<span id="cb9-141"><a href="#cb9-141"></a>    <span class="co"># Prepare data for plotting for upper and lower boundaries</span></span>
<span id="cb9-142"><a href="#cb9-142"></a>    tp_upper, tn_upper <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>upper_boundary)</span>
<span id="cb9-143"><a href="#cb9-143"></a>    tp_lower, tn_lower <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>lower_boundary)</span>
<span id="cb9-144"><a href="#cb9-144"></a></span>
<span id="cb9-145"><a href="#cb9-145"></a>    <span class="co"># Plot the results for upper boundary</span></span>
<span id="cb9-146"><a href="#cb9-146"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb9-147"><a href="#cb9-147"></a>    plt.plot(tp_upper, tn_upper, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">"Upper Boundary"</span>)</span>
<span id="cb9-148"><a href="#cb9-148"></a>    plt.plot(tp_lower, tn_lower, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">"Lower Boundary"</span>)</span>
<span id="cb9-149"><a href="#cb9-149"></a>    plt.title(<span class="st">"True Positive vs. True Negative Rates (Upper &amp; Lower Boundaries)"</span>)</span>
<span id="cb9-150"><a href="#cb9-150"></a>    plt.xlabel(<span class="st">"True Positive Rate (TP)"</span>)</span>
<span id="cb9-151"><a href="#cb9-151"></a>    plt.ylabel(<span class="st">"True Negative Rate (TN)"</span>)</span>
<span id="cb9-152"><a href="#cb9-152"></a>    plt.legend()</span>
<span id="cb9-153"><a href="#cb9-153"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb9-154"><a href="#cb9-154"></a>    plt.show()</span>
<span id="cb9-155"><a href="#cb9-155"></a></span>
<span id="cb9-156"><a href="#cb9-156"></a><span class="kw">def</span> start_search():</span>
<span id="cb9-157"><a href="#cb9-157"></a>    <span class="co">"""</span></span>
<span id="cb9-158"><a href="#cb9-158"></a><span class="co">    Starts the theta search using the LPM-based oracle and prints the search range per iteration.</span></span>
<span id="cb9-159"><a href="#cb9-159"></a><span class="co">    """</span></span>
<span id="cb9-160"><a href="#cb9-160"></a>    lower_bound <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-161"><a href="#cb9-161"></a>    upper_bound <span class="op">=</span> torch.pi <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb9-162"><a href="#cb9-162"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(<span class="dv">10</span>), desc<span class="op">=</span><span class="st">"LPM Search"</span>):</span>
<span id="cb9-163"><a href="#cb9-163"></a>        <span class="bu">print</span>(<span class="ss">f"Theta Search Space: [</span><span class="sc">{</span>lower_bound<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>upper_bound<span class="sc">}</span><span class="ss">]"</span>)</span>
<span id="cb9-164"><a href="#cb9-164"></a>        lower_bound, upper_bound <span class="op">=</span> search_theta(oracle, data_dist, lower_bound<span class="op">=</span>lower_bound, upper_bound<span class="op">=</span>upper_bound)</span>
<span id="cb9-165"><a href="#cb9-165"></a>    <span class="bu">print</span>(<span class="ss">f"Theta Search Space: [</span><span class="sc">{</span>lower_bound<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>upper_bound<span class="sc">}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-4-d-optimal-design-with-logistic-model-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-4-d-optimal-design-with-logistic-model-30-points">Question 4: D-optimal Design with Logistic Model (30 points)</h3>
<p>In this question, we explore D-optimal designs in the context of the Bradley-Terry model. The Bradley-Terry model is a logistic regression model used for paired comparison data. Given two items <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, the probability that item <span class="math inline">\(x_1\)</span> is preferred over <span class="math inline">\(x_2\)</span> is modeled as:</p>
<p><span class="math display">\[P(x_1 \succ x_2 | \theta) = \frac{e^{\theta^\top x_1}}{e^{\theta^\top x_1} + e^{\theta^\top x_2}} = \frac{1}{1 + e^{\theta^\top (x_2 - x_1)}}\]</span></p>
<p>where <span class="math inline">\(\theta \in \mathbb{R}^d\)</span> represents the unknown model parameters, and <span class="math inline">\(x_1, x_2 \in \mathbb{R}^d\)</span> are the feature vectors associated with the two items. D-optimal design aims to maximize the determinant of the Fisher information matrix, thus minimizing the volume of the confidence ellipsoid for the estimated parameters. In this exercise, you will analyze D-optimal designs for this model.</p>
<ol type="a">
<li><p><strong>Fisher Information Matrix for the Bradley-Terry Model (12 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 6 points).</strong> Derive the Fisher information matrix for the Bradley-Terry model at a design point <span class="math inline">\((x_1, x_2)\)</span>. Show that the Fisher information matrix at a design point is: <span class="math display">\[I(x_1, x_2, \theta) = w(x_1, x_2, \theta) (x_1 - x_2)(x_1 - x_2)^\top,\]</span> where <span class="math inline">\(w(x_1, x_2, \theta)\)</span> is a weight function given by: <span class="math display">\[w(x_1, x_2, \theta) = \frac{e^{\theta^\top x_1} e^{\theta^\top x_2}}{\left(e^{\theta^\top x_1} + e^{\theta^\top x_2}\right)^2} =\sigma'(\theta^\top (x_1-x_2)).\]</span> <span class="math inline">\(\sigma'\)</span> is the derivative of the sigmoid function.</p></li>
<li><p><strong>(Coding, 6 points).</strong> Implement <code>fisher_matrix</code> in <code>d_optimal/main.py</code> based on the derived expression.</p></li>
</ol></li>
<li><p><strong>D-optimal Design Criterion (18 points)</strong></p>
<ol type="i">
<li><p><strong>(Coding, 11 points).</strong> In the context of the Bradley-Terry model, a D-optimal design maximizes the determinant of the Fisher information matrix. Suppose we have a set of candidate items <span class="math inline">\(\{x_1, \dots, x_n\}\)</span>, and we can choose <span class="math inline">\(N\)</span> comparisons to make. Formally, the D-optimal design maximizes: <span class="math display">\[\det\left( \sum_{i=1}^N w(x_{i1}, x_{i2}, \theta) (x_{i1} - x_{i2})(x_{i1} - x_{i2})^\top \right),\]</span> where <span class="math inline">\((x_{i1}, x_{i2})\)</span> denotes a pair of compared items in the design. Implement a greedy algorithm to approximate the D-optimal design. Given a set of <span class="math inline">\(n\)</span> items and their feature vectors <span class="math inline">\(\{x_1, \dots, x_n\}\)</span>, your task is to iteratively select the pair of items <span class="math inline">\((x_{i1}, x_{i2})\)</span> that maximizes the determinant of the Fisher information matrix. Please implement <code>greedy_fisher</code>. Note that the setup in the code assumes we have a dataset of all possible differences between pairs of items as opposed to directly selecting the pairs.</p></li>
<li><p><strong>(Written + Coding, 7 points).</strong> Notice that <code>posterior_inv_cov</code> uses a Laplace approximation for the posterior centered around the ground truth weights after labeling the chosen points. However, it turns out this approximation doesn’t actually depend on the labels when taking the Hessian. Please run the file <code>d_optimal/main.py</code> and attach a plot of the norm of the covariance matrix of the posterior. What difference do you observe between greedy and random sampling? What is the win rate of greedy?</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="e9753deb" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb10-6"><a href="#cb10-6"></a>    <span class="co">"""Helper function to compute the sigmoid of x."""</span></span>
<span id="cb10-7"><a href="#cb10-7"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="kw">class</span> LogisticData:</span>
<span id="cb10-10"><a href="#cb10-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weights, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="co">"""</span></span>
<span id="cb10-12"><a href="#cb10-12"></a><span class="co">        Initializes the LogisticData class with specified weights and seed.</span></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="co">        </span></span>
<span id="cb10-14"><a href="#cb10-14"></a><span class="co">        Args:</span></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="co">        - weights (np.array): True weights for data generation.</span></span>
<span id="cb10-16"><a href="#cb10-16"></a><span class="co">        - seed (int): Random seed for reproducibility.</span></span>
<span id="cb10-17"><a href="#cb10-17"></a><span class="co">        """</span></span>
<span id="cb10-18"><a href="#cb10-18"></a>        <span class="va">self</span>.rng <span class="op">=</span> np.random.default_rng(seed)</span>
<span id="cb10-19"><a href="#cb10-19"></a>        <span class="va">self</span>.weights <span class="op">=</span> weights</span>
<span id="cb10-20"><a href="#cb10-20"></a>    </span>
<span id="cb10-21"><a href="#cb10-21"></a>    <span class="kw">def</span> generate_data(<span class="va">self</span>, N):</span>
<span id="cb10-22"><a href="#cb10-22"></a>        <span class="co">"""</span></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="co">        Generates synthetic data for logistic regression.</span></span>
<span id="cb10-24"><a href="#cb10-24"></a><span class="co">        </span></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="co">        Args:</span></span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="co">        - N (int): Number of data points.</span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="co">        </span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="co">        Returns:</span></span>
<span id="cb10-29"><a href="#cb10-29"></a><span class="co">        - tuple: Generated data and labels.</span></span>
<span id="cb10-30"><a href="#cb10-30"></a><span class="co">        """</span></span>
<span id="cb10-31"><a href="#cb10-31"></a>        data <span class="op">=</span> <span class="va">self</span>.rng.standard_normal((N, <span class="bu">len</span>(<span class="va">self</span>.weights)))</span>
<span id="cb10-32"><a href="#cb10-32"></a>        probs <span class="op">=</span> sigmoid(data <span class="op">@</span> <span class="va">self</span>.weights)</span>
<span id="cb10-33"><a href="#cb10-33"></a>        labels <span class="op">=</span> (<span class="va">self</span>.rng.random(N) <span class="op">&lt;</span> probs).astype(<span class="bu">int</span>)</span>
<span id="cb10-34"><a href="#cb10-34"></a>        <span class="cf">return</span> data, labels</span>
<span id="cb10-35"><a href="#cb10-35"></a></span>
<span id="cb10-36"><a href="#cb10-36"></a><span class="kw">def</span> fisher_matrix(difference_vector, weights):</span>
<span id="cb10-37"><a href="#cb10-37"></a>    <span class="co">"""</span></span>
<span id="cb10-38"><a href="#cb10-38"></a><span class="co">    Computes the Fisher information matrix for a single data point.</span></span>
<span id="cb10-39"><a href="#cb10-39"></a><span class="co">    </span></span>
<span id="cb10-40"><a href="#cb10-40"></a><span class="co">    Args:</span></span>
<span id="cb10-41"><a href="#cb10-41"></a><span class="co">    - difference_vector (np.array): Difference vector (input data point).</span></span>
<span id="cb10-42"><a href="#cb10-42"></a><span class="co">    - weights (np.array): Weights for the logistic model.</span></span>
<span id="cb10-43"><a href="#cb10-43"></a><span class="co">    </span></span>
<span id="cb10-44"><a href="#cb10-44"></a><span class="co">    Returns:</span></span>
<span id="cb10-45"><a href="#cb10-45"></a><span class="co">    - np.array: Fisher information matrix for the data point.</span></span>
<span id="cb10-46"><a href="#cb10-46"></a><span class="co">    """</span></span>
<span id="cb10-47"><a href="#cb10-47"></a>    <span class="co"># YOUR CODE HERE (~2-4 lines)</span></span>
<span id="cb10-48"><a href="#cb10-48"></a>    <span class="cf">pass</span></span>
<span id="cb10-49"><a href="#cb10-49"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb10-50"><a href="#cb10-50"></a></span>
<span id="cb10-51"><a href="#cb10-51"></a><span class="co"># Initialization</span></span>
<span id="cb10-52"><a href="#cb10-52"></a>true_weights <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.3356</span>, <span class="op">-</span><span class="fl">1.4104</span>, <span class="fl">0.3144</span>, <span class="op">-</span><span class="fl">0.5591</span>, <span class="fl">1.0426</span>, <span class="fl">0.6036</span>, <span class="op">-</span><span class="fl">0.7549</span>, <span class="op">-</span><span class="fl">1.1909</span>, <span class="fl">1.4779</span>, <span class="op">-</span><span class="fl">0.7513</span>])</span>
<span id="cb10-53"><a href="#cb10-53"></a>data_dim <span class="op">=</span> <span class="bu">len</span>(true_weights)</span>
<span id="cb10-54"><a href="#cb10-54"></a>dataset_generator <span class="op">=</span> LogisticData(weights<span class="op">=</span>true_weights)</span>
<span id="cb10-55"><a href="#cb10-55"></a></span>
<span id="cb10-56"><a href="#cb10-56"></a><span class="co"># Number of iterations for sampling 500 points</span></span>
<span id="cb10-57"><a href="#cb10-57"></a>num_iterations <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb10-58"><a href="#cb10-58"></a></span>
<span id="cb10-59"><a href="#cb10-59"></a><span class="co"># Store covariance matrix norms for comparison</span></span>
<span id="cb10-60"><a href="#cb10-60"></a>cov_norms_greedy <span class="op">=</span> []</span>
<span id="cb10-61"><a href="#cb10-61"></a>cov_norms_random <span class="op">=</span> []</span>
<span id="cb10-62"><a href="#cb10-62"></a></span>
<span id="cb10-63"><a href="#cb10-63"></a><span class="kw">def</span> greedy_fisher(data, curr_fisher_matrix, selected_indices):</span>
<span id="cb10-64"><a href="#cb10-64"></a>    <span class="co">"""</span></span>
<span id="cb10-65"><a href="#cb10-65"></a><span class="co">    Selects the data point that maximizes the Fisher information determinant.</span></span>
<span id="cb10-66"><a href="#cb10-66"></a><span class="co">    </span></span>
<span id="cb10-67"><a href="#cb10-67"></a><span class="co">    Args:</span></span>
<span id="cb10-68"><a href="#cb10-68"></a><span class="co">    - data (np.array): The data matrix.</span></span>
<span id="cb10-69"><a href="#cb10-69"></a><span class="co">    - curr_fisher_matrix (np.array): Fisher matrix of already selected indices.</span></span>
<span id="cb10-70"><a href="#cb10-70"></a><span class="co">    - selected_indices (list): List of already selected indices.</span></span>
<span id="cb10-71"><a href="#cb10-71"></a><span class="co">    </span></span>
<span id="cb10-72"><a href="#cb10-72"></a><span class="co">    Returns:</span></span>
<span id="cb10-73"><a href="#cb10-73"></a><span class="co">    - int: Index of the selected data point.</span></span>
<span id="cb10-74"><a href="#cb10-74"></a><span class="co">    """</span></span>
<span id="cb10-75"><a href="#cb10-75"></a>    best_det <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb10-76"><a href="#cb10-76"></a>    best_index <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb10-77"><a href="#cb10-77"></a>    </span>
<span id="cb10-78"><a href="#cb10-78"></a>    <span class="co"># Iterate over data points to find the one maximizing Fisher determinant.</span></span>
<span id="cb10-79"><a href="#cb10-79"></a>    <span class="cf">for</span> i, difference_vector <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb10-80"><a href="#cb10-80"></a>        <span class="co"># YOUR CODE HERE (~5-10 lines)</span></span>
<span id="cb10-81"><a href="#cb10-81"></a>        <span class="co"># Make sure to skip already selected data points!</span></span>
<span id="cb10-82"><a href="#cb10-82"></a>        <span class="cf">pass</span></span>
<span id="cb10-83"><a href="#cb10-83"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb10-84"><a href="#cb10-84"></a>    <span class="cf">return</span> best_index</span>
<span id="cb10-85"><a href="#cb10-85"></a></span>
<span id="cb10-86"><a href="#cb10-86"></a><span class="kw">def</span> posterior_inv_cov(X, laplace_center):</span>
<span id="cb10-87"><a href="#cb10-87"></a>    <span class="co">"""</span></span>
<span id="cb10-88"><a href="#cb10-88"></a><span class="co">    Computes the posterior inverse covariance matrix using Laplace approximation.</span></span>
<span id="cb10-89"><a href="#cb10-89"></a><span class="co">    </span></span>
<span id="cb10-90"><a href="#cb10-90"></a><span class="co">    Args:</span></span>
<span id="cb10-91"><a href="#cb10-91"></a><span class="co">    - X (np.array): Data matrix.</span></span>
<span id="cb10-92"><a href="#cb10-92"></a><span class="co">    - laplace_center (np.array): Center point (weights).</span></span>
<span id="cb10-93"><a href="#cb10-93"></a><span class="co">    </span></span>
<span id="cb10-94"><a href="#cb10-94"></a><span class="co">    Returns:</span></span>
<span id="cb10-95"><a href="#cb10-95"></a><span class="co">    - np.array: Posterior inverse covariance matrix.</span></span>
<span id="cb10-96"><a href="#cb10-96"></a><span class="co">    """</span></span>
<span id="cb10-97"><a href="#cb10-97"></a>    <span class="co"># Calculate probabilities for logistic regression model.</span></span>
<span id="cb10-98"><a href="#cb10-98"></a>    probs <span class="op">=</span> sigmoid(X <span class="op">@</span> laplace_center)</span>
<span id="cb10-99"><a href="#cb10-99"></a>    W <span class="op">=</span> np.diag(probs <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> probs))</span>
<span id="cb10-100"><a href="#cb10-100"></a>    </span>
<span id="cb10-101"><a href="#cb10-101"></a>    <span class="co"># Compute inverse covariance matrix assuming standard Gaussian prior.</span></span>
<span id="cb10-102"><a href="#cb10-102"></a>    inv_cov <span class="op">=</span> X.T <span class="op">@</span> W <span class="op">@</span> X <span class="op">+</span> np.eye(<span class="bu">len</span>(true_weights))</span>
<span id="cb10-103"><a href="#cb10-103"></a>    <span class="cf">return</span> inv_cov</span>
<span id="cb10-104"><a href="#cb10-104"></a></span>
<span id="cb10-105"><a href="#cb10-105"></a><span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_iterations)):</span>
<span id="cb10-106"><a href="#cb10-106"></a>    <span class="co"># Generate a new sample of 500 data points</span></span>
<span id="cb10-107"><a href="#cb10-107"></a>    data, _ <span class="op">=</span> dataset_generator.generate_data(N<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb10-108"><a href="#cb10-108"></a>    </span>
<span id="cb10-109"><a href="#cb10-109"></a>    <span class="co"># Greedy selection of best 30 data points</span></span>
<span id="cb10-110"><a href="#cb10-110"></a>    selected_indices <span class="op">=</span> []</span>
<span id="cb10-111"><a href="#cb10-111"></a>    curr_fisher_matrix <span class="op">=</span> np.zeros((data_dim, data_dim))</span>
<span id="cb10-112"><a href="#cb10-112"></a></span>
<span id="cb10-113"><a href="#cb10-113"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">30</span>):</span>
<span id="cb10-114"><a href="#cb10-114"></a>        <span class="co"># Select the data point maximizing Fisher information determinant.</span></span>
<span id="cb10-115"><a href="#cb10-115"></a>        best_index <span class="op">=</span> greedy_fisher(data, curr_fisher_matrix, selected_indices)</span>
<span id="cb10-116"><a href="#cb10-116"></a>        selected_indices.append(best_index)</span>
<span id="cb10-117"><a href="#cb10-117"></a>        curr_fisher_matrix <span class="op">+=</span> fisher_matrix(data[best_index], true_weights)</span>
<span id="cb10-118"><a href="#cb10-118"></a></span>
<span id="cb10-119"><a href="#cb10-119"></a>    <span class="co"># Prepare greedy and random samples</span></span>
<span id="cb10-120"><a href="#cb10-120"></a>    X_greedy <span class="op">=</span> data[selected_indices]</span>
<span id="cb10-121"><a href="#cb10-121"></a></span>
<span id="cb10-122"><a href="#cb10-122"></a>    <span class="co"># Generate 30 random samples for comparison</span></span>
<span id="cb10-123"><a href="#cb10-123"></a>    random_indices <span class="op">=</span> np.random.choice(<span class="bu">len</span>(data), <span class="dv">30</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-124"><a href="#cb10-124"></a>    X_random <span class="op">=</span> data[random_indices]</span>
<span id="cb10-125"><a href="#cb10-125"></a></span>
<span id="cb10-126"><a href="#cb10-126"></a>    <span class="co"># Compute posterior inverse covariance matrices for both strategies</span></span>
<span id="cb10-127"><a href="#cb10-127"></a>    posterior_inv_cov_greedy <span class="op">=</span> posterior_inv_cov(X_greedy, laplace_center<span class="op">=</span>true_weights) </span>
<span id="cb10-128"><a href="#cb10-128"></a>    posterior_inv_cov_random <span class="op">=</span> posterior_inv_cov(X_random, laplace_center<span class="op">=</span>true_weights)</span>
<span id="cb10-129"><a href="#cb10-129"></a></span>
<span id="cb10-130"><a href="#cb10-130"></a>    <span class="co"># Calculate covariance matrices (inverse of posterior inverse covariance)</span></span>
<span id="cb10-131"><a href="#cb10-131"></a>    cov_matrix_greedy <span class="op">=</span> np.linalg.inv(posterior_inv_cov_greedy)</span>
<span id="cb10-132"><a href="#cb10-132"></a>    cov_matrix_random <span class="op">=</span> np.linalg.inv(posterior_inv_cov_random)</span>
<span id="cb10-133"><a href="#cb10-133"></a></span>
<span id="cb10-134"><a href="#cb10-134"></a>    <span class="co"># Measure the norm (Frobenius norm) of the covariance matrices</span></span>
<span id="cb10-135"><a href="#cb10-135"></a>    cov_norm_greedy <span class="op">=</span> np.linalg.norm(cov_matrix_greedy, <span class="st">'fro'</span>)</span>
<span id="cb10-136"><a href="#cb10-136"></a>    cov_norm_random <span class="op">=</span> np.linalg.norm(cov_matrix_random, <span class="st">'fro'</span>)</span>
<span id="cb10-137"><a href="#cb10-137"></a></span>
<span id="cb10-138"><a href="#cb10-138"></a>    <span class="co"># Store norms for analysis</span></span>
<span id="cb10-139"><a href="#cb10-139"></a>    cov_norms_greedy.append(cov_norm_greedy)</span>
<span id="cb10-140"><a href="#cb10-140"></a>    cov_norms_random.append(cov_norm_random)</span>
<span id="cb10-141"><a href="#cb10-141"></a></span>
<span id="cb10-142"><a href="#cb10-142"></a><span class="co"># Display comparison results</span></span>
<span id="cb10-143"><a href="#cb10-143"></a><span class="bu">print</span>(<span class="ss">f'Greedy mean: </span><span class="sc">{</span>np<span class="sc">.</span>mean(cov_norms_greedy)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-144"><a href="#cb10-144"></a><span class="bu">print</span>(<span class="ss">f'Random mean: </span><span class="sc">{</span>np<span class="sc">.</span>mean(cov_norms_random)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-145"><a href="#cb10-145"></a><span class="bu">print</span>(<span class="ss">f'Greedy win rate: </span><span class="sc">{</span>(np.array(cov_norms_greedy) <span class="op">&lt;</span> np.array(cov_norms_random))<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-146"><a href="#cb10-146"></a></span>
<span id="cb10-147"><a href="#cb10-147"></a><span class="co"># Plot the distributions of covariance matrix norms</span></span>
<span id="cb10-148"><a href="#cb10-148"></a>plt.hist(cov_norms_greedy, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Greedy'</span>)</span>
<span id="cb10-149"><a href="#cb10-149"></a>plt.hist(cov_norms_random, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Random'</span>)</span>
<span id="cb10-150"><a href="#cb10-150"></a>plt.xlabel(<span class="st">'L2 Norm of Covariance Matrix'</span>)</span>
<span id="cb10-151"><a href="#cb10-151"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb10-152"><a href="#cb10-152"></a>plt.title(<span class="st">'Comparison of Covariance Norms (Greedy vs. Random) Across Iterations'</span>)</span>
<span id="cb10-153"><a href="#cb10-153"></a>plt.legend()</span>
<span id="cb10-154"><a href="#cb10-154"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-question-5-nonparametric-metric-elicitation-30-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="sec-question-5-nonparametric-metric-elicitation-30-points">Question 5: Nonparametric Metric Elicitation (30 points)</h3>
<p>In this question, we explore the problem of performance metric elicitation using a Gaussian Process (GP) to map the elements of the confusion matrix, specifically false positives (FP) and false negatives (FN), to an unknown performance metric. The goal is to learn a non-linear function that maps FP and FN to the metric, using relative preferences from pairwise classifier comparisons. We will use elliptical slice sampling for posterior inference.</p>
<ol type="a">
<li><p><strong>Gaussian Process for Metric Elicitation (10 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 2 points).</strong> Assume that the performance metric <span class="math inline">\(\phi(C)\)</span> is a non-linear function of the confusion matrix <span class="math inline">\(C\)</span>. For simplicity, assume that <span class="math inline">\(\phi\)</span> depends only on FP and FN, i.e., <span class="math display">\[\phi(\text{FP}, \text{FN}) \sim \mathcal{GP}(0, k((\text{FP}, \text{FN}), (\text{FP}', \text{FN}'))),\]</span> where <span class="math inline">\(k\)</span> is the covariance kernel function of the Gaussian Process. Explain why using a GP allows for flexible modeling of the metric <span class="math inline">\(\phi\)</span> as a non-linear function of FP and FN. What are the advantages of using a GP over a linear model in this context?</p></li>
<li><p><strong>(Written, 2 points).</strong> Suppose we observe pairwise comparisons between classifiers, where a user provides feedback on which classifier they prefer based on the unknown metric <span class="math inline">\(\phi\)</span>. Given two classifiers with confusion matrices <span class="math inline">\(C_1 = (\text{FP}_1, \text{FN}_1)\)</span> and <span class="math inline">\(C_2 = (\text{FP}_2, \text{FN}_2)\)</span>, the user indicates their relative preference. Let the observed preference be modeled by Bradley-Terry as: <span class="math display">\[\Pr(C_1 \succ C_2) = \sigma(\phi(\text{FP}_1, \text{FN}_1) - \phi(\text{FP}_2, \text{FN}_2)).\]</span> where we view <span class="math inline">\(\phi\)</span> as the reward function. How does this likelihood affect the posterior inference in the GP? Where does it introduce additional complexity?</p></li>
<li><p><strong>(Written + Coding, 6 points).</strong> Given a set of observed pairwise comparisons, derive the posterior distribution over the latent function values <span class="math inline">\(\phi\)</span> given a set of confusion matrices preferences using Bayes’ rule. Express the posterior distribution in terms of the GP prior and the pairwise likelihood function. You do not need to include the normalization constant. Implement the likelihood function in <code>loglik_from_preferences</code>.</p></li>
</ol></li>
<li><p><strong>Elliptical Slice Sampling for Posterior Inference (20 points)</strong></p>
<ol type="i">
<li><p><strong>(Written, 3 points).</strong> Read <a href="https://proceedings.mlr.press/v9/murray10a/murray10a.pdf" class="uri">https://proceedings.mlr.press/v9/murray10a/murray10a.pdf</a>. Elliptical slice sampling is a sampling method used to generate samples from the posterior distribution of a Gaussian Process. Explain the key idea behind elliptical slice sampling and why it is well-suited for sampling from the GP posterior in this context.</p></li>
<li><p><strong>(Coding, 10 points).</strong> Implement elliptical slice sampling in <code>npme/elliptical_sampler.py</code> by following Figure 2 in the paper.</p></li>
<li><p><strong>(Written, 3 points).</strong> Run the algorithm on a synthetic preference dataset of confusion matrices with pairwise preferences. The synthetic data will be constructed using the metric <span class="math display">\[\phi_{\text{true}}(\text{FP}, \text{FN}) = \log(1 + \text{FP}) + \log(1 + \text{FN}),\]</span> which captures the idea that the human oracle perceives both false positives and false negatives in a way that flattens out as these values increase (i.e., marginal increases in FP and FN have diminishing effects on the performance metric). Explain the psychological motivation behind this non-linear function. Why might a logarithmic form be appropriate for modeling human perception of classification errors?</p>
<p>Run the file <code>npme/main.py</code> and attach the plot of <span class="math inline">\(\phi_{\text{true}}\)</span> vs your elicited metric. What do you notice in the plot?</p></li>
<li><p><strong>(Written + Coding, 4 points).</strong> Once the GP has been trained and posterior samples of the function <span class="math inline">\(\phi(\text{FP}, \text{FN})\)</span> have been obtained, how can we evaluate the quality of the elicited metric? Propose a method to evaluate how well the elicited metric <span class="math inline">\(\phi\)</span> aligns with the user’s true preferences and implement it in <code>evaluate_elicited_metric</code> taking into the plot you saw in part (iii).</p></li>
</ol></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="8fc706b4" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="kw">class</span> EllipticalSliceSampler:</span>
<span id="cb11-8"><a href="#cb11-8"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb11-9"><a href="#cb11-9"></a>                 prior_cov: np.ndarray,</span>
<span id="cb11-10"><a href="#cb11-10"></a>                 loglik: Callable):</span>
<span id="cb11-11"><a href="#cb11-11"></a>        <span class="co">"""</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co">        Initializes the Elliptical Slice Sampler.</span></span>
<span id="cb11-13"><a href="#cb11-13"></a><span class="co">        </span></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="co">        Args:</span></span>
<span id="cb11-15"><a href="#cb11-15"></a><span class="co">        - prior_cov (np.ndarray): Prior covariance matrix.</span></span>
<span id="cb11-16"><a href="#cb11-16"></a><span class="co">        - loglik (Callable): Log-likelihood function.</span></span>
<span id="cb11-17"><a href="#cb11-17"></a><span class="co">        """</span></span>
<span id="cb11-18"><a href="#cb11-18"></a>        <span class="va">self</span>.prior_cov <span class="op">=</span> prior_cov</span>
<span id="cb11-19"><a href="#cb11-19"></a>        <span class="va">self</span>.loglik <span class="op">=</span> loglik</span>
<span id="cb11-20"><a href="#cb11-20"></a></span>
<span id="cb11-21"><a href="#cb11-21"></a>        <span class="va">self</span>._n <span class="op">=</span> prior_cov.shape[<span class="dv">0</span>]  <span class="co"># Dimensionality of the space</span></span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="va">self</span>._chol <span class="op">=</span> np.linalg.cholesky(prior_cov)  <span class="co"># Cache Cholesky decomposition</span></span>
<span id="cb11-23"><a href="#cb11-23"></a></span>
<span id="cb11-24"><a href="#cb11-24"></a>        <span class="co"># Initialize state by sampling from prior</span></span>
<span id="cb11-25"><a href="#cb11-25"></a>        <span class="va">self</span>._state_f <span class="op">=</span> <span class="va">self</span>._chol <span class="op">@</span> np.random.randn(<span class="va">self</span>._n)</span>
<span id="cb11-26"><a href="#cb11-26"></a></span>
<span id="cb11-27"><a href="#cb11-27"></a>    <span class="kw">def</span> _indiv_sample(<span class="va">self</span>):</span>
<span id="cb11-28"><a href="#cb11-28"></a>        <span class="co">"""</span></span>
<span id="cb11-29"><a href="#cb11-29"></a><span class="co">        Main algorithm for generating an individual sample using Elliptical Slice Sampling.</span></span>
<span id="cb11-30"><a href="#cb11-30"></a><span class="co">        """</span></span>
<span id="cb11-31"><a href="#cb11-31"></a>        f <span class="op">=</span> <span class="va">self</span>._state_f  <span class="co"># Previous state</span></span>
<span id="cb11-32"><a href="#cb11-32"></a>        nu <span class="op">=</span> <span class="va">self</span>._chol <span class="op">@</span> np.random.randn(<span class="va">self</span>._n)  <span class="co"># Sample from prior for the ellipse</span></span>
<span id="cb11-33"><a href="#cb11-33"></a>        log_y <span class="op">=</span> <span class="va">self</span>.loglik(f) <span class="op">+</span> np.log(np.random.uniform())  <span class="co"># Log-likelihood threshold</span></span>
<span id="cb11-34"><a href="#cb11-34"></a></span>
<span id="cb11-35"><a href="#cb11-35"></a>        theta <span class="op">=</span> np.random.uniform(<span class="fl">0.</span>, <span class="dv">2</span> <span class="op">*</span> np.pi)  <span class="co"># Initial proposal angle</span></span>
<span id="cb11-36"><a href="#cb11-36"></a>        theta_min, theta_max <span class="op">=</span> theta <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.pi, theta  <span class="co"># Define bracketing interval</span></span>
<span id="cb11-37"><a href="#cb11-37"></a></span>
<span id="cb11-38"><a href="#cb11-38"></a>        <span class="co"># Main loop: Accept sample if it meets log-likelihood threshold; otherwise, shrink the bracket.</span></span>
<span id="cb11-39"><a href="#cb11-39"></a>        <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-40"><a href="#cb11-40"></a>            <span class="co"># YOUR CODE HERE (~10 lines)</span></span>
<span id="cb11-41"><a href="#cb11-41"></a>            <span class="co"># 1. Generate a new sample point based on the current angle.</span></span>
<span id="cb11-42"><a href="#cb11-42"></a>            <span class="co"># 2. Check if the proposed point meets the acceptance criterion.            </span></span>
<span id="cb11-43"><a href="#cb11-43"></a>            <span class="co"># 3. If not accepted, adjust the bracket and select a new angle.</span></span>
<span id="cb11-44"><a href="#cb11-44"></a>            <span class="cf">break</span></span>
<span id="cb11-45"><a href="#cb11-45"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb11-46"><a href="#cb11-46"></a></span>
<span id="cb11-47"><a href="#cb11-47"></a>    <span class="kw">def</span> sample(<span class="va">self</span>,</span>
<span id="cb11-48"><a href="#cb11-48"></a>               n_samples: <span class="bu">int</span>,</span>
<span id="cb11-49"><a href="#cb11-49"></a>               n_burn: <span class="bu">int</span> <span class="op">=</span> <span class="dv">500</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb11-50"><a href="#cb11-50"></a>        <span class="co">"""</span></span>
<span id="cb11-51"><a href="#cb11-51"></a><span class="co">        Generates samples using Elliptical Slice Sampling.</span></span>
<span id="cb11-52"><a href="#cb11-52"></a></span>
<span id="cb11-53"><a href="#cb11-53"></a><span class="co">        Args:</span></span>
<span id="cb11-54"><a href="#cb11-54"></a><span class="co">        - n_samples (int): Total number of samples to return.</span></span>
<span id="cb11-55"><a href="#cb11-55"></a><span class="co">        - n_burn (int): Number of initial samples to discard (burn-in).</span></span>
<span id="cb11-56"><a href="#cb11-56"></a></span>
<span id="cb11-57"><a href="#cb11-57"></a><span class="co">        Returns:</span></span>
<span id="cb11-58"><a href="#cb11-58"></a><span class="co">        - np.ndarray: Array of samples after burn-in.</span></span>
<span id="cb11-59"><a href="#cb11-59"></a><span class="co">        """</span></span>
<span id="cb11-60"><a href="#cb11-60"></a>        samples <span class="op">=</span> []</span>
<span id="cb11-61"><a href="#cb11-61"></a>        <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(n_samples), desc<span class="op">=</span><span class="st">"Sampling"</span>):</span>
<span id="cb11-62"><a href="#cb11-62"></a>            <span class="va">self</span>._indiv_sample()</span>
<span id="cb11-63"><a href="#cb11-63"></a>            <span class="cf">if</span> i <span class="op">&gt;</span> n_burn:</span>
<span id="cb11-64"><a href="#cb11-64"></a>                samples.append(<span class="va">self</span>._state_f.copy())  <span class="co"># Store sample post burn-in</span></span>
<span id="cb11-65"><a href="#cb11-65"></a></span>
<span id="cb11-66"><a href="#cb11-66"></a>        <span class="cf">return</span> np.stack(samples)</span>
<span id="cb11-67"><a href="#cb11-67"></a></span>
<span id="cb11-68"><a href="#cb11-68"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb11-69"><a href="#cb11-69"></a>    <span class="co">"""Sigmoid function to map values between 0 and 1."""</span></span>
<span id="cb11-70"><a href="#cb11-70"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb11-71"><a href="#cb11-71"></a></span>
<span id="cb11-72"><a href="#cb11-72"></a><span class="co"># Step 1: Define a New Two-Dimensional Non-linear Function</span></span>
<span id="cb11-73"><a href="#cb11-73"></a><span class="kw">def</span> nonlinear_function(x1, x2):</span>
<span id="cb11-74"><a href="#cb11-74"></a>    <span class="co">"""</span></span>
<span id="cb11-75"><a href="#cb11-75"></a><span class="co">    Computes a non-linear function of x1 and x2.</span></span>
<span id="cb11-76"><a href="#cb11-76"></a><span class="co">    </span></span>
<span id="cb11-77"><a href="#cb11-77"></a><span class="co">    Args:</span></span>
<span id="cb11-78"><a href="#cb11-78"></a><span class="co">    - x1 (np.array): First input array.</span></span>
<span id="cb11-79"><a href="#cb11-79"></a><span class="co">    - x2 (np.array): Second input array.</span></span>
<span id="cb11-80"><a href="#cb11-80"></a><span class="co">    </span></span>
<span id="cb11-81"><a href="#cb11-81"></a><span class="co">    Returns:</span></span>
<span id="cb11-82"><a href="#cb11-82"></a><span class="co">    - np.array: Computed function values.</span></span>
<span id="cb11-83"><a href="#cb11-83"></a><span class="co">    """</span></span>
<span id="cb11-84"><a href="#cb11-84"></a>    <span class="cf">return</span> np.log(<span class="dv">1</span> <span class="op">+</span> x1) <span class="op">+</span> np.log(<span class="dv">1</span> <span class="op">+</span> x2)</span>
<span id="cb11-85"><a href="#cb11-85"></a></span>
<span id="cb11-86"><a href="#cb11-86"></a><span class="co"># Generate a 2D grid of points</span></span>
<span id="cb11-87"><a href="#cb11-87"></a>x1 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>)</span>
<span id="cb11-88"><a href="#cb11-88"></a>x2 <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>)</span>
<span id="cb11-89"><a href="#cb11-89"></a>x1_grid, x2_grid <span class="op">=</span> np.meshgrid(x1, x2)</span>
<span id="cb11-90"><a href="#cb11-90"></a>x_grid_points <span class="op">=</span> np.vstack([x1_grid.ravel(), x2_grid.ravel()]).T</span>
<span id="cb11-91"><a href="#cb11-91"></a>f_values <span class="op">=</span> nonlinear_function(x_grid_points[:, <span class="dv">0</span>], x_grid_points[:, <span class="dv">1</span>])</span>
<span id="cb11-92"><a href="#cb11-92"></a></span>
<span id="cb11-93"><a href="#cb11-93"></a><span class="co"># Step 2: Generate Preferences Using Bradley-Terry Model Over the Grid</span></span>
<span id="cb11-94"><a href="#cb11-94"></a><span class="kw">def</span> generate_preferences(f_vals, num_prefs<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb11-95"><a href="#cb11-95"></a>    <span class="co">"""</span></span>
<span id="cb11-96"><a href="#cb11-96"></a><span class="co">    Generates preferences based on the Bradley-Terry model.</span></span>
<span id="cb11-97"><a href="#cb11-97"></a><span class="co">    </span></span>
<span id="cb11-98"><a href="#cb11-98"></a><span class="co">    Args:</span></span>
<span id="cb11-99"><a href="#cb11-99"></a><span class="co">    - f_vals (np.array): Function values at grid points.</span></span>
<span id="cb11-100"><a href="#cb11-100"></a><span class="co">    - num_prefs (int): Number of preference pairs to generate.</span></span>
<span id="cb11-101"><a href="#cb11-101"></a><span class="co">    </span></span>
<span id="cb11-102"><a href="#cb11-102"></a><span class="co">    Returns:</span></span>
<span id="cb11-103"><a href="#cb11-103"></a><span class="co">    - list of tuple: Generated preference pairs (i, j).</span></span>
<span id="cb11-104"><a href="#cb11-104"></a><span class="co">    """</span></span>
<span id="cb11-105"><a href="#cb11-105"></a>    preferences <span class="op">=</span> []</span>
<span id="cb11-106"><a href="#cb11-106"></a>    num_points <span class="op">=</span> <span class="bu">len</span>(f_vals)</span>
<span id="cb11-107"><a href="#cb11-107"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_prefs):</span>
<span id="cb11-108"><a href="#cb11-108"></a>        i, j <span class="op">=</span> np.random.choice(num_points, size<span class="op">=</span><span class="dv">2</span>, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-109"><a href="#cb11-109"></a>        <span class="co"># Probability of preference using Bradley-Terry model</span></span>
<span id="cb11-110"><a href="#cb11-110"></a>        p_ij <span class="op">=</span> sigmoid(f_vals[i] <span class="op">-</span> f_vals[j])</span>
<span id="cb11-111"><a href="#cb11-111"></a>        <span class="co"># Decide preference based on random draw</span></span>
<span id="cb11-112"><a href="#cb11-112"></a>        <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> p_ij:</span>
<span id="cb11-113"><a href="#cb11-113"></a>            preferences.append((i, j))</span>
<span id="cb11-114"><a href="#cb11-114"></a>        <span class="cf">else</span>:</span>
<span id="cb11-115"><a href="#cb11-115"></a>            preferences.append((j, i))</span>
<span id="cb11-116"><a href="#cb11-116"></a>    <span class="cf">return</span> preferences</span>
<span id="cb11-117"><a href="#cb11-117"></a></span>
<span id="cb11-118"><a href="#cb11-118"></a>preferences <span class="op">=</span> generate_preferences(f_values)</span>
<span id="cb11-119"><a href="#cb11-119"></a></span>
<span id="cb11-120"><a href="#cb11-120"></a><span class="co"># Step 3: Define the Likelihood Function for Elliptical Slice Sampling</span></span>
<span id="cb11-121"><a href="#cb11-121"></a><span class="kw">def</span> loglik_from_preferences(f):</span>
<span id="cb11-122"><a href="#cb11-122"></a>    <span class="co">"""</span></span>
<span id="cb11-123"><a href="#cb11-123"></a><span class="co">    Log-likelihood function using Bradley-Terry model for preferences.</span></span>
<span id="cb11-124"><a href="#cb11-124"></a><span class="co">    </span></span>
<span id="cb11-125"><a href="#cb11-125"></a><span class="co">    Args:</span></span>
<span id="cb11-126"><a href="#cb11-126"></a><span class="co">    - f (np.array): Sampled function values.</span></span>
<span id="cb11-127"><a href="#cb11-127"></a><span class="co">    </span></span>
<span id="cb11-128"><a href="#cb11-128"></a><span class="co">    Returns:</span></span>
<span id="cb11-129"><a href="#cb11-129"></a><span class="co">    - float: Log-likelihood value.</span></span>
<span id="cb11-130"><a href="#cb11-130"></a><span class="co">    """</span></span>
<span id="cb11-131"><a href="#cb11-131"></a>    log_lik <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-132"><a href="#cb11-132"></a>    <span class="cf">for</span> idx_i, idx_j <span class="kw">in</span> preferences:</span>
<span id="cb11-133"><a href="#cb11-133"></a>        <span class="co"># YOUR CODE HERE (~2 lines)</span></span>
<span id="cb11-134"><a href="#cb11-134"></a>        <span class="cf">pass</span></span>
<span id="cb11-135"><a href="#cb11-135"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb11-136"><a href="#cb11-136"></a>    <span class="cf">return</span> log_lik</span>
<span id="cb11-137"><a href="#cb11-137"></a></span>
<span id="cb11-138"><a href="#cb11-138"></a><span class="co"># Step 4: Define the RBF Kernel to Compute Prior Covariance Matrix</span></span>
<span id="cb11-139"><a href="#cb11-139"></a><span class="kw">def</span> rbf_kernel(X1, X2, length_scale<span class="op">=</span><span class="fl">1.0</span>, sigma_f<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb11-140"><a href="#cb11-140"></a>    <span class="co">"""</span></span>
<span id="cb11-141"><a href="#cb11-141"></a><span class="co">    Computes the Radial Basis Function (RBF) kernel between two sets of points.</span></span>
<span id="cb11-142"><a href="#cb11-142"></a><span class="co">    </span></span>
<span id="cb11-143"><a href="#cb11-143"></a><span class="co">    Args:</span></span>
<span id="cb11-144"><a href="#cb11-144"></a><span class="co">    - X1, X2 (np.array): Input data points.</span></span>
<span id="cb11-145"><a href="#cb11-145"></a><span class="co">    - length_scale (float): Kernel length scale parameter.</span></span>
<span id="cb11-146"><a href="#cb11-146"></a><span class="co">    - sigma_f (float): Kernel output scale.</span></span>
<span id="cb11-147"><a href="#cb11-147"></a><span class="co">    </span></span>
<span id="cb11-148"><a href="#cb11-148"></a><span class="co">    Returns:</span></span>
<span id="cb11-149"><a href="#cb11-149"></a><span class="co">    - np.array: RBF kernel matrix.</span></span>
<span id="cb11-150"><a href="#cb11-150"></a><span class="co">    """</span></span>
<span id="cb11-151"><a href="#cb11-151"></a>    sqdist <span class="op">=</span> np.<span class="bu">sum</span>(X1<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> np.<span class="bu">sum</span>(X2<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> np.dot(X1, X2.T)</span>
<span id="cb11-152"><a href="#cb11-152"></a>    <span class="cf">return</span> sigma_f<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">/</span> length_scale<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> sqdist)</span>
<span id="cb11-153"><a href="#cb11-153"></a></span>
<span id="cb11-154"><a href="#cb11-154"></a><span class="co"># Define prior covariance (prior mean is zero vector)</span></span>
<span id="cb11-155"><a href="#cb11-155"></a>sigma_prior <span class="op">=</span> rbf_kernel(x_grid_points, x_grid_points, length_scale<span class="op">=</span><span class="fl">1.0</span>, sigma_f<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb11-156"><a href="#cb11-156"></a></span>
<span id="cb11-157"><a href="#cb11-157"></a><span class="co"># Add small jitter to diagonal for numerical stability</span></span>
<span id="cb11-158"><a href="#cb11-158"></a>jitter <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb11-159"><a href="#cb11-159"></a>sigma_prior <span class="op">+=</span> jitter <span class="op">*</span> np.eye(sigma_prior.shape[<span class="dv">0</span>])</span>
<span id="cb11-160"><a href="#cb11-160"></a></span>
<span id="cb11-161"><a href="#cb11-161"></a><span class="co"># Ensure the matrix is symmetric to avoid numerical issues</span></span>
<span id="cb11-162"><a href="#cb11-162"></a>sigma_prior <span class="op">=</span> (sigma_prior <span class="op">+</span> sigma_prior.T) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb11-163"><a href="#cb11-163"></a></span>
<span id="cb11-164"><a href="#cb11-164"></a><span class="co"># Step 5: Run Elliptical Slice Sampling</span></span>
<span id="cb11-165"><a href="#cb11-165"></a>sampler <span class="op">=</span> EllipticalSliceSampler(sigma_prior, loglik_from_preferences)</span>
<span id="cb11-166"><a href="#cb11-166"></a>samples <span class="op">=</span> sampler.sample(<span class="dv">1000</span>, n_burn<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb11-167"><a href="#cb11-167"></a>average_samples <span class="op">=</span> np.mean(samples, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-168"><a href="#cb11-168"></a></span>
<span id="cb11-169"><a href="#cb11-169"></a><span class="co"># Generate true function values on grid points</span></span>
<span id="cb11-170"><a href="#cb11-170"></a>true_values_on_grid <span class="op">=</span> nonlinear_function(x_grid_points[:, <span class="dv">0</span>], x_grid_points[:, <span class="dv">1</span>])</span>
<span id="cb11-171"><a href="#cb11-171"></a></span>
<span id="cb11-172"><a href="#cb11-172"></a><span class="kw">def</span> evaluate_elicited_metric(true_metric, elicited_metric):</span>
<span id="cb11-173"><a href="#cb11-173"></a>    <span class="co">"""</span></span>
<span id="cb11-174"><a href="#cb11-174"></a><span class="co">    Evaluates and prints the mean and standard deviation of the difference</span></span>
<span id="cb11-175"><a href="#cb11-175"></a><span class="co">    between true and elicited metrics.</span></span>
<span id="cb11-176"><a href="#cb11-176"></a><span class="co">    </span></span>
<span id="cb11-177"><a href="#cb11-177"></a><span class="co">    Args:</span></span>
<span id="cb11-178"><a href="#cb11-178"></a><span class="co">    - true_metric (np.array): True values of the function.</span></span>
<span id="cb11-179"><a href="#cb11-179"></a><span class="co">    - elicited_metric (np.array): Elicited (estimated) function values.</span></span>
<span id="cb11-180"><a href="#cb11-180"></a><span class="co">    """</span></span>
<span id="cb11-181"><a href="#cb11-181"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb11-182"><a href="#cb11-182"></a>    <span class="cf">pass</span></span>
<span id="cb11-183"><a href="#cb11-183"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb11-184"><a href="#cb11-184"></a></span>
<span id="cb11-185"><a href="#cb11-185"></a>evaluate_elicited_metric(true_values_on_grid, average_samples)</span>
<span id="cb11-186"><a href="#cb11-186"></a></span>
<span id="cb11-187"><a href="#cb11-187"></a><span class="co"># Step 6: Plot the True Non-linear Function and Elicited Metric in 3D</span></span>
<span id="cb11-188"><a href="#cb11-188"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb11-189"><a href="#cb11-189"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>, projection<span class="op">=</span><span class="st">'3d'</span>)</span>
<span id="cb11-190"><a href="#cb11-190"></a></span>
<span id="cb11-191"><a href="#cb11-191"></a><span class="co"># Plot the true function</span></span>
<span id="cb11-192"><a href="#cb11-192"></a>x1_fine <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb11-193"><a href="#cb11-193"></a>x2_fine <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb11-194"><a href="#cb11-194"></a>x1_fine_grid, x2_fine_grid <span class="op">=</span> np.meshgrid(x1_fine, x2_fine)</span>
<span id="cb11-195"><a href="#cb11-195"></a>true_f_values <span class="op">=</span> nonlinear_function(x1_fine_grid, x2_fine_grid)</span>
<span id="cb11-196"><a href="#cb11-196"></a>ax.plot_surface(x1_fine_grid, x2_fine_grid, true_f_values, color<span class="op">=</span><span class="st">'blue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'True Function'</span>)</span>
<span id="cb11-197"><a href="#cb11-197"></a></span>
<span id="cb11-198"><a href="#cb11-198"></a><span class="co"># Plot the averaged samples as a surface</span></span>
<span id="cb11-199"><a href="#cb11-199"></a>x1_avg <span class="op">=</span> x_grid_points[:, <span class="dv">0</span>].reshape(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb11-200"><a href="#cb11-200"></a>x2_avg <span class="op">=</span> x_grid_points[:, <span class="dv">1</span>].reshape(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb11-201"><a href="#cb11-201"></a>avg_values <span class="op">=</span> average_samples.reshape(<span class="dv">20</span>, <span class="dv">20</span>)</span>
<span id="cb11-202"><a href="#cb11-202"></a>ax.plot_surface(x1_avg, x2_avg, avg_values, color<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Estimated Function'</span>)</span>
<span id="cb11-203"><a href="#cb11-203"></a></span>
<span id="cb11-204"><a href="#cb11-204"></a><span class="co"># Customize plot</span></span>
<span id="cb11-205"><a href="#cb11-205"></a>ax.set_xlabel(<span class="st">'x1'</span>)</span>
<span id="cb11-206"><a href="#cb11-206"></a>ax.set_ylabel(<span class="st">'x2'</span>)</span>
<span id="cb11-207"><a href="#cb11-207"></a>ax.set_zlabel(<span class="st">'f(x1, x2)'</span>)</span>
<span id="cb11-208"><a href="#cb11-208"></a>ax.set_title(<span class="st">'True Function vs. Averaged Estimated Function'</span>)</span>
<span id="cb11-209"><a href="#cb11-209"></a>plt.legend()</span>
<span id="cb11-210"><a href="#cb11-210"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-unnoisy_humans" class="csl-entry" role="listitem">
Amershi, Saleema, Maya Cakmak, W. Bradley Knox, and Todd Kulesza. 2014. <span>“Power to the People: The Role of Humans in Interactive Machine Learning.”</span> <em>AI Magazine</em>.
</div>
<div id="ref-AL_committee" class="csl-entry" role="listitem">
Beluch, William H., Tim Genewein, A. Nürnberger, and Jan M. Köhler. 2018. <span>“The Power of Ensembles for Active Learning in Image Classification.”</span> <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9368–77. <a href="https://api.semanticscholar.org/CorpusID:52838058">https://api.semanticscholar.org/CorpusID:52838058</a>.
</div>
<div id="ref-AL_usercentered" class="csl-entry" role="listitem">
Bernard, J., Matthias Zeppelzauer, Markus Lehmann, Martin Müller, and Michael Sedlmair. 2018. <span>“Towards User‐centered Active Learning Algorithms.”</span> <em>Computer Graphics Forum</em> 37. <a href="https://api.semanticscholar.org/CorpusID:51875861">https://api.semanticscholar.org/CorpusID:51875861</a>.
</div>
<div id="ref-pmlr-v87-biyik18a" class="csl-entry" role="listitem">
Biyik, Erdem, and Dorsa Sadigh. 2018. <span>“Batch Active Preference-Based Learning of Reward Functions.”</span> In <em>Proceedings of the 2nd Conference on Robot Learning</em>, edited by Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, 87:519–28. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v87/biyik18a.html">https://proceedings.mlr.press/v87/biyik18a.html</a>.
</div>
<div id="ref-bommasani2022opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2022. <span>“On the Opportunities and Risks of Foundation Models.”</span> <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a>.
</div>
<div id="ref-AL_exploreexploit" class="csl-entry" role="listitem">
Bouneffouf, Djallel, Romain Laroche, Tanguy Urvoy, Raphaël Féraud, and Robin Allesiardo. 2014. <span>“Contextual Bandit for Active Learning: Active Thompson Sampling.”</span> In <em>International Conference on Neural Information Processing</em>. <a href="https://api.semanticscholar.org/CorpusID:1701357">https://api.semanticscholar.org/CorpusID:1701357</a>.
</div>
<div id="ref-pref4" class="csl-entry" role="listitem">
Braziunas, Darius, and Craig Boutilier. 2012. <span>“Minimax Regret Based Elicitation of Generalized Additive Utilities.”</span> <a href="https://arxiv.org/abs/1206.5255">https://arxiv.org/abs/1206.5255</a>.
</div>
<div id="ref-brohan2023rt2" class="csl-entry" role="listitem">
Brohan, Anthony, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, et al. 2023. <span>“RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control.”</span> <a href="https://arxiv.org/abs/2307.15818">https://arxiv.org/abs/2307.15818</a>.
</div>
<div id="ref-bulow-klemperer1996" class="csl-entry" role="listitem">
Bulow, Jeremy, and Paul Klemperer. 1996. <span>“Auctions Versus Negotiations.”</span> <em>The American Economic Review</em> 86 (1): 180–94. <a href="http://www.jstor.org/stable/2118262">http://www.jstor.org/stable/2118262</a>.
</div>
<div id="ref-AL_expmodelchange" class="csl-entry" role="listitem">
Cai, Wenbin, Ya Zhang, and Jun Zhou. 2013. <span>“Maximizing Expected Model Change for Active Learning in Regression.”</span> In <em>2013 IEEE 13th International Conference on Data Mining</em>, 51–60. <a href="https://doi.org/10.1109/ICDM.2013.104">https://doi.org/10.1109/ICDM.2013.104</a>.
</div>
<div id="ref-AL_variance" class="csl-entry" role="listitem">
Cohn, David A., Zoubin Ghahramani, and Michael I. Jordan. 1996. <span>“Active Learning with Statistical Models.”</span> <em>CoRR</em> cs.AI/9603104. <a href="https://arxiv.org/abs/cs/9603104">https://arxiv.org/abs/cs/9603104</a>.
</div>
<div id="ref-deng2009imagenet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. IEEE.
</div>
<div id="ref-geo_paper" class="csl-entry" role="listitem">
G., Jamieson Kevin, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> <em>Advances in Neural Information Processing Systems</em> 24.
</div>
<div id="ref-gandhi2022eliciting" class="csl-entry" role="listitem">
Gandhi, Kanishk, Siddharth Karamcheti, Madeline Liao, and Dorsa Sadigh. 2022. <span>“Eliciting Compatible Demonstrations for Multi-Human Imitation Learning.”</span> In <em>Proceedings of the 6th Conference on Robot Learning (CoRL)</em>.
</div>
<div id="ref-bias_variance_orig_paper" class="csl-entry" role="listitem">
Geman, Stuart, Elie Bienenstock, and René Doursat. 1992. <span>“Neural Networks and the Bias/Variance Dilemma.”</span> <em>Neural Computation</em> 4: 1–58. <a href="https://api.semanticscholar.org/CorpusID:14215320">https://api.semanticscholar.org/CorpusID:14215320</a>.
</div>
<div id="ref-monte-carlo" class="csl-entry" role="listitem">
Ghojogh, Benyamin, Hadi Nekoei, Aydin Ghojogh, Fakhri Karray, and Mark Crowley. 2020. <span>“Sampling Algorithms, from Survey Sampling to Monte Carlo Methods: Tutorial and Literature Review.”</span> <a href="https://arxiv.org/abs/2011.00901">https://arxiv.org/abs/2011.00901</a>.
</div>
<div id="ref-grauman2022ego4d" class="csl-entry" role="listitem">
Grauman, Kristen, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, et al. 2022. <span>“Ego4D: Around the World in 3,000 Hours of Egocentric Video.”</span> <a href="https://arxiv.org/abs/2110.07058">https://arxiv.org/abs/2110.07058</a>.
</div>
<div id="ref-noisy_humans" class="csl-entry" role="listitem">
Guillory, Andrew, and Jeff Bilmes. 2011. <span>“Simultaneous Learning and Covering with Adversarial Noise.”</span> <em>ICML</em>.
</div>
<div id="ref-max_halford" class="csl-entry" role="listitem">
Halford, Max. 2023. <span>“Online Active Learning in 80 Lines of Python.”</span>
</div>
<div id="ref-jasonH2020" class="csl-entry" role="listitem">
Hartline, Jason D., Yingkai Li, Liren Shan, and Yifan Wu. 2020. <span>“Optimization of Scoring Rules.”</span> <em>CoRR</em> abs/2007.02905. <a href="https://arxiv.org/abs/2007.02905">https://arxiv.org/abs/2007.02905</a>.
</div>
<div id="ref-jasonH2023" class="csl-entry" role="listitem">
Hartline, Jason D., Liren Shan, Yingkai Li, and Yifan Wu. 2023. <span>“Optimal Scoring Rules for Multi-Dimensional Effort.”</span> In <em>Proceedings of Thirty Sixth Conference on Learning Theory</em>, edited by Gergely Neu and Lorenzo Rosasco, 195:2624–50. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v195/hartline23a.html">https://proceedings.mlr.press/v195/hartline23a.html</a>.
</div>
<div id="ref-he2020momentum" class="csl-entry" role="listitem">
He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. <span>“Momentum Contrast for Unsupervised Visual Representation Learning.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 9729–38. IEEE.
</div>
<div id="ref-pmlr-v89-hiranandani19a" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi Koyejo. 2019a. <span>“Performance Metric Elicitation from Pairwise Classifier Comparisons.”</span> In <em>Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</em>, edited by Kamalika Chaudhuri and Masashi Sugiyama, 89:371–79. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v89/hiranandani19a.html">https://proceedings.mlr.press/v89/hiranandani19a.html</a>.
</div>
<div id="ref-NEURIPS2019_1fd09c5f" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Shant Boodaghians, Ruta Mehta, and Oluwasanmi O Koyejo. 2019b. <span>“Multiclass Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2019/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf</a>.
</div>
<div id="ref-nips" class="csl-entry" role="listitem">
Hiranandani, Gaurush, Harikrishna Narasimhan, and Sanmi Koyejo. 2020. <span>“Fair Performance Metric Elicitation.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:11083–95. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2020/file/7ec2442aa04c157590b2fa1a7d093a33-Paper.pdf</a>.
</div>
<div id="ref-claus" class="csl-entry" role="listitem">
Holladay, Rachel, Shervin Javdani, Anca Dragan, and Siddhartha Srinivasa. 2016. <span>“Active Comparison Based Learning Incorporating User Uncertainty and Noise.”</span> <em>Proceedings of RSS ’16 Workshop on Model Learning for Human-Robot Communication</em>.
</div>
<div id="ref-AL_BALD" class="csl-entry" role="listitem">
Houlsby, Neil, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. 2011. <span>“Bayesian Active Learning for Classification and Preference Learning.”</span> <em>arXiv Preprint arXiv:1112.5745</em>.
</div>
<div id="ref-AL_app_autonomous" class="csl-entry" role="listitem">
Jarl, Sanna, Linus Aronsson, Sadegh Rahrovani, and Morteza Haghir Chehreghani. 2021. <span>“Active Learning of Driving Scenario Trajectories.”</span> <em>Eng. Appl. Artif. Intell.</em> 113: 104972. <a href="https://api.semanticscholar.org/CorpusID:249113683">https://api.semanticscholar.org/CorpusID:249113683</a>.
</div>
<div id="ref-karamcheti2023languagedriven" class="csl-entry" role="listitem">
Karamcheti, Siddharth, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. 2023. <span>“Language-Driven Representation Learning for Robotics.”</span> <a href="https://arxiv.org/abs/2302.12766">https://arxiv.org/abs/2302.12766</a>.
</div>
<div id="ref-kongschoenebeck2019" class="csl-entry" role="listitem">
Kong, Yuqing, and Grant Schoenebeck. 2019. <span>“An Information Theoretic Framework for Designing Information Elicitation Mechanisms That Reward Truth-Telling.”</span> <em>ACM Trans. Econ. Comput.</em> 7 (1). <a href="https://doi.org/10.1145/3296670">https://doi.org/10.1145/3296670</a>.
</div>
<div id="ref-kwon2021targeted" class="csl-entry" role="listitem">
Kwon, Minae, Siddharth Karamcheti, Mariano-Florentino Cuellar, and Dorsa Sadigh. 2021. <span>“Targeted Data Acquisition for Evolving Negotiation Agents.”</span> <a href="https://arxiv.org/abs/2106.07728">https://arxiv.org/abs/2106.07728</a>.
</div>
<div id="ref-Li_2021" class="csl-entry" role="listitem">
Li, Kejun, Maegan Tucker, Erdem Biyik, Ellen Novoseller, Joel W. Burdick, Yanan Sui, Dorsa Sadigh, Yisong Yue, and Aaron D. Ames. 2021. <span>“ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes.”</span> In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE. <a href="https://doi.org/10.1109/icra48506.2021.9560840">https://doi.org/10.1109/icra48506.2021.9560840</a>.
</div>
<div id="ref-AL_partition" class="csl-entry" role="listitem">
Ma, Jiaqi, Ziqiao Ma, Joyce Chai, and Qiaozhu Mei. 2022. <span>“Partition-Based Active Learning for Graph Neural Networks.”</span> <em>ArXiv</em> abs/2201.09391. <a href="https://api.semanticscholar.org/CorpusID:246240846">https://api.semanticscholar.org/CorpusID:246240846</a>.
</div>
<div id="ref-AL_conformal" class="csl-entry" role="listitem">
Makili, Lázaro Emílio, Jesús A. Vega Sánchez, and Sebastián Dormido-Canto. 2012. <span>“Active Learning Using Conformal Predictors: Application to Image Classification.”</span> <em>Fusion Science and Technology</em> 62: 347–55. <a href="https://api.semanticscholar.org/CorpusID:115384000">https://api.semanticscholar.org/CorpusID:115384000</a>.
</div>
<div id="ref-AL_app_LLMs" class="csl-entry" role="listitem">
Margatina, Katerina, Timo Schick, Nikolaos Aletras, and Jane Dwivedi-Yu. 2023. <span>“Active Learning Principles for in-Context Learning with Large Language Models.”</span> <em>ArXiv</em> abs/2305.14264. <a href="https://api.semanticscholar.org/CorpusID:258841313">https://api.semanticscholar.org/CorpusID:258841313</a>.
</div>
<div id="ref-pref2" class="csl-entry" role="listitem">
Mas-Colell, Andreu. 1977. <span>“The Recoverability of Consumers’ Preferences from Market Demand Behavior.”</span> <em>Econometrica</em> 45 (6): 1409–30. <a href="http://www.jstor.org/stable/1912308">http://www.jstor.org/stable/1912308</a>.
</div>
<div id="ref-mcafee-87" class="csl-entry" role="listitem">
McAfee, R. Preston, and John McMillan. 1987. <span>“Auctions and Bidding.”</span> <em>Journal of Economic Literature</em> 25 (2): 699–738. <a href="http://www.jstor.org/stable/2726107">http://www.jstor.org/stable/2726107</a>.
</div>
<div id="ref-AL_experrorredn" class="csl-entry" role="listitem">
Mussmann, Stephen, Julia Reisler, Daniel Tsai, Ehsan Mousavi, Shayne O’Brien, and Moises Goldszmidt. 2022. <span>“Active Learning with Expected Error Reduction.”</span> <a href="https://arxiv.org/abs/2211.09283">https://arxiv.org/abs/2211.09283</a>.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-nair2022r3m" class="csl-entry" role="listitem">
Nair, Suraj, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. 2022. <span>“R3M: A Universal Visual Representation for Robot Manipulation.”</span> <a href="https://arxiv.org/abs/2203.12601">https://arxiv.org/abs/2203.12601</a>.
</div>
<div id="ref-pmlr-v37-narasimhanb15" class="csl-entry" role="listitem">
Narasimhan, Harikrishna, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. 2015. <span>“Consistent Multiclass Algorithms for Complex Performance Measures.”</span> In <em>Proceedings of the 32nd International Conference on Machine Learning</em>, edited by Francis Bach and David Blei, 37:2398–2407. Proceedings of Machine Learning Research. Lille, France: PMLR. <a href="https://proceedings.mlr.press/v37/narasimhanb15.html">https://proceedings.mlr.press/v37/narasimhanb15.html</a>.
</div>
<div id="ref-radford2021learning" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> <em>arXiv Preprint arXiv:2103.00020</em>.
</div>
<div id="ref-pref1" class="csl-entry" role="listitem">
Samuelson, P. A. 1938. <span>“A Note on the Pure Theory of Consumer’s Behaviour.”</span> <em>Economica</em> 5 (17): 61–71. <a href="http://www.jstor.org/stable/2548836">http://www.jstor.org/stable/2548836</a>.
</div>
<div id="ref-lus-shep" class="csl-entry" role="listitem">
Shepard, Roger N. 1957. <span>“Stimulus and Response Generalization: A Stochastic Model Relating Generalization to Distance in Psychological Space.”</span> <em>Psychometrika</em> 22(4):325–345.
</div>
<div id="ref-AL_app_sensors" class="csl-entry" role="listitem">
Singh, Aarti, Robert D. Nowak, and Parameswaran Ramanathan. 2006. <span>“Active Learning for Adaptive Mobile Sensing Networks.”</span> <em>2006 5th International Conference on Information Processing in Sensor Networks</em>, 60–68. <a href="https://api.semanticscholar.org/CorpusID:17590956">https://api.semanticscholar.org/CorpusID:17590956</a>.
</div>
<div id="ref-ab" class="csl-entry" role="listitem">
Tamburrelli, Giordano, and Alessandro Margara. 2014. <span>“Towards Automated a/b Testing.”</span> In <em>Search-Based Software Engineering</em>. <a href="https://doi.org/10.1007/978-3-319-09940-8_13">https://doi.org/10.1007/978-3-319-09940-8_13</a>.
</div>
<div id="ref-AL_app_robotics" class="csl-entry" role="listitem">
Taylor, Annalisa T., Thomas A. Berrueta, and Todd D. Murphey. 2021. <span>“Active Learning in Robotics: A Review of Control Principles.”</span> <em>ArXiv</em> abs/2106.13697. <a href="https://api.semanticscholar.org/CorpusID:235652039">https://api.semanticscholar.org/CorpusID:235652039</a>.
</div>
<div id="ref-pref3" class="csl-entry" role="listitem">
Varian, Hal R. 2006. <span>“Revealed Preference.”</span> In <em>The SAGE Encyclopedia of Business Ethics and Society</em>. <a href="https://api.semanticscholar.org/CorpusID:1632873">https://api.semanticscholar.org/CorpusID:1632873</a>.
</div>
<div id="ref-lus-log" class="csl-entry" role="listitem">
Viappiani, Paolo, and Craig Boutilier. 2010. <span>“Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.”</span> <em>NIPS</em>, 2352–60.
</div>
<div id="ref-walke2023bridgedata" class="csl-entry" role="listitem">
Walke, Homer, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, et al. 2023. <span>“BridgeData V2: A Dataset for Robot Learning at Scale.”</span> <a href="https://arxiv.org/abs/2308.12952">https://arxiv.org/abs/2308.12952</a>.
</div>
<div id="ref-xiao2022masked" class="csl-entry" role="listitem">
Xiao, Tete, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. 2022. <span>“Masked Visual Pre-Training for Motor Control.”</span> <a href="https://arxiv.org/abs/2203.06173">https://arxiv.org/abs/2203.06173</a>.
</div>
<div id="ref-ask_help" class="csl-entry" role="listitem">
Xie, Annie, Fahim Tajwar, Archit Sharma, and Chelsea Finn. 2022. <span>“When to Ask for Help: Proactive Interventions in Autonomous Reinforcement Learning.”</span> <a href="https://arxiv.org/abs/2210.10765">https://arxiv.org/abs/2210.10765</a>.
</div>
<div id="ref-YangNaiman+2014+477+496" class="csl-entry" role="listitem">
Yang, Sitan, and Daniel Q. Naiman. 2014. <span>“Multiclass Cancer Classification Based on Gene Expression Comparison.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 13 (4): 477–96. <a href="https://doi.org/doi:10.1515/sagmb-2013-0053">https://doi.org/doi:10.1515/sagmb-2013-0053</a>.
</div>
<div id="ref-AL_mismatch" class="csl-entry" role="listitem">
Zhao, Shuyang, Toni Heittola, and Tuomas Virtanen. 2020. <span>“Active Learning for Sound Event Detection.”</span> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 28: 2895–905. <a href="https://api.semanticscholar.org/CorpusID:211082815">https://api.semanticscholar.org/CorpusID:211082815</a>.
</div>
<div id="ref-AL_uncertainty" class="csl-entry" role="listitem">
Zhu, Jingbo, Huizhen Wang, Benjamin Ka-Yin T’sou, and Matthew Y. Ma. 2010. <span>“Active Learning with Sampling by Uncertainty and Density for Data Annotations.”</span> <em>IEEE Transactions on Audio, Speech, and Language Processing</em> 18: 1323–31. <a href="https://api.semanticscholar.org/CorpusID:5777911">https://api.semanticscholar.org/CorpusID:5777911</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../src/002-reward_model.html" class="pagination-link" aria-label="Human Decision Making and Choice Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/004-optim.html" class="pagination-link" aria-label="Model-Free Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1"></a><span class="fu"># Model-Based Preference Optimization {#ch-model-based}</span></span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>::: {.content-visible when-format="html"}</span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a>&lt;iframe</span>
<span id="cb12-6"><a href="#cb12-6"></a>  src="https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/"</span>
<span id="cb12-7"><a href="#cb12-7"></a>  style="width:45%; height:225px;"</span>
<span id="cb12-8"><a href="#cb12-8"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb12-9"><a href="#cb12-9"></a>&lt;iframe</span>
<span id="cb12-10"><a href="#cb12-10"></a>  src="https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/"</span>
<span id="cb12-11"><a href="#cb12-11"></a>  style="width:45%; height:225px;"</span>
<span id="cb12-12"><a href="#cb12-12"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co">[</span><span class="ot">Fullscreen - AL</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/3.1.active_learning/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="co">[</span><span class="ot">Fullscreen - ME</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/3.2.metric_elicitation/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb12-15"><a href="#cb12-15"></a></span>
<span id="cb12-16"><a href="#cb12-16"></a>:::</span>
<span id="cb12-17"><a href="#cb12-17"></a></span>
<span id="cb12-18"><a href="#cb12-18"></a><span class="fu">## Active Preference Learning</span></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="fu">### Introduction to Active Learning</span></span>
<span id="cb12-20"><a href="#cb12-20"></a></span>
<span id="cb12-21"><a href="#cb12-21"></a>In real-world scenarios, data is often scarce, and acquiring labeled data can be expensive. </span>
<span id="cb12-22"><a href="#cb12-22"></a>Active learning is a machine learning paradigm that aims to reduce the amount of labeled data </span>
<span id="cb12-23"><a href="#cb12-23"></a>required to train a model to achieve high accuracy. Active learning algorithms iteratively </span>
<span id="cb12-24"><a href="#cb12-24"></a>select an input datapoint for an oracle (e.g., a human annotator) to label such that when the </span>
<span id="cb12-25"><a href="#cb12-25"></a>label is observed, the model improves the most. The goal of AL algorithms is to minimize the number of labels</span>
<span id="cb12-26"><a href="#cb12-26"></a>required to achieve a desired level of performance. This technique is particularly useful in</span>
<span id="cb12-27"><a href="#cb12-27"></a>situations where labeling data is expensive, time-consuming, or requires domain expertise.</span>
<span id="cb12-28"><a href="#cb12-28"></a></span>
<span id="cb12-29"><a href="#cb12-29"></a>There are two primary setups in active learning:</span>
<span id="cb12-30"><a href="#cb12-30"></a></span>
<span id="cb12-31"><a href="#cb12-31"></a><span class="ss">-   </span>**Pool-based:** The model selects samples from a large unlabeled pool of data. </span>
<span id="cb12-32"><a href="#cb12-32"></a>  For example, a model for text classification selects the most uncertain texts from a large pool </span>
<span id="cb12-33"><a href="#cb12-33"></a>  to ask a human annotator to label.</span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-35"><a href="#cb12-35"></a><span class="ss">-   </span>**Stream-based:** The model receives samples sequentially (one sample at a time) and decides </span>
<span id="cb12-36"><a href="#cb12-36"></a>  whether to label them. The data is gone if the decision maker decides not to label it. </span>
<span id="cb12-37"><a href="#cb12-37"></a>  For example, a system monitoring sensor data decides on-the-fly whether new sensor readings are </span>
<span id="cb12-38"><a href="#cb12-38"></a>  valuable enough to label.</span>
<span id="cb12-39"><a href="#cb12-39"></a></span>
<span id="cb12-40"><a href="#cb12-40"></a>A common AL process is shown in @fig-schema:</span>
<span id="cb12-41"><a href="#cb12-41"></a></span>
<span id="cb12-42"><a href="#cb12-42"></a><span class="ss">-   </span>Current model trained on current dataset $\mathcal{D}$, potential points $\tilde{x}_1 \dots \tilde{x}_m$ </span>
<span id="cb12-43"><a href="#cb12-43"></a>are being investigated. AL will choose one of them to add to the dataset. </span>
<span id="cb12-44"><a href="#cb12-44"></a></span>
<span id="cb12-45"><a href="#cb12-45"></a><span class="ss">-   </span>Relative to the model, a proxy highlights the relative value of each point to model improvement </span>
<span id="cb12-46"><a href="#cb12-46"></a>$(v(\tilde{x}_1) \dots v(\tilde{x}_m) )$. A naive proxy is the model's uncertainty about the point.</span>
<span id="cb12-47"><a href="#cb12-47"></a></span>
<span id="cb12-48"><a href="#cb12-48"></a><span class="ss">-   </span>The cycle repeats until we collect enough data or the model is good enough.</span>
<span id="cb12-49"><a href="#cb12-49"></a></span>
<span id="cb12-50"><a href="#cb12-50"></a>![The current model is trained on the current data set $\mathcal{D}$. </span>
<span id="cb12-51"><a href="#cb12-51"></a>Potential points $\tilde{x}_1, \ldots, \tilde{x}_m$ are being investigated, </span>
<span id="cb12-52"><a href="#cb12-52"></a>and one of them will be chosen and added to the data set. A proxy highlights </span>
<span id="cb12-53"><a href="#cb12-53"></a>the relative value of each point in terms of improving the model, denoted by </span>
<span id="cb12-54"><a href="#cb12-54"></a>$v(\tilde{x}_1), \ldots, v(\tilde{x}_m)$. The point with the highest value is </span>
<span id="cb12-55"><a href="#cb12-55"></a>selected and added to $\mathcal{D}$. This cycle repeats until enough data has </span>
<span id="cb12-56"><a href="#cb12-56"></a>been collected or the model is good enough.](Figures/active_learning_schema.png){#fig-schema width="40%"}</span>
<span id="cb12-57"><a href="#cb12-57"></a></span>
<span id="cb12-58"><a href="#cb12-58"></a>Active learning has been successfully applied to various domains to enhance real-world systems, </span>
<span id="cb12-59"><a href="#cb12-59"></a>including computer vision, natural language processing, and recommender systems. For example, active </span>
<span id="cb12-60"><a href="#cb12-60"></a>learning can improve the computer vision models used in autonomous vehicles <span class="co">[</span><span class="ot">@AL_app_autonomous</span><span class="co">]</span>, </span>
<span id="cb12-61"><a href="#cb12-61"></a>here driving scenes can take infinitely many forms, making it impossible to gather an exhaustive dataset. </span>
<span id="cb12-62"><a href="#cb12-62"></a>Instead, probing a model to understand what type of data it would benefit from is more practical. </span>
<span id="cb12-63"><a href="#cb12-63"></a>In robotics, autonomous agents may query humans when unsure how to act or when facing new situations </span>
<span id="cb12-64"><a href="#cb12-64"></a><span class="co">[</span><span class="ot">@AL_app_robotics</span><span class="co">]</span>. In this field, collecting data often incurs significant financial and time costs: </span>
<span id="cb12-65"><a href="#cb12-65"></a>the robot must act in real-time in the real world, and while parallelization is possible, being strategic </span>
<span id="cb12-66"><a href="#cb12-66"></a>about which examples to collect can best benefit the model. In meteorology, active learning can help decide </span>
<span id="cb12-67"><a href="#cb12-67"></a>where to place additional sensors for weather predictions <span class="co">[</span><span class="ot">@AL_app_sensors</span><span class="co">]</span>. Sensor placement involves </span>
<span id="cb12-68"><a href="#cb12-68"></a>deploying teams to remote locations and expensive construction for an extra data point. Choosing these </span>
<span id="cb12-69"><a href="#cb12-69"></a>locations and allocating resources wisely is of interest to governments and businesses. Active learning </span>
<span id="cb12-70"><a href="#cb12-70"></a>could also be employed to select data for fine-tuning large language models (LLMs) for specific downstream </span>
<span id="cb12-71"><a href="#cb12-71"></a>tasks <span class="co">[</span><span class="ot">@AL_app_LLMs</span><span class="co">]</span>. In this context, it might be difficult to fully describe an NLP task one might want an </span>
<span id="cb12-72"><a href="#cb12-72"></a>LLM to solve. Often, instead of defining a task via a dataset of examples, it may be easier for a human to </span>
<span id="cb12-73"><a href="#cb12-73"></a>interact with the LLM for a specific use case, identify gaps in the model, and address those using active learning.</span>
<span id="cb12-74"><a href="#cb12-74"></a></span>
<span id="cb12-75"><a href="#cb12-75"></a></span>
<span id="cb12-76"><a href="#cb12-76"></a><span class="fu">### Introduction to Active Preference Learning</span></span>
<span id="cb12-77"><a href="#cb12-77"></a></span>
<span id="cb12-78"><a href="#cb12-78"></a>Consider the scenario where a robot is being trained to assist individuals with feeding. How can such a robot be </span>
<span id="cb12-79"><a href="#cb12-79"></a>effectively taught to perform necessary tasks, such as determining the appropriate distance to reach, detecting the </span>
<span id="cb12-80"><a href="#cb12-80"></a>location of a person's mouth, and, most importantly, understanding human preferences? Typically, robots learn by </span>
<span id="cb12-81"><a href="#cb12-81"></a>observing human demonstrations, replicating the ways a person performs the task. However, this method poses </span>
<span id="cb12-82"><a href="#cb12-82"></a>significant challenges. Expert demonstrations are often limited, and training a supervised learning model would </span>
<span id="cb12-83"><a href="#cb12-83"></a>require vast amounts of demonstration data, which is difficult to obtain at scale. Moreover, demonstrations tend </span>
<span id="cb12-84"><a href="#cb12-84"></a>to be variable, reflecting the actions of individual humans, making the data collection process inconsistent. To </span>
<span id="cb12-85"><a href="#cb12-85"></a>address these limitations, alternative approaches have been proposed, such as using pairwise comparisons, where </span>
<span id="cb12-86"><a href="#cb12-86"></a>humans evaluate two action trajectories to determine the superior one, or employing physical corrections, in which </span>
<span id="cb12-87"><a href="#cb12-87"></a>reward functions are learned through human-robot interactions, with humans guiding the robot’s actions during the task.</span>
<span id="cb12-88"><a href="#cb12-88"></a></span>
<span id="cb12-89"><a href="#cb12-89"></a>Active learning algorithms can be employed in preference learning tasks, such as the previously mentioned example, </span>
<span id="cb12-90"><a href="#cb12-90"></a>where the objective is to develop a model that aligns with human preferences while minimizing the need for extensive </span>
<span id="cb12-91"><a href="#cb12-91"></a>labeled data or reducing the high cost of annotations. This chapter will explore the theoretical foundations of </span>
<span id="cb12-92"><a href="#cb12-92"></a>pairwise comparisons and active preference learning, along with extensions to these methods that address known </span>
<span id="cb12-93"><a href="#cb12-93"></a>limitations. Practical examples where these approaches prove beneficial will also be discussed. Additionally, we </span>
<span id="cb12-94"><a href="#cb12-94"></a>will examine the role of LLMs in assisting robots through corrective feedback and highlight the applications of these techniques.</span>
<span id="cb12-95"><a href="#cb12-95"></a></span>
<span id="cb12-96"><a href="#cb12-96"></a></span>
<span id="cb12-97"><a href="#cb12-97"></a><span class="fu">### Uncertainty Qualification</span></span>
<span id="cb12-98"><a href="#cb12-98"></a></span>
<span id="cb12-99"><a href="#cb12-99"></a>**Problem Setup**: In this section, we consider a binary classification problem. The model is trained on a small </span>
<span id="cb12-100"><a href="#cb12-100"></a>labeled dataset $\mathcal{D} = <span class="sc">\{</span>(x_1, y_1), \ldots, (x_n, y_n)<span class="sc">\}</span>$, where $x_i$ represents the input data and $y_i$ </span>
<span id="cb12-101"><a href="#cb12-101"></a>is the corresponding label. The model is uncertain about the class labels of some data points and can query an oracle </span>
<span id="cb12-102"><a href="#cb12-102"></a>to obtain the true labels of these data points. The goal is to minimize the number of queries to the oracle while </span>
<span id="cb12-103"><a href="#cb12-103"></a>maximizing the model's performance.</span>
<span id="cb12-104"><a href="#cb12-104"></a></span>
<span id="cb12-105"><a href="#cb12-105"></a>Uncertainty quantification (UQ) is a critical aspect of active learning that allows models to evaluate the informativeness </span>
<span id="cb12-106"><a href="#cb12-106"></a>of new data points. In machine learning (ML), two primary types of uncertainty are often considered: epistemic and </span>
<span id="cb12-107"><a href="#cb12-107"></a>aleatoric uncertainty. **Epistemic uncertainty**, or model uncertainty, arises from a lack of knowledge and can be </span>
<span id="cb12-108"><a href="#cb12-108"></a>reduced by acquiring more data. This type of uncertainty is especially significant when the model lacks confidence </span>
<span id="cb12-109"><a href="#cb12-109"></a>due to insufficient or incomplete information in its training set. On the other hand, **aleatoric uncertainty**, or </span>
<span id="cb12-110"><a href="#cb12-110"></a>data uncertainty, stems from the inherent randomness within the data itself. Unlike epistemic uncertainty, aleatoric </span>
<span id="cb12-111"><a href="#cb12-111"></a>uncertainty cannot be reduced, even with additional data, as it reflects noise or unpredictability in the real </span>
<span id="cb12-112"><a href="#cb12-112"></a>data-generating process. Several approaches exist to quantify uncertainty in active learning, each with its strengths and limitations. </span>
<span id="cb12-113"><a href="#cb12-113"></a></span>
<span id="cb12-114"><a href="#cb12-114"></a>**Bayesian methods**, such as Bayesian Neural Networks (BNNs) and Gaussian Processes (GPs), offer a principled way </span>
<span id="cb12-115"><a href="#cb12-115"></a>of estimating uncertainty by incorporating prior knowledge into the model. These approaches can generate meaningful </span>
<span id="cb12-116"><a href="#cb12-116"></a>uncertainty estimates that aid in choosing informative samples for labeling. However, they can become computationally </span>
<span id="cb12-117"><a href="#cb12-117"></a>prohibitive, especially for large and complex models, limiting their applicability in some practical scenarios.</span>
<span id="cb12-118"><a href="#cb12-118"></a></span>
<span id="cb12-119"><a href="#cb12-119"></a>Another common technique for uncertainty quantification is the use of **ensemble methods**, such as Random Forests or </span>
<span id="cb12-120"><a href="#cb12-120"></a>Gradient Boosting Machines. These methods involve training multiple models and combining their predictions to provide </span>
<span id="cb12-121"><a href="#cb12-121"></a>an estimate of uncertainty. Ensemble methods are relatively easy to implement and can give valuable insights into model </span>
<span id="cb12-122"><a href="#cb12-122"></a>uncertainty. However, they can be computationally expensive and may not always produce well-calibrated uncertainty </span>
<span id="cb12-123"><a href="#cb12-123"></a>estimates. Moreover, they do not integrate prior knowledge, which can be a disadvantage in certain applications.</span>
<span id="cb12-124"><a href="#cb12-124"></a></span>
<span id="cb12-125"><a href="#cb12-125"></a>**Conformal prediction methods** also provide a framework for estimating uncertainty by offering a measure of confidence </span>
<span id="cb12-126"><a href="#cb12-126"></a>in predictions based on the conformity of a given instance with the training data. While these methods are useful in </span>
<span id="cb12-127"><a href="#cb12-127"></a>some contexts, this book focuses primarily on the Bayesian approach due to its theoretical robustness and capacity to </span>
<span id="cb12-128"><a href="#cb12-128"></a>quantify uncertainty in a more comprehensive manner.</span>
<span id="cb12-129"><a href="#cb12-129"></a></span>
<span id="cb12-130"><a href="#cb12-130"></a></span>
<span id="cb12-131"><a href="#cb12-131"></a><span class="fu">### Acquisition Function</span></span>
<span id="cb12-132"><a href="#cb12-132"></a></span>
<span id="cb12-133"><a href="#cb12-133"></a>Uncertainty quantification plays a vital role in **acquisition functions**, which are central to active learning strategies. </span>
<span id="cb12-134"><a href="#cb12-134"></a>These functions determine which samples are most valuable to label by evaluating their utility based on the model's </span>
<span id="cb12-135"><a href="#cb12-135"></a>current uncertainty estimates. Common acquisition functions include **uncertainty sampling** <span class="co">[</span><span class="ot">@AL_uncertainty</span><span class="co">]</span>, </span>
<span id="cb12-136"><a href="#cb12-136"></a>which selects samples the model is least confident about, **query-by-committee** <span class="co">[</span><span class="ot">@AL_committee</span><span class="co">]</span>, which utilizes a set </span>
<span id="cb12-137"><a href="#cb12-137"></a>of models to choose the most uncertain samples, and **Bayesian Active Learning by Disagreement (BALD)** <span class="co">[</span><span class="ot">@AL_BALD</span><span class="co">]</span>, </span>
<span id="cb12-138"><a href="#cb12-138"></a>which selects samples that maximize information gain by reducing model uncertainty. Through careful uncertainty quantification, </span>
<span id="cb12-139"><a href="#cb12-139"></a>acquisition functions guide the active learning process, improving the model's efficiency in learning from limited data. </span>
<span id="cb12-140"><a href="#cb12-140"></a>Other acquisition functions that can be employed include:</span>
<span id="cb12-141"><a href="#cb12-141"></a></span>
<span id="cb12-142"><a href="#cb12-142"></a><span class="ss">-   </span>**Active Thompson Sampling** <span class="co">[</span><span class="ot">@AL_exploreexploit</span><span class="co">]</span>: This method leverages the Thompson Sampling algorithm to select a </span>
<span id="cb12-143"><a href="#cb12-143"></a>    posterior sample from the model's distribution and compute the expected utility of labeling using that sample. By doing so, </span>
<span id="cb12-144"><a href="#cb12-144"></a>    the algorithm balances exploration and exploitation, leading to effective active learning.</span>
<span id="cb12-145"><a href="#cb12-145"></a></span>
<span id="cb12-146"><a href="#cb12-146"></a><span class="ss">-   </span>**Expected model change** <span class="co">[</span><span class="ot">@AL_expmodelchange</span><span class="co">]</span>: This approach focuses on labeling points that would have the most </span>
<span id="cb12-147"><a href="#cb12-147"></a>    impact on changing the current model parameters.</span>
<span id="cb12-148"><a href="#cb12-148"></a></span>
<span id="cb12-149"><a href="#cb12-149"></a><span class="ss">-   </span>**Expected error reduction** <span class="co">[</span><span class="ot">@AL_experrorredn</span><span class="co">]</span>: Points that would most effectively reduce the model's generalization </span>
<span id="cb12-150"><a href="#cb12-150"></a>    error are labeled using this strategy.</span>
<span id="cb12-151"><a href="#cb12-151"></a></span>
<span id="cb12-152"><a href="#cb12-152"></a><span class="ss">-   </span>**Variance reduction** <span class="co">[</span><span class="ot">@AL_variance</span><span class="co">]</span>: This approach labels points that would minimize output variance, which is one </span>
<span id="cb12-153"><a href="#cb12-153"></a>    component of error. By selecting points that reduce variability in the model's predictions, it aims to improve overall performance.</span>
<span id="cb12-154"><a href="#cb12-154"></a></span>
<span id="cb12-155"><a href="#cb12-155"></a><span class="ss">-   </span>**User Centered Labeling Strategies** <span class="co">[</span><span class="ot">@AL_usercentered</span><span class="co">]</span>: This approach involves actively involving the user in </span>
<span id="cb12-156"><a href="#cb12-156"></a>    the labeling process by visualizing data through dimensionality reduction techniques. The user then provides labels </span>
<span id="cb12-157"><a href="#cb12-157"></a>    for the compiled data based on their domain expertise and preferences. This strategy leverages user input to improve </span>
<span id="cb12-158"><a href="#cb12-158"></a>    the quality and relevance of the labeled data.</span>
<span id="cb12-159"><a href="#cb12-159"></a></span>
<span id="cb12-160"><a href="#cb12-160"></a><span class="ss">-   </span>**Querying from diverse subspaces or partitions** <span class="co">[</span><span class="ot">@AL_partition</span><span class="co">]</span>: When using a forest of trees as the underlying model, </span>
<span id="cb12-161"><a href="#cb12-161"></a>    the leaf nodes can represent overlapping partitions of the feature space. This strategy selects instances from non-overlapping </span>
<span id="cb12-162"><a href="#cb12-162"></a>    or minimally overlapping partitions for labeling.</span>
<span id="cb12-163"><a href="#cb12-163"></a></span>
<span id="cb12-164"><a href="#cb12-164"></a><span class="ss">-   </span>**Conformal prediction** <span class="co">[</span><span class="ot">@AL_conformal</span><span class="co">]</span>: This method predicts that a new data point will have a label similar to old </span>
<span id="cb12-165"><a href="#cb12-165"></a>    data points in some specified way. The degree of similarity within the old examples is used to estimate the confidence </span>
<span id="cb12-166"><a href="#cb12-166"></a>    in the prediction.</span>
<span id="cb12-167"><a href="#cb12-167"></a></span>
<span id="cb12-168"><a href="#cb12-168"></a><span class="ss">-   </span>**Mismatch-first farthest-traversal** <span class="co">[</span><span class="ot">@AL_mismatch</span><span class="co">]</span>: This strategy first prioritizes data points that are wrongly </span>
<span id="cb12-169"><a href="#cb12-169"></a>    predicted by the current model compared to the nearest-neighbor prediction. The second criterion is the distance to </span>
<span id="cb12-170"><a href="#cb12-170"></a>    previously selected data, with preference given to those that are farthest away. The goal is to optimize both the </span>
<span id="cb12-171"><a href="#cb12-171"></a>    correction of mispredictions and the diversity of the selected data.</span>
<span id="cb12-172"><a href="#cb12-172"></a></span>
<span id="cb12-173"><a href="#cb12-173"></a><span class="fu">#### Uncertainty Sampling {.unnumbered}</span></span>
<span id="cb12-174"><a href="#cb12-174"></a>Uncertainty sampling <span class="co">[</span><span class="ot">@AL_uncertainty</span><span class="co">]</span> is a widely used acquisition function in active learning that selects data </span>
<span id="cb12-175"><a href="#cb12-175"></a>points for which the model exhibits the greatest uncertainty. This method aims to improve model performance by focusing </span>
<span id="cb12-176"><a href="#cb12-176"></a>labeling efforts on ambiguous samples, where additional information is likely to yield the greatest benefit. Let $x$ </span>
<span id="cb12-177"><a href="#cb12-177"></a>represent the input, and $p(y|x)$ the probability distribution of the output $y$ given $x$. Several acquisition strategies </span>
<span id="cb12-178"><a href="#cb12-178"></a>fall under uncertainty sampling, including **entropy sampling**, **margin sampling**, and **least confidence sampling**, </span>
<span id="cb12-179"><a href="#cb12-179"></a>each providing a unique measure of uncertainty.</span>
<span id="cb12-180"><a href="#cb12-180"></a></span>
<span id="cb12-181"><a href="#cb12-181"></a><span class="ss">-   </span>**Entropy sampling** measures uncertainty by calculating the entropy of the predicted probability distribution. </span>
<span id="cb12-182"><a href="#cb12-182"></a>    The acquisition function is given by $\alpha(x) = - \sum_{y} p(y|x) \log p(y|x)$, with higher entropy values indicating </span>
<span id="cb12-183"><a href="#cb12-183"></a>    higher uncertainty.</span>
<span id="cb12-184"><a href="#cb12-184"></a></span>
<span id="cb12-185"><a href="#cb12-185"></a><span class="ss">-   </span>**Margin sampling** focuses on the difference between the two highest predicted probabilities for a sample. The </span>
<span id="cb12-186"><a href="#cb12-186"></a>    acquisition function is given by $\alpha(x) = p(y_1|x) - p(y_2|x)$, where $y_1$ and $y_2$ are two most likely classes. </span>
<span id="cb12-187"><a href="#cb12-187"></a>    Smaller margins signify greater uncertainty.</span>
<span id="cb12-188"><a href="#cb12-188"></a></span>
<span id="cb12-189"><a href="#cb12-189"></a><span class="ss">-   </span>**Least confidence sampling** measures uncertainty by identifying the sample with the lowest predicted probability </span>
<span id="cb12-190"><a href="#cb12-190"></a>    for its most likely class. The acquisition function is $\alpha(x) = 1 - p(y_{\text{max}}|x)$, where $y_{\text{max}}$ </span>
<span id="cb12-191"><a href="#cb12-191"></a>    is the class with the highest probability.</span>
<span id="cb12-192"><a href="#cb12-192"></a></span>
<span id="cb12-193"><a href="#cb12-193"></a>**Example:** Consider a binary classification problem with two classes $y_1$ and $y_2$. </span>
<span id="cb12-194"><a href="#cb12-194"></a>We have three samples $x_1, x_2, x_3$ and the corresponding predictive distributions are as follows:</span>
<span id="cb12-195"><a href="#cb12-195"></a>$$\begin{aligned}</span>
<span id="cb12-196"><a href="#cb12-196"></a>p(y_1|x_1) &amp;= 0.6, \quad p(y_2|x_1) = 0.4<span class="sc">\\</span></span>
<span id="cb12-197"><a href="#cb12-197"></a>p(y_1|x_2) &amp;= 0.3, \quad p(y_2|x_2) = 0.7<span class="sc">\\</span></span>
<span id="cb12-198"><a href="#cb12-198"></a>p(y_1|x_3) &amp;= 0.8, \quad p(y_2|x_3) = 0.2</span>
<span id="cb12-199"><a href="#cb12-199"></a>\end{aligned}$$ {#eq-eq3.1}</span>
<span id="cb12-200"><a href="#cb12-200"></a></span>
<span id="cb12-201"><a href="#cb12-201"></a><span class="ss">-   </span>**Entropy Sampling** </span>
<span id="cb12-202"><a href="#cb12-202"></a><span class="ss">    +   </span>$\alpha(x_1) = -0.6 \log (0.6) - 0.4 \log (0.4) = 0.29$</span>
<span id="cb12-203"><a href="#cb12-203"></a><span class="ss">    +   </span>$\alpha(x_2) = -0.3 \log (0.3) - 0.7 \log (0.7) = 0.27$</span>
<span id="cb12-204"><a href="#cb12-204"></a><span class="ss">    +   </span>$\alpha(x_3) = -0.8 \log (0.8) - 0.2 \log (0.2) = 0.22$</span>
<span id="cb12-205"><a href="#cb12-205"></a></span>
<span id="cb12-206"><a href="#cb12-206"></a>We would select $x_1$ for labeling as it has the highest entropy, indicating the model is most uncertain about its prediction at $x_1$.</span>
<span id="cb12-207"><a href="#cb12-207"></a></span>
<span id="cb12-208"><a href="#cb12-208"></a><span class="ss">-   </span>**Margin Sampling** </span>
<span id="cb12-209"><a href="#cb12-209"></a><span class="ss">    +   </span>$\alpha(x_1) = 0.6 - 0.4 = 0.2$</span>
<span id="cb12-210"><a href="#cb12-210"></a><span class="ss">    +   </span>$\alpha(x_2) = 0.7 - 0.3 = 0.4$</span>
<span id="cb12-211"><a href="#cb12-211"></a><span class="ss">    +   </span>$\alpha(x_3) = 0.8 - 0.2 = 0.6$</span>
<span id="cb12-212"><a href="#cb12-212"></a></span>
<span id="cb12-213"><a href="#cb12-213"></a>We would select $x_1$ for labeling as it has the smallest margin, indicating the model is most uncertain about the prediction at $x_1$.</span>
<span id="cb12-214"><a href="#cb12-214"></a></span>
<span id="cb12-215"><a href="#cb12-215"></a><span class="ss">-   </span>**Least Confidence Sampling** </span>
<span id="cb12-216"><a href="#cb12-216"></a><span class="ss">    +   </span>$\alpha(x_1) = 1 - 0.6 = 0.4$</span>
<span id="cb12-217"><a href="#cb12-217"></a><span class="ss">    +   </span>$\alpha(x_2) = 1 - 0.7 = 0.3$</span>
<span id="cb12-218"><a href="#cb12-218"></a><span class="ss">    +   </span>$\alpha(x_3) = 1 - 0.8 = 0.2$</span>
<span id="cb12-219"><a href="#cb12-219"></a></span>
<span id="cb12-220"><a href="#cb12-220"></a>We would select $x_1$ for labeling as it has the lowest confidence, indicating the model is most uncertain about the prediction at $x_1$.</span>
<span id="cb12-221"><a href="#cb12-221"></a></span>
<span id="cb12-222"><a href="#cb12-222"></a>In summary, uncertainty sampling methods, whether based on entropy, margin, or least confidence, help prioritize data </span>
<span id="cb12-223"><a href="#cb12-223"></a>points that the model struggles with the most. By focusing on these uncertain samples, the model can more efficiently </span>
<span id="cb12-224"><a href="#cb12-224"></a>improve its performance, making uncertainty sampling a key tool in active learning.</span>
<span id="cb12-225"><a href="#cb12-225"></a></span>
<span id="cb12-226"><a href="#cb12-226"></a><span class="fu">#### Query-by-Committee {.unnumbered}</span></span>
<span id="cb12-227"><a href="#cb12-227"></a>Query-by-Committee <span class="co">[</span><span class="ot">@AL_committee</span><span class="co">]</span> is an active learning strategy where a committee of models selects samples for </span>
<span id="cb12-228"><a href="#cb12-228"></a>labeling based on the level of disagreement among the committee members. Several acquisition functions can be employed </span>
<span id="cb12-229"><a href="#cb12-229"></a>under this framework to quantify the disagreement:</span>
<span id="cb12-230"><a href="#cb12-230"></a></span>
<span id="cb12-231"><a href="#cb12-231"></a><span class="ss">-   </span>**Vote Entropy:** The vote entropy measures the uncertainty based on how often the committee members vote for each class. </span>
<span id="cb12-232"><a href="#cb12-232"></a>    The acquisition function is defined as $\alpha(x) = \mathbb{H}\left<span class="co">[</span><span class="ot">\frac{V(y)}{C}\right</span><span class="co">]</span>$, where $V(y)$ is the number of </span>
<span id="cb12-233"><a href="#cb12-233"></a>    votes for class $y$ and $C$ is the number of committee members.</span>
<span id="cb12-234"><a href="#cb12-234"></a></span>
<span id="cb12-235"><a href="#cb12-235"></a><span class="ss">-   </span>**Consensus Entropy:** This acquisition function measures the entropy of the average probability distribution across committee </span>
<span id="cb12-236"><a href="#cb12-236"></a>    members. It is given by $\alpha(x) = \mathbb{H}<span class="co">[</span><span class="ot">P_C(y|x)</span><span class="co">]</span>$, where $P_C(y|x)$ is the average probability distribution for sample </span>
<span id="cb12-237"><a href="#cb12-237"></a>    $x$ across all committee members.</span>
<span id="cb12-238"><a href="#cb12-238"></a></span>
<span id="cb12-239"><a href="#cb12-239"></a><span class="ss">-   </span>**KL Divergence:** The KL divergence quantifies the disagreement by comparing the probability distribution of each </span>
<span id="cb12-240"><a href="#cb12-240"></a>    committee member to the average distribution. The acquisition function is given by </span>
<span id="cb12-241"><a href="#cb12-241"></a>    $\alpha(x) = \frac{1}{C} \sum_{c=1}^{C} D_{KL}<span class="co">[</span><span class="ot">P_c(y|x) || P_C(y|x)</span><span class="co">]</span>$, </span>
<span id="cb12-242"><a href="#cb12-242"></a>    where $P_c(y|x)$ is the probability distribution of committee member $c$ and $P_C(y|x)$ is the average distribution </span>
<span id="cb12-243"><a href="#cb12-243"></a>    across the committee.</span>
<span id="cb12-244"><a href="#cb12-244"></a></span>
<span id="cb12-245"><a href="#cb12-245"></a>**Example:** Consider a binary classification problem with two classes $y_1$ and $y_2$. We have three committee members and three samples: $x_1$, $x_2$, and $x_3$. The predictive distributions for each committee member are given below:</span>
<span id="cb12-246"><a href="#cb12-246"></a></span>
<span id="cb12-247"><a href="#cb12-247"></a>|   $x$ | $p_1(y_1 \vert \cdot)$  | $p_1(y_2 \vert \cdot)$ | $p_2(y_1 \vert \cdot)$ | $p_2(y_2 \vert \cdot)$ | $p_3(y_1 \vert \cdot)$ | $p_3(y_2 \vert \cdot)$ |</span>
<span id="cb12-248"><a href="#cb12-248"></a>| ----- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- | --------------------- |</span>
<span id="cb12-249"><a href="#cb12-249"></a>| $x_1$ | 0.6                   | 0.4                   | 0.7                   | 0.3                   | 0.3                   | 0.7                   |</span>
<span id="cb12-250"><a href="#cb12-250"></a>| $x_2$ | 0.3                   | 0.7                   | 0.4                   | 0.6                   | 0.4                   | 0.6                   |</span>
<span id="cb12-251"><a href="#cb12-251"></a>| $x_3$ | 0.8                   | 0.2                   | 0.9                   | 0.1                   | 0.7                   | 0.3                   |</span>
<span id="cb12-252"><a href="#cb12-252"></a></span>
<span id="cb12-253"><a href="#cb12-253"></a>**Query-by-Committee: Vote Entropy**</span>
<span id="cb12-254"><a href="#cb12-254"></a></span>
<span id="cb12-255"><a href="#cb12-255"></a><span class="ss">-   </span>For sample $x_1$, the votes for $y_1$ and $y_2$ are $V(y_1) = 2$ and $V(y_2) = 1$. The vote entropy is $\alpha(x_1) = - \frac{2}{3} \log (\frac{2}{3}) - \frac{1}{3} \log (\frac{1}{3}) = 0.28$.</span>
<span id="cb12-256"><a href="#cb12-256"></a><span class="ss">-   </span>For sample $x_2$, the votes are $V(y_1) = 0$ and $V(y_2) = 3$, resulting in vote entropy $\alpha(x_2) = 0$.</span>
<span id="cb12-257"><a href="#cb12-257"></a><span class="ss">-   </span>For sample $x_3$, the votes are $V(y_1) = 3$ and $V(y_2) = 0$, resulting in vote entropy $\alpha(x_3) = 0$.</span>
<span id="cb12-258"><a href="#cb12-258"></a></span>
<span id="cb12-259"><a href="#cb12-259"></a>Thus, sample $x_1$ would be selected for labeling as it has the highest vote entropy, indicating the greatest disagreement among the committee members.</span>
<span id="cb12-260"><a href="#cb12-260"></a></span>
<span id="cb12-261"><a href="#cb12-261"></a>**Query-by-Committee: Consensus Entropy**</span>
<span id="cb12-262"><a href="#cb12-262"></a></span>
<span id="cb12-263"><a href="#cb12-263"></a>The first step is to compute the consensus probability of each class for each sample:</span>
<span id="cb12-264"><a href="#cb12-264"></a></span>
<span id="cb12-265"><a href="#cb12-265"></a><span class="ss">-   </span>For $x_1$, $p_c(y_1|x_1) = \frac{0.6 + 0.7 + 0.3}{3} = 0.53$ and $p_c(y_2|x_1) = \frac{0.4 + 0.3 + 0.7}{3} = 0.47$.</span>
<span id="cb12-266"><a href="#cb12-266"></a><span class="ss">-   </span>For $x_2$, $p_c(y_1|x_2) = \frac{0.3 + 0.4 + 0.4}{3} = 0.37$ and $p_c(y_2|x_2) = \frac{0.7 + 0.6 + 0.6}{3} = 0.63$.</span>
<span id="cb12-267"><a href="#cb12-267"></a><span class="ss">-   </span>For $x_3$, $p_c(y_1|x_3) = \frac{0.8 + 0.9 + 0.7}{3} = 0.8$ and $p_c(y_2|x_3) = \frac{0.2 + 0.1 + 0.3}{3} = 0.2$.</span>
<span id="cb12-268"><a href="#cb12-268"></a></span>
<span id="cb12-269"><a href="#cb12-269"></a>Next, we compute the entropy of these consensus probabilities:</span>
<span id="cb12-270"><a href="#cb12-270"></a></span>
<span id="cb12-271"><a href="#cb12-271"></a><span class="ss">-   </span>For $x_1$, $\mathbb{H}<span class="co">[</span><span class="ot">p_c(y|x_1)</span><span class="co">]</span> = -0.53 \log (0.53) - 0.47 \log (0.47) = 0.30$.</span>
<span id="cb12-272"><a href="#cb12-272"></a><span class="ss">-   </span>For $x_2$, $\mathbb{H}<span class="co">[</span><span class="ot">p_c(y|x_2)</span><span class="co">]</span> = -0.37 \log (0.37) - 0.63 \log (0.63) = 0.29$.</span>
<span id="cb12-273"><a href="#cb12-273"></a><span class="ss">-   </span>For $x_3$, $\mathbb{H}<span class="co">[</span><span class="ot">p_c(y|x_3)</span><span class="co">]</span> = -0.8 \log (0.8) - 0.2 \log (0.2) = 0.22$.</span>
<span id="cb12-274"><a href="#cb12-274"></a></span>
<span id="cb12-275"><a href="#cb12-275"></a>Thus, $x_1$ would be selected for labeling as it has the highest consensus entropy, indicating the highest level of disagreement among the committee members.</span>
<span id="cb12-276"><a href="#cb12-276"></a></span>
<span id="cb12-277"><a href="#cb12-277"></a><span class="fu">#### Bayesian Active Learning by Disagreement {.unnumbered}</span></span>
<span id="cb12-278"><a href="#cb12-278"></a></span>
<span id="cb12-279"><a href="#cb12-279"></a>Bayesian Active Learning by Disagreement (BALD) <span class="co">[</span><span class="ot">@AL_BALD</span><span class="co">]</span> selects the samples for which the model expects to gain the most Shannon information when corresponding labels are observed:</span>
<span id="cb12-280"><a href="#cb12-280"></a></span>
<span id="cb12-281"><a href="#cb12-281"></a>$$</span>
<span id="cb12-282"><a href="#cb12-282"></a>\mathbb{I}(\theta; y|x, \mathcal{D}) = \mathbb{H}<span class="co">[</span><span class="ot">p(y|x, \mathcal{D})</span><span class="co">]</span> - \mathbb{E}_{p(\theta | \mathcal{D})} <span class="co">[</span><span class="ot">\mathbb{H}[p(y|x, \theta, \mathcal{D})]</span><span class="co">]</span></span>
<span id="cb12-283"><a href="#cb12-283"></a>$$ {#eq-eq3.2}</span>
<span id="cb12-284"><a href="#cb12-284"></a></span>
<span id="cb12-285"><a href="#cb12-285"></a>where $\mathbb{H}<span class="co">[</span><span class="ot">\cdot</span><span class="co">]</span>$ denotes entropy. When there is significant disagreement among models, the predictive entropy (the first term) will be large, while the expected entropy (the second term) will be smaller. This difference represents the degree to which the models disagree. BALD selects points where this disagreement is maximized.</span>
<span id="cb12-286"><a href="#cb12-286"></a></span>
<span id="cb12-287"><a href="#cb12-287"></a></span>
<span id="cb12-288"><a href="#cb12-288"></a><span class="ss">-   </span>To compute the first term, we can derive the following expression:</span>
<span id="cb12-289"><a href="#cb12-289"></a></span>
<span id="cb12-290"><a href="#cb12-290"></a>$$\begin{aligned}</span>
<span id="cb12-291"><a href="#cb12-291"></a>\mathbb{H}<span class="co">[</span><span class="ot">p(y|x, \mathcal{D})</span><span class="co">]</span> &amp;= \mathbb{H}\left<span class="co">[</span><span class="ot">\int_{\theta} p(y|x, \theta, \mathcal{D}) p(\theta | \mathcal{D}) d\theta\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb12-292"><a href="#cb12-292"></a>&amp;\approx \mathbb{H}\left<span class="co">[</span><span class="ot">\frac{1}{N}\sum_{i=1}^{N} p(y|x, \theta_i, \mathcal{D})\right</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb12-293"><a href="#cb12-293"></a>&amp;= \mathbb{H}\left<span class="co">[</span><span class="ot">\overline{p}(y|x, \mathcal{D})\right</span><span class="co">]</span></span>
<span id="cb12-294"><a href="#cb12-294"></a>\end{aligned}$$ {#eq-eq3.3}</span>
<span id="cb12-295"><a href="#cb12-295"></a></span>
<span id="cb12-296"><a href="#cb12-296"></a><span class="ss">-   </span>To compute the second term, we can derive the following expression:</span>
<span id="cb12-297"><a href="#cb12-297"></a></span>
<span id="cb12-298"><a href="#cb12-298"></a>$$\begin{aligned}</span>
<span id="cb12-299"><a href="#cb12-299"></a>\mathbb{E}_{p(\theta|\mathcal{D})} [\mathbb{H}[p(y|x, \theta, \mathcal{D})]] &amp;= \mathbb{E}_{p(\theta|\mathcal{D})} \left[ - \sum_{y} p(y|x, \theta, \mathcal{D}) \log p(y|x, \theta, \mathcal{D}) \right] <span class="sc">\\</span></span>
<span id="cb12-300"><a href="#cb12-300"></a>&amp;\approx - \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{y} p(y|x, \theta_i, \mathcal{D}) \log p(y|x, \theta_i, \mathcal{D}) \right)</span>
<span id="cb12-301"><a href="#cb12-301"></a>\end{aligned}$$ {#eq-eq3.4}</span>
<span id="cb12-302"><a href="#cb12-302"></a></span>
<span id="cb12-303"><a href="#cb12-303"></a></span>
<span id="cb12-304"><a href="#cb12-304"></a>**Example:** Consider a binary classification problem with two classes, $y_1$ and $y_2$. We have two samples, $x_1$ and $x_2$, and the model’s predictive distributions are as follows:</span>
<span id="cb12-305"><a href="#cb12-305"></a></span>
<span id="cb12-306"><a href="#cb12-306"></a><span class="ss">- </span>**First-time inference** (with $\theta_1 \sim p(\theta | \mathcal{D})$):</span>
<span id="cb12-307"><a href="#cb12-307"></a>  $$</span>
<span id="cb12-308"><a href="#cb12-308"></a>  p(y_1|x_1, \theta_1, \mathcal{D}) = 0.6, \quad p(y_2|x_1, \theta_1, \mathcal{D}) = 0.4</span>
<span id="cb12-309"><a href="#cb12-309"></a>  $$ {#eq-eq3.5}</span>
<span id="cb12-310"><a href="#cb12-310"></a>  $$</span>
<span id="cb12-311"><a href="#cb12-311"></a>  p(y_1|x_2, \theta_1, \mathcal{D}) = 0.4, \quad p(y_2|x_2, \theta_1, \mathcal{D}) = 0.6</span>
<span id="cb12-312"><a href="#cb12-312"></a>  $$ {#eq-eq3.6}</span>
<span id="cb12-313"><a href="#cb12-313"></a></span>
<span id="cb12-314"><a href="#cb12-314"></a><span class="ss">- </span>**Second-time inference** (with $\theta_2 \sim p(\theta | \mathcal{D})$):</span>
<span id="cb12-315"><a href="#cb12-315"></a>  $$</span>
<span id="cb12-316"><a href="#cb12-316"></a>  p(y_1|x_1, \theta_2, \mathcal{D}) = 0.8, \quad p(y_2|x_1, \theta_2, \mathcal{D}) = 0.2</span>
<span id="cb12-317"><a href="#cb12-317"></a>  $$ {#eq-eq3.7}</span>
<span id="cb12-318"><a href="#cb12-318"></a>  $$</span>
<span id="cb12-319"><a href="#cb12-319"></a>  p(y_1|x_2, \theta_2, \mathcal{D}) = 0.5, \quad p(y_2|x_2, \theta_2, \mathcal{D}) = 0.5</span>
<span id="cb12-320"><a href="#cb12-320"></a>  $$ {#eq-eq3.8}</span>
<span id="cb12-321"><a href="#cb12-321"></a></span>
<span id="cb12-322"><a href="#cb12-322"></a>**Solution:**</span>
<span id="cb12-323"><a href="#cb12-323"></a></span>
<span id="cb12-324"><a href="#cb12-324"></a>**Step 1:** Compute the entropy of the model's predictive distribution for each sample:</span>
<span id="cb12-325"><a href="#cb12-325"></a></span>
<span id="cb12-326"><a href="#cb12-326"></a><span class="ss">-   </span>$\overline{p}_{\theta}(y_1|x_1, \theta, \mathcal{D}) = 0.7$</span>
<span id="cb12-327"><a href="#cb12-327"></a><span class="ss">-   </span>$\overline{p}_{\theta}(y_2|x_1, \theta, \mathcal{D}) = 0.3$</span>
<span id="cb12-328"><a href="#cb12-328"></a><span class="ss">-   </span>$\overline{p}_{\theta}(y_1|x_2, \theta, \mathcal{D}) = 0.45$</span>
<span id="cb12-329"><a href="#cb12-329"></a><span class="ss">-   </span>$\overline{p}_{\theta}(y_2|x_2, \theta, \mathcal{D}) = 0.55$</span>
<span id="cb12-330"><a href="#cb12-330"></a></span>
<span id="cb12-331"><a href="#cb12-331"></a>Now, we compute the entropy for each sample using the formula:</span>
<span id="cb12-332"><a href="#cb12-332"></a></span>
<span id="cb12-333"><a href="#cb12-333"></a>$$</span>
<span id="cb12-334"><a href="#cb12-334"></a>\mathbb{H}<span class="co">[</span><span class="ot">p(y|x, \mathcal{D})</span><span class="co">]</span> = - p(y_1|x, \mathcal{D}) \log(p(y_1|x, \mathcal{D})) - p(y_2|x, \mathcal{D}) \log(p(y_2|x, \mathcal{D}))</span>
<span id="cb12-335"><a href="#cb12-335"></a>$$ {#eq-eq3.9}</span>
<span id="cb12-336"><a href="#cb12-336"></a></span>
<span id="cb12-337"><a href="#cb12-337"></a>For $x_1$:</span>
<span id="cb12-338"><a href="#cb12-338"></a></span>
<span id="cb12-339"><a href="#cb12-339"></a>$$</span>
<span id="cb12-340"><a href="#cb12-340"></a>\mathbb{H}<span class="co">[</span><span class="ot">p(y|x_1, \mathcal{D})</span><span class="co">]</span> = - 0.7 \log(0.7) - 0.3 \log(0.3) = 0.27</span>
<span id="cb12-341"><a href="#cb12-341"></a>$$ {#eq-eq3.10}</span>
<span id="cb12-342"><a href="#cb12-342"></a></span>
<span id="cb12-343"><a href="#cb12-343"></a>For $x_2$:</span>
<span id="cb12-344"><a href="#cb12-344"></a></span>
<span id="cb12-345"><a href="#cb12-345"></a>$$</span>
<span id="cb12-346"><a href="#cb12-346"></a>\mathbb{H}<span class="co">[</span><span class="ot">p(y|x_2, \mathcal{D})</span><span class="co">]</span> = - 0.45 \log(0.45) - 0.55 \log(0.55) = 0.30</span>
<span id="cb12-347"><a href="#cb12-347"></a>$$ {#eq-eq3.11}</span>
<span id="cb12-348"><a href="#cb12-348"></a></span>
<span id="cb12-349"><a href="#cb12-349"></a>**Step 2:** Compute the expected entropy of the model's predictive distribution for each sample.</span>
<span id="cb12-350"><a href="#cb12-350"></a></span>
<span id="cb12-351"><a href="#cb12-351"></a>For $x_1$:</span>
<span id="cb12-352"><a href="#cb12-352"></a></span>
<span id="cb12-353"><a href="#cb12-353"></a><span class="ss">-   </span>$\mathbb{H}_{\theta_1}<span class="co">[</span><span class="ot">p(y|x_1, \theta, \mathcal{D})</span><span class="co">]</span> = -0.6 \log(0.6) - 0.4 \log(0.4) = 0.29$</span>
<span id="cb12-354"><a href="#cb12-354"></a><span class="ss">-   </span>$\mathbb{H}_{\theta_2}<span class="co">[</span><span class="ot">p(y|x_1, \theta, \mathcal{D})</span><span class="co">]</span> = -0.8 \log(0.8) - 0.2 \log(0.2) = 0.22$</span>
<span id="cb12-355"><a href="#cb12-355"></a></span>
<span id="cb12-356"><a href="#cb12-356"></a>Average the results:</span>
<span id="cb12-357"><a href="#cb12-357"></a></span>
<span id="cb12-358"><a href="#cb12-358"></a>$$</span>
<span id="cb12-359"><a href="#cb12-359"></a>\mathbb{E}_{p(\theta|\mathcal{D})}<span class="co">[</span><span class="ot">\mathbb{H}[p(y|x_1, \theta, \mathcal{D})]</span><span class="co">]</span> \approx \frac{0.29 + 0.22}{2} = 0.255</span>
<span id="cb12-360"><a href="#cb12-360"></a>$$ {#eq-eq3.12}</span>
<span id="cb12-361"><a href="#cb12-361"></a></span>
<span id="cb12-362"><a href="#cb12-362"></a>For $x_2$:</span>
<span id="cb12-363"><a href="#cb12-363"></a></span>
<span id="cb12-364"><a href="#cb12-364"></a><span class="ss">-   </span>$\mathbb{H}_{\theta_1}<span class="co">[</span><span class="ot">p(y|x_2, \theta, \mathcal{D})</span><span class="co">]</span> = -0.4 \log(0.4) - 0.6 \log(0.6) = 0.29$</span>
<span id="cb12-365"><a href="#cb12-365"></a><span class="ss">-   </span>$\mathbb{H}_{\theta_2}<span class="co">[</span><span class="ot">p(y|x_2, \theta, \mathcal{D})</span><span class="co">]</span> = -0.5 \log(0.5) - 0.5 \log(0.5) = 0.30$</span>
<span id="cb12-366"><a href="#cb12-366"></a></span>
<span id="cb12-367"><a href="#cb12-367"></a>Average the results:</span>
<span id="cb12-368"><a href="#cb12-368"></a></span>
<span id="cb12-369"><a href="#cb12-369"></a>$$</span>
<span id="cb12-370"><a href="#cb12-370"></a>\mathbb{E}_{p(\theta|\mathcal{D})}<span class="co">[</span><span class="ot">\mathbb{H}[p(y|x_2, \theta, \mathcal{D})]</span><span class="co">]</span> \approx \frac{0.29 + 0.30}{2} = 0.295</span>
<span id="cb12-371"><a href="#cb12-371"></a>$$ {#eq-eq3.13}</span>
<span id="cb12-372"><a href="#cb12-372"></a></span>
<span id="cb12-373"><a href="#cb12-373"></a>**Step 3:** Compute the BALD score for each sample.</span>
<span id="cb12-374"><a href="#cb12-374"></a></span>
<span id="cb12-375"><a href="#cb12-375"></a>The BALD score $\alpha(x)$ is the difference between the predictive entropy and the expected entropy:</span>
<span id="cb12-376"><a href="#cb12-376"></a></span>
<span id="cb12-377"><a href="#cb12-377"></a>For $x_1$:</span>
<span id="cb12-378"><a href="#cb12-378"></a></span>
<span id="cb12-379"><a href="#cb12-379"></a>$$</span>
<span id="cb12-380"><a href="#cb12-380"></a>\alpha(x_1) = \mathbb{H}<span class="co">[</span><span class="ot">p(y|x_1, \mathcal{D})</span><span class="co">]</span> - \mathbb{E}_{p(\theta|\mathcal{D})}<span class="co">[</span><span class="ot">\mathbb{H}[p(y|x_1, \theta, \mathcal{D})]</span><span class="co">]</span> = 0.27 - 0.255 = 0.015</span>
<span id="cb12-381"><a href="#cb12-381"></a>$$ {#eq-eq3.14}</span>
<span id="cb12-382"><a href="#cb12-382"></a></span>
<span id="cb12-383"><a href="#cb12-383"></a>For $x_2$:</span>
<span id="cb12-384"><a href="#cb12-384"></a></span>
<span id="cb12-385"><a href="#cb12-385"></a>$$</span>
<span id="cb12-386"><a href="#cb12-386"></a>\alpha(x_2) = \mathbb{H}<span class="co">[</span><span class="ot">p(y|x_2, \mathcal{D})</span><span class="co">]</span> - \mathbb{E}_{p(\theta|\mathcal{D})}<span class="co">[</span><span class="ot">\mathbb{H}[p(y|x_2, \theta, \mathcal{D})]</span><span class="co">]</span> = 0.30 - 0.295 = 0.005</span>
<span id="cb12-387"><a href="#cb12-387"></a>$$ {#eq-eq3.15}</span>
<span id="cb12-388"><a href="#cb12-388"></a></span>
<span id="cb12-389"><a href="#cb12-389"></a>We would select $x_1$ for labeling since it has the highest BALD score, indicating that labeling $x_1$ will provide the most information gain for the model.</span>
<span id="cb12-390"><a href="#cb12-390"></a></span>
<span id="cb12-391"><a href="#cb12-391"></a><span class="fu">### Active Learning by Variance Reduction</span></span>
<span id="cb12-392"><a href="#cb12-392"></a></span>
<span id="cb12-393"><a href="#cb12-393"></a>Active Learning by Variance Reduction <span class="co">[</span><span class="ot">@AL_variance</span><span class="co">]</span> is an algorithm designed to select the next data point for labeling </span>
<span id="cb12-394"><a href="#cb12-394"></a>based on the anticipated reduction in the model's variance. The objective is to identify the point $\tilde{x} \sim p(x)$ </span>
<span id="cb12-395"><a href="#cb12-395"></a>that, when labeled ($y(\tilde{x})$), will most effectively decrease the model's variance. To quantify the expected error </span>
<span id="cb12-396"><a href="#cb12-396"></a>at a given input $x$, we can mathematically express it as follows:</span>
<span id="cb12-397"><a href="#cb12-397"></a></span>
<span id="cb12-398"><a href="#cb12-398"></a>$$</span>
<span id="cb12-399"><a href="#cb12-399"></a>\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2</span>
<span id="cb12-400"><a href="#cb12-400"></a>$$ {#eq-eq3.16}</span>
<span id="cb12-401"><a href="#cb12-401"></a></span>
<span id="cb12-402"><a href="#cb12-402"></a>In @eq-eq3.16, $\hat{y}$ represents the model's prediction, while $y$ denotes the true label corresponding </span>
<span id="cb12-403"><a href="#cb12-403"></a>to the input $x$. This formulation captures the average squared difference between the predicted and actual values, </span>
<span id="cb12-404"><a href="#cb12-404"></a>providing a measure of the model's accuracy. Utilizing concepts from bias-variance decomposition as outlined in the </span>
<span id="cb12-405"><a href="#cb12-405"></a>literature <span class="co">[</span><span class="ot">@bias_variance_orig_paper</span><span class="co">]</span>, we can expand the expected error term. The expansion is given by:</span>
<span id="cb12-406"><a href="#cb12-406"></a></span>
<span id="cb12-407"><a href="#cb12-407"></a>$$\begin{aligned}</span>
<span id="cb12-408"><a href="#cb12-408"></a>\mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x), y \sim p(y|x)} (\hat{y} - y)^2 &amp;= \mathbb{E}_{\hat{y}, y}<span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[y|x]) + (\mathbb{E}[y|x] - y)</span><span class="co">]</span>^2 <span class="sc">\\</span></span>
<span id="cb12-409"><a href="#cb12-409"></a>&amp;= \mathbb{E}_{\hat{y}, y} <span class="co">[</span><span class="ot">(y - \mathbb{E}[y|x])^2</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb12-410"><a href="#cb12-410"></a>&amp;+ 2\mathbb{E}_{\hat{y}, y} <span class="co">[</span><span class="ot">(\hat{y} - \mathbb{E}[y|x])(\mathbb{E}[y|x] - y)</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb12-411"><a href="#cb12-411"></a>&amp;+ \mathbb{E}_{\hat{y}, y}(\hat{y} - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2</span>
<span id="cb12-412"><a href="#cb12-412"></a>\end{aligned}$$ {#eq-eq3.17}</span>
<span id="cb12-413"><a href="#cb12-413"></a></span>
<span id="cb12-414"><a href="#cb12-414"></a>In @eq-eq3.17, the first term represents the variance of the true label $y$, the second term evaluates to zero, </span>
<span id="cb12-415"><a href="#cb12-415"></a>and the third term accounts for the variance of the model's prediction $\hat{y}$. </span>
<span id="cb12-416"><a href="#cb12-416"></a>To clarify why the second term is zero, we note that:</span>
<span id="cb12-417"><a href="#cb12-417"></a></span>
<span id="cb12-418"><a href="#cb12-418"></a>$$</span>
<span id="cb12-419"><a href="#cb12-419"></a>\mathbb{E}_{\hat{y}, y}<span class="co">[</span><span class="ot">\mathbb{E}[y|x] - y</span><span class="co">]</span> = 0</span>
<span id="cb12-420"><a href="#cb12-420"></a>$$ {#eq-eq3.18}</span>
<span id="cb12-421"><a href="#cb12-421"></a></span>
<span id="cb12-422"><a href="#cb12-422"></a>This indicates that the expected deviation of the true label from its conditional mean is null, </span>
<span id="cb12-423"><a href="#cb12-423"></a>as $\mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>$ is, by definition, the average of $y$ given $x$. Focusing on the third term, we derive it as follows:</span>
<span id="cb12-424"><a href="#cb12-424"></a></span>
<span id="cb12-425"><a href="#cb12-425"></a>$$\begin{aligned}</span>
<span id="cb12-426"><a href="#cb12-426"></a>\mathbb{E}_{\hat{y}, y}(\hat{y} - \mathbb{E}[y|x])^2 &amp;= \mathbb{E}_{\hat{y}, y}[(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}] + \mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2] <span class="sc">\\</span></span>
<span id="cb12-427"><a href="#cb12-427"></a>&amp;= \mathbb{E}_{\hat{y}, y}[(\hat{y} - \mathbb{E}_{\hat{y}}[\hat{y}])^2] + (\mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span> - \mathbb{E}<span class="co">[</span><span class="ot">y|x</span><span class="co">]</span>)^2</span>
<span id="cb12-428"><a href="#cb12-428"></a>\end{aligned}$$ {#eq-eq3.19}</span>
<span id="cb12-429"><a href="#cb12-429"></a></span>
<span id="cb12-430"><a href="#cb12-430"></a>Here, $\mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span>$ represents the expected model prediction conditioned on the </span>
<span id="cb12-431"><a href="#cb12-431"></a>data $\mathcal{D}$ and input $x$. Combining the results of our analysis, we arrive at the total expected error as:</span>
<span id="cb12-432"><a href="#cb12-432"></a></span>
<span id="cb12-433"><a href="#cb12-433"></a>$$</span>
<span id="cb12-434"><a href="#cb12-434"></a>\mathbb{E}_{y} [(y - \mathbb{E}[y|x])^2] + (\mathbb{E}_{\hat{y}} [\hat{y} - \mathbb{E}[y|x]] )^2 + \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span>)^2]</span>
<span id="cb12-435"><a href="#cb12-435"></a>$$ {#eq-eq3.20}</span>
<span id="cb12-436"><a href="#cb12-436"></a></span>
<span id="cb12-437"><a href="#cb12-437"></a>In this equation, the first term signifies the variance of the true label, which remains constant for </span>
<span id="cb12-438"><a href="#cb12-438"></a>a given $x$. The second term captures the bias of the model, reflecting how much the average model prediction </span>
<span id="cb12-439"><a href="#cb12-439"></a>deviates from the expected true label. The third term quantifies the model's uncertainty concerning the selected input $x$.</span>
<span id="cb12-440"><a href="#cb12-440"></a></span>
<span id="cb12-441"><a href="#cb12-441"></a>Referring to <span class="co">[</span><span class="ot">@AL_variance</span><span class="co">]</span>, we can denote the uncertainty term as:</span>
<span id="cb12-442"><a href="#cb12-442"></a></span>
<span id="cb12-443"><a href="#cb12-443"></a>$$</span>
<span id="cb12-444"><a href="#cb12-444"></a>\sigma^2_{\hat{y}} (x | \mathcal{D}) = \mathbb{E}_{\hat{y}} [(\hat{y} - \mathbb{E}_{\hat{y}}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span>)^2]</span>
<span id="cb12-445"><a href="#cb12-445"></a>$$ {#eq-eq3.21}</span>
<span id="cb12-446"><a href="#cb12-446"></a></span>
<span id="cb12-447"><a href="#cb12-447"></a>This term explicitly represents the variance of the model predictions at the input $x$ given the dataset </span>
<span id="cb12-448"><a href="#cb12-448"></a>$\mathcal{D}$. More explicitly, it can be expressed as:</span>
<span id="cb12-449"><a href="#cb12-449"></a></span>
<span id="cb12-450"><a href="#cb12-450"></a>$$</span>
<span id="cb12-451"><a href="#cb12-451"></a>\sigma^2_{\hat{y}} (x | \mathcal{D}) =  \mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x)} [(\hat{y} - \mathbb{E}_{\hat{y} \sim p(\hat{y} | \mathcal{D}; x)}<span class="co">[</span><span class="ot">\hat{y}</span><span class="co">]</span>)^2]</span>
<span id="cb12-452"><a href="#cb12-452"></a>$$ {#eq-eq3.22}</span>
<span id="cb12-453"><a href="#cb12-453"></a></span>
<span id="cb12-454"><a href="#cb12-454"></a>This formulation emphasizes the variability of the model's predictions around their mean, providing </span>
<span id="cb12-455"><a href="#cb12-455"></a>insights into the model's reliability in its estimations. The active learning by variance reduction algorithm </span>
<span id="cb12-456"><a href="#cb12-456"></a>can be summarized as follows:</span>
<span id="cb12-457"><a href="#cb12-457"></a></span>
<span id="cb12-458"><a href="#cb12-458"></a><span class="ss">1. </span>**Sampling Candidates**: Sample candidate points $\tilde{x}_1, \dots, \tilde{x}_m$ from $p(x)$.</span>
<span id="cb12-459"><a href="#cb12-459"></a><span class="ss">2. </span>**Compute Expected Variance Reduction**: For each candidate $\tilde{x}_i$, compute:</span>
<span id="cb12-460"><a href="#cb12-460"></a>$$</span>
<span id="cb12-461"><a href="#cb12-461"></a>   \mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]</span>
<span id="cb12-462"><a href="#cb12-462"></a>$$ {#eq-eq3.23}</span>
<span id="cb12-463"><a href="#cb12-463"></a><span class="ss">3.  </span>**Select the Best Candidate**: Choose the point that minimizes expected variance reduction:</span>
<span id="cb12-464"><a href="#cb12-464"></a>$$</span>
<span id="cb12-465"><a href="#cb12-465"></a>   \tilde{x}^* = \arg\min_{\tilde{x}_i} \mathbb{E}_{p(x)} [\sigma^2_{\hat{y}} (x | \tilde{\mathcal{D}})]</span>
<span id="cb12-466"><a href="#cb12-466"></a>$$ {#eq-eq3.24}</span>
<span id="cb12-467"><a href="#cb12-467"></a><span class="ss">4. </span>**Update Model**: Incorporate the newly labeled data and repeat the process.</span>
<span id="cb12-468"><a href="#cb12-468"></a></span>
<span id="cb12-469"><a href="#cb12-469"></a>While there is no general recipe for the number of iterations to</span>
<span id="cb12-470"><a href="#cb12-470"></a>perform, one could imagine relying on some empirical measure like a loss</span>
<span id="cb12-471"><a href="#cb12-471"></a>on left-out labelled data to gauge model improvement (as seen in <span class="co">[</span><span class="ot">@fig-empirical:gauss; @fig-empirical:regress</span><span class="co">]</span>). </span>
<span id="cb12-472"><a href="#cb12-472"></a>Intuitively, the size of the data</span>
<span id="cb12-473"><a href="#cb12-473"></a>set and its relationship to the loss is intimately tied to the model</span>
<span id="cb12-474"><a href="#cb12-474"></a>complexity which impacts its data-thirstiness.</span>
<span id="cb12-475"><a href="#cb12-475"></a></span>
<span id="cb12-476"><a href="#cb12-476"></a>We note to the reader that $P(X=x)$ is a distribution with</span>
<span id="cb12-477"><a href="#cb12-477"></a>potentially-infinite support and the authors do not compute this</span>
<span id="cb12-478"><a href="#cb12-478"></a>integral exactly. Instead, the computational estimate of that integral</span>
<span id="cb12-479"><a href="#cb12-479"></a>consists of sampling several points $x \sim P(X=x)$ and averaging the</span>
<span id="cb12-480"><a href="#cb12-480"></a>quantity inside the integral over these points until convergence using</span>
<span id="cb12-481"><a href="#cb12-481"></a>Monte-Carlo sampling approaches (see <span class="co">[</span><span class="ot">@monte-carlo</span><span class="co">]</span>).</span>
<span id="cb12-482"><a href="#cb12-482"></a></span>
<span id="cb12-483"><a href="#cb12-483"></a>![Two models were empirically explored. These two models lead to closed-form,</span>
<span id="cb12-484"><a href="#cb12-484"></a>accurately and efficiently-computed expected learner variance which can</span>
<span id="cb12-485"><a href="#cb12-485"></a>be plugged into the algorithm.](Figures/1_two_models.png){#fig-two_models</span>
<span id="cb12-486"><a href="#cb12-486"></a>width="100%"}</span>
<span id="cb12-487"><a href="#cb12-487"></a></span>
<span id="cb12-488"><a href="#cb12-488"></a>Arm2D (@fig-arm2D) is a kinematics problem where learner has to</span>
<span id="cb12-489"><a href="#cb12-489"></a>predict the tip position of a robotic arm given a set of joint angles</span>
<span id="cb12-490"><a href="#cb12-490"></a>$\mathbf{\theta_1}, \mathbf{\theta_2}$. In this analysis, the two models</span>
<span id="cb12-491"><a href="#cb12-491"></a>seen in @fig-two_models, namely the Gaussian mixture model and</span>
<span id="cb12-492"><a href="#cb12-492"></a>locally-weighted regression (LOESS).</span>
<span id="cb12-493"><a href="#cb12-493"></a></span>
<span id="cb12-494"><a href="#cb12-494"></a>![The arm kinematics problem. The learner attempts to predict tip</span>
<span id="cb12-495"><a href="#cb12-495"></a>position given a set of joint angles</span>
<span id="cb12-496"><a href="#cb12-496"></a>$\mathbf{\theta_1}, \mathbf{\theta_2}$](Figures/1_experiment_setup.png){#fig-arm2D</span>
<span id="cb12-497"><a href="#cb12-497"></a>width="40%"}</span>
<span id="cb12-498"><a href="#cb12-498"></a></span>
<span id="cb12-499"><a href="#cb12-499"></a>![Arm2D domain. Dotted lines denote standard error for average of</span>
<span id="cb12-500"><a href="#cb12-500"></a>10 runs, each started with one initial random</span>
<span id="cb12-501"><a href="#cb12-501"></a>example.](Figures/1_experiment_results_gaussian.png){#fig-empirical:gauss</span>
<span id="cb12-502"><a href="#cb12-502"></a>width="100%"}</span>
<span id="cb12-503"><a href="#cb12-503"></a></span>
<span id="cb12-504"><a href="#cb12-504"></a>The results shown in <span class="co">[</span><span class="ot">@fig-empirical:gauss; @fig-empirical:regress</span><span class="co">]</span> are intriguing. </span>
<span id="cb12-505"><a href="#cb12-505"></a>As expected, the variance of the learner decreases because the authors selected points to minimize </span>
<span id="cb12-506"><a href="#cb12-506"></a>expected variance. Additionally, we observe a related decrease in the mean square error (MSE) of both </span>
<span id="cb12-507"><a href="#cb12-507"></a>models as the dataset size increases. This is a notable outcome because the expected learner variance </span>
<span id="cb12-508"><a href="#cb12-508"></a>for these models can be computed accurately and efficiently relative to a new point. When integrated </span>
<span id="cb12-509"><a href="#cb12-509"></a>into the general active learning loop (<span class="co">[</span><span class="ot">@fig-schema</span><span class="co">]</span>), this significantly enhances model performance.</span>
<span id="cb12-510"><a href="#cb12-510"></a></span>
<span id="cb12-511"><a href="#cb12-511"></a>In the case of the locally-weighted regression model (<span class="co">[</span><span class="ot">@fig-empirical:regress</span><span class="co">]</span>), it is surprising </span>
<span id="cb12-512"><a href="#cb12-512"></a>that if points were chosen randomly, the MSE would be highly unstable, with sharp fluctuations. </span>
<span id="cb12-513"><a href="#cb12-513"></a>However, when active learning by variance reduction is applied, using expected learner variance </span>
<span id="cb12-514"><a href="#cb12-514"></a>as a proxy, the MSE decreases almost smoothly, aside from some initial instabilities.</span>
<span id="cb12-515"><a href="#cb12-515"></a></span>
<span id="cb12-516"><a href="#cb12-516"></a>![Variance and MSE learning curves for LOESS model trained on the Arm2D</span>
<span id="cb12-517"><a href="#cb12-517"></a>domain. Dotted lines denote standard error for average of 60 runs, each</span>
<span id="cb12-518"><a href="#cb12-518"></a>started with a single initial random</span>
<span id="cb12-519"><a href="#cb12-519"></a>example.](Figures/1_experiment_results_regression.png){#fig-empirical:regress</span>
<span id="cb12-520"><a href="#cb12-520"></a>width="100%"}</span>
<span id="cb12-521"><a href="#cb12-521"></a></span>
<span id="cb12-522"><a href="#cb12-522"></a><span class="fu">### Active Learning in Ranking and Comparison</span></span>
<span id="cb12-523"><a href="#cb12-523"></a></span>
<span id="cb12-524"><a href="#cb12-524"></a>Many researchers have shown that making comparisons is easier and more convenient </span>
<span id="cb12-525"><a href="#cb12-525"></a>for users than assigning a specific score to each item. Individual comparisons yield a </span>
<span id="cb12-526"><a href="#cb12-526"></a>complete ranking over a set of $n$ objects $\Theta = (\theta_1, \theta_2, \cdots, \theta_n)$. </span>
<span id="cb12-527"><a href="#cb12-527"></a>This ranking is defined as a mapping $\sigma : <span class="sc">\{</span>1, \cdots, n<span class="sc">\}</span> \rightarrow <span class="sc">\{</span>1,\cdots, n<span class="sc">\}</span>$ </span>
<span id="cb12-528"><a href="#cb12-528"></a>that orders the set of objects $\Theta$. Specifically, for a single $\sigma$, </span>
<span id="cb12-529"><a href="#cb12-529"></a>$\sigma(\Theta) = \theta_{\sigma(1)} &lt; \theta_{\sigma(2)} &lt; \cdots &lt; \theta_{\sigma(n-1)} &lt; \theta_{\sigma(n)}$, </span>
<span id="cb12-530"><a href="#cb12-530"></a>where $\theta_{i} &lt; \theta_{j}$ means that $\theta_{i}$ is rated lower than $\theta_{j}$.</span>
<span id="cb12-531"><a href="#cb12-531"></a></span>
<span id="cb12-532"><a href="#cb12-532"></a>For any $n$ elements to be ranked, there are $n!$ possible orderings that can result </span>
<span id="cb12-533"><a href="#cb12-533"></a>in the correct complete ranking. Given that a lower bound on sorting is $n\log n$, </span>
<span id="cb12-534"><a href="#cb12-534"></a>obtaining a guaranteed true rating over $n$ objects requires $n\log n$ pairwise </span>
<span id="cb12-535"><a href="#cb12-535"></a>comparisons if those comparisons are chosen at random. This number can be quite high </span>
<span id="cb12-536"><a href="#cb12-536"></a>and costly in many applications, especially since most ranking information comes from </span>
<span id="cb12-537"><a href="#cb12-537"></a>humans. The more comparisons they have to make, the more money and time is spent. This </span>
<span id="cb12-538"><a href="#cb12-538"></a>process can also be inefficient, as some comparisons provide more value to the learning </span>
<span id="cb12-539"><a href="#cb12-539"></a>process than others, making some comparisons a waste. This inefficiency can be detrimental </span>
<span id="cb12-540"><a href="#cb12-540"></a>in fields like psychology and market research, where comparisons are heavily utilized, and </span>
<span id="cb12-541"><a href="#cb12-541"></a>a faster process could offer significant benefits.</span>
<span id="cb12-542"><a href="#cb12-542"></a></span>
<span id="cb12-543"><a href="#cb12-543"></a>The reason the lower bound on the number of comparisons is $n\log n$ is that it assumes no </span>
<span id="cb12-544"><a href="#cb12-544"></a>prior information about the underlying space and field, so comparisons are chosen at random. </span>
<span id="cb12-545"><a href="#cb12-545"></a>However, leveraging the structures within the comparison space can provide more information </span>
<span id="cb12-546"><a href="#cb12-546"></a>about which comparisons are most valuable. For example, <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> discusses how eye doctors </span>
<span id="cb12-547"><a href="#cb12-547"></a>have a wide range of options when assigning prescriptions for glasses, yet patients do not see </span>
<span id="cb12-548"><a href="#cb12-548"></a>them making many comparisons before deciding on the best option. This is because eye doctors </span>
<span id="cb12-549"><a href="#cb12-549"></a>incorporate domain knowledge into the process and only ask clients for comparisons when necessary. </span>
<span id="cb12-550"><a href="#cb12-550"></a>Applying similar knowledge in the ranking field leads to an active learning approach that </span>
<span id="cb12-551"><a href="#cb12-551"></a>selects data based on the relevance of a comparison query toward finding the final $\sigma(\Theta)$.</span>
<span id="cb12-552"><a href="#cb12-552"></a></span>
<span id="cb12-553"><a href="#cb12-553"></a><span class="fu">#### Geometric Approach to Comparisons {.unnumbered}</span></span>
<span id="cb12-554"><a href="#cb12-554"></a></span>
<span id="cb12-555"><a href="#cb12-555"></a>In this section, we will review the paper <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span>, which explores active learning within data that can be embedded in a multi-dimensional space. In this context, comparisons between two different objects divide the space into halves, with one object being superior in each half. By leveraging such spatial information, the paper develops a geometric approach to ranking and active learning. This spatial information serves as the domain knowledge that informs which comparisons to perform to achieve the ranking.</span>
<span id="cb12-556"><a href="#cb12-556"></a></span>
<span id="cb12-557"><a href="#cb12-557"></a>For this application, the following terms are defined:</span>
<span id="cb12-558"><a href="#cb12-558"></a></span>
<span id="cb12-559"><a href="#cb12-559"></a><span class="ss">1. </span>**$R^d$**: The space in which objects can be embedded.</span>
<span id="cb12-560"><a href="#cb12-560"></a></span>
<span id="cb12-561"><a href="#cb12-561"></a><span class="ss">2. </span>**$\theta_1, \cdots,\theta_n$**: The objects, now representing their locations in $R^d$.</span>
<span id="cb12-562"><a href="#cb12-562"></a></span>
<span id="cb12-563"><a href="#cb12-563"></a><span class="ss">3. </span>For each ranking $\sigma$, there is a reference point $r_{\sigma} \in R^d$, such that if, </span>
<span id="cb12-564"><a href="#cb12-564"></a>according to ranking $\sigma$, $\theta_{i} &lt; \theta_{j}$ (object $i$ is worse than $j$),</span>
<span id="cb12-565"><a href="#cb12-565"></a> then $||\theta_i - r_{\sigma}|| &lt; ||\theta_j - r_{\sigma}||$. In other words, object $i$ is </span>
<span id="cb12-566"><a href="#cb12-566"></a> closer to the reference point $r_{\sigma}$ of the ranking than object $j$.</span>
<span id="cb12-567"><a href="#cb12-567"></a></span>
<span id="cb12-568"><a href="#cb12-568"></a><span class="ss">4. </span>**$\Sigma_{n,d}$**: The set of all possible rankings of the $n$ objects that satisfy the</span>
<span id="cb12-569"><a href="#cb12-569"></a> embedding distances in the space $R^d$ as defined above. Note that not all possible rankings </span>
<span id="cb12-570"><a href="#cb12-570"></a> will satisfy the embedding conditions, but multiple rankings might satisfy all those conditions.</span>
<span id="cb12-571"><a href="#cb12-571"></a></span>
<span id="cb12-572"><a href="#cb12-572"></a><span class="ss">5. </span>For every ranking $\sigma$, there is $M_n(\sigma)$, the number of pairwise comparisons </span>
<span id="cb12-573"><a href="#cb12-573"></a>needed to identify the ranking. When comparisons are done at random, $E<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = n\log n$. </span>
<span id="cb12-574"><a href="#cb12-574"></a>The paper <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> examines this quantity to demonstrate that it can be reduced by incorporating </span>
<span id="cb12-575"><a href="#cb12-575"></a>spatial knowledge.</span>
<span id="cb12-576"><a href="#cb12-576"></a></span>
<span id="cb12-577"><a href="#cb12-577"></a><span class="ss">6. </span>**$q_{i,j}$**: The query of comparison between objects $i$ and $j$.</span>
<span id="cb12-578"><a href="#cb12-578"></a></span>
<span id="cb12-579"><a href="#cb12-579"></a><span class="fu">#### Embedding Space {.unnumbered}</span></span>
<span id="cb12-580"><a href="#cb12-580"></a></span>
<span id="cb12-581"><a href="#cb12-581"></a>![Objects $\theta_1, \theta_2, \theta_3$ and queries in $R^2$. The</span>
<span id="cb12-582"><a href="#cb12-582"></a>$r_\theta$ lies in the shaded region which represents</span>
<span id="cb12-583"><a href="#cb12-583"></a>$\Sigma_{n,2}$(consistent with the labels of</span>
<span id="cb12-584"><a href="#cb12-584"></a>$q_{1,2}, q_{1,3}, q_{2,3}$). The dotted (dashed) lines represent new</span>
<span id="cb12-585"><a href="#cb12-585"></a>queries whose labels are (are not)</span>
<span id="cb12-586"><a href="#cb12-586"></a>ambiguous.](Figures/SPACE.png){#fig-dim-space width="40%"}</span>
<span id="cb12-587"><a href="#cb12-587"></a></span>
<span id="cb12-588"><a href="#cb12-588"></a>To properly understand how to select the most </span>
<span id="cb12-589"><a href="#cb12-589"></a>valuable queries, it is essential to examine the space where the objects exist and how </span>
<span id="cb12-590"><a href="#cb12-590"></a>the queries divide that space to determine the proper rankings. For this example, </span>
<span id="cb12-591"><a href="#cb12-591"></a>in @fig-dim-space, the paper <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> operates in $R^2$ space with three objects: </span>
<span id="cb12-592"><a href="#cb12-592"></a>$\theta_1$, $\theta_2$, and $\theta_3$. There are pairwise queries $q_{1,3}$, $q_{2,3}$, </span>
<span id="cb12-593"><a href="#cb12-593"></a>and $q_{1,2}$ between them, denoted by solid lines equidistant from the two objects they compare. </span>
<span id="cb12-594"><a href="#cb12-594"></a>These lines split the $R^2$ space into halves, with each half closer to one of the two objects. </span>
<span id="cb12-595"><a href="#cb12-595"></a>The paper colors the side of the worse object for each query in dark grey and takes the intersection </span>
<span id="cb12-596"><a href="#cb12-596"></a>of these halves, resulting in the dark grey region in the image. This region indicates </span>
<span id="cb12-597"><a href="#cb12-597"></a>$\Sigma_{n,2}$ since all points follow the embedding conditions. Specifically, for every point </span>
<span id="cb12-598"><a href="#cb12-598"></a>$r$ in the dark grey area, $||\theta_3 - r|| &lt; ||\theta_2 - r|| &lt; ||\theta_1 - r||$, </span>
<span id="cb12-599"><a href="#cb12-599"></a>meaning $\theta_3 &lt; \theta_2 &lt; \theta_1$. Thus, every point $r$ is one of the $r_\sigma$ representing </span>
<span id="cb12-600"><a href="#cb12-600"></a>their respective rankings $\sigma \in \Sigma_{n,2}$. In other words, the paper aims to have the </span>
<span id="cb12-601"><a href="#cb12-601"></a>reference points and dark grey region closest to the worst object and furthest from the best object.</span>
<span id="cb12-602"><a href="#cb12-602"></a></span>
<span id="cb12-603"><a href="#cb12-603"></a>The authors also denote the label for each query $q_{i,j}$, such as label $y_{i,j} = 1<span class="sc">\{</span>q_{i,j}<span class="sc">\}</span>$ </span>
<span id="cb12-604"><a href="#cb12-604"></a>(for example, $y_{1,2} = 0, y_{3,2} = 1$). This allows for deciding how to label new queries represented </span>
<span id="cb12-605"><a href="#cb12-605"></a>by dashed and dotted lines, depending on which objects each query compares. Focusing on the dotted line, </span>
<span id="cb12-606"><a href="#cb12-606"></a>called $q_{i,4}$, where $i={1,2,3}$, and considering potential locations of $\theta_4$, the line must be </span>
<span id="cb12-607"><a href="#cb12-607"></a>equidistant from one of the three objects in the picture and $\theta_4$, meaning $\theta_4$ can be placed </span>
<span id="cb12-608"><a href="#cb12-608"></a>in three different locations. If the query performed is $q_{2,4}$, then $\theta_4$ will be closer to the </span>
<span id="cb12-609"><a href="#cb12-609"></a>dark grey area than $\theta_2$, thus $y_{2,4} = 0$. However, if $q_{1,4}$ or $q_{3,4}$ are performed, </span>
<span id="cb12-610"><a href="#cb12-610"></a>$\theta_4$ will be further from the dark grey area than $\theta_1$ or $\theta_3$, meaning $y_{1,4} = y_{3,4} = 1$. </span>
<span id="cb12-611"><a href="#cb12-611"></a>In this case, the labels are contradictory and depend on which object they are compared with, making such </span>
<span id="cb12-612"><a href="#cb12-612"></a>a query $q_{i,4}$ ambiguous.</span>
<span id="cb12-613"><a href="#cb12-613"></a></span>
<span id="cb12-614"><a href="#cb12-614"></a>In contrast, the authors analyze the dashed line, called $q_{i,5}$, where $i={1,2,3}$, and consider </span>
<span id="cb12-615"><a href="#cb12-615"></a>potential locations of $\theta_5$. Since the line must be equidistant from one of the three objects in the </span>
<span id="cb12-616"><a href="#cb12-616"></a>picture and $\theta_5$, it can be placed in three different locations. If one of the three potential </span>
<span id="cb12-617"><a href="#cb12-617"></a>queries is performed, $\theta_5$ will be closer to the dark grey area than $\theta_1$, $\theta_2$, and </span>
<span id="cb12-618"><a href="#cb12-618"></a>$\theta_3$, meaning $y_{1,5} = y_{2,5} = y_{3,5} = 0$. In this case, all labels are the same regardless </span>
<span id="cb12-619"><a href="#cb12-619"></a>of which object is used, meaning such a query will not be contradictory, as all agree on the label.</span>
<span id="cb12-620"><a href="#cb12-620"></a></span>
<span id="cb12-621"><a href="#cb12-621"></a>The goal is to perform as many ambiguous queries as possible and skip non-ambiguous queries to decrease </span>
<span id="cb12-622"><a href="#cb12-622"></a>the total $M_n(\sigma)$. Intuitively, if there is contradictory information about a query, it needs to be </span>
<span id="cb12-623"><a href="#cb12-623"></a>erformed so that a human can clarify its direction. Conversely, if all sources of information from the domain </span>
<span id="cb12-624"><a href="#cb12-624"></a>space agree on the query's label, that information can be used without asking a human, incorporating the </span>
<span id="cb12-625"><a href="#cb12-625"></a>knowledge of the embedding distances.</span>
<span id="cb12-626"><a href="#cb12-626"></a></span>
<span id="cb12-627"><a href="#cb12-627"></a>Lastly, to consider the general case of the $R^d$ space, rather than discussing halves of the image, it </span>
<span id="cb12-628"><a href="#cb12-628"></a>is essential to discuss half-spaces. Similarly, consider the half-space that assigns a label of $1$ to </span>
<span id="cb12-629"><a href="#cb12-629"></a>the query and the half-space assigning a label of $0$. If both half-spaces exist, they have conflicting </span>
<span id="cb12-630"><a href="#cb12-630"></a>information on the query, making the query ambiguous. However, if one of the half-spaces does not exist, </span>
<span id="cb12-631"><a href="#cb12-631"></a>it means the other is the full space, representing consistency in the label assignment and a non-ambiguous query.</span>
<span id="cb12-632"><a href="#cb12-632"></a></span>
<span id="cb12-633"><a href="#cb12-633"></a><span class="fu">##### Algorithms for Ambiguous Query Selection {.unnumbered}</span></span>
<span id="cb12-634"><a href="#cb12-634"></a></span>
<span id="cb12-635"><a href="#cb12-635"></a><span class="in">```pseudocode</span></span>
<span id="cb12-636"><a href="#cb12-636"></a><span class="in">#| label: alg-qsa</span></span>
<span id="cb12-637"><a href="#cb12-637"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb12-638"><a href="#cb12-638"></a><span class="in">    \caption{Query Selection Algorithm}</span></span>
<span id="cb12-639"><a href="#cb12-639"></a><span class="in">    \begin{algorithmic}</span></span>
<span id="cb12-640"><a href="#cb12-640"></a><span class="in">        \State \textbf{input:} $n$ objects in $\mathbb{R}^d$</span></span>
<span id="cb12-641"><a href="#cb12-641"></a><span class="in">        \State \textbf{initialize:} objects $\theta_1, \dots, \theta_n$ in uniformly random order</span></span>
<span id="cb12-642"><a href="#cb12-642"></a><span class="in">        \For{$j=2, \dots, n$}</span></span>
<span id="cb12-643"><a href="#cb12-643"></a><span class="in">            \For{$i=1, \dots, j-1$}</span></span>
<span id="cb12-644"><a href="#cb12-644"></a><span class="in">                \If{$q_{i,j}$ is ambiguous}</span></span>
<span id="cb12-645"><a href="#cb12-645"></a><span class="in">                    \State request $q_{i,j}$'s label from reference</span></span>
<span id="cb12-646"><a href="#cb12-646"></a><span class="in">                \Else</span></span>
<span id="cb12-647"><a href="#cb12-647"></a><span class="in">                    \State impute $q_{i,j}$'s label from previously labeled queries</span></span>
<span id="cb12-648"><a href="#cb12-648"></a><span class="in">                \EndIf</span></span>
<span id="cb12-649"><a href="#cb12-649"></a><span class="in">            \EndFor</span></span>
<span id="cb12-650"><a href="#cb12-650"></a><span class="in">        \EndFor</span></span>
<span id="cb12-651"><a href="#cb12-651"></a><span class="in">        \State \textbf{output:} ranking of $n$ objects</span></span>
<span id="cb12-652"><a href="#cb12-652"></a><span class="in">    \end{algorithmic}</span></span>
<span id="cb12-653"><a href="#cb12-653"></a><span class="in">\end{algorithm}</span></span>
<span id="cb12-654"><a href="#cb12-654"></a><span class="in">```</span></span>
<span id="cb12-655"><a href="#cb12-655"></a></span>
<span id="cb12-656"><a href="#cb12-656"></a>The standard algorithm in @alg-qsa requests labels for $q_{i,j}$ if those queries </span>
<span id="cb12-657"><a href="#cb12-657"></a>are ambiguous; otherwise, it infers the information from prior comparisons and their labels.</span>
<span id="cb12-658"><a href="#cb12-658"></a></span>
<span id="cb12-659"><a href="#cb12-659"></a>It is important to demonstrate that the number of comparisons decreases. Specifically, </span>
<span id="cb12-660"><a href="#cb12-660"></a><span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> shows that this algorithm has $E<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = O(d\log n)$, where $d$ is the </span>
<span id="cb12-661"><a href="#cb12-661"></a>dimension of the space and $d &lt; n$, which improves on the $O(n\log n)$ baseline. The proof </span>
<span id="cb12-662"><a href="#cb12-662"></a>can be studied in detail in the paper itself, but at a high level, it starts by reasoning </span>
<span id="cb12-663"><a href="#cb12-663"></a>about the probability of a query being ambiguous and a comparison being requested from a </span>
<span id="cb12-664"><a href="#cb12-664"></a>human, thus representing $M_n = \Sigma_{k=1}^{n-1}\Sigma_{i=1}^k 1<span class="sc">\{</span>Requestq_{i,k+1}<span class="sc">\}</span>$. </span>
<span id="cb12-665"><a href="#cb12-665"></a>For that, the authors define $Q(i,j)$, which represents the number of different rankings </span>
<span id="cb12-666"><a href="#cb12-666"></a>that exist for $i$ elements in $j$-dimensional space (e.g., $Q(1,d) = 1, Q(n,0) = 1, Q(n,1) = n!$). </span>
<span id="cb12-667"><a href="#cb12-667"></a>In that case, $|\Sigma_{n,d}| = Q(n,d)$. Further, using recurrence relations for $Q(i,j)$, the </span>
<span id="cb12-668"><a href="#cb12-668"></a>authors derive that $|\Sigma_{n,d}| = Q(n,d) = O(n^{2d})$, which is omitted here. Analogously, </span>
<span id="cb12-669"><a href="#cb12-669"></a>the authors define $P(i,j)$, which represents the number of rankings in $\Sigma_{n,d}$ that will </span>
<span id="cb12-670"><a href="#cb12-670"></a>still be possible with the addition of a new element $i+1$ to the ranking objects. Referring back </span>
<span id="cb12-671"><a href="#cb12-671"></a>to @fig-dim-space, $P(i,j)$ estimates how much of the dark grey area will still exist after making </span>
<span id="cb12-672"><a href="#cb12-672"></a>a query for $i+1$. As indicated there, the dotted line ambiguous query did not change the dark grey a</span>
<span id="cb12-673"><a href="#cb12-673"></a>rea at all ($P(n,d) = Q(n,d)$), whereas the dashed non-ambiguous query would cut a piece from it </span>
<span id="cb12-674"><a href="#cb12-674"></a>($P(n,d) &lt; Q(n,d)$). Thus, $Request q_{i,k+1} = P(k,d) / Q(k,d)$, so a higher value indicates </span>
<span id="cb12-675"><a href="#cb12-675"></a>more possible rankings and an ambiguous query that needs to be requested to obtain more useful </span>
<span id="cb12-676"><a href="#cb12-676"></a>information. With this in mind, the authors derive that $E<span class="co">[</span><span class="ot">M_n(\sigma)</span><span class="co">]</span> = O(d\log n)$, showing </span>
<span id="cb12-677"><a href="#cb12-677"></a>that fewer queries are needed for effective ranking.</span>
<span id="cb12-678"><a href="#cb12-678"></a></span>
<span id="cb12-679"><a href="#cb12-679"></a>The issue with this algorithm is that only one human provides the answers to the requested queries, </span>
<span id="cb12-680"><a href="#cb12-680"></a>which means it does not account for their biases. An alternative approach is a Robust Query Selection </span>
<span id="cb12-681"><a href="#cb12-681"></a>Algorithm (RQSA) <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span>, which uses majority voting for every query to indicate the ground </span>
<span id="cb12-682"><a href="#cb12-682"></a>truth of the query's label. However, the authors consider that a group of people can still give </span>
<span id="cb12-683"><a href="#cb12-683"></a>incorrect or divided responses. If the votes for each answer are almost equal in number, the authors </span>
<span id="cb12-684"><a href="#cb12-684"></a>push that query to the end of the algorithm to see if it can become a non-ambiguous query with more </span>
<span id="cb12-685"><a href="#cb12-685"></a>information learned. If it does not, an odd number of voters is used to determine the final ranking.</span>
<span id="cb12-686"><a href="#cb12-686"></a></span>
<span id="cb12-687"><a href="#cb12-687"></a><span class="fu">##### Performance Analysis {#sec-QSA .unnumbered}</span></span>
<span id="cb12-688"><a href="#cb12-688"></a></span>
<span id="cb12-689"><a href="#cb12-689"></a>![Mean and standard deviation of requested queries (solid) in the</span>
<span id="cb12-690"><a href="#cb12-690"></a>noiseless case for $n = 100$; $\log_2|\Sigma_{n,d}|$ is a lower bound</span>
<span id="cb12-691"><a href="#cb12-691"></a>(dashed).](Figures/Dim:query_graph.png){#fig-rand_n width="60%"}</span>
<span id="cb12-692"><a href="#cb12-692"></a></span>
<span id="cb12-693"><a href="#cb12-693"></a>::: {#tbl-geo_acc}</span>
<span id="cb12-694"><a href="#cb12-694"></a>  Dimension                           2      3</span>
<span id="cb12-695"><a href="#cb12-695"></a>  --------------- ----------------- ------ ------</span>
<span id="cb12-696"><a href="#cb12-696"></a>  \% of queries         mean         14.5   18.5</span>
<span id="cb12-697"><a href="#cb12-697"></a>                        std          5.3     6</span>
<span id="cb12-698"><a href="#cb12-698"></a>  Average error    $d(\bar{y}, y)$   0.23   0.21</span>
<span id="cb12-699"><a href="#cb12-699"></a>                   $d(\bar{y}, y)$   0.31   0.29</span>
<span id="cb12-700"><a href="#cb12-700"></a></span>
<span id="cb12-701"><a href="#cb12-701"></a>  : Statistics for the Robust Query Selection Algorithm (RQSA)</span>
<span id="cb12-702"><a href="#cb12-702"></a>  <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> discussed at the end of @sec-QSA and the baseline of conducting all</span>
<span id="cb12-703"><a href="#cb12-703"></a>  comparisons. $y$ serves as a noisy ground truth, $\tilde{y}$ is the</span>
<span id="cb12-704"><a href="#cb12-704"></a>  result of all comparisons, and $\hat{y}$ is the output of the RQSA.</span>
<span id="cb12-705"><a href="#cb12-705"></a>:::</span>
<span id="cb12-706"><a href="#cb12-706"></a></span>
<span id="cb12-707"><a href="#cb12-707"></a>@fig-rand_n shows that the number of comparisons fits within the expected bounds, as</span>
<span id="cb12-708"><a href="#cb12-708"></a>$\log|\Sigma_{n,d}| = \log(n^d) = d\log n$. To derive that graph,</span>
<span id="cb12-709"><a href="#cb12-709"></a>authors <span class="co">[</span><span class="ot">@geo_paper</span><span class="co">]</span> sampled 100 random data points in a $R^d$ space,</span>
<span id="cb12-710"><a href="#cb12-710"></a>where $d$ took on 10 different values as indicated on the graph. Each</span>
<span id="cb12-711"><a href="#cb12-711"></a>dimension's experiments were repeated 25 times for consistency.</span>
<span id="cb12-712"><a href="#cb12-712"></a></span>
<span id="cb12-713"><a href="#cb12-713"></a>With regard to the accuracy and performance of the method, the authors</span>
<span id="cb12-714"><a href="#cb12-714"></a>did a ranking experiment on 100 different audio signals, results of</span>
<span id="cb12-715"><a href="#cb12-715"></a>which can be seen in @tbl-geo_acc. The ground truth labels came from humans,</span>
<span id="cb12-716"><a href="#cb12-716"></a>indicated by $y$ in the table. That resulted in the existence of noise</span>
<span id="cb12-717"><a href="#cb12-717"></a>and potential errors in the ground truth, which could influence the</span>
<span id="cb12-718"><a href="#cb12-718"></a>performance of both the baseline algorithm that does all comparisons</span>
<span id="cb12-719"><a href="#cb12-719"></a>($\tilde{y}$) and the Robust Query Selection Algorithm (RQSA) proposed</span>
<span id="cb12-720"><a href="#cb12-720"></a>in @sec-QSA ($\hat{y}$). As can be seen in both 2 and</span>
<span id="cb12-721"><a href="#cb12-721"></a>3-dimensional spaces RQSA performed worse by $8\%$ compared to the</span>
<span id="cb12-722"><a href="#cb12-722"></a>baseline, which indicates that active learning that uses the domain</span>
<span id="cb12-723"><a href="#cb12-723"></a>information can still be erroneous due to the inference of certain</span>
<span id="cb12-724"><a href="#cb12-724"></a>comparisons that sometimes may not be entirely correct. However, as can</span>
<span id="cb12-725"><a href="#cb12-725"></a>be seen by the upper part of @tbl-geo_acc,</span>
<span id="cb12-726"><a href="#cb12-726"></a>significantly less queries were requested compared to the baseline,</span>
<span id="cb12-727"><a href="#cb12-727"></a>which means that the approach can have a significant benefit at a cost</span>
<span id="cb12-728"><a href="#cb12-728"></a>of slight loss in accuracy.</span>
<span id="cb12-729"><a href="#cb12-729"></a></span>
<span id="cb12-730"><a href="#cb12-730"></a><span class="fu">#### User Information as Domain Knowledge for Active Learning {#sec-geo_app .unnumbered}</span></span>
<span id="cb12-731"><a href="#cb12-731"></a></span>
<span id="cb12-732"><a href="#cb12-732"></a>An alternative source of domain knowledge could be users themselves, who</span>
<span id="cb12-733"><a href="#cb12-733"></a>can indicate their uncertainty when it comes to comparing two objects.</span>
<span id="cb12-734"><a href="#cb12-734"></a>Prior studies have shown <span class="co">[</span><span class="ot">@unnoisy_humans</span><span class="co">]</span> that when presented with only</span>
<span id="cb12-735"><a href="#cb12-735"></a>two options when selecting which object is better, but not being able to</span>
<span id="cb12-736"><a href="#cb12-736"></a>properly decide, users would get frustrated and tend to respond more</span>
<span id="cb12-737"><a href="#cb12-737"></a>faultyly, creating noise and incorrect responses in the data. Through</span>
<span id="cb12-738"><a href="#cb12-738"></a>feedback and other studies <span class="co">[</span><span class="ot">@noisy_humans</span><span class="co">]</span> it was determined that</span>
<span id="cb12-739"><a href="#cb12-739"></a>presenting users with an option of indifference between the two objects</span>
<span id="cb12-740"><a href="#cb12-740"></a>can remove those problems. Moreover, in connection to active learning,</span>
<span id="cb12-741"><a href="#cb12-741"></a>the authors show that such an option helps to select more informative</span>
<span id="cb12-742"><a href="#cb12-742"></a>queries since it provides more domain knowledge that can be used,</span>
<span id="cb12-743"><a href="#cb12-743"></a>resulting in a decrease in the number of queries required.</span>
<span id="cb12-744"><a href="#cb12-744"></a></span>
<span id="cb12-745"><a href="#cb12-745"></a>For this problem, the following terms are defined:</span>
<span id="cb12-746"><a href="#cb12-746"></a></span>
<span id="cb12-747"><a href="#cb12-747"></a><span class="ss">1.  </span>$c$ - a cost function that represents user preferences, and the</span>
<span id="cb12-748"><a href="#cb12-748"></a>    result the model has to determine at the end of training. The</span>
<span id="cb12-749"><a href="#cb12-749"></a>    preferred items will have lower costs, and less preferred ones will</span>
<span id="cb12-750"><a href="#cb12-750"></a>    have higher costs. The goal is to determine this function with the</span>
<span id="cb12-751"><a href="#cb12-751"></a>    fewest possible number of queries using active learning.</span>
<span id="cb12-752"><a href="#cb12-752"></a></span>
<span id="cb12-753"><a href="#cb12-753"></a><span class="ss">2.  </span>$H$ - a set of hypotheses over the possible cost functions, where</span>
<span id="cb12-754"><a href="#cb12-754"></a>    for each $h \in H$ there is a cost function $c_h$ associated with</span>
<span id="cb12-755"><a href="#cb12-755"></a>    it.</span>
<span id="cb12-756"><a href="#cb12-756"></a></span>
<span id="cb12-757"><a href="#cb12-757"></a><span class="ss">3.  </span>$h^*$ - a true hypothesis that the model needs to determine, which</span>
<span id="cb12-758"><a href="#cb12-758"></a>    has cost $c_{h^*}$ associated with it</span>
<span id="cb12-759"><a href="#cb12-759"></a></span>
<span id="cb12-760"><a href="#cb12-760"></a><span class="ss">4.  </span>$t(x,y)$ - a test performed to compare items $x$ and $y$ (the user</span>
<span id="cb12-761"><a href="#cb12-761"></a>    is being asked to provide a response to which item is better). Those</span>
<span id="cb12-762"><a href="#cb12-762"></a>    tests result in changes and adjustments to $H$ as more information</span>
<span id="cb12-763"><a href="#cb12-763"></a>    is learned.</span>
<span id="cb12-764"><a href="#cb12-764"></a></span>
<span id="cb12-765"><a href="#cb12-765"></a><span class="ss">5.  </span>$o(x,y)$ - observation or result of $t(x,y)$, where</span>
<span id="cb12-766"><a href="#cb12-766"></a>    $o(x,y) \in <span class="sc">\{</span>x&lt;y, x&gt;y<span class="sc">\}</span>$</span>
<span id="cb12-767"><a href="#cb12-767"></a></span>
<span id="cb12-768"><a href="#cb12-768"></a><span class="ss">6.  </span>$S = <span class="sc">\{</span>(t_1, o_1), (t_2, o_2),...,(t_m, o_m)<span class="sc">\}</span>$ - a sequence of $m$</span>
<span id="cb12-769"><a href="#cb12-769"></a>    pairs of tests and observations</span>
<span id="cb12-770"><a href="#cb12-770"></a></span>
<span id="cb12-771"><a href="#cb12-771"></a><span class="ss">7.  </span>$w(H|S)$ - probability mass of all hypotheses that are still</span>
<span id="cb12-772"><a href="#cb12-772"></a>    consistent with the observations (similar to the dark grey area from</span>
<span id="cb12-773"><a href="#cb12-773"></a>    @fig-dim-space and $Q(i,j)$ discussed in @sec-QSA. This means that if $h \in H$ is</span>
<span id="cb12-774"><a href="#cb12-774"></a>    inconsistent with user responses received, it is removed from $H$.</span>
<span id="cb12-775"><a href="#cb12-775"></a></span>
<span id="cb12-776"><a href="#cb12-776"></a>With the key terms defined, let's consider the noiseless base setting</span>
<span id="cb12-777"><a href="#cb12-777"></a>where users only have two options for response. Those components will</span>
<span id="cb12-778"><a href="#cb12-778"></a>also later be translated to the setting with the third option so the</span>
<span id="cb12-779"><a href="#cb12-779"></a>true cost function can be determined there. $w(H|S)$ is the sum of the</span>
<span id="cb12-780"><a href="#cb12-780"></a>weights of all hypotheses that are still consistent with the evidence.</span>
<span id="cb12-781"><a href="#cb12-781"></a>$$\begin{aligned}</span>
<span id="cb12-782"><a href="#cb12-782"></a>    w(H|S) = \sum_{h \in H} w(h | S)<span class="sc">\\</span></span>
<span id="cb12-783"><a href="#cb12-783"></a>\end{aligned}$$  {#eq-eq3.25}</span>
<span id="cb12-784"><a href="#cb12-784"></a>Each $w(h|S)$ is a probability of the evidence's</span>
<span id="cb12-785"><a href="#cb12-785"></a>existence given such hypothesis: </span>
<span id="cb12-786"><a href="#cb12-786"></a>$$\begin{aligned}</span>
<span id="cb12-787"><a href="#cb12-787"></a>    w(h|S) = p(S|h)</span>
<span id="cb12-788"><a href="#cb12-788"></a>\end{aligned}$$  {#eq-eq3.26}</span>
<span id="cb12-789"><a href="#cb12-789"></a>Such probability comes from the test-observation pairs</span>
<span id="cb12-790"><a href="#cb12-790"></a>since they compose the set $S$. Moreover, each test is independent of</span>
<span id="cb12-791"><a href="#cb12-791"></a>other tests, which gives:</span>
<span id="cb12-792"><a href="#cb12-792"></a> $$\begin{aligned}</span>
<span id="cb12-793"><a href="#cb12-793"></a>    p(S|h) = \prod_{(t,o) \in S} p((t,o) | h)</span>
<span id="cb12-794"><a href="#cb12-794"></a>\end{aligned}$$  {#eq-eq3.27}</span>
<span id="cb12-795"><a href="#cb12-795"></a>In the noiseless setting, users will select an option</span>
<span id="cb12-796"><a href="#cb12-796"></a>that minimizes their cost function (selecting more preferred items),</span>
<span id="cb12-797"><a href="#cb12-797"></a>mathematically defined as: </span>
<span id="cb12-798"><a href="#cb12-798"></a>$$\begin{aligned}</span>
<span id="cb12-799"><a href="#cb12-799"></a>    p((t, o = x) | h) = </span>
<span id="cb12-800"><a href="#cb12-800"></a>    \begin{cases}</span>
<span id="cb12-801"><a href="#cb12-801"></a>        1 &amp; c_h(x) &lt; c_h(y)<span class="sc">\\</span></span>
<span id="cb12-802"><a href="#cb12-802"></a>        0 &amp; else</span>
<span id="cb12-803"><a href="#cb12-803"></a>    \end{cases}</span>
<span id="cb12-804"><a href="#cb12-804"></a>\end{aligned}$$ {#eq-prob_base}</span>
<span id="cb12-805"><a href="#cb12-805"></a></span>
<span id="cb12-806"><a href="#cb12-806"></a>**6.3.3.1 User Noise Modeling**</span>
<span id="cb12-807"><a href="#cb12-807"></a></span>
<span id="cb12-808"><a href="#cb12-808"></a>As has been discussed, users are not perfect evaluators and even get</span>
<span id="cb12-809"><a href="#cb12-809"></a>frustrated if unable to select the better option. Prior work</span>
<span id="cb12-810"><a href="#cb12-810"></a><span class="co">[</span><span class="ot">@unnoisy_humans</span><span class="co">]</span> has shown that treating users as perfect can lead to</span>
<span id="cb12-811"><a href="#cb12-811"></a>poor performance. That gave rise to accounting for noise in users'</span>
<span id="cb12-812"><a href="#cb12-812"></a>responses, but a majority of such work applies the same noise to all</span>
<span id="cb12-813"><a href="#cb12-813"></a>queries and all responses. While those led to great performance results</span>
<span id="cb12-814"><a href="#cb12-814"></a><span class="co">[</span><span class="ot">@noisy_humans</span><span class="co">]</span>, they don't accurately reflect the real world, which</span>
<span id="cb12-815"><a href="#cb12-815"></a>gave rise to the idea of creating query-based noise.</span>
<span id="cb12-816"><a href="#cb12-816"></a></span>
<span id="cb12-817"><a href="#cb12-817"></a>Effectively, for some of the queries it is important to incorporate the</span>
<span id="cb12-818"><a href="#cb12-818"></a>fact that the user is unsure and noisy, but for others, if the user is</span>
<span id="cb12-819"><a href="#cb12-819"></a>confident, noise in the response is not needed at all. For</span>
<span id="cb12-820"><a href="#cb12-820"></a>comparison-based learning, this means that the noise is related to the</span>
<span id="cb12-821"><a href="#cb12-821"></a>costs of the two items compared. Specifically for items $x$ and $y$, if</span>
<span id="cb12-822"><a href="#cb12-822"></a>$c_{h^*}(x) \simeq c_{h^*}(y)$ then the items are hard to distinguish</span>
<span id="cb12-823"><a href="#cb12-823"></a>for the user, so here it is preferred to incorporate user uncertainty</span>
<span id="cb12-824"><a href="#cb12-824"></a>and noise. But if $c_{h^*}(x) &gt;&gt; c_{h^*}(y)$, the user will certainly</span>
<span id="cb12-825"><a href="#cb12-825"></a>select $y$ and the other way around, which is where the noise is not</span>
<span id="cb12-826"><a href="#cb12-826"></a>needed.</span>
<span id="cb12-827"><a href="#cb12-827"></a></span>
<span id="cb12-828"><a href="#cb12-828"></a>Query-dependent noise is also supported in the psychology literature,</span>
<span id="cb12-829"><a href="#cb12-829"></a>which means that such an approach is more related to the real world. In</span>
<span id="cb12-830"><a href="#cb12-830"></a>particular, psychologists talk about the Luce-Sheppard Choice rule</span>
<span id="cb12-831"><a href="#cb12-831"></a><span class="co">[</span><span class="ot">@lus-shep</span><span class="co">]</span> when talking about comparisons. This rule previously gave</span>
<span id="cb12-832"><a href="#cb12-832"></a>rise to a logistic model based on the noise <span class="co">[</span><span class="ot">@lus-log</span><span class="co">]</span> where the</span>
<span id="cb12-833"><a href="#cb12-833"></a>probability of observation for a given test is: </span>
<span id="cb12-834"><a href="#cb12-834"></a>$$\begin{aligned}</span>
<span id="cb12-835"><a href="#cb12-835"></a>    p((t, o = x) | h) \propto exp(-\gamma * c_h(x))</span>
<span id="cb12-836"><a href="#cb12-836"></a>\end{aligned}$$ {#eq-noise_model}</span>
<span id="cb12-837"><a href="#cb12-837"></a></span>
<span id="cb12-838"><a href="#cb12-838"></a>![User response model in the noiseless</span>
<span id="cb12-839"><a href="#cb12-839"></a>setting](Figures/Noiseless probs.png){#fig-noiseless_1 width="100%"}</span>
<span id="cb12-840"><a href="#cb12-840"></a></span>
<span id="cb12-841"><a href="#cb12-841"></a>![User response with Luce Sheppard noise</span>
<span id="cb12-842"><a href="#cb12-842"></a>model](Figures/Noise probs.png){#fig-noiseless_2 width="100%"}</span>
<span id="cb12-843"><a href="#cb12-843"></a></span>
<span id="cb12-844"><a href="#cb12-844"></a><span class="co">[</span><span class="ot">@fig-noiseless_1; @fig-noiseless_2</span><span class="co">]</span> demonstrate the difference between the</span>
<span id="cb12-845"><a href="#cb12-845"></a>noiseless setting and incorporating the Luce-Sheppard Choice rule. GBS</span>
<span id="cb12-846"><a href="#cb12-846"></a>is the baseline model with only 2 response options, and CLAUS is the</span>
<span id="cb12-847"><a href="#cb12-847"></a>model with the uncertainty option added. The figures show how</span>
<span id="cb12-848"><a href="#cb12-848"></a>incorporating such noise influences and smoothes the probability</span>
<span id="cb12-849"><a href="#cb12-849"></a>distribution of the user's response.</span>
<span id="cb12-850"><a href="#cb12-850"></a></span>
<span id="cb12-851"><a href="#cb12-851"></a>**6.3.3.2 User Uncertainty**</span>
<span id="cb12-852"><a href="#cb12-852"></a></span>
<span id="cb12-853"><a href="#cb12-853"></a>We will now discuss the functionality of CLAUS, which is an algorithm</span>
<span id="cb12-854"><a href="#cb12-854"></a>designed by <span class="co">[</span><span class="ot">@claus</span><span class="co">]</span> that allows users to select an uncertain response</span>
<span id="cb12-855"><a href="#cb12-855"></a>about the two options that they need to rank. The authors model such</span>
<span id="cb12-856"><a href="#cb12-856"></a>uncertainty as $\epsilon$ and it is associated with each $c_h$, so now</span>
<span id="cb12-857"><a href="#cb12-857"></a>every hypothesis $h$ is defined over a pair of $(c_h, \epsilon_h)$. It</span>
<span id="cb12-858"><a href="#cb12-858"></a>is important to note that the goal is to still learn and maintain our</span>
<span id="cb12-859"><a href="#cb12-859"></a>objective on $c$, $\epsilon$ is only necessary to model the users'</span>
<span id="cb12-860"><a href="#cb12-860"></a>responses. The uncertainty relates to the cost function in the following</span>
<span id="cb12-861"><a href="#cb12-861"></a>way: </span>
<span id="cb12-862"><a href="#cb12-862"></a>$$\begin{aligned}</span>
<span id="cb12-863"><a href="#cb12-863"></a>    |c_h(x) - c_h(y)| &lt; \epsilon_h</span>
<span id="cb12-864"><a href="#cb12-864"></a>\end{aligned}$$  {#eq-eq3.30}</span>
<span id="cb12-865"><a href="#cb12-865"></a>this means that the user is uncertain between items $x$</span>
<span id="cb12-866"><a href="#cb12-866"></a>and $y$ and their cost difference is negligible such that the user is</span>
<span id="cb12-867"><a href="#cb12-867"></a>not able to select which item is better. This in turn gives more</span>
<span id="cb12-868"><a href="#cb12-868"></a>information about the real value of the two items, as a binary response</span>
<span id="cb12-869"><a href="#cb12-869"></a>would indicate the user's preference towards one item, which will not be</span>
<span id="cb12-870"><a href="#cb12-870"></a>real and will skew the cost functions.</span>
<span id="cb12-871"><a href="#cb12-871"></a></span>
<span id="cb12-872"><a href="#cb12-872"></a>This causes modifications of the problem set-up:</span>
<span id="cb12-873"><a href="#cb12-873"></a></span>
<span id="cb12-874"><a href="#cb12-874"></a><span class="ss">1.  </span>For test $t(x,y)$ the observation will be</span>
<span id="cb12-875"><a href="#cb12-875"></a>    $o(x,y) \in <span class="sc">\{</span>x&lt;y, x&gt;y, \tilde{xy}<span class="sc">\}</span>$, where $\tilde{xy}$ is the</span>
<span id="cb12-876"><a href="#cb12-876"></a>    uncertain response.</span>
<span id="cb12-877"><a href="#cb12-877"></a></span>
<span id="cb12-878"><a href="#cb12-878"></a><span class="ss">2.  </span>The probability distribution over the user's response</span>
<span id="cb12-879"><a href="#cb12-879"></a>    (<span class="co">[</span><span class="ot">@eq-prob_base</span><span class="co">]</span>) will now be defined as:</span>
<span id="cb12-880"><a href="#cb12-880"></a>$$\begin{aligned}</span>
<span id="cb12-881"><a href="#cb12-881"></a>    p((t, o = x) | h) = </span>
<span id="cb12-882"><a href="#cb12-882"></a>    \begin{cases}</span>
<span id="cb12-883"><a href="#cb12-883"></a>        1 &amp; c_h(x) &lt; c_h(y) - \epsilon_h<span class="sc">\\</span></span>
<span id="cb12-884"><a href="#cb12-884"></a>        0 &amp; else</span>
<span id="cb12-885"><a href="#cb12-885"></a>    \end{cases}</span>
<span id="cb12-886"><a href="#cb12-886"></a>\end{aligned}$$ {#eq-eq3.31}</span>
<span id="cb12-887"><a href="#cb12-887"></a>    </span>
<span id="cb12-888"><a href="#cb12-888"></a>$$\begin{aligned}</span>
<span id="cb12-889"><a href="#cb12-889"></a>    p((t, o = \tilde{xy}) | h) = </span>
<span id="cb12-890"><a href="#cb12-890"></a>    \begin{cases}</span>
<span id="cb12-891"><a href="#cb12-891"></a>        1 &amp; |c_h(x) - c_h(y)|^2 &lt; \epsilon_h^2<span class="sc">\\</span></span>
<span id="cb12-892"><a href="#cb12-892"></a>        0 &amp; else</span>
<span id="cb12-893"><a href="#cb12-893"></a>    \end{cases}</span>
<span id="cb12-894"><a href="#cb12-894"></a>\end{aligned}$$ {#eq-eq3.32}</span>
<span id="cb12-895"><a href="#cb12-895"></a></span>
<span id="cb12-896"><a href="#cb12-896"></a>This means the user confidently selects $x$ when it is better than $y$ by more than $\epsilon$, but if the squared</span>
<span id="cb12-897"><a href="#cb12-897"></a>difference of the cost functions of two items is negligible by $\epsilon$ user will choose the indifferent option.</span>
<span id="cb12-898"><a href="#cb12-898"></a></span>
<span id="cb12-899"><a href="#cb12-899"></a><span class="ss">3.  </span>Finally this also updates the noise model (<span class="co">[</span><span class="ot">@eq-noise_model</span><span class="co">]</span>):</span>
<span id="cb12-900"><a href="#cb12-900"></a>$$\begin{aligned}</span>
<span id="cb12-901"><a href="#cb12-901"></a>    p((t, o = x) | h) \propto \exp(-\gamma * <span class="co">[</span><span class="ot">c_h(x) - c_h(y)</span><span class="co">]</span>)</span>
<span id="cb12-902"><a href="#cb12-902"></a>\end{aligned}$$ {#eq-eq3.33}</span>
<span id="cb12-903"><a href="#cb12-903"></a></span>
<span id="cb12-904"><a href="#cb12-904"></a>$$\begin{aligned}</span>
<span id="cb12-905"><a href="#cb12-905"></a>    p((t, o = \tilde{xy}) | h) \propto exp(-1/\epsilon_h^2 * <span class="co">[</span><span class="ot">c_h(x) - c_h(y)</span><span class="co">]</span>^2)</span>
<span id="cb12-906"><a href="#cb12-906"></a>\end{aligned}$$ {#eq-eq3.34}</span>
<span id="cb12-907"><a href="#cb12-907"></a></span>
<span id="cb12-908"><a href="#cb12-908"></a>**6.3.3.3 Performance Analysis**</span>
<span id="cb12-909"><a href="#cb12-909"></a></span>
<span id="cb12-910"><a href="#cb12-910"></a>![CLAUS using equivalence classes. Each cost function $c$ corresponds to</span>
<span id="cb12-911"><a href="#cb12-911"></a>an equivalence class (blue ellipse). Hypotheses (black dots) are</span>
<span id="cb12-912"><a href="#cb12-912"></a>$<span class="sc">\{</span>c_h,\epsilon_h<span class="sc">\}</span>$ pairs. Hypotheses sharing a cost $c$ are said to be</span>
<span id="cb12-913"><a href="#cb12-913"></a>inside the equivalence class of $c$. After performing a test and</span>
<span id="cb12-914"><a href="#cb12-914"></a>receiving an observation, the evidence results in downweighting</span>
<span id="cb12-915"><a href="#cb12-915"></a>connections among some of the hypotheses.](Figures/equiv.png){#fig-equiv_c width="60%"}</span>
<span id="cb12-916"><a href="#cb12-916"></a></span>
<span id="cb12-917"><a href="#cb12-917"></a>Before diving deeper into the comparisons of performance, it is</span>
<span id="cb12-918"><a href="#cb12-918"></a>important to indicate that rather than predicting a specific pair</span>
<span id="cb12-919"><a href="#cb12-919"></a>$(c_h, \epsilon_h)$, the algorithm focuses on predicting a group of</span>
<span id="cb12-920"><a href="#cb12-920"></a>pairs that are similar to one another, otherwise called equivalence</span>
<span id="cb12-921"><a href="#cb12-921"></a>class (@fig-equiv_c), which indicates not essentially different</span>
<span id="cb12-922"><a href="#cb12-922"></a>hypothesis for the cost function and uncertainty. That information is</span>
<span id="cb12-923"><a href="#cb12-923"></a>learned through each new test, as the algorithm updates the information</span>
<span id="cb12-924"><a href="#cb12-924"></a>about $c$ and $\epsilon$ that distinguishes between the distinct $h$,</span>
<span id="cb12-925"><a href="#cb12-925"></a>finding the equivalence groups among them. Moreover, the authors tweaked</span>
<span id="cb12-926"><a href="#cb12-926"></a>the parameter responsible for the size of the equivalence class (how</span>
<span id="cb12-927"><a href="#cb12-927"></a>many hypotheses can be grouped together at a time).</span>
<span id="cb12-928"><a href="#cb12-928"></a></span>
<span id="cb12-929"><a href="#cb12-929"></a><span class="al">![Performance of GBS and its variants](Figures/GBS:CLAUS.png)</span>{#fig-claus_num</span>
<span id="cb12-930"><a href="#cb12-930"></a>width="60%"}</span>
<span id="cb12-931"><a href="#cb12-931"></a></span>
<span id="cb12-932"><a href="#cb12-932"></a>::: {#tbl-claus_tab}</span>
<span id="cb12-933"><a href="#cb12-933"></a>  **Category**                        **Accuracy**                       **Query Count**</span>
<span id="cb12-934"><a href="#cb12-934"></a>  --------------------- ------------------------------------ ------------------------------------</span>
<span id="cb12-935"><a href="#cb12-935"></a>  GBS - About Equal               $94.15 \pm 0.52$                     $36.02 \pm 0.03$</span>
<span id="cb12-936"><a href="#cb12-936"></a>  GBS - Not Sure         $\textbf{94.66} \pm \textbf{0.55}$            $35.95 \pm 0.04$</span>
<span id="cb12-937"><a href="#cb12-937"></a>  CLAUS - About Equal             $91.56 \pm 0.84$            $\textbf{25.93} \pm \textbf{0.41}$</span>
<span id="cb12-938"><a href="#cb12-938"></a>  CLAUS - Not Sure                $90.86 \pm 0.74$                     $26.98 \pm 0.47$</span>
<span id="cb12-939"><a href="#cb12-939"></a></span>
<span id="cb12-940"><a href="#cb12-940"></a>  : Performance of GBS and CLAUS with different labels for the uncertainty</span>
<span id="cb12-941"><a href="#cb12-941"></a>:::</span>
<span id="cb12-942"><a href="#cb12-942"></a></span>
<span id="cb12-943"><a href="#cb12-943"></a>The first performance evaluation is done on the number of queries and</span>
<span id="cb12-944"><a href="#cb12-944"></a>confirms that it decreases in @fig-claus_num.</span>
<span id="cb12-945"><a href="#cb12-945"></a>The GBS model serves as the baseline, as it will do all of the</span>
<span id="cb12-946"><a href="#cb12-946"></a>comparison queries using the binary response options. The CLAUS model is</span>
<span id="cb12-947"><a href="#cb12-947"></a>measured over different values of $\epsilon$ on the x-axis and over</span>
<span id="cb12-948"><a href="#cb12-948"></a>different sizes of the equivalence sets indicated by different shades of</span>
<span id="cb12-949"><a href="#cb12-949"></a>blue. Figure shows that all variants of CLAUS use approximately 10 fewer</span>
<span id="cb12-950"><a href="#cb12-950"></a>queries on average compared to GBS. Moreover, using bigger-sized</span>
<span id="cb12-951"><a href="#cb12-951"></a>equivalence classes can further decrease the number of needed queries.</span>
<span id="cb12-952"><a href="#cb12-952"></a>The most optimal $\epsilon \simeq 0.07$, after which higher $\epsilon$</span>
<span id="cb12-953"><a href="#cb12-953"></a>does not provide any benefit.</span>
<span id="cb12-954"><a href="#cb12-954"></a></span>
<span id="cb12-955"><a href="#cb12-955"></a>Lastly, the authors considered the performance difference, which is</span>
<span id="cb12-956"><a href="#cb12-956"></a>indicated in @tbl-claus_tab. For that authors used two different labels</span>
<span id="cb12-957"><a href="#cb12-957"></a>for the uncertainty button in CLAUS, it was either labeled as \"About</span>
<span id="cb12-958"><a href="#cb12-958"></a>Equal\" or \"Not Sure\" as those can provoke different responses and</span>
<span id="cb12-959"><a href="#cb12-959"></a>feelings in users. Moreover, GBS and CLAUS-type responses were mixed in</span>
<span id="cb12-960"><a href="#cb12-960"></a>the same set of questions to the user, which splits the metrics for both</span>
<span id="cb12-961"><a href="#cb12-961"></a>in two as can be seen in @tbl-claus_tab.</span>
<span id="cb12-962"><a href="#cb12-962"></a>The performance of CLAUS is lower by $3\%$ on average, indicating</span>
<span id="cb12-963"><a href="#cb12-963"></a>similar results to @sec-geo_app, showing that a smaller number of queries can</span>
<span id="cb12-964"><a href="#cb12-964"></a>still lead to a performance loss. However, the second column of @tbl-claus_tab</span>
<span id="cb12-965"><a href="#cb12-965"></a>supports the information in @fig-claus_num,</span>
<span id="cb12-966"><a href="#cb12-966"></a>as it also shows that 10 fewer queries were conducted on average.</span>
<span id="cb12-967"><a href="#cb12-967"></a></span>
<span id="cb12-968"><a href="#cb12-968"></a><span class="fu">### Active Preference-Based Learning of Reward Functions</span></span>
<span id="cb12-969"><a href="#cb12-969"></a></span>
<span id="cb12-970"><a href="#cb12-970"></a>Active learning can be essential in learning within dynamic systems and</span>
<span id="cb12-971"><a href="#cb12-971"></a>environments. Say we have an agent in an environment, and we want it to</span>
<span id="cb12-972"><a href="#cb12-972"></a>conform to a certain behavior as set by a human. How exactly do we go</span>
<span id="cb12-973"><a href="#cb12-973"></a>about doing this? In a traditional RL setting, this is solved by a class</span>
<span id="cb12-974"><a href="#cb12-974"></a>of algorithms under Inverse Reinforcement Learning. Techniques such as</span>
<span id="cb12-975"><a href="#cb12-975"></a>VICE and GAIL attempt to learn a reward function that can distinguish</span>
<span id="cb12-976"><a href="#cb12-976"></a>between states visited by the agent and states desired to be visited as</span>
<span id="cb12-977"><a href="#cb12-977"></a>defined by a human. In effect, a human will demonstrate what it would</span>
<span id="cb12-978"><a href="#cb12-978"></a>like the agent to do in the environment, and from there, learning is</span>
<span id="cb12-979"><a href="#cb12-979"></a>done. However, what if humans do not precisely know how an agent should</span>
<span id="cb12-980"><a href="#cb12-980"></a>optimally behave in an environment but still have some opinion on what</span>
<span id="cb12-981"><a href="#cb12-981"></a>trajectories would be better than others? This is where a paper like</span>
<span id="cb12-982"><a href="#cb12-982"></a>Active Preference-Based Learning of Reward Functions comes into the</span>
<span id="cb12-983"><a href="#cb12-983"></a>picture. The paper aims to use human preferences to aid an agent's</span>
<span id="cb12-984"><a href="#cb12-984"></a>learning within a dynamic system.</span>
<span id="cb12-985"><a href="#cb12-985"></a></span>
<span id="cb12-986"><a href="#cb12-986"></a>A dynamic system contains human input, robotic input, and an environment</span>
<span id="cb12-987"><a href="#cb12-987"></a>state. The transitions between states is defined by $f_{HR}$, so that we</span>
<span id="cb12-988"><a href="#cb12-988"></a>have: </span>
<span id="cb12-989"><a href="#cb12-989"></a>$$x^{t+1} = f_{HR}(x^t, u_R, u_H)$$  {#eq-eq3.35}</span>
<span id="cb12-990"><a href="#cb12-990"></a>At a given time step $t$, we</span>
<span id="cb12-991"><a href="#cb12-991"></a>have $x_t$, $u_R^t$, and $u_H^t$. This can be encapsulated into a single</span>
<span id="cb12-992"><a href="#cb12-992"></a>$d$ dimensional feature vector that the authors denote as $\phi$. The</span>
<span id="cb12-993"><a href="#cb12-993"></a>paper then assumes that the underlying reward model we are trying to</span>
<span id="cb12-994"><a href="#cb12-994"></a>learn can be represented linearly. If we have our human reward</span>
<span id="cb12-995"><a href="#cb12-995"></a>preference function defined as $r_H$, this means we can write $r_H$ as:</span>
<span id="cb12-996"><a href="#cb12-996"></a>$$r_H(x^t, u_R^t, u_H^t) = w^{\intercal}\phi(x^t, u_R^t, u_H^t)$$ {#eq-eq3.35}</span>
<span id="cb12-997"><a href="#cb12-997"></a>Because the reward function is linear, we can take the weight vector out</span>
<span id="cb12-998"><a href="#cb12-998"></a>of the summation if we want to calculate the reward over an entire</span>
<span id="cb12-999"><a href="#cb12-999"></a>trajectory:</span>
<span id="cb12-1000"><a href="#cb12-1000"></a>$$\begin{aligned}</span>
<span id="cb12-1001"><a href="#cb12-1001"></a>R_{H}(x^0, u_R, u_H) &amp;= \sum_{t=0}^{N} r_{H}(x^t, u^t, u_H^t)<span class="sc">\\</span></span>
<span id="cb12-1002"><a href="#cb12-1002"></a>\Phi &amp;= \sum \phi(x^t, u_R^t, u_H^t)<span class="sc">\\</span> </span>
<span id="cb12-1003"><a href="#cb12-1003"></a>R_H(traj) &amp;= w\cdot\Phi(traj)\end{aligned}$$ {#eq-eq3.36}</span>
<span id="cb12-1004"><a href="#cb12-1004"></a></span>
<span id="cb12-1005"><a href="#cb12-1005"></a><span class="fu">#### Properties of $W$ {.unnumbered}</span></span>
<span id="cb12-1006"><a href="#cb12-1006"></a></span>
<span id="cb12-1007"><a href="#cb12-1007"></a>First, the scale of $w$ does not matter because we only care about the</span>
<span id="cb12-1008"><a href="#cb12-1008"></a>relative rewards produced with $w$ (given two different trajectories, we</span>
<span id="cb12-1009"><a href="#cb12-1009"></a>want to answer the question of which trajectory a human would prefer,</span>
<span id="cb12-1010"><a href="#cb12-1010"></a>i.e. which one has a higher preference reward). This means we can</span>
<span id="cb12-1011"><a href="#cb12-1011"></a>constrain $||w|| &lt;= 1$, so the initial prior is uniform over a unit</span>
<span id="cb12-1012"><a href="#cb12-1012"></a>ball. From here, we can determine a probabilistic expression to assess</span>
<span id="cb12-1013"><a href="#cb12-1013"></a>whether we should prefer trajectory A or B (because it can be noisy with</span>
<span id="cb12-1014"><a href="#cb12-1014"></a>human input). Let $I_t = +1$ if the human prefers trajectory $A$ and let</span>
<span id="cb12-1015"><a href="#cb12-1015"></a>$I_t = -1$ if the human prefers trajectory $B$. We get the following for</span>
<span id="cb12-1016"><a href="#cb12-1016"></a>$p(I_t | w)$.</span>
<span id="cb12-1017"><a href="#cb12-1017"></a></span>
<span id="cb12-1018"><a href="#cb12-1018"></a>$$\begin{aligned}</span>
<span id="cb12-1019"><a href="#cb12-1019"></a>p(I_t = +1|w) &amp;= \frac{exp(R_H(traj_A))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}<span class="sc">\\</span></span>
<span id="cb12-1020"><a href="#cb12-1020"></a>p(I_t = -1|w) &amp;= \frac{exp(R_H(traj_B))}{exp(R_H(traj_A)) + exp(R_H(traj_B))}</span>
<span id="cb12-1021"><a href="#cb12-1021"></a>\end{aligned}$$ {#eq-eq3.37}</span>
<span id="cb12-1022"><a href="#cb12-1022"></a></span>
<span id="cb12-1023"><a href="#cb12-1023"></a>We can re-write this expression to make it cleaner, using the following</span>
<span id="cb12-1024"><a href="#cb12-1024"></a>substitution: $$\psi = \Phi(traj_a) - \Phi(traj_b)$$ {#eq-eq3.38}</span>
<span id="cb12-1025"><a href="#cb12-1025"></a>$$f_{\psi} (w) = p(I_t|w) = \frac{1}{1 + exp(-I_tw^{\intercal}\psi)}$$ {#eq-eq3.39}</span>
<span id="cb12-1026"><a href="#cb12-1026"></a></span>
<span id="cb12-1027"><a href="#cb12-1027"></a>The idea now is that we can update $p(w)$ everytime we get a result from</span>
<span id="cb12-1028"><a href="#cb12-1028"></a>a human preference query using Bayes:</span>
<span id="cb12-1029"><a href="#cb12-1029"></a></span>
<span id="cb12-1030"><a href="#cb12-1030"></a>$$p(w|I_t) &lt;- p(w) \cdot p(I_t|w)$$ {#eq-eq3.40}</span>
<span id="cb12-1031"><a href="#cb12-1031"></a></span>
<span id="cb12-1032"><a href="#cb12-1032"></a>We do not need to know $p(I_t)$ because we can use an algorithm like the</span>
<span id="cb12-1033"><a href="#cb12-1033"></a>Metropolis algorithm to actually sample.</span>
<span id="cb12-1034"><a href="#cb12-1034"></a></span>
<span id="cb12-1035"><a href="#cb12-1035"></a><span class="fu">#### Generating Queries {.unnumbered}</span></span>
<span id="cb12-1036"><a href="#cb12-1036"></a></span>
<span id="cb12-1037"><a href="#cb12-1037"></a>This is where the interesting part of the paper comes into play. How do</span>
<span id="cb12-1038"><a href="#cb12-1038"></a>we actually generate queries for the user to pick between? This paper</span>
<span id="cb12-1039"><a href="#cb12-1039"></a>synthetically generates queries through an optimization process and then</span>
<span id="cb12-1040"><a href="#cb12-1040"></a>presents them to a human to pick between. The idea is that we want to</span>
<span id="cb12-1041"><a href="#cb12-1041"></a>generate a query that maximizes the conditional entropy $H(I|w)$. There</span>
<span id="cb12-1042"><a href="#cb12-1042"></a>are a few ways to think about this -- intuitively we want to pick a</span>
<span id="cb12-1043"><a href="#cb12-1043"></a>query that we are most uncertain about given our current weights (thus</span>
<span id="cb12-1044"><a href="#cb12-1044"></a>having the highest conditional entropy given the weights). The way the</span>
<span id="cb12-1045"><a href="#cb12-1045"></a>authors of the paper frame this originally in the paper is that \"we</span>
<span id="cb12-1046"><a href="#cb12-1046"></a>want to find the next query such that it will help us remove as much</span>
<span id="cb12-1047"><a href="#cb12-1047"></a>volume (the integral of the unnormalized pdf over w) as possible from</span>
<span id="cb12-1048"><a href="#cb12-1048"></a>the space of possible rewards.\" Mathematically this can be written as:</span>
<span id="cb12-1049"><a href="#cb12-1049"></a></span>
<span id="cb12-1050"><a href="#cb12-1050"></a>$$max_{x^0, u_R, u_H^A, u_H^B} min<span class="sc">\{</span>E<span class="co">[</span><span class="ot">1-f_{\psi}(w)</span><span class="co">]</span>, E<span class="co">[</span><span class="ot">1 - f_{-\psi}(w)</span><span class="co">]</span><span class="sc">\}</span>$$ {#eq-eq3.41}</span>
<span id="cb12-1051"><a href="#cb12-1051"></a></span>
<span id="cb12-1052"><a href="#cb12-1052"></a>But how exactly do we optimize this expression mathematically? After</span>
<span id="cb12-1053"><a href="#cb12-1053"></a>all, we need to use this expression to generate synthetic queries. The</span>
<span id="cb12-1054"><a href="#cb12-1054"></a>answer is to sample $w_1, ... w_m$ from $p(w)$. We can assume we are</span>
<span id="cb12-1055"><a href="#cb12-1055"></a>sampling points from a point cloud, thus approximating the distribution</span>
<span id="cb12-1056"><a href="#cb12-1056"></a>$p(w)$ as</span>
<span id="cb12-1057"><a href="#cb12-1057"></a></span>
<span id="cb12-1058"><a href="#cb12-1058"></a>$$p(w) = \frac{1}{M} \sum \delta (w_i).$$  {#eq-eq3.42}</span>
<span id="cb12-1059"><a href="#cb12-1059"></a>We can now approximate the expectation expression like so:</span>
<span id="cb12-1060"><a href="#cb12-1060"></a>$$E<span class="co">[</span><span class="ot">1 - f_{\psi}(w)</span><span class="co">]</span> = \frac{1}{M} (\sum 1 - f_{\psi}(w_i))$$ {#eq-eq3.43}</span>
<span id="cb12-1061"><a href="#cb12-1061"></a></span>
<span id="cb12-1062"><a href="#cb12-1062"></a>and now we can optimize the expression to generate a synthetic query!</span>
<span id="cb12-1063"><a href="#cb12-1063"></a>Altogether, the algorithm looks like the following:</span>
<span id="cb12-1064"><a href="#cb12-1064"></a></span>
<span id="cb12-1065"><a href="#cb12-1065"></a><span class="in">```pseudocode</span></span>
<span id="cb12-1066"><a href="#cb12-1066"></a><span class="in">#| label: alg-design</span></span>
<span id="cb12-1067"><a href="#cb12-1067"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb12-1068"><a href="#cb12-1068"></a><span class="in">\caption{Preference-Based Learning of Reward Functions}</span></span>
<span id="cb12-1069"><a href="#cb12-1069"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb12-1070"><a href="#cb12-1070"></a><span class="in">    \State \textbf{input:} features $\phi$, horizon $N$, dynamics $f$, $iter$</span></span>
<span id="cb12-1071"><a href="#cb12-1071"></a><span class="in">    \State \textbf{initialize:} $p(w) \sim Uniform(B)$, for a unit ball $B$</span></span>
<span id="cb12-1072"><a href="#cb12-1072"></a><span class="in">    \While{$t &lt; iter$}</span></span>
<span id="cb12-1073"><a href="#cb12-1073"></a><span class="in">        \State $W \gets M$ samples from $AdaptiveMetropolis(p(w))$</span></span>
<span id="cb12-1074"><a href="#cb12-1074"></a><span class="in">        \State $(x^0, u_R, u^A_H, u^B_H) \gets SynthExps(W,f)$</span></span>
<span id="cb12-1075"><a href="#cb12-1075"></a><span class="in">        \State $I_t \gets QueryHuman(x^0, u_R, u^A_H, u^B_H)$</span></span>
<span id="cb12-1076"><a href="#cb12-1076"></a><span class="in">        \State $\varphi = \Phi(x^0, u_R, u^A_H) - \Phi(x^0, u_R, u^B_H)$</span></span>
<span id="cb12-1077"><a href="#cb12-1077"></a><span class="in">        \State $f_\varphi(w) = \min(1, I_t\exp(w^\top \varphi))$</span></span>
<span id="cb12-1078"><a href="#cb12-1078"></a><span class="in">        \State $p(w) \gets p(w) \cdot f_\varphi(w)$</span></span>
<span id="cb12-1079"><a href="#cb12-1079"></a><span class="in">        \State $t \gets t+1$</span></span>
<span id="cb12-1080"><a href="#cb12-1080"></a><span class="in">    \EndWhile</span></span>
<span id="cb12-1081"><a href="#cb12-1081"></a><span class="in">    \State \textbf{output:} distribution of $w: p(w)$</span></span>
<span id="cb12-1082"><a href="#cb12-1082"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb12-1083"><a href="#cb12-1083"></a><span class="in">\end{algorithm}</span></span>
<span id="cb12-1084"><a href="#cb12-1084"></a><span class="in">```</span></span>
<span id="cb12-1085"><a href="#cb12-1085"></a></span>
<span id="cb12-1086"><a href="#cb12-1086"></a><span class="fu">#### Batching Queries {.unnumbered}</span></span>
<span id="cb12-1087"><a href="#cb12-1087"></a></span>
<span id="cb12-1088"><a href="#cb12-1088"></a>The algorithm itself works well, however there ends up being a bottle</span>
<span id="cb12-1089"><a href="#cb12-1089"></a>neck that each query needs to be synthesized before being sent to the</span>
<span id="cb12-1090"><a href="#cb12-1090"></a>human -- one at a time. In other words, the human gives their feedback,</span>
<span id="cb12-1091"><a href="#cb12-1091"></a>waits for a query to be synthesized, and then gives another data point</span>
<span id="cb12-1092"><a href="#cb12-1092"></a>of feedback. There is no room for parallelization and so the authors</span>
<span id="cb12-1093"><a href="#cb12-1093"></a>proposed a second algorithm in a separate paper that allows for the</span>
<span id="cb12-1094"><a href="#cb12-1094"></a>batching of queries. Simply put, we change the mathematical expression</span>
<span id="cb12-1095"><a href="#cb12-1095"></a>to the following:</span>
<span id="cb12-1096"><a href="#cb12-1096"></a></span>
<span id="cb12-1097"><a href="#cb12-1097"></a>$$max_{\xi_{ib+1_A}, \xi_{ib+1_B}, ... , \xi_{ib+b_A}, \xi_{ib+b_B} H(I_{ib+1}, I_{ib+2}, .., I_{ib+b} | w)}$$ {#eq-eq3.44}</span>
<span id="cb12-1098"><a href="#cb12-1098"></a></span>
<span id="cb12-1099"><a href="#cb12-1099"></a>Naively, we could consider optimizing this in the greedy fashion. This</span>
<span id="cb12-1100"><a href="#cb12-1100"></a>would mean just synthetically generating $b$ independent queries. The</span>
<span id="cb12-1101"><a href="#cb12-1101"></a>obvious drawback of this method would be that the queries would likely</span>
<span id="cb12-1102"><a href="#cb12-1102"></a>be very similar to each other. The authors propose a few other</span>
<span id="cb12-1103"><a href="#cb12-1103"></a>heuristics that would help guide the algorithm away from generating very</span>
<span id="cb12-1104"><a href="#cb12-1104"></a>similar queries. As an example, the authors propose Medioid Selection</span>
<span id="cb12-1105"><a href="#cb12-1105"></a>where we have to cluster $B$ greedy vectors into $b &lt; B$ groups and pick</span>
<span id="cb12-1106"><a href="#cb12-1106"></a>one vector from each group (the medioid). The authors also propose two</span>
<span id="cb12-1107"><a href="#cb12-1107"></a>other methods rooted in providing different queries: boundary medioids</span>
<span id="cb12-1108"><a href="#cb12-1108"></a>selection and successive elimination. They are best visually depicted</span>
<span id="cb12-1109"><a href="#cb12-1109"></a>as:</span>
<span id="cb12-1110"><a href="#cb12-1110"></a></span>
<span id="cb12-1111"><a href="#cb12-1111"></a><span class="al">![Different selection strategies](Figures/greedy.png)</span>{#fig-selection-strategy width="75%"}</span>
<span id="cb12-1112"><a href="#cb12-1112"></a></span>
<span id="cb12-1113"><a href="#cb12-1113"></a><span class="fu">#### Results {.unnumbered}</span></span>
<span id="cb12-1114"><a href="#cb12-1114"></a></span>
<span id="cb12-1115"><a href="#cb12-1115"></a>The authors test both the non-batched and variety of batched learning</span>
<span id="cb12-1116"><a href="#cb12-1116"></a>algorithms on multiple environments:</span>
<span id="cb12-1117"><a href="#cb12-1117"></a></span>
<span id="cb12-1118"><a href="#cb12-1118"></a><span class="al">![Comparison between batched and non-batched algorithms](Figures/activeresults.png)</span>{#fig-batch-nonbatch width="75%"}</span>
<span id="cb12-1119"><a href="#cb12-1119"></a></span>
<span id="cb12-1120"><a href="#cb12-1120"></a>What is interesting to note is that when graphed over $N$ the</span>
<span id="cb12-1121"><a href="#cb12-1121"></a>non-batched active learning approach does in the same ball-park of</span>
<span id="cb12-1122"><a href="#cb12-1122"></a>performance as the batched approaches. However, if you graph it over</span>
<span id="cb12-1123"><a href="#cb12-1123"></a>time, we see that learning is a much slower process when not-batched.</span>
<span id="cb12-1124"><a href="#cb12-1124"></a></span>
<span id="cb12-1125"><a href="#cb12-1125"></a><span class="fu">### Application: Foundation Models for Robotics</span></span>
<span id="cb12-1126"><a href="#cb12-1126"></a></span>
<span id="cb12-1127"><a href="#cb12-1127"></a>Modern foundation models have been ubiquitous in discussions of</span>
<span id="cb12-1128"><a href="#cb12-1128"></a>powerful, general purpose AI systems that can accomplish myriad tasks</span>
<span id="cb12-1129"><a href="#cb12-1129"></a>across many disciplines such as programming, medicine, law, open</span>
<span id="cb12-1130"><a href="#cb12-1130"></a>question-answering and much more, with rapidly increasing capabilities</span>
<span id="cb12-1131"><a href="#cb12-1131"></a><span class="co">[</span><span class="ot">@bommasani2022opportunities</span><span class="co">]</span>. However, despite successes from large</span>
<span id="cb12-1132"><a href="#cb12-1132"></a>labs in controlled environments <span class="co">[</span><span class="ot">@brohan2023rt2</span><span class="co">]</span> foundation models have</span>
<span id="cb12-1133"><a href="#cb12-1133"></a>not seen ubiquitous use in robotics due to shifting robot morphology,</span>
<span id="cb12-1134"><a href="#cb12-1134"></a>lack of data, and the sim to real gap in robotics</span>
<span id="cb12-1135"><a href="#cb12-1135"></a><span class="co">[</span><span class="ot">@walke2023bridgedata</span><span class="co">]</span>. For this subsection we explore two promising</span>
<span id="cb12-1136"><a href="#cb12-1136"></a>approaches known as R3M and Voltron which are the first to leverage</span>
<span id="cb12-1137"><a href="#cb12-1137"></a>pre-training on vast amounts of data towards performance improvement on</span>
<span id="cb12-1138"><a href="#cb12-1138"></a>downstream robotic tasks despite the aforementioned issues</span>
<span id="cb12-1139"><a href="#cb12-1139"></a><span class="co">[</span><span class="ot">@nair2022r3m; @karamcheti2023languagedriven</span><span class="co">]</span>.</span>
<span id="cb12-1140"><a href="#cb12-1140"></a></span>
<span id="cb12-1141"><a href="#cb12-1141"></a><span class="fu">#### R3M: Universal Visual Representation for Robotics {.unnumbered}</span></span>
<span id="cb12-1142"><a href="#cb12-1142"></a></span>
<span id="cb12-1143"><a href="#cb12-1143"></a><span class="al">![R3M pipeline](Figures/r3m.png)</span>{#fig-r3m-pipline width="95%"}</span>
<span id="cb12-1144"><a href="#cb12-1144"></a></span>
<span id="cb12-1145"><a href="#cb12-1145"></a>R3M represents a significant advancement in the field of robotic</span>
<span id="cb12-1146"><a href="#cb12-1146"></a>manipulation and learning. This model diverges from traditional</span>
<span id="cb12-1147"><a href="#cb12-1147"></a>approaches that rely on training from scratch within the same domain on</span>
<span id="cb12-1148"><a href="#cb12-1148"></a>the same robot data as instead it leverags pretraining on large</span>
<span id="cb12-1149"><a href="#cb12-1149"></a>datasets, akin to the practices in computer vision and natural language</span>
<span id="cb12-1150"><a href="#cb12-1150"></a>processing (NLP) where models are trained on diverse, large-scale</span>
<span id="cb12-1151"><a href="#cb12-1151"></a>datasets to create reusable, general-purpose representations.</span>
<span id="cb12-1152"><a href="#cb12-1152"></a></span>
<span id="cb12-1153"><a href="#cb12-1153"></a>The core principle behind R3M is its training methodology. It is</span>
<span id="cb12-1154"><a href="#cb12-1154"></a>pre-trained on a wide array of human videos, encompassing various</span>
<span id="cb12-1155"><a href="#cb12-1155"></a>activities and interactions. This diverse dataset enables the model to</span>
<span id="cb12-1156"><a href="#cb12-1156"></a>capture a broad spectrum of physical interactions and dynamics, which</span>
<span id="cb12-1157"><a href="#cb12-1157"></a>are crucial for effective robotic manipulation known as EGO4D</span>
<span id="cb12-1158"><a href="#cb12-1158"></a><span class="co">[</span><span class="ot">@grauman2022ego4d</span><span class="co">]</span>. However, prior papers could not fit this dataset</span>
<span id="cb12-1159"><a href="#cb12-1159"></a>well, and R3M leveraged. The training utilizes a unique objective that</span>
<span id="cb12-1160"><a href="#cb12-1160"></a>combines time contrastive learning, video-language alignment, and a</span>
<span id="cb12-1161"><a href="#cb12-1161"></a>sparsity penalty. This objective ensures that R3M not only understands</span>
<span id="cb12-1162"><a href="#cb12-1162"></a>the temporal dynamics of scenes (i.e., how states transition over time)</span>
<span id="cb12-1163"><a href="#cb12-1163"></a>but also focuses on semantically relevant features, such as objects and</span>
<span id="cb12-1164"><a href="#cb12-1164"></a>their interrelations, while maintaining a compact and efficient</span>
<span id="cb12-1165"><a href="#cb12-1165"></a>representation.</span>
<span id="cb12-1166"><a href="#cb12-1166"></a></span>
<span id="cb12-1167"><a href="#cb12-1167"></a>What sets R3M apart in the realm of robotics is its efficiency and</span>
<span id="cb12-1168"><a href="#cb12-1168"></a>effectiveness in learning from a limited amount of data. The model</span>
<span id="cb12-1169"><a href="#cb12-1169"></a>demonstrates remarkable performance in learning tasks in the real world</span>
<span id="cb12-1170"><a href="#cb12-1170"></a>with minimal human supervision -- typically less than 10 minutes. This</span>
<span id="cb12-1171"><a href="#cb12-1171"></a>is a stark contrast to traditional models that require extensive and</span>
<span id="cb12-1172"><a href="#cb12-1172"></a>often prohibitively large datasets for training. Furthermore, R3M's</span>
<span id="cb12-1173"><a href="#cb12-1173"></a>pre-trained nature allows for its application across a variety of tasks</span>
<span id="cb12-1174"><a href="#cb12-1174"></a>and environments without the need for retraining from scratch, making it</span>
<span id="cb12-1175"><a href="#cb12-1175"></a>a versatile tool in robotic manipulation. The empirical results from</span>
<span id="cb12-1176"><a href="#cb12-1176"></a>using R3M are compelling, leading to a 10% improvement over training</span>
<span id="cb12-1177"><a href="#cb12-1177"></a>from a pretrained image-net model, self-supervised approaches such as</span>
<span id="cb12-1178"><a href="#cb12-1178"></a>MoCo or even CLIP</span>
<span id="cb12-1179"><a href="#cb12-1179"></a><span class="co">[</span><span class="ot">@deng2009imagenet; @he2020momentum; @radford2021learning</span><span class="co">]</span>. Note</span>
<span id="cb12-1180"><a href="#cb12-1180"></a>however, that R3m does **not** use any language data which leaves quite</span>
<span id="cb12-1181"><a href="#cb12-1181"></a>a bit of supervision to be desired.</span>
<span id="cb12-1182"><a href="#cb12-1182"></a></span>
<span id="cb12-1183"><a href="#cb12-1183"></a><span class="fu">#### Voltron: Language Driven Representation Learning for Robotics {.unnumbered}</span></span>
<span id="cb12-1184"><a href="#cb12-1184"></a></span>
<span id="cb12-1185"><a href="#cb12-1185"></a>Building off the success of R3M, Voltron proposes a further extension of</span>
<span id="cb12-1186"><a href="#cb12-1186"></a>leveraging self-supervision and advancements in foundation models, and</span>
<span id="cb12-1187"><a href="#cb12-1187"></a>multi-modality. Voltron takes on an intuitive and simple dual use</span>
<span id="cb12-1188"><a href="#cb12-1188"></a>objective, where the trained model alternates between predicting the</span>
<span id="cb12-1189"><a href="#cb12-1189"></a>task in an image through natural language and classifying images based</span>
<span id="cb12-1190"><a href="#cb12-1190"></a>on a natural text label. This forces a nuanced understanding of both</span>
<span id="cb12-1191"><a href="#cb12-1191"></a>modalities <span class="co">[</span><span class="ot">@radford2021learning</span><span class="co">]</span>.</span>
<span id="cb12-1192"><a href="#cb12-1192"></a></span>
<span id="cb12-1193"><a href="#cb12-1193"></a>Voltron's approach is distinguished by its versatility and depth of</span>
<span id="cb12-1194"><a href="#cb12-1194"></a>learning. It is adept at handling a wide range of robotic tasks, from</span>
<span id="cb12-1195"><a href="#cb12-1195"></a>low-level spatial feature recognition to high-level semantic</span>
<span id="cb12-1196"><a href="#cb12-1196"></a>understanding required in language-conditioned imitation and intent</span>
<span id="cb12-1197"><a href="#cb12-1197"></a>scoring. This flexibility makes it suitable for various applications in</span>
<span id="cb12-1198"><a href="#cb12-1198"></a>robotic manipulation, from grasping objects based on descriptive</span>
<span id="cb12-1199"><a href="#cb12-1199"></a>language to performing complex sequences of actions in response to</span>
<span id="cb12-1200"><a href="#cb12-1200"></a>verbal instructions.</span>
<span id="cb12-1201"><a href="#cb12-1201"></a></span>
<span id="cb12-1202"><a href="#cb12-1202"></a><span class="al">![Voltron pipeline](Figures/voltron.png)</span>{#fig-voltron-pipeline width="95%"}</span>
<span id="cb12-1203"><a href="#cb12-1203"></a></span>
<span id="cb12-1204"><a href="#cb12-1204"></a>The authors rigorously test Voltron in scenarios such as dense</span>
<span id="cb12-1205"><a href="#cb12-1205"></a>segmentation for grasp affordance prediction, object detection in</span>
<span id="cb12-1206"><a href="#cb12-1206"></a>cluttered scenes, and learning multi-task language-conditioned policies</span>
<span id="cb12-1207"><a href="#cb12-1207"></a>for real-world manipulation with up to 15% improvement over baselines.</span>
<span id="cb12-1208"><a href="#cb12-1208"></a>In each of these domains, Voltron has shown a remarkable ability to</span>
<span id="cb12-1209"><a href="#cb12-1209"></a>outperform existing models like MVP and R3M, showcasing its superior</span>
<span id="cb12-1210"><a href="#cb12-1210"></a>adaptability and learning capabilities <span class="co">[</span><span class="ot">@xiao2022masked</span><span class="co">]</span>.</span>
<span id="cb12-1211"><a href="#cb12-1211"></a></span>
<span id="cb12-1212"><a href="#cb12-1212"></a>Moreover, Voltron's framework allows for a balance between encoding</span>
<span id="cb12-1213"><a href="#cb12-1213"></a>low-level and high-level features, which is critical in the context of</span>
<span id="cb12-1214"><a href="#cb12-1214"></a>robotics. This balance enables the model to excel in both control tasks</span>
<span id="cb12-1215"><a href="#cb12-1215"></a>and those requiring deeper semantic understanding, offering a</span>
<span id="cb12-1216"><a href="#cb12-1216"></a>comprehensive solution in the realm of robotic vision and manipulation.</span>
<span id="cb12-1217"><a href="#cb12-1217"></a></span>
<span id="cb12-1218"><a href="#cb12-1218"></a>Voltron stands as a groundbreaking approach in the field of robotics,</span>
<span id="cb12-1219"><a href="#cb12-1219"></a>offering a language-driven, versatile, and efficient approach to</span>
<span id="cb12-1220"><a href="#cb12-1220"></a>learning and manipulation. Its ability to seamlessly integrate visual</span>
<span id="cb12-1221"><a href="#cb12-1221"></a>and linguistic data makes it a potent tool in the ever-evolving</span>
<span id="cb12-1222"><a href="#cb12-1222"></a>landscape of robotic technology, with potential applications that extend</span>
<span id="cb12-1223"><a href="#cb12-1223"></a>far beyond current capabilities. Interesting the authors show Voltron</span>
<span id="cb12-1224"><a href="#cb12-1224"></a>does not beat R3M off the shelf but only when trained on similar amounts</span>
<span id="cb12-1225"><a href="#cb12-1225"></a>of data. Nevertheless, Voltron's success in diverse tasks and</span>
<span id="cb12-1226"><a href="#cb12-1226"></a>environments heralds a new era in robotic manipulation, where language</span>
<span id="cb12-1227"><a href="#cb12-1227"></a>and vision coalesce to create more intelligent, adaptable, and capable</span>
<span id="cb12-1228"><a href="#cb12-1228"></a>robotic systems.</span>
<span id="cb12-1229"><a href="#cb12-1229"></a></span>
<span id="cb12-1230"><a href="#cb12-1230"></a><span class="fu">### Conclusion</span></span>
<span id="cb12-1231"><a href="#cb12-1231"></a></span>
<span id="cb12-1232"><a href="#cb12-1232"></a>On the note of applying active learning to RL and environment settings,</span>
<span id="cb12-1233"><a href="#cb12-1233"></a>there have been many recent papers that have attempted to extend this to</span>
<span id="cb12-1234"><a href="#cb12-1234"></a>more modern RL environments. For example, the paper \"When to Ask for</span>
<span id="cb12-1235"><a href="#cb12-1235"></a>Help\" <span class="co">[</span><span class="ot">@ask_help</span><span class="co">]</span> examines the intersection of autonomous and active</span>
<span id="cb12-1236"><a href="#cb12-1236"></a>learning. Instead of just expecting an RL agent to autonomously solve a</span>
<span id="cb12-1237"><a href="#cb12-1237"></a>task, making the assumption that an agent could get stuck and need human</span>
<span id="cb12-1238"><a href="#cb12-1238"></a>input to get \"unstuck\" is a key insight of the paper. In general,</span>
<span id="cb12-1239"><a href="#cb12-1239"></a>there has been an emphasis in recent literature in robotics on not just</span>
<span id="cb12-1240"><a href="#cb12-1240"></a>blindly using demonstration data as a form of human input, but rather</span>
<span id="cb12-1241"><a href="#cb12-1241"></a>actively querying a human and using this to better synthesize correct</span>
<span id="cb12-1242"><a href="#cb12-1242"></a>actions.</span>
<span id="cb12-1243"><a href="#cb12-1243"></a></span>
<span id="cb12-1244"><a href="#cb12-1244"></a>Active learning holds promise for enhancing AI models in real-world</span>
<span id="cb12-1245"><a href="#cb12-1245"></a>scenarios, yet several challenges persist. This discussion aims to</span>
<span id="cb12-1246"><a href="#cb12-1246"></a>provide an overview of these challenges.</span>
<span id="cb12-1247"><a href="#cb12-1247"></a></span>
<span id="cb12-1248"><a href="#cb12-1248"></a>**Task-Specific Considerations:**</span>
<span id="cb12-1249"><a href="#cb12-1249"></a></span>
<span id="cb12-1250"><a href="#cb12-1250"></a>For certain tasks, the input space of a model may have some rare yet</span>
<span id="cb12-1251"><a href="#cb12-1251"></a>extremely important pockets which may never be discovered by active</span>
<span id="cb12-1252"><a href="#cb12-1252"></a>learning and may cause severe blindspots in the model. In medical</span>
<span id="cb12-1253"><a href="#cb12-1253"></a>imaging for instance, there can be rare yet critical diseases. Designing</span>
<span id="cb12-1254"><a href="#cb12-1254"></a>AL strategies for medical image analysis must prioritize rare classes,</span>
<span id="cb12-1255"><a href="#cb12-1255"></a>such as various forms of cancers. Oftentimes, collecting data around</span>
<span id="cb12-1256"><a href="#cb12-1256"></a>those rare classes is not a recommendation of the active learning</span>
<span id="cb12-1257"><a href="#cb12-1257"></a>process because these examples constitute heavy distribution drifts from</span>
<span id="cb12-1258"><a href="#cb12-1258"></a>the input distribution a model has seen.</span>
<span id="cb12-1259"><a href="#cb12-1259"></a></span>
<span id="cb12-1260"><a href="#cb12-1260"></a>**Complex Task Adaptation:**</span>
<span id="cb12-1261"><a href="#cb12-1261"></a></span>
<span id="cb12-1262"><a href="#cb12-1262"></a>AL has predominantly been adopted for simple classification tasks,</span>
<span id="cb12-1263"><a href="#cb12-1263"></a>leaving more other types of tasks (generative ones for instance), less</span>
<span id="cb12-1264"><a href="#cb12-1264"></a>explored. In Natural Language Processing, tasks like natural language</span>
<span id="cb12-1265"><a href="#cb12-1265"></a>inference, question-answering pose additional complexities that affect</span>
<span id="cb12-1266"><a href="#cb12-1266"></a>the direct application of the active learning process. While machine</span>
<span id="cb12-1267"><a href="#cb12-1267"></a>translation has seen AL applications, generation tasks in NLP require</span>
<span id="cb12-1268"><a href="#cb12-1268"></a>more thorough exploration. Challenges arise in obtaining unlabeled data,</span>
<span id="cb12-1269"><a href="#cb12-1269"></a>particularly for tasks with intricate inputs.</span>
<span id="cb12-1270"><a href="#cb12-1270"></a></span>
<span id="cb12-1271"><a href="#cb12-1271"></a>**Unsupervised and Semi-Supervised Approaches:**</span>
<span id="cb12-1272"><a href="#cb12-1272"></a></span>
<span id="cb12-1273"><a href="#cb12-1273"></a>In the presence of large datasets without sufficient labels,</span>
<span id="cb12-1274"><a href="#cb12-1274"></a>unsupervised and semi-supervised approaches become crucial. These</span>
<span id="cb12-1275"><a href="#cb12-1275"></a>methods offer a means to extract information without relying on labeled</span>
<span id="cb12-1276"><a href="#cb12-1276"></a>data for every data point, potentially revolutionizing fields like</span>
<span id="cb12-1277"><a href="#cb12-1277"></a>medical image analysis. There is an ongoing need for methods that</span>
<span id="cb12-1278"><a href="#cb12-1278"></a>combine self/semi-supervised learning with active learning.</span>
<span id="cb12-1279"><a href="#cb12-1279"></a></span>
<span id="cb12-1280"><a href="#cb12-1280"></a>**Algorithm Scalability:**</span>
<span id="cb12-1281"><a href="#cb12-1281"></a></span>
<span id="cb12-1282"><a href="#cb12-1282"></a>Scalability is a critical concern for online AL algorithms, particularly</span>
<span id="cb12-1283"><a href="#cb12-1283"></a>when dealing with large datasets and high-velocity data streams. The</span>
<span id="cb12-1284"><a href="#cb12-1284"></a>computational demands of AL can become prohibitive as data volume</span>
<span id="cb12-1285"><a href="#cb12-1285"></a>increases, posing challenges for practical deployment. Issues of</span>
<span id="cb12-1286"><a href="#cb12-1286"></a>catastrophic forgetting and model plasticity further complicate</span>
<span id="cb12-1287"><a href="#cb12-1287"></a>scalability, requiring careful consideration in algorithm design.</span>
<span id="cb12-1288"><a href="#cb12-1288"></a></span>
<span id="cb12-1289"><a href="#cb12-1289"></a>**Labeling Quality Assurance:**</span>
<span id="cb12-1290"><a href="#cb12-1290"></a></span>
<span id="cb12-1291"><a href="#cb12-1291"></a>The effectiveness of most online AL strategies hinges on the quality of</span>
<span id="cb12-1292"><a href="#cb12-1292"></a>labeled data. Ensuring labeling accuracy in real-world scenarios is</span>
<span id="cb12-1293"><a href="#cb12-1293"></a>challenging, with human annotators prone to errors, biases, and diverse</span>
<span id="cb12-1294"><a href="#cb12-1294"></a>interpretations. Addressing imperfections in labeling through</span>
<span id="cb12-1295"><a href="#cb12-1295"></a>considerations of oracle imperfections becomes essential in real-life AL</span>
<span id="cb12-1296"><a href="#cb12-1296"></a>applications. Solutions for cleaning up data and verifying its quality</span>
<span id="cb12-1297"><a href="#cb12-1297"></a>need to be more aggressively pursued.</span>
<span id="cb12-1298"><a href="#cb12-1298"></a></span>
<span id="cb12-1299"><a href="#cb12-1299"></a>**Data Drift Challenges:**</span>
<span id="cb12-1300"><a href="#cb12-1300"></a></span>
<span id="cb12-1301"><a href="#cb12-1301"></a>Real-world settings introduce data drift, where distributions shift over</span>
<span id="cb12-1302"><a href="#cb12-1302"></a>time, challenging models to adapt for accurate predictions. These shifts</span>
<span id="cb12-1303"><a href="#cb12-1303"></a>can impact the quality of labeled data acquired in the AL process. For</span>
<span id="cb12-1304"><a href="#cb12-1304"></a>example, the criterion or proxy used for selecting informative instances</span>
<span id="cb12-1305"><a href="#cb12-1305"></a>may be thrown off when the distribution a model is trained on, and the</span>
<span id="cb12-1306"><a href="#cb12-1306"></a>distribution we want it to perform well on, are too far away from one</span>
<span id="cb12-1307"><a href="#cb12-1307"></a>another.</span>
<span id="cb12-1308"><a href="#cb12-1308"></a></span>
<span id="cb12-1309"><a href="#cb12-1309"></a>**Evaluation in Real-Life Scenarios**:</span>
<span id="cb12-1310"><a href="#cb12-1310"></a></span>
<span id="cb12-1311"><a href="#cb12-1311"></a>While AL methods are often evaluated assuming access to ground-truth</span>
<span id="cb12-1312"><a href="#cb12-1312"></a>labels, the real motivation for AL lies in label scarcity. Assessing the</span>
<span id="cb12-1313"><a href="#cb12-1313"></a>effectiveness of AL strategies becomes challenging in real-life</span>
<span id="cb12-1314"><a href="#cb12-1314"></a>scenarios where ground-truth labels may be limited. In other words, one</span>
<span id="cb12-1315"><a href="#cb12-1315"></a>may verify the goodness of an AL algorithm within the lab, but once the</span>
<span id="cb12-1316"><a href="#cb12-1316"></a>algorithm is deployed for improving all sorts of models on all sorts of</span>
<span id="cb12-1317"><a href="#cb12-1317"></a>data distributions, verifying whether AL is actually improving a model</span>
<span id="cb12-1318"><a href="#cb12-1318"></a>is tricky, especially when collecting and labeling data from the target</span>
<span id="cb12-1319"><a href="#cb12-1319"></a>distribution is expensive and defeats the purpose of using AL in the</span>
<span id="cb12-1320"><a href="#cb12-1320"></a>first place.</span>
<span id="cb12-1321"><a href="#cb12-1321"></a></span>
<span id="cb12-1322"><a href="#cb12-1322"></a>By systematically addressing these challenges, the field of active</span>
<span id="cb12-1323"><a href="#cb12-1323"></a>learning in AI can progress towards more effective and practical</span>
<span id="cb12-1324"><a href="#cb12-1324"></a>applications.</span>
<span id="cb12-1325"><a href="#cb12-1325"></a></span>
<span id="cb12-1326"><a href="#cb12-1326"></a>In summary, active learning is a promising modern tool to model training</span>
<span id="cb12-1327"><a href="#cb12-1327"></a>that presents potential benefits. As was mentioned at the start, there</span>
<span id="cb12-1328"><a href="#cb12-1328"></a>are numerous approaches that can be employed by active learning,</span>
<span id="cb12-1329"><a href="#cb12-1329"></a>starting from reducing error of model's prediction, reducing variance,</span>
<span id="cb12-1330"><a href="#cb12-1330"></a>to more conformal predictions. The flavor of active learning heavily</span>
<span id="cb12-1331"><a href="#cb12-1331"></a>depends on the applications, which include robotics, LLM, autonomous</span>
<span id="cb12-1332"><a href="#cb12-1332"></a>vehicles, and more. We discussed in more detail how to perform active</span>
<span id="cb12-1333"><a href="#cb12-1333"></a>learning for variance reduction in the case of predicting kinematics of</span>
<span id="cb12-1334"><a href="#cb12-1334"></a>the robotic arms, which showed decrease in MSE as well as more stable</span>
<span id="cb12-1335"><a href="#cb12-1335"></a>reduction in it. Next we talked about using active learning for reducing</span>
<span id="cb12-1336"><a href="#cb12-1336"></a>the number of comparisons required to create a ranking of objects, and</span>
<span id="cb12-1337"><a href="#cb12-1337"></a>the examples discussed were able to achieve that but with some loss in</span>
<span id="cb12-1338"><a href="#cb12-1338"></a>the prediction accuracy. Finally, we discussed how active learning can</span>
<span id="cb12-1339"><a href="#cb12-1339"></a>be used for modeling of reward functions within a dynamical system,</span>
<span id="cb12-1340"><a href="#cb12-1340"></a>which demonstrated improvements in performance and time required to</span>
<span id="cb12-1341"><a href="#cb12-1341"></a>achieve it. For a more hands-on experience with active learning and</span>
<span id="cb12-1342"><a href="#cb12-1342"></a>demonstrated example, we encourage the readers to explore a blogpost by</span>
<span id="cb12-1343"><a href="#cb12-1343"></a>Max Halford <span class="co">[</span><span class="ot">@max_halford</span><span class="co">]</span>.</span>
<span id="cb12-1344"><a href="#cb12-1344"></a></span>
<span id="cb12-1345"><a href="#cb12-1345"></a></span>
<span id="cb12-1346"><a href="#cb12-1346"></a><span class="fu">## Metric Elicitation {#sec-metric-elicitation}</span></span>
<span id="cb12-1347"><a href="#cb12-1347"></a></span>
<span id="cb12-1348"><a href="#cb12-1348"></a><span class="fu">### Introduction to Performance Metric Elicitation</span></span>
<span id="cb12-1349"><a href="#cb12-1349"></a></span>
<span id="cb12-1350"><a href="#cb12-1350"></a>In binary classification problems, selecting an appropriate performance metric that aligns with the real-world </span>
<span id="cb12-1351"><a href="#cb12-1351"></a>task is crucial. The problem of *metric elicitation* aims to characterize and discover the performance metric of a </span>
<span id="cb12-1352"><a href="#cb12-1352"></a>practitioner, reflecting the rewards or costs associated with correct or incorrect classification. For instance, </span>
<span id="cb12-1353"><a href="#cb12-1353"></a>in medical contexts such as diagnosing a disease or determining the appropriateness of a treatment, trade-offs are </span>
<span id="cb12-1354"><a href="#cb12-1354"></a>made for incorrect decisions. Not administering a treatment could lead to the worsening of a disease (a false negative), </span>
<span id="cb12-1355"><a href="#cb12-1355"></a>whereas delivering the wrong treatment could cause adverse side effects worse than not treating the condition (a false positive).</span>
<span id="cb12-1356"><a href="#cb12-1356"></a></span>
<span id="cb12-1357"><a href="#cb12-1357"></a>Rather than choosing from a limited set of default choices like the F1-score or weighted accuracy, metric elicitation </span>
<span id="cb12-1358"><a href="#cb12-1358"></a>considers the process of devising a metric that best matches the preferences of practitioners or users. This is </span>
<span id="cb12-1359"><a href="#cb12-1359"></a>achieved by querying an "oracle" who provides feedback on proposed potential metrics through pairwise comparisons. </span>
<span id="cb12-1360"><a href="#cb12-1360"></a>Since queries to humans are often expensive, the goal is to minimize the number of comparisons needed.</span>
<span id="cb12-1361"><a href="#cb12-1361"></a></span>
<span id="cb12-1362"><a href="#cb12-1362"></a>**Note:** Contents in this section are derived from "Performance Metric Elicitation from Pairwise </span>
<span id="cb12-1363"><a href="#cb12-1363"></a>Classifier Comparisons" by <span class="co">[</span><span class="ot">@pmlr-v89-hiranandani19a</span><span class="co">]</span>, which introduced the problem of metric </span>
<span id="cb12-1364"><a href="#cb12-1364"></a>elicitation and the framework for binary-class metric elicitation from pairwise comparisons. This section aims to </span>
<span id="cb12-1365"><a href="#cb12-1365"></a>present their work expository while providing additional motivation and intuitive explanations to supplement their work.</span>
<span id="cb12-1366"><a href="#cb12-1366"></a></span>
<span id="cb12-1367"><a href="#cb12-1367"></a>The motivation for the pairwise comparison aspect of metric elicitation stems from a rich history of literature in </span>
<span id="cb12-1368"><a href="#cb12-1368"></a>psychology, economics, and computer science <span class="co">[</span><span class="ot">@pref1; @pref2; @pref3; @pref4; @ab</span><span class="co">]</span>, demonstrating that humans are often </span>
<span id="cb12-1369"><a href="#cb12-1369"></a>ineffective at providing absolute feedback on aspects such as potential prices, user interfaces, or even ML model </span>
<span id="cb12-1370"><a href="#cb12-1370"></a>outputs (hence the comparison-based structure of RLHF, for instance). Additionally, confusion matrices accurately </span>
<span id="cb12-1371"><a href="#cb12-1371"></a>capture binary metrics such as accuracy, $F_\beta$, and Jaccard similarity by recording the number of false positives, </span>
<span id="cb12-1372"><a href="#cb12-1372"></a>true positives, false negatives, and true negatives obtained by a classifier. The main goal of this chapter is to </span>
<span id="cb12-1373"><a href="#cb12-1373"></a>introduce two binary-search procedures that can approximate the oracle's performance metric for two types of metrics </span>
<span id="cb12-1374"><a href="#cb12-1374"></a>(linear and linear-fractional performance metrics) by presenting the oracle with confusion matrices generated by </span>
<span id="cb12-1375"><a href="#cb12-1375"></a>various classifiers. Essentially, we are learning an optimal threshold for classification given a decision boundary </span>
<span id="cb12-1376"><a href="#cb12-1376"></a>for a binary classification problem.</span>
<span id="cb12-1377"><a href="#cb12-1377"></a></span>
<span id="cb12-1378"><a href="#cb12-1378"></a>First, we introduce some relevant notation that will later be used to formalize notions of oracle queries, </span>
<span id="cb12-1379"><a href="#cb12-1379"></a>classifiers, and metrics. In this context, $X \in \mathcal{X}$ represents an input random variable, while </span>
<span id="cb12-1380"><a href="#cb12-1380"></a>$Y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ denotes the output random variable. We learn from a dataset of size $n$, denoted by </span>
<span id="cb12-1381"><a href="#cb12-1381"></a>$<span class="sc">\{</span>(x, y)_i\}^n_{i=1}$, which is generated independently and identically distributed (i.i.d.) from some </span>
<span id="cb12-1382"><a href="#cb12-1382"></a>distribution $\mathbb{P}(X, Y)$. The conditional probability of the positive class, given some sample $x$, </span>
<span id="cb12-1383"><a href="#cb12-1383"></a>is denoted by $\eta(\vec{x}) = \mathbb{P}(Y=1 | X=x)$. The marginal probability of the positive class </span>
<span id="cb12-1384"><a href="#cb12-1384"></a>is represented by $\zeta = \mathbb{P}(Y=1)$. </span>
<span id="cb12-1385"><a href="#cb12-1385"></a></span>
<span id="cb12-1386"><a href="#cb12-1386"></a>The set of all potential classifiers is $\mathcal{H} = <span class="sc">\{</span>h : \mathcal{X} \rightarrow <span class="sc">\{</span>0,1<span class="sc">\}\}</span>$. The confusion </span>
<span id="cb12-1387"><a href="#cb12-1387"></a>matrix for a classifier $h$ is $C(h, \mathbb{P}) \in \mathbb{R}^{2 \times 2}$, where $C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j)$ </span>
<span id="cb12-1388"><a href="#cb12-1388"></a>for $i, j \in <span class="sc">\{</span>0,1<span class="sc">\}</span>$. These entries represent the false positives, true positives, false negatives, and true negatives, </span>
<span id="cb12-1389"><a href="#cb12-1389"></a>ensuring that $\sum_{i,j}C_{ij}=1$. The set of all confusion matrices is denoted by $\mathcal{C}$. Since </span>
<span id="cb12-1390"><a href="#cb12-1390"></a>$FN(h, \mathbb{P}) = \zeta - TP(h, \mathbb{P})$ and $FP(h, \mathbb{P}) = 1 - \zeta - TN(h, \mathbb{P})$, $\mathcal{C}$ </span>
<span id="cb12-1391"><a href="#cb12-1391"></a>is actually a 2-dimensional space, not a 4-dimensional space.</span>
<span id="cb12-1392"><a href="#cb12-1392"></a></span>
<span id="cb12-1393"><a href="#cb12-1393"></a>Any hyperplane in the $(tp, tn)$ space is given by $\ell := a \cdot tp + b \cdot tn = c$, where $a, b, c \in \mathbb{R}$. </span>
<span id="cb12-1394"><a href="#cb12-1394"></a>Given a classifier $h$, we define a performance metric $\phi : <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^{2 \times 2} \rightarrow \mathbb{R}$. The value </span>
<span id="cb12-1395"><a href="#cb12-1395"></a>$\phi(C(h))$, which represents the performance of a classifier with respect to a certain metric, is referred to as the </span>
<span id="cb12-1396"><a href="#cb12-1396"></a>*utility* of the classifier $h$. We assume, without loss of generality, that a higher value of $\phi$ indicates a better </span>
<span id="cb12-1397"><a href="#cb12-1397"></a>performance metric for $h$. Our focus is to recover some metric $\phi$ using comparisons between confusion matrices $C(h)$, </span>
<span id="cb12-1398"><a href="#cb12-1398"></a>determined by classifiers $h$, which approximates the oracle's "ground-truth" metric $\phi^*$.</span>
<span id="cb12-1399"><a href="#cb12-1399"></a></span>
<span id="cb12-1400"><a href="#cb12-1400"></a>Next, we introduce two classes of performance metrics—*Linear Performance Metrics (LPM)* and *Linear-Fractional </span>
<span id="cb12-1401"><a href="#cb12-1401"></a>Performance Metrics (LFPM)*—for which we will present two elicitation algorithms. </span>
<span id="cb12-1402"><a href="#cb12-1402"></a></span>
<span id="cb12-1403"><a href="#cb12-1403"></a>An *LPM*, given constants $<span class="sc">\{</span>a_{11}, a_{01}, a_{10}, a_{00}<span class="sc">\}</span> \in \mathbb{R}^{4}$, is defined as:</span>
<span id="cb12-1404"><a href="#cb12-1404"></a></span>
<span id="cb12-1405"><a href="#cb12-1405"></a>$$\begin{aligned}</span>
<span id="cb12-1406"><a href="#cb12-1406"></a>\phi(C) &amp;= a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN<span class="sc">\\</span></span>
<span id="cb12-1407"><a href="#cb12-1407"></a>&amp;= m_{11} TP + m_{00} TN + m_{0},</span>
<span id="cb12-1408"><a href="#cb12-1408"></a>\end{aligned}$$ {#eq-eqlpm}</span>
<span id="cb12-1409"><a href="#cb12-1409"></a></span>
<span id="cb12-1410"><a href="#cb12-1410"></a>where $m_{11} = (a_{11} - a_{10})$, $m_{00} = (a_{00} - a_{01})$, and $m_{0} = a_{10} \zeta + a_{01} (1 - \zeta)$. </span>
<span id="cb12-1411"><a href="#cb12-1411"></a>This reparametrization simplifies the metric by reducing dimensionality, making it more tractable for elicitation. </span>
<span id="cb12-1412"><a href="#cb12-1412"></a>One example of an LPM is *weighted accuracy*, defined as $WA = w_1TP + w_2TN$, where adjusting $w_1$ and $w_2$ controls </span>
<span id="cb12-1413"><a href="#cb12-1413"></a>the relative importance of different types of misclassification.</span>
<span id="cb12-1414"><a href="#cb12-1414"></a></span>
<span id="cb12-1415"><a href="#cb12-1415"></a>An *LFPM*, defined by constants $<span class="sc">\{</span>a_{11}, a_{01}, a_{10}, a_{00}, b_{11}, b_{01}, b_{10}, b_{00}<span class="sc">\}</span> \in \mathbb{R}^{8}$, is given by:</span>
<span id="cb12-1416"><a href="#cb12-1416"></a></span>
<span id="cb12-1417"><a href="#cb12-1417"></a>$$\begin{aligned}</span>
<span id="cb12-1418"><a href="#cb12-1418"></a>\phi(C) &amp;= \frac{a_{11} TP + a_{01} FP + a_{10} FN + a_{00} TN}{b_{11} TP + b_{01} FP + b_{10} FN + b_{00} TN}<span class="sc">\\</span></span>
<span id="cb12-1419"><a href="#cb12-1419"></a>&amp;= \frac{p_{11} TP + p_{00} TN + p_{0}}{q_{11} TP + q_{00} TN + q_{0}},</span>
<span id="cb12-1420"><a href="#cb12-1420"></a>\end{aligned}$$ {#eq-eqlfpm}</span>
<span id="cb12-1421"><a href="#cb12-1421"></a></span>
<span id="cb12-1422"><a href="#cb12-1422"></a>where $p_{11} = (a_{11} - a_{10})$, $p_{00} = (a_{00} - a_{01})$, $q_{11} = (b_{11} - b_{10})$, $q_{00} = (b_{00} - b_{01})$, </span>
<span id="cb12-1423"><a href="#cb12-1423"></a>$p_{0} = a_{10} \zeta + a_{01} (1 - \zeta)$, and $q_{0} = b_{10} \zeta + b_{01} (1 - \zeta)$. This parametrization also </span>
<span id="cb12-1424"><a href="#cb12-1424"></a>simplifies the elicitation process by reducing the number of variables. Common LFPMs include the $F_\beta$ score and Jaccard </span>
<span id="cb12-1425"><a href="#cb12-1425"></a>similarity, defined as:</span>
<span id="cb12-1426"><a href="#cb12-1426"></a></span>
<span id="cb12-1427"><a href="#cb12-1427"></a>$$F_{\beta} = \frac{TP}{\frac{TP}{1+\beta^{2}} - \frac{TN}{1+\beta^{2}} + \frac{\beta^{2} \zeta + 1 - \zeta}{1+\beta^{2}}}, \quad JAC = \frac{TP}{1 - TN}.$$ {#eq-lfpm_metrics}</span>
<span id="cb12-1428"><a href="#cb12-1428"></a></span>
<span id="cb12-1429"><a href="#cb12-1429"></a>Setting $\beta = 1$ gives the F1 score, which is widely used as a classification metric in machine learning.</span>
<span id="cb12-1430"><a href="#cb12-1430"></a></span>
<span id="cb12-1431"><a href="#cb12-1431"></a><span class="fu">### Preliminaries {#sec-me-preliminaries}</span></span>
<span id="cb12-1432"><a href="#cb12-1432"></a><span class="fu">#### Confusion Matrices {#sec-confusion-matrices .unnumbered}</span></span>
<span id="cb12-1433"><a href="#cb12-1433"></a></span>
<span id="cb12-1434"><a href="#cb12-1434"></a>Since we are considering all possible metrics in the LPM and LFPM families, we need to make certain assumptions </span>
<span id="cb12-1435"><a href="#cb12-1435"></a>about $\mathcal{C}$. Particularly, we will assume that $g(t) = \mathbb{P}<span class="co">[</span><span class="ot">\eta(X) \geq t</span><span class="co">]</span>$ is continuous and </span>
<span id="cb12-1436"><a href="#cb12-1436"></a>strictly decreasing for $t \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$; essentially, $\eta$ has positive density and zero probability.</span>
<span id="cb12-1437"><a href="#cb12-1437"></a></span>
<span id="cb12-1438"><a href="#cb12-1438"></a>Additionally, $\mathcal{C}$ is convex, closed, and contained within the rectangle $<span class="co">[</span><span class="ot">0, \zeta</span><span class="co">]</span> \times <span class="co">[</span><span class="ot">0, 1-\zeta</span><span class="co">]</span>$, </span>
<span id="cb12-1439"><a href="#cb12-1439"></a>and is rotationally symmetric around its center, $(\frac{\zeta}{2}, \frac{1-\zeta}{2})$, where the axes represent the </span>
<span id="cb12-1440"><a href="#cb12-1440"></a>proportion of true positives and negatives. The only vertices of $\mathcal{C}$ are $(0, 1-\zeta)$ and $(\zeta, 0)$, </span>
<span id="cb12-1441"><a href="#cb12-1441"></a>corresponding to predicting all $0$'s or all $1$'s on a given dataset. Therefore, $\mathcal{C}$ is strictly convex, </span>
<span id="cb12-1442"><a href="#cb12-1442"></a>and any line tangent to it is tangent at exactly one point, corresponding to one particular confusion matrix; these </span>
<span id="cb12-1443"><a href="#cb12-1443"></a>properties can be visually observed in @fig-c.</span>
<span id="cb12-1444"><a href="#cb12-1444"></a></span>
<span id="cb12-1445"><a href="#cb12-1445"></a><span class="al">![Visual representation of $\mathcal{C}$](Figures/Screenshot 2023-11-13 at 6.56.44 PM.png)</span>{#fig-c width="50%"}</span>
<span id="cb12-1446"><a href="#cb12-1446"></a></span>
<span id="cb12-1447"><a href="#cb12-1447"></a>Next, recall that an LPM is represented in terms of three parameters ($\phi = m_{11}TP + m_{00}TN + m_0$). We have </span>
<span id="cb12-1448"><a href="#cb12-1448"></a>just seen that this LPM and its corresponding confusion matrix correspond to a certain point on the boundary of </span>
<span id="cb12-1449"><a href="#cb12-1449"></a>$\mathcal{C}$. We first note that this point is independent of $m_0$. Additionally, we only care about the relative </span>
<span id="cb12-1450"><a href="#cb12-1450"></a>weightings of $m_{11}$ and $m_{00}$, not their actual values—they are scale invariant. Therefore, we can parametrize </span>
<span id="cb12-1451"><a href="#cb12-1451"></a>the space of LPMs as $\varphi_{LPM} = <span class="sc">\{</span>\mathbf{m} = (\cos \theta, \sin \theta) : \theta \in <span class="co">[</span><span class="ot">0, 2\pi</span><span class="co">]</span><span class="sc">\}</span>$, where </span>
<span id="cb12-1452"><a href="#cb12-1452"></a>$\cos \theta$ corresponds to $m_{00}$ and $\sin \theta$ corresponds to $m_{11}$. As we already know, we can recover </span>
<span id="cb12-1453"><a href="#cb12-1453"></a>the Bayes classifier given $\mathbf{m}$, and it is unique, corresponding to one point on the boundary of $\mathcal{C}$ </span>
<span id="cb12-1454"><a href="#cb12-1454"></a>due to its convexity. The supporting hyperplane at this point is defined as</span>
<span id="cb12-1455"><a href="#cb12-1455"></a></span>
<span id="cb12-1456"><a href="#cb12-1456"></a>$$\bar{\ell}_{\mathbf{m}} := m_{11} \cdot tp + m_{00} \cdot tn = m_{11} \overline{TP}_{\mathbf{m}} + m_{00} \overline{TN}_{\mathbf{m}}$$  {#eq-eq3.47}</span>
<span id="cb12-1457"><a href="#cb12-1457"></a></span>
<span id="cb12-1458"><a href="#cb12-1458"></a>We note that if $m_{00}$ and $m_{11}$ have opposite signs, then $\bar{h}_m$ is the trivial classifier predicting </span>
<span id="cb12-1459"><a href="#cb12-1459"></a>all 1's or all 0's, since either predicting true positives or true negatives results in negative reward. This corresponds </span>
<span id="cb12-1460"><a href="#cb12-1460"></a>to a supporting hyperplane with a positive slope, so it can only be tangent at the vertices.</span>
<span id="cb12-1461"><a href="#cb12-1461"></a></span>
<span id="cb12-1462"><a href="#cb12-1462"></a>Additionally, the boundary $\partial \mathcal{C}$ can be split into upper and lower boundaries </span>
<span id="cb12-1463"><a href="#cb12-1463"></a>($\partial \mathcal{C}_{+}, \partial \mathcal{C}_{-}$), corresponding to $\theta \in (0, \pi/2)$ and $\theta \in (\pi, 3\pi/2)$ </span>
<span id="cb12-1464"><a href="#cb12-1464"></a>respectively (and whether $m_{00}, m_{11}$ are positive or negative).</span>
<span id="cb12-1465"><a href="#cb12-1465"></a></span>
<span id="cb12-1466"><a href="#cb12-1466"></a><span class="fu">#### Bayes Optimal and Inverse-Optimal Classifiers {.unnumbered}</span></span>
<span id="cb12-1467"><a href="#cb12-1467"></a></span>
<span id="cb12-1468"><a href="#cb12-1468"></a>We also define the notions of Bayes optimal and inverse-optimal classifiers. Given a performance metric $\phi$, we define:</span>
<span id="cb12-1469"><a href="#cb12-1469"></a></span>
<span id="cb12-1470"><a href="#cb12-1470"></a><span class="ss">-   </span>The *Bayes utility* as $\bar{\tau} := \sup_{h \in \mathcal{H}} \phi(C(h)) = \sup_{C \in \mathcal{C}} \phi(C)$; </span>
<span id="cb12-1471"><a href="#cb12-1471"></a>    this is the highest achievable utility (using the metric $\phi$) over all classifiers $h \in $\mathcal{H}$ for a given problem.</span>
<span id="cb12-1472"><a href="#cb12-1472"></a><span class="ss">-   </span>The *Bayes classifier* as $\bar{h} := \arg \max_{h \in \mathcal{H}} \phi(C(h))$; this is the classifier $h$ corresponding to the Bayes utility.</span>
<span id="cb12-1473"><a href="#cb12-1473"></a><span class="ss">-   </span>The *Bayes confusion matrix* as $\bar{C} := \arg \max_{C \in \mathcal{C}} \phi(C)$; this is the confusion matrix </span>
<span id="cb12-1474"><a href="#cb12-1474"></a>    corresponding to the Bayes utility and classifier.</span>
<span id="cb12-1475"><a href="#cb12-1475"></a></span>
<span id="cb12-1476"><a href="#cb12-1476"></a>Similarly, the inverse Bayes utility, classifier, and confusion matrix can be defined by replacing "$\sup$" with "$\inf$"; </span>
<span id="cb12-1477"><a href="#cb12-1477"></a>they represent the classifier and confusion matrix corresponding to the lower bound on utility for a given problem.</span>
<span id="cb12-1478"><a href="#cb12-1478"></a></span>
<span id="cb12-1479"><a href="#cb12-1479"></a>We also have the following useful proposition:</span>
<span id="cb12-1480"><a href="#cb12-1480"></a></span>
<span id="cb12-1481"><a href="#cb12-1481"></a>::: {.callout-note title="proposition"}</span>
<span id="cb12-1482"><a href="#cb12-1482"></a>::: {#prp-prp3.1}</span>
<span id="cb12-1483"><a href="#cb12-1483"></a>Let $\phi \in \varphi_{LPM}$. Then</span>
<span id="cb12-1484"><a href="#cb12-1484"></a></span>
<span id="cb12-1485"><a href="#cb12-1485"></a>::: {.content-visible when-format="html"}</span>
<span id="cb12-1486"><a href="#cb12-1486"></a>$$\bar{h}(x) = \left<span class="sc">\{</span>\begin{array}{lr}</span>
<span id="cb12-1487"><a href="#cb12-1487"></a>\unicode{x1D7D9} \left<span class="co">[</span><span class="ot">\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right</span><span class="co">]</span>, &amp; m_{11} + m_{00} \geq 0 <span class="sc">\\</span></span>
<span id="cb12-1488"><a href="#cb12-1488"></a>\unicode{x1D7D9} \left<span class="co">[</span><span class="ot">\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right</span><span class="co">]</span>, &amp; \text { o.w. }</span>
<span id="cb12-1489"><a href="#cb12-1489"></a>\end{array}\right<span class="sc">\}</span>$$  {#eq-eq3.45}</span>
<span id="cb12-1490"><a href="#cb12-1490"></a>:::</span>
<span id="cb12-1491"><a href="#cb12-1491"></a></span>
<span id="cb12-1492"><a href="#cb12-1492"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb12-1493"><a href="#cb12-1493"></a>$$\bar{h}(x) = \left<span class="sc">\{</span>\begin{array}{lr}</span>
<span id="cb12-1494"><a href="#cb12-1494"></a>\mathbbm{1}\left<span class="co">[</span><span class="ot">\eta(x) \geq \frac{m_{00}}{m_{11} + m_{00}}\right</span><span class="co">]</span>, &amp; m_{11} + m_{00} \geq 0 <span class="sc">\\</span></span>
<span id="cb12-1495"><a href="#cb12-1495"></a>\mathbbm{1}\left<span class="co">[</span><span class="ot">\frac{m_{00}}{m_{11} + m_{00}} \geq \eta(x)\right</span><span class="co">]</span>, &amp; \text { o.w. }</span>
<span id="cb12-1496"><a href="#cb12-1496"></a>\end{array}\right<span class="sc">\}</span>$$  {#eq-eq3.46}</span>
<span id="cb12-1497"><a href="#cb12-1497"></a>:::</span>
<span id="cb12-1498"><a href="#cb12-1498"></a></span>
<span id="cb12-1499"><a href="#cb12-1499"></a>is a Bayes optimal classifier with respect to $\phi$. The inverse Bayes classifier is given by $\underline{h} = 1 - \bar{h}$.</span>
<span id="cb12-1500"><a href="#cb12-1500"></a>:::</span>
<span id="cb12-1501"><a href="#cb12-1501"></a>:::</span>
<span id="cb12-1502"><a href="#cb12-1502"></a></span>
<span id="cb12-1503"><a href="#cb12-1503"></a>This is a simple derivation based on the fact that we only get rewards from true positives and true negatives. </span>
<span id="cb12-1504"><a href="#cb12-1504"></a>Essentially, if we recover an LPM, we can use it to determine the best-performing classifier, obtained by placing </span>
<span id="cb12-1505"><a href="#cb12-1505"></a>a threshold on the conditional probability of a given sample, that corresponds to a confusion matrix. Therefore, </span>
<span id="cb12-1506"><a href="#cb12-1506"></a>the three notions of Bayes utility, classifier, and confusion matrix are functionally equivalent in our setting.</span>
<span id="cb12-1507"><a href="#cb12-1507"></a></span>
<span id="cb12-1508"><a href="#cb12-1508"></a></span>
<span id="cb12-1509"><a href="#cb12-1509"></a><span class="fu">### Problem Setup {#sec-metric-elicitation-setup}</span></span>
<span id="cb12-1510"><a href="#cb12-1510"></a></span>
<span id="cb12-1511"><a href="#cb12-1511"></a>We will now formalize the problem of metric elicitation. Given two classifiers $h$ and $h'$ (or equivalently, </span>
<span id="cb12-1512"><a href="#cb12-1512"></a>two confusion matrices $C$ and $C'$), we define an *oracle query* as the function:</span>
<span id="cb12-1513"><a href="#cb12-1513"></a></span>
<span id="cb12-1514"><a href="#cb12-1514"></a>::: {.content-visible when-format="html"}</span>
<span id="cb12-1515"><a href="#cb12-1515"></a>$$\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\unicode{x1D7D9}\left<span class="co">[</span><span class="ot">\phi(C)&gt;\phi\left(C^{\prime}\right)\right</span><span class="co">]</span>=: \unicode{x1D7D9} \left<span class="co">[</span><span class="ot">C \succ C^{\prime}\right</span><span class="co">]</span>,$$ {#eq-oracle}</span>
<span id="cb12-1516"><a href="#cb12-1516"></a>:::</span>
<span id="cb12-1517"><a href="#cb12-1517"></a></span>
<span id="cb12-1518"><a href="#cb12-1518"></a>::: {.content-visible when-format="pdf"}</span>
<span id="cb12-1519"><a href="#cb12-1519"></a>$$\Gamma\left(h, h^{\prime}\right)=\Omega\left(C, C^{\prime}\right)=\mathbbm{1}\left<span class="co">[</span><span class="ot">\phi(C)&gt;\phi\left(C^{\prime}\right)\right</span><span class="co">]</span>=: \mathbbm{1} \left<span class="co">[</span><span class="ot">C \succ C^{\prime}\right</span><span class="co">]</span>,$$ {#eq-oracle}</span>
<span id="cb12-1520"><a href="#cb12-1520"></a>:::</span>
<span id="cb12-1521"><a href="#cb12-1521"></a></span>
<span id="cb12-1522"><a href="#cb12-1522"></a>which represents the classifier preferred by the practitioner. We can then define the metric elicitation problem for populations:</span>
<span id="cb12-1523"><a href="#cb12-1523"></a></span>
<span id="cb12-1524"><a href="#cb12-1524"></a>::: {.callout-note title="definition"}</span>
<span id="cb12-1525"><a href="#cb12-1525"></a>::: {#def-def3.1}</span>
<span id="cb12-1526"><a href="#cb12-1526"></a>Suppose the true (oracle) performance metric is $\phi$. The goal is to recover a metric $\hat{\phi}$ by </span>
<span id="cb12-1527"><a href="#cb12-1527"></a>querying the oracle for as few pairwise comparisons of the form $\Omega\left(C, C^{\prime}\right)$ so that </span>
<span id="cb12-1528"><a href="#cb12-1528"></a>$\|\phi - \hat{\phi}\|_{--} &lt; \kappa$ for a sufficiently small $\kappa &gt; 0$ and for any suitable norm $\|\cdot\|_{--}$.</span>
<span id="cb12-1529"><a href="#cb12-1529"></a>:::</span>
<span id="cb12-1530"><a href="#cb12-1530"></a>:::</span>
<span id="cb12-1531"><a href="#cb12-1531"></a></span>
<span id="cb12-1532"><a href="#cb12-1532"></a>In practice, we do not have access to the true probability distribution or the population, which would </span>
<span id="cb12-1533"><a href="#cb12-1533"></a>provide the true values of $C$ and $C'$. However, we can subtly alter this problem description to use </span>
<span id="cb12-1534"><a href="#cb12-1534"></a>$\hat{C}$ and $\hat{C}^{\prime}$, which are derived from our dataset of $n$ samples:</span>
<span id="cb12-1535"><a href="#cb12-1535"></a></span>
<span id="cb12-1536"><a href="#cb12-1536"></a>::: {.callout-note title="definition"}</span>
<span id="cb12-1537"><a href="#cb12-1537"></a>::: {#def-def3.2}</span>
<span id="cb12-1538"><a href="#cb12-1538"></a>Suppose the true (oracle) performance metric is $\phi$. The aim is to recover a metric $\hat{\phi}$ </span>
<span id="cb12-1539"><a href="#cb12-1539"></a>by querying the oracle for as few pairwise comparisons of the form $\Omega\left(\hat{C}, \hat{C}^{\prime}\right)$ </span>
<span id="cb12-1540"><a href="#cb12-1540"></a>so that $\|\phi - \hat{\phi}\|_{--} &lt; \kappa$ for a sufficiently small $\kappa &gt; 0$ and for any suitable norm $\|\cdot\|_{--}$.</span>
<span id="cb12-1541"><a href="#cb12-1541"></a>:::</span>
<span id="cb12-1542"><a href="#cb12-1542"></a>:::</span>
<span id="cb12-1543"><a href="#cb12-1543"></a></span>
<span id="cb12-1544"><a href="#cb12-1544"></a>As is common in theoretical ML research, we solve the population problem and then consider ways to extend this to </span>
<span id="cb12-1545"><a href="#cb12-1545"></a>practical settings where we only have limited datasets of samples. In our case, this corresponds to calculating the </span>
<span id="cb12-1546"><a href="#cb12-1546"></a>confusion matrices from a portion of the dataset we have access to.</span>
<span id="cb12-1547"><a href="#cb12-1547"></a></span>
<span id="cb12-1548"><a href="#cb12-1548"></a></span>
<span id="cb12-1549"><a href="#cb12-1549"></a><span class="fu">### Linear Performance Metric Elicitation {#sec-orgb6dac4e}</span></span>
<span id="cb12-1550"><a href="#cb12-1550"></a></span>
<span id="cb12-1551"><a href="#cb12-1551"></a>For LPM elicitation, we need one more proposition.</span>
<span id="cb12-1552"><a href="#cb12-1552"></a></span>
<span id="cb12-1553"><a href="#cb12-1553"></a>::: {.callout-note title="proposition"}</span>
<span id="cb12-1554"><a href="#cb12-1554"></a>::: {#prp-prp3.2}</span>
<span id="cb12-1555"><a href="#cb12-1555"></a>For a metric $\psi$ (quasiconvex and monotone increasing in TP/TN) or</span>
<span id="cb12-1556"><a href="#cb12-1556"></a>$\phi$ (quasiconcave and monotone increasing), and parametrization</span>
<span id="cb12-1557"><a href="#cb12-1557"></a>$\rho^+$/$\rho^-$ of upper/lower boundary, composition</span>
<span id="cb12-1558"><a href="#cb12-1558"></a>$\psi \circ \rho^-$ is quasiconvex and unimodal on <span class="sc">\[</span>0, 1<span class="sc">\]</span>, and</span>
<span id="cb12-1559"><a href="#cb12-1559"></a>$\phi \circ \rho^+$ is quasiconcave and unimodal on <span class="sc">\[</span>0, 1<span class="sc">\]</span>.</span>
<span id="cb12-1560"><a href="#cb12-1560"></a>:::</span>
<span id="cb12-1561"><a href="#cb12-1561"></a>:::</span>
<span id="cb12-1562"><a href="#cb12-1562"></a></span>
<span id="cb12-1563"><a href="#cb12-1563"></a>Quasiconcavity and quasiconvexity are slightly more general variations</span>
<span id="cb12-1564"><a href="#cb12-1564"></a>on concavity and convexity. Their main useful property in our setting is</span>
<span id="cb12-1565"><a href="#cb12-1565"></a>that they are unimodal (they have a singular extremum), so we can devise</span>
<span id="cb12-1566"><a href="#cb12-1566"></a>a binary-search-style algorithm for eliciting the Bayes optimal and</span>
<span id="cb12-1567"><a href="#cb12-1567"></a>inverse-optimal confusion matrices for a given setting, as well as the</span>
<span id="cb12-1568"><a href="#cb12-1568"></a>corresponding $\phi$'s.</span>
<span id="cb12-1569"><a href="#cb12-1569"></a></span>
<span id="cb12-1570"><a href="#cb12-1570"></a>We first note that to maximize a quasiconcave metric, in which $\phi$ is</span>
<span id="cb12-1571"><a href="#cb12-1571"></a>monotonically increasing in $TP$ and $TN$, we note that the resulting</span>
<span id="cb12-1572"><a href="#cb12-1572"></a>maximizer (and supporting hyperplane) will occur on the upper boundary</span>
<span id="cb12-1573"><a href="#cb12-1573"></a>of $\mathcal{C}$. We thus set our initial search range to be</span>
<span id="cb12-1574"><a href="#cb12-1574"></a>$<span class="co">[</span><span class="ot">0, \pi/2</span><span class="co">]</span>$ and repeatedly divide it into four regions. Then, we</span>
<span id="cb12-1575"><a href="#cb12-1575"></a>calculate the resulting confusion matrix on the 5 resulting boundaries of these</span>
<span id="cb12-1576"><a href="#cb12-1576"></a>regions and query the oracle $4$ times. We repeat this in each iteration</span>
<span id="cb12-1577"><a href="#cb12-1577"></a>of the binary search until a maximizer is found.</span>
<span id="cb12-1578"><a href="#cb12-1578"></a></span>
<span id="cb12-1579"><a href="#cb12-1579"></a>::: {.callout-note title="remark"}</span>
<span id="cb12-1580"><a href="#cb12-1580"></a>::: {#rem-explaination_binary_search}</span>
<span id="cb12-1581"><a href="#cb12-1581"></a>In the case of quasiconcave and quasiconvex search ranges, a slightly more sophisticated variation on typical </span>
<span id="cb12-1582"><a href="#cb12-1582"></a>binary search must be used. To illustrate this, consider the two distributions in @fig-bsearch:</span>
<span id="cb12-1583"><a href="#cb12-1583"></a></span>
<span id="cb12-1584"><a href="#cb12-1584"></a>::: {#fig-bsearch layout-ncol=2}</span>
<span id="cb12-1585"><a href="#cb12-1585"></a><span class="al">![First distribution](Figures/normaldistribution.png)</span>{#fig-normal-distribution width="100%"}</span>
<span id="cb12-1586"><a href="#cb12-1586"></a></span>
<span id="cb12-1587"><a href="#cb12-1587"></a><span class="al">![Second distribution](Figures/normaldistribution copy.png)</span>{#fig-normal-distribution-copy width="94%"}</span>
<span id="cb12-1588"><a href="#cb12-1588"></a>:::</span>
<span id="cb12-1589"><a href="#cb12-1589"></a></span>
<span id="cb12-1590"><a href="#cb12-1590"></a>For both the symmetric and skewed distributions, if we were to divide the search range into two portions </span>
<span id="cb12-1591"><a href="#cb12-1591"></a>and compare $A$, $C$, and $E$, we would find that $C &gt; A$ and $C &gt; E$. In both cases, this does not help </span>
<span id="cb12-1592"><a href="#cb12-1592"></a>us reduce our search range, since the true maximum could lie on either of the two intervals (as in the second case), </span>
<span id="cb12-1593"><a href="#cb12-1593"></a>or at $C$ itself (as in the first case). Therefore, we must make comparisons between all five points $A, B, C, D, and E$. </span>
<span id="cb12-1594"><a href="#cb12-1594"></a>This allows us to correctly restrict our search range to $<span class="co">[</span><span class="ot">B, D</span><span class="co">]</span>$ in the first case and $<span class="co">[</span><span class="ot">C, E</span><span class="co">]</span>$ in the second. </span>
<span id="cb12-1595"><a href="#cb12-1595"></a>These extra search requirements are due to the quasiconcavity of the search space we are considering, in which </span>
<span id="cb12-1596"><a href="#cb12-1596"></a>there exists a maximum but we need to make several comparisons at various points throughout the search space </span>
<span id="cb12-1597"><a href="#cb12-1597"></a>to be able to reduce its size in each iteration.</span>
<span id="cb12-1598"><a href="#cb12-1598"></a>:::</span>
<span id="cb12-1599"><a href="#cb12-1599"></a>:::</span>
<span id="cb12-1600"><a href="#cb12-1600"></a></span>
<span id="cb12-1601"><a href="#cb12-1601"></a><span class="in">```pseudocode</span></span>
<span id="cb12-1602"><a href="#cb12-1602"></a><span class="in">#| label: alg-lpm</span></span>
<span id="cb12-1603"><a href="#cb12-1603"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb12-1604"><a href="#cb12-1604"></a><span class="in">    \caption{Quasiconcave Metric Maximization}</span></span>
<span id="cb12-1605"><a href="#cb12-1605"></a><span class="in">    \begin{algorithmic}</span></span>
<span id="cb12-1606"><a href="#cb12-1606"></a><span class="in">        \State \textbf{input:} $\epsilon &gt; 0$ and oracle $\Omega$</span></span>
<span id="cb12-1607"><a href="#cb12-1607"></a><span class="in">        \State \textbf{initialize:} $\theta_a = 0, \theta_b = \frac{\pi}{2}$</span></span>
<span id="cb12-1608"><a href="#cb12-1608"></a><span class="in">        \While{$|\theta_b - \theta_a| &gt; \epsilon$}</span></span>
<span id="cb12-1609"><a href="#cb12-1609"></a><span class="in">            \State set $\theta_c = \frac{3\theta_a+\theta_b}{4}$, $\theta_d = \frac{\theta_a+\theta_b}{2}$, and $\theta_e = \frac{\theta_a+3\theta_b}{4}$</span></span>
<span id="cb12-1610"><a href="#cb12-1610"></a><span class="in">            </span></span>
<span id="cb12-1611"><a href="#cb12-1611"></a><span class="in">            \State obtain $h\theta_a, h\theta_c, h\theta_d, h\theta_e, h\theta_b$ using Proposition 1</span></span>
<span id="cb12-1612"><a href="#cb12-1612"></a><span class="in">            </span></span>
<span id="cb12-1613"><a href="#cb12-1613"></a><span class="in">            \State Compute $C\theta_a, C\theta_c, C\theta_d, C\theta_e, C\theta_b$ using (1)</span></span>
<span id="cb12-1614"><a href="#cb12-1614"></a><span class="in">            </span></span>
<span id="cb12-1615"><a href="#cb12-1615"></a><span class="in">            \State Query $\Omega(C\theta_c, C\theta_a), \Omega(C\theta_d, C\theta_c), \Omega(C\theta_e, C\theta_d)$, and $\Omega(C\theta_b, C\theta_e)$</span></span>
<span id="cb12-1616"><a href="#cb12-1616"></a></span>
<span id="cb12-1617"><a href="#cb12-1617"></a><span class="in">            \If{$q_{i,j}$ is ambiguous}</span></span>
<span id="cb12-1618"><a href="#cb12-1618"></a><span class="in">                \State request $q_{i,j}$'s label from reference</span></span>
<span id="cb12-1619"><a href="#cb12-1619"></a><span class="in">            \Else</span></span>
<span id="cb12-1620"><a href="#cb12-1620"></a><span class="in">                \State impute $q_{i,j}$'s label from previously labeled queries</span></span>
<span id="cb12-1621"><a href="#cb12-1621"></a><span class="in">            \EndIf</span></span>
<span id="cb12-1622"><a href="#cb12-1622"></a><span class="in">            </span></span>
<span id="cb12-1623"><a href="#cb12-1623"></a><span class="in">            \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$}</span></span>
<span id="cb12-1624"><a href="#cb12-1624"></a><span class="in">                \State assume the default order $C\theta \prec C\theta' \prec C\theta''$</span></span>
<span id="cb12-1625"><a href="#cb12-1625"></a><span class="in">            \EndIf</span></span>
<span id="cb12-1626"><a href="#cb12-1626"></a></span>
<span id="cb12-1627"><a href="#cb12-1627"></a><span class="in">            \If{$C\theta' \succ C\theta'' \succ C\theta'''$ for consecutive $\theta &lt; \theta' &lt; \theta''$}</span></span>
<span id="cb12-1628"><a href="#cb12-1628"></a><span class="in">                \State assume the default order $C\theta \prec C\theta' \prec C\theta''$</span></span>
<span id="cb12-1629"><a href="#cb12-1629"></a><span class="in">            \EndIf</span></span>
<span id="cb12-1630"><a href="#cb12-1630"></a><span class="in">            </span></span>
<span id="cb12-1631"><a href="#cb12-1631"></a><span class="in">            \If{$C\theta_a \succ C\theta_c$} </span></span>
<span id="cb12-1632"><a href="#cb12-1632"></a><span class="in">                \State Set $\theta_b = \theta_d$ </span></span>
<span id="cb12-1633"><a href="#cb12-1633"></a><span class="in">            \ElsIf{$C\theta_a \prec C\theta_c \succ C\theta_d$} </span></span>
<span id="cb12-1634"><a href="#cb12-1634"></a><span class="in">                \State Set $\theta_b = \theta_d$ </span></span>
<span id="cb12-1635"><a href="#cb12-1635"></a><span class="in">            \ElsIf{$C\theta_c \prec C\theta_d \succ C\theta_e$} </span></span>
<span id="cb12-1636"><a href="#cb12-1636"></a><span class="in">                \State Set $\theta_a = \theta_c$ </span></span>
<span id="cb12-1637"><a href="#cb12-1637"></a><span class="in">                \State Set $\theta_b = \theta_e$ </span></span>
<span id="cb12-1638"><a href="#cb12-1638"></a><span class="in">            \ElsIf{$C\theta_d \prec C\theta_e \succ C\theta_b$} </span></span>
<span id="cb12-1639"><a href="#cb12-1639"></a><span class="in">                \State Set $\theta_a = \theta_d$ </span></span>
<span id="cb12-1640"><a href="#cb12-1640"></a><span class="in">            \Else </span></span>
<span id="cb12-1641"><a href="#cb12-1641"></a><span class="in">                \State Set $\theta_a = \theta_d$ </span></span>
<span id="cb12-1642"><a href="#cb12-1642"></a><span class="in">            \EndIf</span></span>
<span id="cb12-1643"><a href="#cb12-1643"></a><span class="in">        \EndWhile</span></span>
<span id="cb12-1644"><a href="#cb12-1644"></a><span class="in">        \State \textbf{output:} $\vec{m}, C$, and $\vec{l}$, where $\vec{m} = m_l(\theta_d), C = C\theta_d$, and $\vec{l} := (\vec{m}, (tp, tn)) = (\vec{m}, C)$</span></span>
<span id="cb12-1645"><a href="#cb12-1645"></a><span class="in">    \end{algorithmic}</span></span>
<span id="cb12-1646"><a href="#cb12-1646"></a><span class="in">\end{algorithm}</span></span>
<span id="cb12-1647"><a href="#cb12-1647"></a><span class="in">```</span></span>
<span id="cb12-1648"><a href="#cb12-1648"></a></span>
<span id="cb12-1649"><a href="#cb12-1649"></a>To elicit LPMs, we run @alg-lpm, querying the oracle in each iteration, and set the</span>
<span id="cb12-1650"><a href="#cb12-1650"></a>elicited metric $\hat{m}$ (which is the maximizer on $\mathcal{C}$) to</span>
<span id="cb12-1651"><a href="#cb12-1651"></a>be the slope of the resulting hyperplane, since the metric is linear.</span>
<span id="cb12-1652"><a href="#cb12-1652"></a></span>
<span id="cb12-1653"><a href="#cb12-1653"></a>::: {.callout-note title="remark"}</span>
<span id="cb12-1654"><a href="#cb12-1654"></a>::: {#rem-explaination_lpm}</span>
<span id="cb12-1655"><a href="#cb12-1655"></a>To find the minimum of a quasiconvex metric, we flip all instances of</span>
<span id="cb12-1656"><a href="#cb12-1656"></a>$\prec$ and $\succ$, and use an initial search range of $<span class="co">[</span><span class="ot">\pi, 3\pi/2</span><span class="co">]</span>$;</span>
<span id="cb12-1657"><a href="#cb12-1657"></a>we use this algorithm, which we refer to as @alg-lfpm, in our</span>
<span id="cb12-1658"><a href="#cb12-1658"></a>elicitation of LFPMs.</span>
<span id="cb12-1659"><a href="#cb12-1659"></a>:::</span>
<span id="cb12-1660"><a href="#cb12-1660"></a>:::</span>
<span id="cb12-1661"><a href="#cb12-1661"></a></span>
<span id="cb12-1662"><a href="#cb12-1662"></a>Next, we provide a Python implementation of @alg-lpm.</span>
<span id="cb12-1663"><a href="#cb12-1663"></a></span>
<span id="cb12-1664"><a href="#cb12-1664"></a>::: {.callout-note title="code"}</span>
<span id="cb12-1667"><a href="#cb12-1667"></a><span class="in">```{python}</span></span>
<span id="cb12-1668"><a href="#cb12-1668"></a><span class="kw">def</span> get_m(theta):</span>
<span id="cb12-1669"><a href="#cb12-1669"></a>    <span class="co">"""</span></span>
<span id="cb12-1670"><a href="#cb12-1670"></a><span class="co">    Inputs: </span></span>
<span id="cb12-1671"><a href="#cb12-1671"></a><span class="co">    - theta: the value that parametrizes m</span></span>
<span id="cb12-1672"><a href="#cb12-1672"></a><span class="co">    Outputs:</span></span>
<span id="cb12-1673"><a href="#cb12-1673"></a><span class="co">    - m_0 and m_1 for the LPM</span></span>
<span id="cb12-1674"><a href="#cb12-1674"></a><span class="co">    """</span></span>
<span id="cb12-1675"><a href="#cb12-1675"></a></span>
<span id="cb12-1676"><a href="#cb12-1676"></a>    <span class="cf">return</span> (math.cos(theta), math.sin(theta))</span>
<span id="cb12-1677"><a href="#cb12-1677"></a></span>
<span id="cb12-1678"><a href="#cb12-1678"></a><span class="kw">def</span> lpm_elicitation(epsilon, oracle):</span>
<span id="cb12-1679"><a href="#cb12-1679"></a>    <span class="co">"""</span></span>
<span id="cb12-1680"><a href="#cb12-1680"></a><span class="co">    Inputs:</span></span>
<span id="cb12-1681"><a href="#cb12-1681"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb12-1682"><a href="#cb12-1682"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb12-1683"><a href="#cb12-1683"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb12-1684"><a href="#cb12-1684"></a><span class="co">    Outputs:</span></span>
<span id="cb12-1685"><a href="#cb12-1685"></a><span class="co">    - estimate for m, which is used to compute the LPM as described above</span></span>
<span id="cb12-1686"><a href="#cb12-1686"></a><span class="co">    """</span></span>
<span id="cb12-1687"><a href="#cb12-1687"></a></span>
<span id="cb12-1688"><a href="#cb12-1688"></a>    a <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-1689"><a href="#cb12-1689"></a>    b <span class="op">=</span> math.pi<span class="op">/</span><span class="dv">2</span></span>
<span id="cb12-1690"><a href="#cb12-1690"></a>    <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb12-1691"><a href="#cb12-1691"></a>        c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb12-1692"><a href="#cb12-1692"></a>        d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb12-1693"><a href="#cb12-1693"></a>        e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb12-1694"><a href="#cb12-1694"></a></span>
<span id="cb12-1695"><a href="#cb12-1695"></a>        m_a, m_b, m_c, m_d, m_e <span class="op">=</span> (get_m(x) <span class="cf">for</span> x <span class="kw">in</span> [a,b,c,d,e]) <span class="co"># using definition of m</span></span>
<span id="cb12-1696"><a href="#cb12-1696"></a>        c_a, c_b, c_c, c_d, c_e <span class="op">=</span> (get_c(x) <span class="cf">for</span> x <span class="kw">in</span> [m_a, m_b, m_c, m_d, m_e]) <span class="co"># compute classifier from m's then calculate confusion matrices</span></span>
<span id="cb12-1697"><a href="#cb12-1697"></a>        </span>
<span id="cb12-1698"><a href="#cb12-1698"></a>        response_ac <span class="op">=</span> oracle(c_a, c_c)</span>
<span id="cb12-1699"><a href="#cb12-1699"></a>        response_cd <span class="op">=</span> oracle(c_c, c_d)</span>
<span id="cb12-1700"><a href="#cb12-1700"></a>        response_de <span class="op">=</span> oracle(c_d, c_e)</span>
<span id="cb12-1701"><a href="#cb12-1701"></a>        response_eb <span class="op">=</span> oracle(c_e, c_b)</span>
<span id="cb12-1702"><a href="#cb12-1702"></a></span>
<span id="cb12-1703"><a href="#cb12-1703"></a>        <span class="co"># update ranges to keep the peak</span></span>
<span id="cb12-1704"><a href="#cb12-1704"></a>        <span class="cf">if</span> response_ac:</span>
<span id="cb12-1705"><a href="#cb12-1705"></a>            b <span class="op">=</span> d</span>
<span id="cb12-1706"><a href="#cb12-1706"></a>        <span class="cf">elif</span> response_cd:</span>
<span id="cb12-1707"><a href="#cb12-1707"></a>            b <span class="op">=</span> d</span>
<span id="cb12-1708"><a href="#cb12-1708"></a>        <span class="cf">elif</span> response_de:</span>
<span id="cb12-1709"><a href="#cb12-1709"></a>            a <span class="op">=</span> c</span>
<span id="cb12-1710"><a href="#cb12-1710"></a>            b <span class="op">=</span> e</span>
<span id="cb12-1711"><a href="#cb12-1711"></a>        <span class="cf">elif</span> response_eb:</span>
<span id="cb12-1712"><a href="#cb12-1712"></a>            a <span class="op">=</span> d</span>
<span id="cb12-1713"><a href="#cb12-1713"></a>        <span class="cf">else</span>:</span>
<span id="cb12-1714"><a href="#cb12-1714"></a>            a <span class="op">=</span> d</span>
<span id="cb12-1715"><a href="#cb12-1715"></a>    <span class="cf">return</span> get_m(d), get_c(d)</span>
<span id="cb12-1716"><a href="#cb12-1716"></a><span class="in">```</span></span>
<span id="cb12-1717"><a href="#cb12-1717"></a>:::</span>
<span id="cb12-1718"><a href="#cb12-1718"></a></span>
<span id="cb12-1719"><a href="#cb12-1719"></a><span class="fu">### Linear-Fractional Performance Metric Elicitation {#sec-lfpm-elicitation}</span></span>
<span id="cb12-1720"><a href="#cb12-1720"></a></span>
<span id="cb12-1721"><a href="#cb12-1721"></a>Now, we present the next main result, which is an algorithm to elicit</span>
<span id="cb12-1722"><a href="#cb12-1722"></a>linear-fractional performance metrics. For this task, we will need the</span>
<span id="cb12-1723"><a href="#cb12-1723"></a>following assumption:</span>
<span id="cb12-1724"><a href="#cb12-1724"></a></span>
<span id="cb12-1725"><a href="#cb12-1725"></a>Let $\phi \in \varphi_{L F P M}$. We assume</span>
<span id="cb12-1726"><a href="#cb12-1726"></a>$p_{11}, p_{00} \geq 0, p_{11} \geq q_{11}, p_{00} \geq q_{00},$</span>
<span id="cb12-1727"><a href="#cb12-1727"></a>$p_{0}=0, q_{0}=$</span>
<span id="cb12-1728"><a href="#cb12-1728"></a>$\left(p_{11}-q_{11}\right) \zeta+\left(p_{00}-q_{00}\right)(1-\zeta)$,</span>
<span id="cb12-1729"><a href="#cb12-1729"></a>and $p_{11}+p_{00}=1$.</span>
<span id="cb12-1730"><a href="#cb12-1730"></a></span>
<span id="cb12-1731"><a href="#cb12-1731"></a>These assumptions guarantee that the LFPM $\phi$ which we are trying to</span>
<span id="cb12-1732"><a href="#cb12-1732"></a>elicit is monotonically increasing in $TP$ and $TN$, just as in the LPM</span>
<span id="cb12-1733"><a href="#cb12-1733"></a>elicitation case.</span>
<span id="cb12-1734"><a href="#cb12-1734"></a></span>
<span id="cb12-1735"><a href="#cb12-1735"></a>We first provide motivation and an overview of the approach for LFPM</span>
<span id="cb12-1736"><a href="#cb12-1736"></a>elicitation and then present pseudocode for the algorithm.</span>
<span id="cb12-1737"><a href="#cb12-1737"></a></span>
<span id="cb12-1738"><a href="#cb12-1738"></a>The general idea of the algorithm is to use @alg-lpm to obtain a</span>
<span id="cb12-1739"><a href="#cb12-1739"></a>maximizer and a minimizer for the given dataset; these result in two</span>
<span id="cb12-1740"><a href="#cb12-1740"></a>systems of equations involving the true LFPM $\phi^*$ with 1 degree of</span>
<span id="cb12-1741"><a href="#cb12-1741"></a>freedom. Then, we run a grid search that is independent of oracle</span>
<span id="cb12-1742"><a href="#cb12-1742"></a>queries to find the point where solutions to the systems match pointwise</span>
<span id="cb12-1743"><a href="#cb12-1743"></a>on the resulting confusion matrices; this occurs close to where the true</span>
<span id="cb12-1744"><a href="#cb12-1744"></a>metric lies.</span>
<span id="cb12-1745"><a href="#cb12-1745"></a></span>
<span id="cb12-1746"><a href="#cb12-1746"></a>More formally, suppose that the true metric is</span>
<span id="cb12-1747"><a href="#cb12-1747"></a>$$\phi^{*}(C)=\frac{p_{11}^{*} T P+p_{00}^{*} T N}{q_{11}^{*} T P+q_{00}^{*} T N+q_{0}^{*}}.$$  {#eq-eq3.48}</span>
<span id="cb12-1748"><a href="#cb12-1748"></a>Then, let $\bar{\tau}$ and $\underline{\tau}$ represent the maximizer</span>
<span id="cb12-1749"><a href="#cb12-1749"></a>and minimizer of $\phi$ over $\mathcal{C}$, respectively. There exists a</span>
<span id="cb12-1750"><a href="#cb12-1750"></a>hyperplane </span>
<span id="cb12-1751"><a href="#cb12-1751"></a>$$\begin{aligned}</span>
<span id="cb12-1752"><a href="#cb12-1752"></a>\bar{\ell}_{f}^{*}:=\left(p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}\right) t n=\bar{\tau}^{*} q_{0}^{*},</span>
<span id="cb12-1753"><a href="#cb12-1753"></a>\end{aligned}$$   {#eq-eq3.49}</span>
<span id="cb12-1754"><a href="#cb12-1754"></a>which touches $\mathcal{C}$ at $\left(\overline{T P}^{*}, \overline{T N}^{*}\right)$ on</span>
<span id="cb12-1755"><a href="#cb12-1755"></a>$\partial \mathcal{C}_{+}$.</span>
<span id="cb12-1756"><a href="#cb12-1756"></a></span>
<span id="cb12-1757"><a href="#cb12-1757"></a>Correspondingly, there also exists a hyperplane </span>
<span id="cb12-1758"><a href="#cb12-1758"></a>$$\begin{aligned}</span>
<span id="cb12-1759"><a href="#cb12-1759"></a>\underline{\ell}_{f}^{*}:=\left(p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}\right) t p+\left(p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}\right) \operatorname{tn}=\underline{\tau}^{*} q_{0}^{*},</span>
<span id="cb12-1760"><a href="#cb12-1760"></a>\end{aligned}$$   {#eq-eq3.50}</span>
<span id="cb12-1761"><a href="#cb12-1761"></a>which touches $\mathcal{C}$ at $\left(\underline{TP}^{*}, \underline{T N}^{*}\right)$ on</span>
<span id="cb12-1762"><a href="#cb12-1762"></a>$\partial \mathcal{C}_{-}$. @fig-minmax illustrates this visually on $\mathcal{C}$.</span>
<span id="cb12-1763"><a href="#cb12-1763"></a></span>
<span id="cb12-1764"><a href="#cb12-1764"></a>![Visual representation of the minimizer and maximizer on</span>
<span id="cb12-1765"><a href="#cb12-1765"></a>$\mathcal{C}$](Figures/Screenshot 2023-11-13 at 6.56.52 PM.png){#fig-minmax</span>
<span id="cb12-1766"><a href="#cb12-1766"></a>width="50%"}</span>
<span id="cb12-1767"><a href="#cb12-1767"></a></span>
<span id="cb12-1768"><a href="#cb12-1768"></a>While we are unable to obtain @eq-eq3.48 and @eq-eq3.49 directly, we can use @alg-lpm to get a hyperplane</span>
<span id="cb12-1769"><a href="#cb12-1769"></a>$$\bar{\ell}:=\bar{m}_{11} t p+\bar{m}_{00} t n= \bar{m}_{11} \overline{T P}^{*}+\bar{m}_{00} \overline{T N}^{*} = \bar{C}_{0},$$  {#eq-eq3.51}</span>
<span id="cb12-1770"><a href="#cb12-1770"></a>which is equivalent to $\bar{\ell}_{f}^{*}$ (@eq-eq3.48) up to a constant</span>
<span id="cb12-1771"><a href="#cb12-1771"></a>multiple. From here, we can obtain the system of equations</span>
<span id="cb12-1772"><a href="#cb12-1772"></a></span>
<span id="cb12-1773"><a href="#cb12-1773"></a>$$p_{11}^{*}-\bar{\tau}^{*} q_{11}^{*}=\alpha \bar{m}_{11}, p_{00}^{*}-\bar{\tau}^{*} q_{00}^{*}=\alpha \bar{m}_{00}, \bar{\tau}^{*} q_{0}^{*}=\alpha \bar{C}_{0},$$  {#eq-eq3.52}</span>
<span id="cb12-1774"><a href="#cb12-1774"></a>where $\alpha &gt; 0$ (we know it is $\geq0$ due to our assumptions earlier</span>
<span id="cb12-1775"><a href="#cb12-1775"></a>and because $\bar{m}$ is positive, but if it is equal to $0$ then</span>
<span id="cb12-1776"><a href="#cb12-1776"></a>$\phi^*$ would be constant. So, our resulting system of equations is</span>
<span id="cb12-1777"><a href="#cb12-1777"></a>$$\begin{aligned}</span>
<span id="cb12-1778"><a href="#cb12-1778"></a>    p_{11}^{\prime}-\bar{\tau}^{*} q_{11}^{\prime}=\bar{m}_{11}, p_{00}^{\prime}-\bar{\tau}^{*} q_{00}^{\prime}=\bar{m}_{00}, \bar{\tau}^{*} q_{0}^{\prime}=\bar{C}_{0}.</span>
<span id="cb12-1779"><a href="#cb12-1779"></a>\end{aligned}$$  {#eq-eq3.53}</span>
<span id="cb12-1780"><a href="#cb12-1780"></a></span>
<span id="cb12-1781"><a href="#cb12-1781"></a>Now, similarly, we can approximate @eq-eq3.49 using the algorithm we defined</span>
<span id="cb12-1782"><a href="#cb12-1782"></a>for quasiconvex metrics (@alg-lfpm), where we altered the search range</span>
<span id="cb12-1783"><a href="#cb12-1783"></a>and comparisons. After finding the minimizer, we obtain the hyperplane</span>
<span id="cb12-1784"><a href="#cb12-1784"></a>$$\underline{\ell}:=\underline{m}_{11} t p+\underline{m}_{00} t n=\underline{m}_{11} \underline{TP}^{*}+\underline{m}_{00} \underline{TN}^{*} = \underline{C}_{0},$$  {#eq-eq3.54}</span>
<span id="cb12-1785"><a href="#cb12-1785"></a>which is equivalent to $\underline{\ell}_{f}^{*}$ (@eq-eq3.49) up to a constant</span>
<span id="cb12-1786"><a href="#cb12-1786"></a>multiple. So then, our system of equations is</span>
<span id="cb12-1787"><a href="#cb12-1787"></a>$$p_{11}^{*}-\underline{\tau}^{*} q_{11}^{*}=\gamma \underline{m}_{11}, p_{00}^{*}-\underline{\tau}^{*} q_{00}^{*}=\gamma \underline{m}_{00}, \underline{\tau}^{*} q_{0}^{*}=\gamma \underline{C}_{0},$$  {#eq-eq3.55}</span>
<span id="cb12-1788"><a href="#cb12-1788"></a>where $\gamma &lt;0$ (for a reason analogous to why we have $\alpha &gt;0$),</span>
<span id="cb12-1789"><a href="#cb12-1789"></a>meaning our resulting system of equations is </span>
<span id="cb12-1790"><a href="#cb12-1790"></a>$$\begin{aligned}</span>
<span id="cb12-1791"><a href="#cb12-1791"></a>    p_{11}^{\prime \prime}-\underline{\tau}^{*} q_{11}^{\prime \prime}=\underline{m}_{11}, p_{00}^{\prime \prime}-\underline{\tau}^{*} q_{00}^{\prime \prime}=\underline{m}_{00}, \underline{\tau}^{*} q_{0}^{\prime \prime}=\underline{C}_{0}.</span>
<span id="cb12-1792"><a href="#cb12-1792"></a>\end{aligned}$$  {#eq-eq3.56}</span>
<span id="cb12-1793"><a href="#cb12-1793"></a></span>
<span id="cb12-1794"><a href="#cb12-1794"></a>@eq-eq3.55 and @eq-eq3.56 form the two systems of equations mentioned in our overview</span>
<span id="cb12-1795"><a href="#cb12-1795"></a>of the algorithm. Next, we demonstrate that they have only one degree of</span>
<span id="cb12-1796"><a href="#cb12-1796"></a>freedom. Note that if we know $p_{11}'$, we could solve both systems of</span>
<span id="cb12-1797"><a href="#cb12-1797"></a>equations as follows: </span>
<span id="cb12-1798"><a href="#cb12-1798"></a>$$\begin{aligned}</span>
<span id="cb12-1799"><a href="#cb12-1799"></a>    p_{00}^{\prime}  &amp;=1-p_{11}^{\prime}, q_{0}^{\prime}=\bar{C}_{0} \frac{P^{\prime}}{Q^{\prime}}<span class="sc">\\</span></span>
<span id="cb12-1800"><a href="#cb12-1800"></a>    q_{11}^{\prime}  &amp;=\left(p_{11}^{\prime}-\bar{m}_{11}\right) \frac{P^{\prime}}{Q^{\prime}} <span class="sc">\\</span></span>
<span id="cb12-1801"><a href="#cb12-1801"></a>    q_{00}^{\prime}&amp;=\left(p_{00}^{\prime}-\bar{m}_{00}\right) \frac{P^{\prime}}{Q^{\prime}},</span>
<span id="cb12-1802"><a href="#cb12-1802"></a>\end{aligned}$$   {#eq-eq3.57}</span>
<span id="cb12-1803"><a href="#cb12-1803"></a>where</span>
<span id="cb12-1804"><a href="#cb12-1804"></a>$P^{\prime}=p_{11}^{\prime} \zeta+p_{00}^{\prime}(1-\zeta)$ and</span>
<span id="cb12-1805"><a href="#cb12-1805"></a>$Q^{\prime}=P^{\prime}+\bar{C}_{0}-$</span>
<span id="cb12-1806"><a href="#cb12-1806"></a>$\bar{m}_{11} \zeta-\bar{m}_{00}(1-\zeta).$</span>
<span id="cb12-1807"><a href="#cb12-1807"></a></span>
<span id="cb12-1808"><a href="#cb12-1808"></a>Now, suppose we know $p_{11}'$. We could use this value to solve both systems @eq-eq3.55 and @eq-eq3.56, </span>
<span id="cb12-1809"><a href="#cb12-1809"></a>yielding two metrics, $\phi'$ and $\phi''$, from the maximizer and minimizer, respectively. Importantly, when</span>
<span id="cb12-1810"><a href="#cb12-1810"></a>$$p_{11}^{*} / p_{00}^{*}=p_{11}^{\prime} / p_{00}^{\prime}=p_{11}^{\prime \prime} / p_{00}^{\prime \prime},$$ {#eq-eq3.58} </span>
<span id="cb12-1811"><a href="#cb12-1811"></a>then</span>
<span id="cb12-1812"><a href="#cb12-1812"></a>$\phi^{*}(C)=\phi^{\prime}(C) / \alpha=-\phi^{\prime \prime}(C) / \gamma$.</span>
<span id="cb12-1813"><a href="#cb12-1813"></a>Essentially, when we find a value of $p_{11}'$ that results in $\phi'$ and $\phi''$ h</span>
<span id="cb12-1814"><a href="#cb12-1814"></a>aving constant ratios at all points on the boundary of $\mathcal{C}$, we can obtain $\phi^*$, </span>
<span id="cb12-1815"><a href="#cb12-1815"></a>as it is derivable from $\phi'$ and $\alpha$ (or, alternatively, $\phi''$ and $\gamma$).</span>
<span id="cb12-1816"><a href="#cb12-1816"></a></span>
<span id="cb12-1817"><a href="#cb12-1817"></a>We will perform a grid search for $p_{11}'$ on $<span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>$. For each point in our search, </span>
<span id="cb12-1818"><a href="#cb12-1818"></a>we will compute $\phi'$ and $\phi''$. Then, we will generate several confusion matrices </span>
<span id="cb12-1819"><a href="#cb12-1819"></a>on the boundaries and calculate the ratio $\phi'' / $\phi'$ for each. We will select the value </span>
<span id="cb12-1820"><a href="#cb12-1820"></a>of $p_{11}'$ for which the ratio $\phi'' / \phi'$ is closest to constant and use it to compute </span>
<span id="cb12-1821"><a href="#cb12-1821"></a>the elicited metric $\hat{\phi}$. The pseudocode for LFPM elicitation is given in @alg-lfpm.</span>
<span id="cb12-1822"><a href="#cb12-1822"></a></span>
<span id="cb12-1823"><a href="#cb12-1823"></a><span class="in">```pseudocode</span></span>
<span id="cb12-1824"><a href="#cb12-1824"></a><span class="in">#| label: alg-lfpm</span></span>
<span id="cb12-1825"><a href="#cb12-1825"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb12-1826"><a href="#cb12-1826"></a><span class="in">    \caption{Grid Search for Best Ratio}</span></span>
<span id="cb12-1827"><a href="#cb12-1827"></a><span class="in">    \begin{algorithmic}</span></span>
<span id="cb12-1828"><a href="#cb12-1828"></a><span class="in">        \State \textbf{Input:} $k, \Delta$.</span></span>
<span id="cb12-1829"><a href="#cb12-1829"></a><span class="in">        \State \textbf{Initialize:} $\sigma_{\text{opt}} = \infty, p'_{11,\text{opt}} = 0$.</span></span>
<span id="cb12-1830"><a href="#cb12-1830"></a><span class="in">        \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3).</span></span>
<span id="cb12-1831"><a href="#cb12-1831"></a><span class="in">        \State Generate $C_1, \dots, C_k$ on $\partial C_+$ and $\partial C_-$ (Section 3).</span></span>
<span id="cb12-1832"><a href="#cb12-1832"></a><span class="in">        \For{$p'_{11} = 0; \; p'_{11} \leq 1; \; p'_{11} = p'_{11} + \Delta$}</span></span>
<span id="cb12-1833"><a href="#cb12-1833"></a><span class="in">            \State Compute $\phi'$, $\phi''$ using Proposition 4. </span></span>
<span id="cb12-1834"><a href="#cb12-1834"></a><span class="in">            \State Compute array $r = \left[ \frac{\phi'(C_1)}{\phi''(C_1)}, \dots, \frac{\phi'(C_k)}{\phi''(C_k)} \right]$.</span></span>
<span id="cb12-1835"><a href="#cb12-1835"></a><span class="in">            \State Set $\sigma = \text{std}(r)$.</span></span>
<span id="cb12-1836"><a href="#cb12-1836"></a><span class="in">            \If{$\sigma &lt; \sigma_{\text{opt}}$}</span></span>
<span id="cb12-1837"><a href="#cb12-1837"></a><span class="in">                \State Set $\sigma_{\text{opt}} = \sigma$ and $p'_{11,\text{opt}} = p'_{11}$.</span></span>
<span id="cb12-1838"><a href="#cb12-1838"></a><span class="in">            \EndIf</span></span>
<span id="cb12-1839"><a href="#cb12-1839"></a><span class="in">        \EndFor</span></span>
<span id="cb12-1840"><a href="#cb12-1840"></a><span class="in">        \State \textbf{Output:} $p'_{11,\text{opt}}$.</span></span>
<span id="cb12-1841"><a href="#cb12-1841"></a><span class="in">    \end{algorithmic}</span></span>
<span id="cb12-1842"><a href="#cb12-1842"></a><span class="in">\end{algorithm}</span></span>
<span id="cb12-1843"><a href="#cb12-1843"></a><span class="in">```</span></span>
<span id="cb12-1844"><a href="#cb12-1844"></a></span>
<span id="cb12-1845"><a href="#cb12-1845"></a>We provide a Python implementation as below.</span>
<span id="cb12-1846"><a href="#cb12-1846"></a></span>
<span id="cb12-1847"><a href="#cb12-1847"></a>::: {.callout-note title="code"}</span>
<span id="cb12-1850"><a href="#cb12-1850"></a><span class="in">```{python}</span></span>
<span id="cb12-1851"><a href="#cb12-1851"></a><span class="kw">def</span> lfpm_elicitation(k, delta):</span>
<span id="cb12-1852"><a href="#cb12-1852"></a>    <span class="co">"""</span></span>
<span id="cb12-1853"><a href="#cb12-1853"></a><span class="co">    Inputs:</span></span>
<span id="cb12-1854"><a href="#cb12-1854"></a><span class="co">    - k: the number of confusion matrices to evaluate on</span></span>
<span id="cb12-1855"><a href="#cb12-1855"></a><span class="co">    - delta: the spacing for the grid search</span></span>
<span id="cb12-1856"><a href="#cb12-1856"></a><span class="co">    Outputs:</span></span>
<span id="cb12-1857"><a href="#cb12-1857"></a><span class="co">    - p_11', which will allow us to compute the elicited LFPM</span></span>
<span id="cb12-1858"><a href="#cb12-1858"></a><span class="co">    """</span></span>
<span id="cb12-1859"><a href="#cb12-1859"></a></span>
<span id="cb12-1860"><a href="#cb12-1860"></a>    sigma_opt <span class="op">=</span> np.inf</span>
<span id="cb12-1861"><a href="#cb12-1861"></a>    p11_opt <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-1862"><a href="#cb12-1862"></a>    C <span class="op">=</span> compute_confusion_matrices(k) <span class="co"># generates k confusion matrices to evaluate on</span></span>
<span id="cb12-1863"><a href="#cb12-1863"></a></span>
<span id="cb12-1864"><a href="#cb12-1864"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">int</span>(<span class="dv">1</span><span class="op">/</span>delta)):</span>
<span id="cb12-1865"><a href="#cb12-1865"></a>        p11 <span class="op">=</span> i <span class="op">*</span> delta</span>
<span id="cb12-1866"><a href="#cb12-1866"></a>        phi1 <span class="op">=</span> compute_upper_metric(p11) <span class="co"># solves the first system of equations with p11 </span></span>
<span id="cb12-1867"><a href="#cb12-1867"></a>        phi2 <span class="op">=</span> compute_lower_metric(p11) <span class="co"># solves the second system of equations with p11 </span></span>
<span id="cb12-1868"><a href="#cb12-1868"></a>        utility_1 <span class="op">=</span> [phi1(c) <span class="cf">for</span> c <span class="kw">in</span> C] <span class="co">#calculate phi for both systems of equations</span></span>
<span id="cb12-1869"><a href="#cb12-1869"></a>        utility_2 <span class="op">=</span> [phi2(c) <span class="cf">for</span> c <span class="kw">in</span> C]</span>
<span id="cb12-1870"><a href="#cb12-1870"></a></span>
<span id="cb12-1871"><a href="#cb12-1871"></a>        r <span class="op">=</span> []</span>
<span id="cb12-1872"><a href="#cb12-1872"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb12-1873"><a href="#cb12-1873"></a>            r.append(utility_1[i] <span class="op">/</span> utility_2[i])</span>
<span id="cb12-1874"><a href="#cb12-1874"></a>        sigma <span class="op">=</span> np.std(r)</span>
<span id="cb12-1875"><a href="#cb12-1875"></a></span>
<span id="cb12-1876"><a href="#cb12-1876"></a>        <span class="cf">if</span>(sigma <span class="op">&lt;</span> sigma_opt):</span>
<span id="cb12-1877"><a href="#cb12-1877"></a>            sigma_opt <span class="op">=</span> sigma</span>
<span id="cb12-1878"><a href="#cb12-1878"></a>            p11_opt <span class="op">=</span> p11</span>
<span id="cb12-1879"><a href="#cb12-1879"></a>    <span class="cf">return</span> p11_opt</span>
<span id="cb12-1880"><a href="#cb12-1880"></a><span class="in">```</span></span>
<span id="cb12-1881"><a href="#cb12-1881"></a>:::</span>
<span id="cb12-1882"><a href="#cb12-1882"></a></span>
<span id="cb12-1883"><a href="#cb12-1883"></a>In summary, to elicit LFPMs, we utilize a special property of the LPM</span>
<span id="cb12-1884"><a href="#cb12-1884"></a>minimizer and maximizer on $\mathcal{C}$--namely, that we can use the</span>
<span id="cb12-1885"><a href="#cb12-1885"></a>corresponding supporting hyperplanes to form a system of equations that</span>
<span id="cb12-1886"><a href="#cb12-1886"></a>can be used to approximate $\phi^*$ if one parameter ($p_{11}'$) is</span>
<span id="cb12-1887"><a href="#cb12-1887"></a>found, and that this parameter can be found using an oracle-independent</span>
<span id="cb12-1888"><a href="#cb12-1888"></a>grid search.</span>
<span id="cb12-1889"><a href="#cb12-1889"></a></span>
<span id="cb12-1890"><a href="#cb12-1890"></a><span class="fu">#### Guarantees {.unnumbered}</span></span>
<span id="cb12-1891"><a href="#cb12-1891"></a></span>
<span id="cb12-1892"><a href="#cb12-1892"></a>Importantly, these algorithms can be shown to satisfy significant theoretical guarantees. We provide </span>
<span id="cb12-1893"><a href="#cb12-1893"></a> formal statement and intuitive interpretation of these guarantees here, with their proofs available </span>
<span id="cb12-1894"><a href="#cb12-1894"></a> in the appendix of the original paper.</span>
<span id="cb12-1895"><a href="#cb12-1895"></a></span>
<span id="cb12-1896"><a href="#cb12-1896"></a>First, we define the oracle noise $\epsilon_{\Omega}$, which arises from the oracle potentially flipping </span>
<span id="cb12-1897"><a href="#cb12-1897"></a>the comparison output on two confusion matrices that are close enough in utility.</span>
<span id="cb12-1898"><a href="#cb12-1898"></a></span>
<span id="cb12-1899"><a href="#cb12-1899"></a>::: {.callout-note title="theorem"}</span>
<span id="cb12-1900"><a href="#cb12-1900"></a>::: {#thm-thm1}</span>
<span id="cb12-1901"><a href="#cb12-1901"></a>Given $\epsilon, \epsilon_{\Omega} \geq 0$ and a metric $\phi$ satisfying our assumptions, @alg-lpm or </span>
<span id="cb12-1902"><a href="#cb12-1902"></a>@alg-lfpm finds an approximate maximizer/minimizer and supporting hyperplane. Additionally, the value of </span>
<span id="cb12-1903"><a href="#cb12-1903"></a>$\phi$ at that point is within $O\left(\sqrt{\epsilon_{\Omega}} + \epsilon\right)$ of the optimum, and the </span>
<span id="cb12-1904"><a href="#cb12-1904"></a>number of queries is $O\left(\log \frac{1}{\epsilon}\right)$.</span>
<span id="cb12-1905"><a href="#cb12-1905"></a>:::</span>
<span id="cb12-1906"><a href="#cb12-1906"></a>:::</span>
<span id="cb12-1907"><a href="#cb12-1907"></a></span>
<span id="cb12-1908"><a href="#cb12-1908"></a>::: {.callout-note title="theorem"}</span>
<span id="cb12-1909"><a href="#cb12-1909"></a>::: {#thm-thm2}</span>
<span id="cb12-1910"><a href="#cb12-1910"></a>Let $\mathbf{m}^{*}$ be the true performance metric. Given $\epsilon &gt; 0$, LPM elicitation outputs a performance </span>
<span id="cb12-1911"><a href="#cb12-1911"></a>metric $\hat{\mathbf{m}}$, such that $\left\|\mathbf{m}^{*} - \hat{\mathbf{m}}\right\|_{\infty} \leq \sqrt{2} \epsilon + \frac{2}{k_{0}} \sqrt{2 k_{1} \epsilon_{\Omega}}$.</span>
<span id="cb12-1912"><a href="#cb12-1912"></a>:::</span>
<span id="cb12-1913"><a href="#cb12-1913"></a>:::</span>
<span id="cb12-1914"><a href="#cb12-1914"></a></span>
<span id="cb12-1915"><a href="#cb12-1915"></a>These two theorems ensure that @alg-lpm and @alg-lfpm find an appropriate maximizer and minimizer in the search </span>
<span id="cb12-1916"><a href="#cb12-1916"></a>space, within a certain range of accuracy that depends on oracle and sample noise, and within a certain number of </span>
<span id="cb12-1917"><a href="#cb12-1917"></a>queries. Both of these statements are guaranteed by the binary search approach.</span>
<span id="cb12-1918"><a href="#cb12-1918"></a></span>
<span id="cb12-1919"><a href="#cb12-1919"></a>::: {.callout-note title="theorem"}</span>
<span id="cb12-1920"><a href="#cb12-1920"></a>::: {#thm-thm3}</span>
<span id="cb12-1921"><a href="#cb12-1921"></a>Let $h_{\theta}$ and $\hat{h}_{\theta}$ be two classifiers estimated using $\eta$ and $\hat{\eta}$, respectively. </span>
<span id="cb12-1922"><a href="#cb12-1922"></a>Further, let $\bar{\theta}$ be such that $h_{\bar{\theta}} = \arg \max _{\theta} \phi\left(h_{\theta}\right)$. </span>
<span id="cb12-1923"><a href="#cb12-1923"></a>Then $\|C(\hat{h}_{\bar{\theta}}) - C\left(h_{\bar{\theta}}\right)\|_{\infty} = O\left(\left\|\hat{\eta}_{n} - \eta\right\|_{\infty}\right)$.</span>
<span id="cb12-1924"><a href="#cb12-1924"></a>:::</span>
<span id="cb12-1925"><a href="#cb12-1925"></a>:::</span>
<span id="cb12-1926"><a href="#cb12-1926"></a></span>
<span id="cb12-1927"><a href="#cb12-1927"></a>This theorem indicates that the drop in elicited metric quality caused by using a dataset of samples rather than </span>
<span id="cb12-1928"><a href="#cb12-1928"></a>population confusion matrices is bounded by the drop in performance of the decision boundary $\eta$. These three </span>
<span id="cb12-1929"><a href="#cb12-1929"></a>guarantees together ensure that oracle noise and sample noise do not amplify drops in performance when using metric </span>
<span id="cb12-1930"><a href="#cb12-1930"></a>elicitation; rather, these drops in performance are bounded by the drops that would typically occur when using the </span>
<span id="cb12-1931"><a href="#cb12-1931"></a>standard machine learning paradigm of training a decision boundary and using a pre-established metric.</span>
<span id="cb12-1932"><a href="#cb12-1932"></a></span>
<span id="cb12-1933"><a href="#cb12-1933"></a>For further interesting exploration of the types of problems that can be</span>
<span id="cb12-1934"><a href="#cb12-1934"></a>solved using the framework of metric elicitation, we refer the reader to</span>
<span id="cb12-1935"><a href="#cb12-1935"></a><span class="co">[</span><span class="ot">@nips</span><span class="co">]</span>, which performs metric elicitation to determine the oracle's</span>
<span id="cb12-1936"><a href="#cb12-1936"></a>ideal tradeoff between the classifier's overall performance and the</span>
<span id="cb12-1937"><a href="#cb12-1937"></a>discrepancy between its performance on certain protected groups.</span>
<span id="cb12-1938"><a href="#cb12-1938"></a></span>
<span id="cb12-1939"><a href="#cb12-1939"></a><span class="fu">### Multiclass Performance Metric Elicitation</span></span>
<span id="cb12-1940"><a href="#cb12-1940"></a></span>
<span id="cb12-1941"><a href="#cb12-1941"></a>Although the previous section only described metric elicitation for</span>
<span id="cb12-1942"><a href="#cb12-1942"></a>binary classification problems, the general framework can still be</span>
<span id="cb12-1943"><a href="#cb12-1943"></a>applied to multiclass classification problems, as described in</span>
<span id="cb12-1944"><a href="#cb12-1944"></a>"Multiclass Performance Metric Elicitation" by <span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>.</span>
<span id="cb12-1945"><a href="#cb12-1945"></a></span>
<span id="cb12-1946"><a href="#cb12-1946"></a>Consider the case of classifying subtypes of</span>
<span id="cb12-1947"><a href="#cb12-1947"></a>leukemia <span class="co">[</span><span class="ot">@YangNaiman+2014+477+496</span><span class="co">]</span>. We can train a neural network to</span>
<span id="cb12-1948"><a href="#cb12-1948"></a>predict conditional probability of a certain leukemia subtype given</span>
<span id="cb12-1949"><a href="#cb12-1949"></a>certain gene expressions. However, it may not be appropriate to classify</span>
<span id="cb12-1950"><a href="#cb12-1950"></a>the subtype purely based on whichever one has the highest confidence.</span>
<span id="cb12-1951"><a href="#cb12-1951"></a>For instance, a treatment for leukemia subtype C1 may be perfect for</span>
<span id="cb12-1952"><a href="#cb12-1952"></a>cases of C1, but it may be ineffective or harmful for certain other</span>
<span id="cb12-1953"><a href="#cb12-1953"></a>subtypes. Therefore, the final response from the classifier may not be</span>
<span id="cb12-1954"><a href="#cb12-1954"></a>as simple as as choosing the class with the highest conditional</span>
<span id="cb12-1955"><a href="#cb12-1955"></a>probability, just like how the threshold for binary classification may</span>
<span id="cb12-1956"><a href="#cb12-1956"></a>not always be 50%.</span>
<span id="cb12-1957"><a href="#cb12-1957"></a></span>
<span id="cb12-1958"><a href="#cb12-1958"></a>With multiclass metric elicitation, we can show confusion matrices to an</span>
<span id="cb12-1959"><a href="#cb12-1959"></a>oracle (like the doctor in the leukemia example) to determine which</span>
<span id="cb12-1960"><a href="#cb12-1960"></a>classifier has the best tradeoffs. In <span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>, the</span>
<span id="cb12-1961"><a href="#cb12-1961"></a>authors focus on eliciting linear performance metrics, which is what we</span>
<span id="cb12-1962"><a href="#cb12-1962"></a>will describe in this chapter.</span>
<span id="cb12-1963"><a href="#cb12-1963"></a></span>
<span id="cb12-1964"><a href="#cb12-1964"></a><span class="fu">#### Preliminaries {.unnumbered}</span></span>
<span id="cb12-1965"><a href="#cb12-1965"></a></span>
<span id="cb12-1966"><a href="#cb12-1966"></a>Most of the notation from Binary Metric Elicitation still persists, just</span>
<span id="cb12-1967"><a href="#cb12-1967"></a>modified to provide categorical responses:</span>
<span id="cb12-1968"><a href="#cb12-1968"></a></span>
<span id="cb12-1969"><a href="#cb12-1969"></a><span class="ss">-   </span>$X \in \mathcal{X}$ is the input random variable.</span>
<span id="cb12-1970"><a href="#cb12-1970"></a></span>
<span id="cb12-1971"><a href="#cb12-1971"></a><span class="ss">-   </span>$Y \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ is the output random variable, where $<span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ is the index</span>
<span id="cb12-1972"><a href="#cb12-1972"></a>    set $<span class="sc">\{</span>1, 2, \dots, k<span class="sc">\}</span>$.</span>
<span id="cb12-1973"><a href="#cb12-1973"></a></span>
<span id="cb12-1974"><a href="#cb12-1974"></a><span class="ss">-   </span>The dataset of size $n$ is denoted by $<span class="sc">\{</span>(\vec{x}, y)<span class="sc">\}</span>_{i=1}^n$</span>
<span id="cb12-1975"><a href="#cb12-1975"></a>    generated independently and identically from $\mathbb{P}(X, Y)$.</span>
<span id="cb12-1976"><a href="#cb12-1976"></a></span>
<span id="cb12-1977"><a href="#cb12-1977"></a><span class="ss">-   </span>$\eta_i(\vec{x}) = \mathbb{P}(Y=i | X=\vec{x})$ gives the</span>
<span id="cb12-1978"><a href="#cb12-1978"></a>    conditional probability of class $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ given an observation.</span>
<span id="cb12-1979"><a href="#cb12-1979"></a></span>
<span id="cb12-1980"><a href="#cb12-1980"></a><span class="ss">-   </span>$\xi_i = \mathbb{P}(Y=i)$ is the marginal probability of class</span>
<span id="cb12-1981"><a href="#cb12-1981"></a>    $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$.</span>
<span id="cb12-1982"><a href="#cb12-1982"></a></span>
<span id="cb12-1983"><a href="#cb12-1983"></a><span class="ss">-   </span>The set of all classifiers is</span>
<span id="cb12-1984"><a href="#cb12-1984"></a>    $\mathcal{H} = <span class="sc">\{</span>h : \mathcal{X} \rightarrow \Delta_k<span class="sc">\}</span>$, where</span>
<span id="cb12-1985"><a href="#cb12-1985"></a>    $\Delta_k$ is (k-1) dimensional simplex. In this case, the outputs</span>
<span id="cb12-1986"><a href="#cb12-1986"></a>    of classifiers are 1-hot vectors of size $k$ where the only index</span>
<span id="cb12-1987"><a href="#cb12-1987"></a>    with value 1 is the predicted class and all other positions have a</span>
<span id="cb12-1988"><a href="#cb12-1988"></a>    value of 0.</span>
<span id="cb12-1989"><a href="#cb12-1989"></a></span>
<span id="cb12-1990"><a href="#cb12-1990"></a><span class="ss">-   </span>The confusion matrix for a classifier, $h$, is</span>
<span id="cb12-1991"><a href="#cb12-1991"></a>    $C(h, \mathbb{P}) \in \mathbb{R}^{k \times k}$, where:</span>
<span id="cb12-1992"><a href="#cb12-1992"></a>    $$C_{ij}(h, \mathbb{P}) = \mathbb{P}(Y=i, h=j) \text{\qquad for } i, j \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$$  {#eq-eq3.59}</span>
<span id="cb12-1993"><a href="#cb12-1993"></a></span>
<span id="cb12-1994"><a href="#cb12-1994"></a>Note that the confusion matrices are $k\times k$ and store the joint</span>
<span id="cb12-1995"><a href="#cb12-1995"></a>probabilities of each type of classification for each possible class.</span>
<span id="cb12-1996"><a href="#cb12-1996"></a>This means that the sum of row $i$ in the confusion matrix equals</span>
<span id="cb12-1997"><a href="#cb12-1997"></a>$\xi_i$, because this is equivalent to adding over all possible</span>
<span id="cb12-1998"><a href="#cb12-1998"></a>classifications. Since we know the sums of each row, all diagonal</span>
<span id="cb12-1999"><a href="#cb12-1999"></a>elements can be reconstructed from just the off-diagonal elements, so a</span>
<span id="cb12-2000"><a href="#cb12-2000"></a>confusion matrix $C(h, \mathbb{P})$ can be expressed as a vector of</span>
<span id="cb12-2001"><a href="#cb12-2001"></a>off-diagonal elements,</span>
<span id="cb12-2002"><a href="#cb12-2002"></a>$\vec{c}(h, \mathbb{P}) = \textit{off-diag}(C(h, \mathbb{P}))$, and</span>
<span id="cb12-2003"><a href="#cb12-2003"></a>$\vec{c} \in \mathbb{R}^q$ where $q := k^2 - k$. The vector $\vec{c}$ is</span>
<span id="cb12-2004"><a href="#cb12-2004"></a>called the vector of *'off-diagonal confusions.'* The space of</span>
<span id="cb12-2005"><a href="#cb12-2005"></a>off-diagonal confusions is</span>
<span id="cb12-2006"><a href="#cb12-2006"></a>$\mathcal{C} = <span class="sc">\{</span>\vec{c}(h, \mathbb{P}) : h \in \mathcal{H}<span class="sc">\}</span>$.</span>
<span id="cb12-2007"><a href="#cb12-2007"></a></span>
<span id="cb12-2008"><a href="#cb12-2008"></a>In cases where the oracle would care about the exact type of</span>
<span id="cb12-2009"><a href="#cb12-2009"></a>misclassification (i.e. misclassifying and object from class 1 as class</span>
<span id="cb12-2010"><a href="#cb12-2010"></a>2), this off-diagonal confusion matrix is necessary. However, there are</span>
<span id="cb12-2011"><a href="#cb12-2011"></a>many cases where the performance of a classifier is determined by just</span>
<span id="cb12-2012"><a href="#cb12-2012"></a>the probability of correct prediction for each class, which just</span>
<span id="cb12-2013"><a href="#cb12-2013"></a>requires the diagonal elements. In these cases, we can define the vector</span>
<span id="cb12-2014"><a href="#cb12-2014"></a>of *'diagonal confusions'* as</span>
<span id="cb12-2015"><a href="#cb12-2015"></a>$\vec{d}(h, \mathbb{P}) = \textit{diag}(C(h, \mathbb{P})) \in \mathbb{R}^k$.</span>
<span id="cb12-2016"><a href="#cb12-2016"></a>The space of diagonal confusions is</span>
<span id="cb12-2017"><a href="#cb12-2017"></a>$\mathcal{D} = <span class="sc">\{</span>\vec{d}(h, \mathbb{P}) : h \in \mathcal{H}<span class="sc">\}</span>$.</span>
<span id="cb12-2018"><a href="#cb12-2018"></a></span>
<span id="cb12-2019"><a href="#cb12-2019"></a>Finally, the setup for metric elicitation is identical to the one</span>
<span id="cb12-2020"><a href="#cb12-2020"></a>examined in the previous chapter. We still assume access to an oracle</span>
<span id="cb12-2021"><a href="#cb12-2021"></a>that can choose between two classifiers or confusion matrices, using</span>
<span id="cb12-2022"><a href="#cb12-2022"></a>notation $\Gamma$ for comparing two classifiers and $\Omega$ for</span>
<span id="cb12-2023"><a href="#cb12-2023"></a>comparing confusion matrices, which returns 1 if the first classifier is</span>
<span id="cb12-2024"><a href="#cb12-2024"></a>better and 0 otherwise. We still assume that the oracle behaves</span>
<span id="cb12-2025"><a href="#cb12-2025"></a>according to some unknown performance metric, and we wish to recover</span>
<span id="cb12-2026"><a href="#cb12-2026"></a>this metric up to some small error tolerance (based on a suitable norm).</span>
<span id="cb12-2027"><a href="#cb12-2027"></a></span>
<span id="cb12-2028"><a href="#cb12-2028"></a>The two different types of confusion vectors result in different</span>
<span id="cb12-2029"><a href="#cb12-2029"></a>algorithms for metric elicitation, which we will explore in later</span>
<span id="cb12-2030"><a href="#cb12-2030"></a>sections.</span>
<span id="cb12-2031"><a href="#cb12-2031"></a></span>
<span id="cb12-2032"><a href="#cb12-2032"></a><span class="fu">#### Introduction to Diagonal Linear Performance Metric Elicitation {.unnumbered}</span></span>
<span id="cb12-2033"><a href="#cb12-2033"></a></span>
<span id="cb12-2034"><a href="#cb12-2034"></a>A Diagonal Linear Performance Metric (DLPM) is a performance metric that</span>
<span id="cb12-2035"><a href="#cb12-2035"></a>only considers the diagonal elements in the confusion matrix. The metric</span>
<span id="cb12-2036"><a href="#cb12-2036"></a>is defined as $\psi(\vec{d}) = \langle \vec{a}, \vec{d} \rangle$, where</span>
<span id="cb12-2037"><a href="#cb12-2037"></a>$\vec{a} \in \mathbb{R}^k$ such that $||\vec{a}||_1 = 1$. It is also</span>
<span id="cb12-2038"><a href="#cb12-2038"></a>called weighted accuracy <span class="co">[</span><span class="ot">@pmlr-v37-narasimhanb15</span><span class="co">]</span>.</span>
<span id="cb12-2039"><a href="#cb12-2039"></a></span>
<span id="cb12-2040"><a href="#cb12-2040"></a>The family of DLPMs is denoted as $\varphi_{DLPM}$. Since these only</span>
<span id="cb12-2041"><a href="#cb12-2041"></a>consider the diagonal elements, which we want to maximize, we can focus</span>
<span id="cb12-2042"><a href="#cb12-2042"></a>on only eliciting monotonically increasing DLPMs, meaning that all</span>
<span id="cb12-2043"><a href="#cb12-2043"></a>elements in $\vec{a}$ are non-negative.</span>
<span id="cb12-2044"><a href="#cb12-2044"></a></span>
<span id="cb12-2045"><a href="#cb12-2045"></a><span class="fu">#### Geometry of Space of Diagonal Confusions $\mathcal{D}$ {.unnumbered}</span></span>
<span id="cb12-2046"><a href="#cb12-2046"></a></span>
<span id="cb12-2047"><a href="#cb12-2047"></a>Consider the trivial classifiers that only predict a single class at all</span>
<span id="cb12-2048"><a href="#cb12-2048"></a>times. The diagonal confusions when only predicting class $i$ are</span>
<span id="cb12-2049"><a href="#cb12-2049"></a>$\vec{v}_i \in \mathbb{R}^k$ with $\xi_i$ at index $i$ and zero</span>
<span id="cb12-2050"><a href="#cb12-2050"></a>elsewhere. Note that this is the maximum possible value in index $i$,</span>
<span id="cb12-2051"><a href="#cb12-2051"></a>because this represents perfectly classifying all points that have a</span>
<span id="cb12-2052"><a href="#cb12-2052"></a>true class of $i$.</span>
<span id="cb12-2053"><a href="#cb12-2053"></a></span>
<span id="cb12-2054"><a href="#cb12-2054"></a>We can consider the space of diagonal confusions, visualized in @fig-diag_geom (taken</span>
<span id="cb12-2055"><a href="#cb12-2055"></a>from <span class="co">[</span><span class="ot">@NEURIPS2019_1fd09c5f</span><span class="co">]</span>). The space of $\mathcal{D}$ is strictly</span>
<span id="cb12-2056"><a href="#cb12-2056"></a>convex, closed, and contained in the box</span>
<span id="cb12-2057"><a href="#cb12-2057"></a>$<span class="co">[</span><span class="ot">0, \xi_1</span><span class="co">]</span> \times \dots \times <span class="co">[</span><span class="ot">0, \xi_k</span><span class="co">]</span>$. We also know that the only</span>
<span id="cb12-2058"><a href="#cb12-2058"></a>vertices are $\vec{v}_i$ for each $i \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>^{(k-1)}$.</span>
<span id="cb12-2059"><a href="#cb12-2059"></a></span>
<span id="cb12-2060"><a href="#cb12-2060"></a>![(a) Geometry of space of diagonal confusions for $k=3$. This is a</span>
<span id="cb12-2061"><a href="#cb12-2061"></a>convex region with three flat areas representing confusions when</span>
<span id="cb12-2062"><a href="#cb12-2062"></a>restricted to only two classes. (b) Geometry of diagonal confusions when</span>
<span id="cb12-2063"><a href="#cb12-2063"></a>restricted to classes $k_1$ and $k_2$. Notice how this is identical to</span>
<span id="cb12-2064"><a href="#cb12-2064"></a>the space of confusion matrices examined in the previous</span>
<span id="cb12-2065"><a href="#cb12-2065"></a>chapter.](Figures/diag_geometry.png){#fig-diag_geom width="<span class="sc">\\</span>textwidth"}</span>
<span id="cb12-2066"><a href="#cb12-2066"></a></span>
<span id="cb12-2067"><a href="#cb12-2067"></a>We know that this is strictly convex under the assumption that an object</span>
<span id="cb12-2068"><a href="#cb12-2068"></a>from any class can be misclassified as any other class. Mathematically,</span>
<span id="cb12-2069"><a href="#cb12-2069"></a>the assumption is that</span>
<span id="cb12-2070"><a href="#cb12-2070"></a>$g_{ij}(r) = \mathbb{P} \left<span class="co">[</span><span class="ot">\frac{\eta_i(X)}{\eta_j(X)} \geq r \right</span><span class="co">]</span>$</span>
<span id="cb12-2071"><a href="#cb12-2071"></a>$\forall i, j \in <span class="co">[</span><span class="ot">k</span><span class="co">]</span>$ are continuous and strictly decreasing for</span>
<span id="cb12-2072"><a href="#cb12-2072"></a>$r \in [0, \infty)$.</span>
<span id="cb12-2073"><a href="#cb12-2073"></a></span>
<span id="cb12-2074"><a href="#cb12-2074"></a>We can also define the space of binary classification confusion matrices</span>
<span id="cb12-2075"><a href="#cb12-2075"></a>confined to classes $k_1$ and $k_2$, which is the 2-D $(k_1, k_2)$</span>
<span id="cb12-2076"><a href="#cb12-2076"></a>axis-aligned face of $\mathcal{D}$, denoted as $\mathcal{D}_{k_1, k_2}$.</span>
<span id="cb12-2077"><a href="#cb12-2077"></a>Note that this is strictly convex, since $\mathcal{D}$ itself is</span>
<span id="cb12-2078"><a href="#cb12-2078"></a>strictly convex, and it has the same geometry as the space of binary</span>
<span id="cb12-2079"><a href="#cb12-2079"></a>confusion matrices examined in the previous chapter. Therefore, we can</span>
<span id="cb12-2080"><a href="#cb12-2080"></a>construct a Restricted Bayes Optimal (RBO) classifier for $\psi \in \varphi_{DLPM}$, parameterized</span>
<span id="cb12-2081"><a href="#cb12-2081"></a>by $\vec{a}$, as follows: </span>
<span id="cb12-2082"><a href="#cb12-2082"></a>$$\begin{aligned}</span>
<span id="cb12-2083"><a href="#cb12-2083"></a>\bar{h}_{k_1, k_2}(\vec{x})= \left<span class="sc">\{</span></span>
<span id="cb12-2084"><a href="#cb12-2084"></a>\begin{array}{ll}</span>
<span id="cb12-2085"><a href="#cb12-2085"></a>      k_1, \text{ if } a_{k_1} \eta_{k_1}(\vec{x}) \geq a_{k_2} \eta_{k_2}(\vec{x})<span class="sc">\\</span></span>
<span id="cb12-2086"><a href="#cb12-2086"></a>k_2, \text{ o.w.}</span>
<span id="cb12-2087"><a href="#cb12-2087"></a>\end{array}</span>
<span id="cb12-2088"><a href="#cb12-2088"></a>\right<span class="sc">\}</span>.</span>
<span id="cb12-2089"><a href="#cb12-2089"></a>\end{aligned}$$ {#eq-rbo_eq}</span>
<span id="cb12-2090"><a href="#cb12-2090"></a></span>
<span id="cb12-2091"><a href="#cb12-2091"></a>We can parameterize the upper boundary of $\mathcal{D}_{k_1, k_2}$,</span>
<span id="cb12-2092"><a href="#cb12-2092"></a>denoted as $\partial \mathcal{D}^{+}_{k_1, k_2}$, using a single</span>
<span id="cb12-2093"><a href="#cb12-2093"></a>parameter $m \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$. Specifically, we can construct a DLPM by</span>
<span id="cb12-2094"><a href="#cb12-2094"></a>setting $a_{k_1} = m$, $a_{k_2} = 1 - m$, and all others to 0. Using</span>
<span id="cb12-2095"><a href="#cb12-2095"></a><span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>, we can get the diagonal confusions, so varying $m$ parameterizes</span>
<span id="cb12-2096"><a href="#cb12-2096"></a>$\partial \mathcal{D}^{+}_{k_1, k_2}$. The parameterization is denoted</span>
<span id="cb12-2097"><a href="#cb12-2097"></a>as $\nu(m; k_1, k_2)$.</span>
<span id="cb12-2098"><a href="#cb12-2098"></a></span>
<span id="cb12-2099"><a href="#cb12-2099"></a><span class="fu">#### Diagonal Linear Performance Metric Elicitation {.unnumbered}</span></span>
<span id="cb12-2100"><a href="#cb12-2100"></a></span>
<span id="cb12-2101"><a href="#cb12-2101"></a>Suppose the oracle follows a true metric, $\psi$, that is linear and</span>
<span id="cb12-2102"><a href="#cb12-2102"></a>monotone increasing across all axes. If we consider the composition</span>
<span id="cb12-2103"><a href="#cb12-2103"></a>$\psi \circ \nu(m; k_1, k_2): <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> \rightarrow \mathbb{R}$, we know it</span>
<span id="cb12-2104"><a href="#cb12-2104"></a>must be concave and unimodal, because $\mathcal{D}_{k_1, k_2}$ is a</span>
<span id="cb12-2105"><a href="#cb12-2105"></a>convex set. Therefore, we can find the value of $m$ that maximizes</span>
<span id="cb12-2106"><a href="#cb12-2106"></a>$\psi \circ \nu(m; k_1, k_2)$ for any given $k_1$ and $k_2$ using a</span>
<span id="cb12-2107"><a href="#cb12-2107"></a>binary search procedure.</span>
<span id="cb12-2108"><a href="#cb12-2108"></a></span>
<span id="cb12-2109"><a href="#cb12-2109"></a>Since the RBO classifier for classes $k_1$ and $k_2$ only rely on the</span>
<span id="cb12-2110"><a href="#cb12-2110"></a>relative weights of the classes in the DLPM (see <span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>), finding</span>
<span id="cb12-2111"><a href="#cb12-2111"></a>the value of $m$ that maximizes $\psi \circ \nu(m; k_1, k_2)$ gives us</span>
<span id="cb12-2112"><a href="#cb12-2112"></a>the true relative ratio between $a_{k_1}$ and $a_{k_2}$. Specifically,</span>
<span id="cb12-2113"><a href="#cb12-2113"></a>from the definition of $\nu$, we know that</span>
<span id="cb12-2114"><a href="#cb12-2114"></a>$\frac{a_{k_2}}{a_{k_1}} = \frac{1-m}{m}$. We can therefore simply</span>
<span id="cb12-2115"><a href="#cb12-2115"></a>calculate the ratio between $a_1$ and all other weights to reconstruct</span>
<span id="cb12-2116"><a href="#cb12-2116"></a>an estimate for the true metric. A python implementation of this</span>
<span id="cb12-2117"><a href="#cb12-2117"></a>algorithm is provided below.</span>
<span id="cb12-2118"><a href="#cb12-2118"></a></span>
<span id="cb12-2119"><a href="#cb12-2119"></a>::: {.callout-note title="code"}</span>
<span id="cb12-2122"><a href="#cb12-2122"></a><span class="in">```{python}</span></span>
<span id="cb12-2123"><a href="#cb12-2123"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2124"><a href="#cb12-2124"></a></span>
<span id="cb12-2125"><a href="#cb12-2125"></a><span class="kw">def</span> rbo_dlpm(m, k1, k2, k):</span>
<span id="cb12-2126"><a href="#cb12-2126"></a>    <span class="co">"""</span></span>
<span id="cb12-2127"><a href="#cb12-2127"></a><span class="co">    This constructs DLPM weights for the upper boundary of the</span></span>
<span id="cb12-2128"><a href="#cb12-2128"></a><span class="co">    restricted diagonal confusions, given a parameter m.</span></span>
<span id="cb12-2129"><a href="#cb12-2129"></a><span class="co">    This is equivalent to </span><span class="ch">\n</span><span class="co">u(m; k1, k2)</span></span>
<span id="cb12-2130"><a href="#cb12-2130"></a><span class="co">    </span></span>
<span id="cb12-2131"><a href="#cb12-2131"></a><span class="co">    Inputs:</span></span>
<span id="cb12-2132"><a href="#cb12-2132"></a><span class="co">    - m: parameter (between 0 and 1) for the upper boundary</span></span>
<span id="cb12-2133"><a href="#cb12-2133"></a><span class="co">    - k1: first axis for this  face</span></span>
<span id="cb12-2134"><a href="#cb12-2134"></a><span class="co">    - k2: second axis for this face</span></span>
<span id="cb12-2135"><a href="#cb12-2135"></a><span class="co">    - k: number of classes</span></span>
<span id="cb12-2136"><a href="#cb12-2136"></a><span class="co">    Outputs:</span></span>
<span id="cb12-2137"><a href="#cb12-2137"></a><span class="co">    - DLPM weights for this point on the upper boundary</span></span>
<span id="cb12-2138"><a href="#cb12-2138"></a><span class="co">    """</span></span>
<span id="cb12-2139"><a href="#cb12-2139"></a>    new_a <span class="op">=</span> np.zeros(k)</span>
<span id="cb12-2140"><a href="#cb12-2140"></a>    new_a[k1] <span class="op">=</span> m</span>
<span id="cb12-2141"><a href="#cb12-2141"></a>    new_a[k2] <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> m</span>
<span id="cb12-2142"><a href="#cb12-2142"></a>    <span class="cf">return</span> new_a</span>
<span id="cb12-2143"><a href="#cb12-2143"></a></span>
<span id="cb12-2144"><a href="#cb12-2144"></a><span class="kw">def</span> dlpm_elicitation(epsilon, oracle, get_d, k):</span>
<span id="cb12-2145"><a href="#cb12-2145"></a>    <span class="co">"""</span></span>
<span id="cb12-2146"><a href="#cb12-2146"></a><span class="co">    Inputs:</span></span>
<span id="cb12-2147"><a href="#cb12-2147"></a><span class="co">    - epsilon: some epsilon &gt; 0 representing threshold of error</span></span>
<span id="cb12-2148"><a href="#cb12-2148"></a><span class="co">    - oracle: some function that accepts 2 confusion matrices and</span></span>
<span id="cb12-2149"><a href="#cb12-2149"></a><span class="co">        returns true if the first is preferred and false otherwise</span></span>
<span id="cb12-2150"><a href="#cb12-2150"></a><span class="co">    - get_d: some function that accepts dlpm weights and returns </span></span>
<span id="cb12-2151"><a href="#cb12-2151"></a><span class="co">        diagonal confusions</span></span>
<span id="cb12-2152"><a href="#cb12-2152"></a><span class="co">    - k: number of classes</span></span>
<span id="cb12-2153"><a href="#cb12-2153"></a><span class="co">    Outputs:</span></span>
<span id="cb12-2154"><a href="#cb12-2154"></a><span class="co">    - estimate for true DLPM weights</span></span>
<span id="cb12-2155"><a href="#cb12-2155"></a><span class="co">    """</span></span>
<span id="cb12-2156"><a href="#cb12-2156"></a>    a_hat <span class="op">=</span> np.zeros(k)</span>
<span id="cb12-2157"><a href="#cb12-2157"></a>    a_hat[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-2158"><a href="#cb12-2158"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, k):</span>
<span id="cb12-2159"><a href="#cb12-2159"></a>        <span class="co"># iterate over each axis to find appropriate ratio</span></span>
<span id="cb12-2160"><a href="#cb12-2160"></a>        a <span class="op">=</span> <span class="dv">0</span>  <span class="co"># lower bound of binary search</span></span>
<span id="cb12-2161"><a href="#cb12-2161"></a>        b <span class="op">=</span> <span class="dv">1</span>  <span class="co"># upper bound of binary search</span></span>
<span id="cb12-2162"><a href="#cb12-2162"></a></span>
<span id="cb12-2163"><a href="#cb12-2163"></a>        <span class="cf">while</span> (b <span class="op">-</span> a <span class="op">&gt;</span> epsilon):</span>
<span id="cb12-2164"><a href="#cb12-2164"></a>            c <span class="op">=</span> (<span class="dv">3</span> <span class="op">*</span> a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb12-2165"><a href="#cb12-2165"></a>            d <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb12-2166"><a href="#cb12-2166"></a>            e <span class="op">=</span> (a <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> b) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb12-2167"><a href="#cb12-2167"></a></span>
<span id="cb12-2168"><a href="#cb12-2168"></a>            <span class="co"># get diagonal confusions for each point</span></span>
<span id="cb12-2169"><a href="#cb12-2169"></a>            d_a, d_c, d_d, d_e, d_b <span class="op">=</span> (get_d(rbo_dlpm(x, <span class="dv">0</span>, i, k)) </span>
<span id="cb12-2170"><a href="#cb12-2170"></a>                <span class="cf">for</span> x <span class="kw">in</span> [a, c, d, e, b])</span>
<span id="cb12-2171"><a href="#cb12-2171"></a></span>
<span id="cb12-2172"><a href="#cb12-2172"></a>            <span class="co"># query oracle for each pair</span></span>
<span id="cb12-2173"><a href="#cb12-2173"></a>            response_ac <span class="op">=</span> oracle(d_a, d_c)</span>
<span id="cb12-2174"><a href="#cb12-2174"></a>            response_cd <span class="op">=</span> oracle(d_c, d_d)</span>
<span id="cb12-2175"><a href="#cb12-2175"></a>            response_de <span class="op">=</span> oracle(d_d, d_e)</span>
<span id="cb12-2176"><a href="#cb12-2176"></a>            response_eb <span class="op">=</span> oracle(d_e, d_b)</span>
<span id="cb12-2177"><a href="#cb12-2177"></a></span>
<span id="cb12-2178"><a href="#cb12-2178"></a>            <span class="co"># update ranges to keep the peak</span></span>
<span id="cb12-2179"><a href="#cb12-2179"></a>            <span class="cf">if</span> response_ac:</span>
<span id="cb12-2180"><a href="#cb12-2180"></a>                b <span class="op">=</span> d</span>
<span id="cb12-2181"><a href="#cb12-2181"></a>            <span class="cf">elif</span> response_cd:</span>
<span id="cb12-2182"><a href="#cb12-2182"></a>                b <span class="op">=</span> d</span>
<span id="cb12-2183"><a href="#cb12-2183"></a>            <span class="cf">elif</span> response_de:</span>
<span id="cb12-2184"><a href="#cb12-2184"></a>                a <span class="op">=</span> c</span>
<span id="cb12-2185"><a href="#cb12-2185"></a>                b <span class="op">=</span> e</span>
<span id="cb12-2186"><a href="#cb12-2186"></a>            <span class="cf">elif</span> response_eb:</span>
<span id="cb12-2187"><a href="#cb12-2187"></a>                a <span class="op">=</span> d</span>
<span id="cb12-2188"><a href="#cb12-2188"></a>            <span class="cf">else</span>:</span>
<span id="cb12-2189"><a href="#cb12-2189"></a>                a <span class="op">=</span> d</span>
<span id="cb12-2190"><a href="#cb12-2190"></a></span>
<span id="cb12-2191"><a href="#cb12-2191"></a>        midpt <span class="op">=</span> (a <span class="op">+</span> b) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb12-2192"><a href="#cb12-2192"></a>        a_hat[i] <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> midpt) <span class="op">/</span> midpt</span>
<span id="cb12-2193"><a href="#cb12-2193"></a>    <span class="cf">return</span> a_hat <span class="op">/</span> np.<span class="bu">sum</span>(a_hat)</span>
<span id="cb12-2194"><a href="#cb12-2194"></a><span class="in">```</span></span>
<span id="cb12-2195"><a href="#cb12-2195"></a>:::</span>
<span id="cb12-2196"><a href="#cb12-2196"></a></span>
<span id="cb12-2197"><a href="#cb12-2197"></a>To use this algorithm for metric elicitation on a real dataset, we need</span>
<span id="cb12-2198"><a href="#cb12-2198"></a>to supply the "oracle" and "get_d" functions. The oracle function is an</span>
<span id="cb12-2199"><a href="#cb12-2199"></a>interface to an expert who judges which of two confusion matrices is</span>
<span id="cb12-2200"><a href="#cb12-2200"></a>better. The get_d function will need to construct a classifier given the</span>
<span id="cb12-2201"><a href="#cb12-2201"></a>DLPM weights, following the principles of the RBO classifier from <span class="co">[</span><span class="ot">@eq-rbo_eq</span><span class="co">]</span>,</span>
<span id="cb12-2202"><a href="#cb12-2202"></a>and calculate the confusion matrix from a validation set.</span>
<span id="cb12-2203"><a href="#cb12-2203"></a></span>
<span id="cb12-2204"><a href="#cb12-2204"></a><span class="fu">#### Guarantees {.unnumbered}</span></span>
<span id="cb12-2205"><a href="#cb12-2205"></a></span>
<span id="cb12-2206"><a href="#cb12-2206"></a>Using the same oracle feedback noise model from the binary metric</span>
<span id="cb12-2207"><a href="#cb12-2207"></a>elicitation, we can make the following guarantees:</span>
<span id="cb12-2208"><a href="#cb12-2208"></a></span>
<span id="cb12-2209"><a href="#cb12-2209"></a>::: {.callout-note title="proposition"}</span>
<span id="cb12-2210"><a href="#cb12-2210"></a>::: {#prop-prop_dlpm}</span>
<span id="cb12-2211"><a href="#cb12-2211"></a>Given $\epsilon, \epsilon_\Omega \geq 0$, and a 1-Lipschitz DLPM</span>
<span id="cb12-2212"><a href="#cb12-2212"></a>$\varphi^*$ parameterized by $\vec{a}^*$. Then the output $\hat{a}$ of</span>
<span id="cb12-2213"><a href="#cb12-2213"></a>the DLPM elicitation algorithm after $O((k-1)\log\frac{1}{\epsilon})$</span>
<span id="cb12-2214"><a href="#cb12-2214"></a>queries to the oracle satisfies</span>
<span id="cb12-2215"><a href="#cb12-2215"></a>$||\vec{a}^* - \hat{a}||_\infty \leq O(\epsilon + \sqrt{\epsilon_\Omega})$,</span>
<span id="cb12-2216"><a href="#cb12-2216"></a>which is equivalent to</span>
<span id="cb12-2217"><a href="#cb12-2217"></a>$||\vec{a}^* - \hat{a}||_2 \leq O(\sqrt{k}(\epsilon + \sqrt{\epsilon_\Omega}))$.</span>
<span id="cb12-2218"><a href="#cb12-2218"></a>:::</span>
<span id="cb12-2219"><a href="#cb12-2219"></a>:::</span>
<span id="cb12-2220"><a href="#cb12-2220"></a></span>
<span id="cb12-2221"><a href="#cb12-2221"></a>In other words, the maximum difference between the estimate and true</span>
<span id="cb12-2222"><a href="#cb12-2222"></a>value along any component (indicated by the L-infinity norm) is linearly</span>
<span id="cb12-2223"><a href="#cb12-2223"></a>bounded by the sum of the epsilon specified by the algorithm and the</span>
<span id="cb12-2224"><a href="#cb12-2224"></a>square root of the oracle's correctness guarantee ($\epsilon_\Omega$).</span>
<span id="cb12-2225"><a href="#cb12-2225"></a></span>
<span id="cb12-2226"><a href="#cb12-2226"></a><span class="fu">### Linear Reward Estimation</span></span>
<span id="cb12-2227"><a href="#cb12-2227"></a></span>
<span id="cb12-2228"><a href="#cb12-2228"></a>How exactly do robots learn human preferences from just the pairwise</span>
<span id="cb12-2229"><a href="#cb12-2229"></a>comparisons, if they need to learn how to act in the environment itself?</span>
<span id="cb12-2230"><a href="#cb12-2230"></a>The comparisons in turn help robots learn the reward function of the</span>
<span id="cb12-2231"><a href="#cb12-2231"></a>human, which allows them to further take actions in real settings.</span>
<span id="cb12-2232"><a href="#cb12-2232"></a></span>
<span id="cb12-2233"><a href="#cb12-2233"></a><span class="fu">#### Geometry of Pairwise Comparisons {.unnumbered}</span></span>
<span id="cb12-2234"><a href="#cb12-2234"></a></span>
<span id="cb12-2235"><a href="#cb12-2235"></a>Let's say there are two trajectories $\xi_A$ and $\xi_B$ that might be</span>
<span id="cb12-2236"><a href="#cb12-2236"></a>taken as the next course of action in any context, like choosing the</span>
<span id="cb12-2237"><a href="#cb12-2237"></a>next turn, or choosing the next chatGPT response. The robot is offering</span>
<span id="cb12-2238"><a href="#cb12-2238"></a>both to a human for comparison. To answer which of them is better, the</span>
<span id="cb12-2239"><a href="#cb12-2239"></a>human would ask themselves if $R(\xi_A)$ or $R(\xi_B)$ is bigger, with</span>
<span id="cb12-2240"><a href="#cb12-2240"></a>$R(\xi) = w * \phi(\xi)$ being the reward function. In this equation $w$</span>
<span id="cb12-2241"><a href="#cb12-2241"></a>and $\phi(\xi)$ are vectors of weights and features of the trajectory,</span>
<span id="cb12-2242"><a href="#cb12-2242"></a>so alternatively, we can express this as:</span>
<span id="cb12-2243"><a href="#cb12-2243"></a></span>
<span id="cb12-2244"><a href="#cb12-2244"></a>$$R(\xi) = \begin{bmatrix} w_1 <span class="sc">\\</span> w_2 <span class="sc">\\</span> ... <span class="sc">\\</span> w_N \end{bmatrix} \cdot \begin{bmatrix} \phi_1(\xi) <span class="sc">\\</span> \phi_2(\xi) <span class="sc">\\</span> ... <span class="sc">\\</span> \phi_N(\xi) \end{bmatrix}$$ {#eq-reward_eq}</span>
<span id="cb12-2245"><a href="#cb12-2245"></a></span>
<span id="cb12-2246"><a href="#cb12-2246"></a>If one says that they preferred $\xi_2$ less than $\xi_1$ then it means</span>
<span id="cb12-2247"><a href="#cb12-2247"></a>$\xi_2 &lt; \xi_1 \implies R(\xi_2) &lt; R(\xi_1) \implies w * \phi(\xi_2) &lt; w * \phi(\xi_1) \implies 0 &lt; w * (\phi(\xi_1) - \phi(\xi_2)) \implies 0 &lt; w * \Phi$.</span>
<span id="cb12-2248"><a href="#cb12-2248"></a>Alternatively, if one preferred $\xi_2$ more than $\xi_1$, the signs</span>
<span id="cb12-2249"><a href="#cb12-2249"></a>would be flipped, resulting in $0 &gt; w * \Phi$. The two results can be</span>
<span id="cb12-2250"><a href="#cb12-2250"></a>represented in the N-dimensional space, where when it is split by the</span>
<span id="cb12-2251"><a href="#cb12-2251"></a>decision boundary, it creates half-spaces indicating preferences for</span>
<span id="cb12-2252"><a href="#cb12-2252"></a>each of the sides. For example in @fig-2dcomp we can</span>
<span id="cb12-2253"><a href="#cb12-2253"></a>see how a query between two objects can split the plain into two halves,</span>
<span id="cb12-2254"><a href="#cb12-2254"></a>indicating preference towards one of the objects. Such an image can be</span>
<span id="cb12-2255"><a href="#cb12-2255"></a>extended into bigger dimensions, where a line would become a separating</span>
<span id="cb12-2256"><a href="#cb12-2256"></a>hyperplane like in @fig-2dcomp.</span>
<span id="cb12-2257"><a href="#cb12-2257"></a></span>
<span id="cb12-2258"><a href="#cb12-2258"></a></span>
<span id="cb12-2259"><a href="#cb12-2259"></a>::: {#fig-2dcomp layout-ncol=2}</span>
<span id="cb12-2260"><a href="#cb12-2260"></a></span>
<span id="cb12-2261"><a href="#cb12-2261"></a>![A single query for a comparison between the two objects splits 2D space</span>
<span id="cb12-2262"><a href="#cb12-2262"></a>into two halves, each of which prefers one of the objects based on</span>
<span id="cb12-2263"><a href="#cb12-2263"></a>feature weights $w_1$ and $w_2$.](Figures/2D-comp.jpg){width="40%"}</span>
<span id="cb12-2264"><a href="#cb12-2264"></a></span>
<span id="cb12-2265"><a href="#cb12-2265"></a><span class="al">![Extension into 3D space](Figures/3D-comp.png)</span>{width="40%"}</span>
<span id="cb12-2266"><a href="#cb12-2266"></a></span>
<span id="cb12-2267"><a href="#cb12-2267"></a>Comparison in 2D and 3D</span>
<span id="cb12-2268"><a href="#cb12-2268"></a>:::</span>
<span id="cb12-2269"><a href="#cb12-2269"></a></span>
<span id="cb12-2270"><a href="#cb12-2270"></a>If one is to truly believe the answers of one person, they would remove</span>
<span id="cb12-2271"><a href="#cb12-2271"></a>everything from the other side of the hyperplane that does not agree</span>
<span id="cb12-2272"><a href="#cb12-2272"></a>with the received human preference. But since humans are noisy, that</span>
<span id="cb12-2273"><a href="#cb12-2273"></a>approach is not optimal, thus most applications up-weight the indicated</span>
<span id="cb12-2274"><a href="#cb12-2274"></a>side of the plane to emphasize that points on that side are better, and</span>
<span id="cb12-2275"><a href="#cb12-2275"></a>down-weight the other side as they do not agree with the provided</span>
<span id="cb12-2276"><a href="#cb12-2276"></a>comparison.</span>
<span id="cb12-2277"><a href="#cb12-2277"></a></span>
<span id="cb12-2278"><a href="#cb12-2278"></a>How should someone choose which queries to conduct, otherwise, what is</span>
<span id="cb12-2279"><a href="#cb12-2279"></a>the most informative query sequence? After completing one query, the</span>
<span id="cb12-2280"><a href="#cb12-2280"></a>next query should be orthogonal to the previous one so that the</span>
<span id="cb12-2281"><a href="#cb12-2281"></a>potential space consistent with the preferences decreases in half. The</span>
<span id="cb12-2282"><a href="#cb12-2282"></a>intuition behind that is the potential space has all of the reward</span>
<span id="cb12-2283"><a href="#cb12-2283"></a>functions that agree with the provided answers, so to find a specific</span>
<span id="cb12-2284"><a href="#cb12-2284"></a>reward function for a human, decreasing the space narrows down the</span>
<span id="cb12-2285"><a href="#cb12-2285"></a>possible options. For example, orthogonal query to the query in @fig-2dcomp is</span>
<span id="cb12-2286"><a href="#cb12-2286"></a>shown in @fig-2dspace. The original query created the blue space, and</span>
<span id="cb12-2287"><a href="#cb12-2287"></a>a new one created a red space, resulting in a purple intersection of the</span>
<span id="cb12-2288"><a href="#cb12-2288"></a>two which is still consistent with both of the queries's results. The</span>
<span id="cb12-2289"><a href="#cb12-2289"></a>image shows that the purple portion is exactly half of the blue portion.</span>
<span id="cb12-2290"><a href="#cb12-2290"></a></span>
<span id="cb12-2291"><a href="#cb12-2291"></a>![Creating further comparisons limits the space that agrees with answers</span>
<span id="cb12-2292"><a href="#cb12-2292"></a>to all of them. The blue area demonstrates a preference for object 1</span>
<span id="cb12-2293"><a href="#cb12-2293"></a>over object 2. The red area demonstrates a preference for object 3 over</span>
<span id="cb12-2294"><a href="#cb12-2294"></a>object 4. Combination (purple area) shows the space that is consistent</span>
<span id="cb12-2295"><a href="#cb12-2295"></a>with both of those preferences.](Figures/2D-space.jpg){#fig-2dspace</span>
<span id="cb12-2296"><a href="#cb12-2296"></a>width="40%"}</span>
<span id="cb12-2297"><a href="#cb12-2297"></a></span>
<span id="cb12-2298"><a href="#cb12-2298"></a>Mathematically, from <span class="co">[</span><span class="ot">@pmlr-v87-biyik18a</span><span class="co">]</span> this can be expressed as set</span>
<span id="cb12-2299"><a href="#cb12-2299"></a>$F$ of potential queries $\phi$, where</span>
<span id="cb12-2300"><a href="#cb12-2300"></a>$F = <span class="sc">\{</span>\phi: \phi = \Phi(\xi_A) - \Phi(\xi_B), \xi_A, \xi_B \in \Xi<span class="sc">\}</span>$</span>
<span id="cb12-2301"><a href="#cb12-2301"></a>(defining that a query is the difference between the features of two</span>
<span id="cb12-2302"><a href="#cb12-2302"></a>trajectories). Using that, the authors define a human update function</span>
<span id="cb12-2303"><a href="#cb12-2303"></a>$f_{\phi}(w) = \min(1, \exp(I^T\phi))$ that accounts for how much of the</span>
<span id="cb12-2304"><a href="#cb12-2304"></a>space will still be consistent with the preferences. Finally, for a</span>
<span id="cb12-2305"><a href="#cb12-2305"></a>specific query, they define the minimum volume removed as</span>
<span id="cb12-2306"><a href="#cb12-2306"></a>$\min<span class="sc">\{</span>\mathbb{E}<span class="co">[</span><span class="ot">1 - f_{\phi}(w)</span><span class="co">]</span>, \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{-\phi}(w)</span><span class="co">]</span><span class="sc">\}</span>$</span>
<span id="cb12-2307"><a href="#cb12-2307"></a>(expected size of the two sides of the remaining space after it is split</span>
<span id="cb12-2308"><a href="#cb12-2308"></a>by a query - purple area in @fig-2dspace), and the final goal is to maximize that amount</span>
<span id="cb12-2309"><a href="#cb12-2309"></a>over all possible queries since it is optimal to get rid of as much</span>
<span id="cb12-2310"><a href="#cb12-2310"></a>space as possible to narrow down the options for the reward function:</span>
<span id="cb12-2311"><a href="#cb12-2311"></a>$\max_{\phi} \min<span class="sc">\{</span> \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{\phi}(w)</span><span class="co">]</span>, \mathbb{E}<span class="co">[</span><span class="ot">1 - f_{-\phi}(w)</span><span class="co">]</span><span class="sc">\}</span>$.</span>
<span id="cb12-2312"><a href="#cb12-2312"></a>Effectively this is finding such $\phi$ that maximizes the information</span>
<span id="cb12-2313"><a href="#cb12-2313"></a>one can get by asking the next comparison query. While this approach</span>
<span id="cb12-2314"><a href="#cb12-2314"></a>uses minimum volume removed, there can be other metrics inside the</span>
<span id="cb12-2315"><a href="#cb12-2315"></a>$\max$ function. Some applications like movie recommendations do not</span>
<span id="cb12-2316"><a href="#cb12-2316"></a>require extra constraints, however in robotics one might want to add</span>
<span id="cb12-2317"><a href="#cb12-2317"></a>more constraints that satisfy certain rules, so that the resulting query</span>
<span id="cb12-2318"><a href="#cb12-2318"></a>follows the dynamics of the physical world.</span>
<span id="cb12-2319"><a href="#cb12-2319"></a></span>
<span id="cb12-2320"><a href="#cb12-2320"></a><span class="fu">#### Driving Simulator Example {.unnumbered}</span></span>
<span id="cb12-2321"><a href="#cb12-2321"></a></span>
<span id="cb12-2322"><a href="#cb12-2322"></a>The first real example of learning reward functions from pairwise</span>
<span id="cb12-2323"><a href="#cb12-2323"></a>comparisons is a 2D driving simulator from <span class="co">[</span><span class="ot">@pmlr-v87-biyik18a</span><span class="co">]</span>. In @fig-car_direct</span>
<span id="cb12-2324"><a href="#cb12-2324"></a>you can see the setting of a 3-lane road with the orange car being</span>
<span id="cb12-2325"><a href="#cb12-2325"></a>controlled by the computer.</span>
<span id="cb12-2326"><a href="#cb12-2326"></a></span>
<span id="cb12-2327"><a href="#cb12-2327"></a>![The choices presented to a human for feedback are represented by green</span>
<span id="cb12-2328"><a href="#cb12-2328"></a>and red trajectories. White trajectory demonstrates the lane change of</span>
<span id="cb12-2329"><a href="#cb12-2329"></a>another vehicle in the space.</span>
<span id="cb12-2330"><a href="#cb12-2330"></a><span class="co">[</span><span class="ot">@pmlr-v87-biyik18a</span><span class="co">]</span>](Figures/car_dir.png){#fig-car_direct width="40%"}</span>
<span id="cb12-2331"><a href="#cb12-2331"></a></span>
<span id="cb12-2332"><a href="#cb12-2332"></a>The queries conducted for this problem are two different trajectories</span>
<span id="cb12-2333"><a href="#cb12-2333"></a>presented to the human, and they are asked to evaluate which one of them</span>
<span id="cb12-2334"><a href="#cb12-2334"></a>is better. For the features that contribute to the reward function, it</span>
<span id="cb12-2335"><a href="#cb12-2335"></a>is important to consider that robots might not find some of the</span>
<span id="cb12-2336"><a href="#cb12-2336"></a>information as informative for the learning process as a human would.</span>
<span id="cb12-2337"><a href="#cb12-2337"></a>For this example, the underlying features included the distance between</span>
<span id="cb12-2338"><a href="#cb12-2338"></a>lane boundaries, distance to other cars, and the heading and speed of</span>
<span id="cb12-2339"><a href="#cb12-2339"></a>the controlled car. The weights toward the last feature were weighted</span>
<span id="cb12-2340"><a href="#cb12-2340"></a>the highest according to the authors, since it takes a lot of effort for</span>
<span id="cb12-2341"><a href="#cb12-2341"></a>the car to change or correct its direction.</span>
<span id="cb12-2342"><a href="#cb12-2342"></a></span>
<span id="cb12-2343"><a href="#cb12-2343"></a>At the start of the learning process, the car had no direction learned</span>
<span id="cb12-2344"><a href="#cb12-2344"></a>and was moving all over the road. In the middle of learning after 30</span>
<span id="cb12-2345"><a href="#cb12-2345"></a>queries, the simulator learned to follow the direction of the road and</span>
<span id="cb12-2346"><a href="#cb12-2346"></a>go straight but still experienced collisions. After 70 queries, the</span>
<span id="cb12-2347"><a href="#cb12-2347"></a>simulator learned to avoid collisions, as well as keep the car within</span>
<span id="cb12-2348"><a href="#cb12-2348"></a>the lane without swerving.</span>
<span id="cb12-2349"><a href="#cb12-2349"></a></span>
<span id="cb12-2350"><a href="#cb12-2350"></a><span class="fu">#### Active Learning for Pairwise Comparisons {.unnumbered}</span></span>
<span id="cb12-2351"><a href="#cb12-2351"></a></span>
<span id="cb12-2352"><a href="#cb12-2352"></a>We have discussed that pairwise comparisons should be selected to</span>
<span id="cb12-2353"><a href="#cb12-2353"></a>maximize the minimum volume of remaining options removed. The question</span>
<span id="cb12-2354"><a href="#cb12-2354"></a>that can come out of the driving example is does it really matter to</span>
<span id="cb12-2355"><a href="#cb12-2355"></a>follow that goal or does random choice of queries performs as well? It</span>
<span id="cb12-2356"><a href="#cb12-2356"></a>turns out that indeed most active learning algorithms (purposefully</span>
<span id="cb12-2357"><a href="#cb12-2357"></a>selecting queries) over time converge with the performance of the random</span>
<span id="cb12-2358"><a href="#cb12-2358"></a>query selection, so in long term the performance is similar. However,</span>
<span id="cb12-2359"><a href="#cb12-2359"></a>what is different is that active learning achieves better performance</span>
<span id="cb12-2360"><a href="#cb12-2360"></a>earlier, which in time-sensitive tasks can be a critical factor.</span>
<span id="cb12-2361"><a href="#cb12-2361"></a></span>
<span id="cb12-2362"><a href="#cb12-2362"></a>One example of such a setting can be exoskeletons for humans as part of</span>
<span id="cb12-2363"><a href="#cb12-2363"></a>the rehabilitation after surgery <span class="co">[</span><span class="ot">@Li_2021</span><span class="co">]</span>. Different people have</span>
<span id="cb12-2364"><a href="#cb12-2364"></a>significantly different walking patterns as well as rehabilitation</span>
<span id="cb12-2365"><a href="#cb12-2365"></a>requirements, so the exoskeleton needs to adapt to the human as soon as</span>
<span id="cb12-2366"><a href="#cb12-2366"></a>possible for a more successful rehabilitation. Figure @fig-robotics</span>
<span id="cb12-2367"><a href="#cb12-2367"></a>demonstrates the difference in the time needed between the two</span>
<span id="cb12-2368"><a href="#cb12-2368"></a>approaches. In general, in robotics, the time differences that might</span>
<span id="cb12-2369"><a href="#cb12-2369"></a>seem small to a human might be detrimental to the final performance.</span>
<span id="cb12-2370"><a href="#cb12-2370"></a></span>
<span id="cb12-2371"><a href="#cb12-2371"></a>![Performance of active learning and random query selection algorithms</span>
<span id="cb12-2372"><a href="#cb12-2372"></a>in the task of exoskeleton learning with human preferences.</span>
<span id="cb12-2373"><a href="#cb12-2373"></a><span class="co">[</span><span class="ot">@Li_2021</span><span class="co">]</span>](Figures/robo_graph.png){#fig-robotics width="60%"}</span>
<span id="cb12-2374"><a href="#cb12-2374"></a></span>
<span id="cb12-2375"><a href="#cb12-2375"></a><span class="fu">#### Multi-Modal Reward Functions for Pairwise Comparisons {.unnumbered}</span></span>
<span id="cb12-2376"><a href="#cb12-2376"></a></span>
<span id="cb12-2377"><a href="#cb12-2377"></a>What if one is working with multiple people and their responses to the</span>
<span id="cb12-2378"><a href="#cb12-2378"></a>queries for comparisons? It will be impossible to recover the different</span>
<span id="cb12-2379"><a href="#cb12-2379"></a>personalities based on the answers, and it might be necessary to conduct</span>
<span id="cb12-2380"><a href="#cb12-2380"></a>a full ranking before it is clear which responses belonged to which</span>
<span id="cb12-2381"><a href="#cb12-2381"></a>person, but the underlying theory for the number of comparisons is</span>
<span id="cb12-2382"><a href="#cb12-2382"></a>non-trivial. For that, the researchers <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> have used</span>
<span id="cb12-2383"><a href="#cb12-2383"></a>multi-modal models for reward function learning, which allows to account</span>
<span id="cb12-2384"><a href="#cb12-2384"></a>for different types of valid behaviours and trajectories that can come</span>
<span id="cb12-2385"><a href="#cb12-2385"></a>from different humans.</span>
<span id="cb12-2386"><a href="#cb12-2386"></a></span>
<span id="cb12-2387"><a href="#cb12-2387"></a>![The negotiation setting with two people and three shared items. Each</span>
<span id="cb12-2388"><a href="#cb12-2388"></a>person has a desired number of items indicated in their utility box.</span>
<span id="cb12-2389"><a href="#cb12-2389"></a>Alice is the controlled agent that has many different response options</span>
<span id="cb12-2390"><a href="#cb12-2390"></a>that are illustrated by the approaches different models might take.</span>
<span id="cb12-2391"><a href="#cb12-2391"></a><span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span>](Figures/negotiation.png){#fig-negotiation</span>
<span id="cb12-2392"><a href="#cb12-2392"></a>width="80%"}</span>
<span id="cb12-2393"><a href="#cb12-2393"></a></span>
<span id="cb12-2394"><a href="#cb12-2394"></a>An example setting for such type of problem is negotiations</span>
<span id="cb12-2395"><a href="#cb12-2395"></a><span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span>. Let's say there are some shared items and two</span>
<span id="cb12-2396"><a href="#cb12-2396"></a>people with different utilities and desires for items, where each person</span>
<span id="cb12-2397"><a href="#cb12-2397"></a>only knows their utility. In a specific case of</span>
<span id="cb12-2398"><a href="#cb12-2398"></a>@fig-negotiation, Bob as a proposing agent and Alice as a</span>
<span id="cb12-2399"><a href="#cb12-2399"></a>controlled agent who has many different ways of responding to Bob's</span>
<span id="cb12-2400"><a href="#cb12-2400"></a>proposals. Different methods can be used to design Alice as an AI agent.</span>
<span id="cb12-2401"><a href="#cb12-2401"></a>The first idea is reinforcement learning, where multiple rounds of</span>
<span id="cb12-2402"><a href="#cb12-2402"></a>negotiations are done, the model simulates game theory and sees how Bob</span>
<span id="cb12-2403"><a href="#cb12-2403"></a>reacts. Authors of this setting <span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span> show that over time</span>
<span id="cb12-2404"><a href="#cb12-2404"></a>the model learns to ask for the same thing over and over again, as Alice</span>
<span id="cb12-2405"><a href="#cb12-2405"></a>is not trained to be human-like or negotiable, and just tries to</span>
<span id="cb12-2406"><a href="#cb12-2406"></a>maximize Alice's utility. The second approach is supervised learning,</span>
<span id="cb12-2407"><a href="#cb12-2407"></a>where the model can be trained on some dataset, learning the history of</span>
<span id="cb12-2408"><a href="#cb12-2408"></a>negotiations. This results in Alice being very agreeable, which</span>
<span id="cb12-2409"><a href="#cb12-2409"></a>demonstrates two polar results of the two approaches, and it would be</span>
<span id="cb12-2410"><a href="#cb12-2410"></a>ideal to find a middle ground and combine both of them. The authors</span>
<span id="cb12-2411"><a href="#cb12-2411"></a>proposed the Targeted acquisition approach, which is based on active</span>
<span id="cb12-2412"><a href="#cb12-2412"></a>learning ideas. The model asks diverse questions at different cases and</span>
<span id="cb12-2413"><a href="#cb12-2413"></a>stages of negotiations like humans, determining which questions are more</span>
<span id="cb12-2414"><a href="#cb12-2414"></a>valuable to be asked throughout learning. Such an approach ended up in</span>
<span id="cb12-2415"><a href="#cb12-2415"></a>more fair and optimal results than supervised or reinforcement learning</span>
<span id="cb12-2416"><a href="#cb12-2416"></a><span class="co">[</span><span class="ot">@kwon2021targeted</span><span class="co">]</span>.</span>
<span id="cb12-2417"><a href="#cb12-2417"></a></span>
<span id="cb12-2418"><a href="#cb12-2418"></a>In conclusion, pairwise comparisons show to be a great way of learning</span>
<span id="cb12-2419"><a href="#cb12-2419"></a>linear reward functions, but at times present challenges or</span>
<span id="cb12-2420"><a href="#cb12-2420"></a>incapabilities that can be further improved with additional</span>
<span id="cb12-2421"><a href="#cb12-2421"></a>incorporations of approaches like Active Learning. That improves many</span>
<span id="cb12-2422"><a href="#cb12-2422"></a>applications in terms of time spent getting to the result in case of</span>
<span id="cb12-2423"><a href="#cb12-2423"></a>exoskeleton adjustments, as well as getting to a middle ground between</span>
<span id="cb12-2424"><a href="#cb12-2424"></a>polar behaviors in applications like negotiations.</span>
<span id="cb12-2425"><a href="#cb12-2425"></a></span>
<span id="cb12-2426"><a href="#cb12-2426"></a><span class="fu">### Truthful Preference Elicitation with Adversary</span></span>
<span id="cb12-2427"><a href="#cb12-2427"></a></span>
<span id="cb12-2428"><a href="#cb12-2428"></a>In our study of social choice models in Chapter</span>
<span id="cb12-2429"><a href="#cb12-2429"></a><span class="co">[</span><span class="ot">\[2model\]</span><span class="co">](#2human-decision-making-choice-models)</span>{reference-type="ref" reference="2model"}, we study</span>
<span id="cb12-2430"><a href="#cb12-2430"></a>how axiomatic properties are implemented to prevent strategic</span>
<span id="cb12-2431"><a href="#cb12-2431"></a>manipulation of a population. This brings us onto the field of</span>
<span id="cb12-2432"><a href="#cb12-2432"></a>**mechanism design**. At its core, mechanism design is the science of</span>
<span id="cb12-2433"><a href="#cb12-2433"></a>making rules. The intent in this field is to design systems so that the</span>
<span id="cb12-2434"><a href="#cb12-2434"></a>strategic behaviour of individuals leads to desirable outcomes. Just</span>
<span id="cb12-2435"><a href="#cb12-2435"></a>thinking about services on the Internet -- file sharing, reputation</span>
<span id="cb12-2436"><a href="#cb12-2436"></a>systems, web search, web advertising, email, Internet auctions,</span>
<span id="cb12-2437"><a href="#cb12-2437"></a>congestion control -- all have to be set up so that an individual's</span>
<span id="cb12-2438"><a href="#cb12-2438"></a>selfish behavior leads to better outcomes for the entire community. A</span>
<span id="cb12-2439"><a href="#cb12-2439"></a>more specific example of this is the phenomenon of "bid-sniping" that</span>
<span id="cb12-2440"><a href="#cb12-2440"></a>was present on eBay in the early 2000s. When people could bid on E-bay,</span>
<span id="cb12-2441"><a href="#cb12-2441"></a>the rule was that the highest bidder by the end of some specified time</span>
<span id="cb12-2442"><a href="#cb12-2442"></a>period would get the item. As a result, people would just wait until the</span>
<span id="cb12-2443"><a href="#cb12-2443"></a>very last minute to bid in order to not raise the price of the item too</span>
<span id="cb12-2444"><a href="#cb12-2444"></a>early. On the other hand, when Amazon still allowed bidding, they had a</span>
<span id="cb12-2445"><a href="#cb12-2445"></a>rule that any time a bid was placed it would extend the time of the bid</span>
<span id="cb12-2446"><a href="#cb12-2446"></a>by ten minutes. This simple difference had drastic effects on bidding</span>
<span id="cb12-2447"><a href="#cb12-2447"></a>prices over time. Mechanism design develops the theoretical framework</span>
<span id="cb12-2448"><a href="#cb12-2448"></a>for learning social choices and eliciting truthful preference.</span>
<span id="cb12-2449"><a href="#cb12-2449"></a></span>
<span id="cb12-2450"><a href="#cb12-2450"></a>We will cover frameworks that model several scenarios that mechanism</span>
<span id="cb12-2451"><a href="#cb12-2451"></a>design is usefully applied to: recommendation systems (where users will</span>
<span id="cb12-2452"><a href="#cb12-2452"></a>selfishly try to stick to their preferences while a planner encourages</span>
<span id="cb12-2453"><a href="#cb12-2453"></a>exploration); auctions (where bidders will try to maximise their reward</span>
<span id="cb12-2454"><a href="#cb12-2454"></a>compared to others); and peer grading (where truthful reporting is not</span>
<span id="cb12-2455"><a href="#cb12-2455"></a>necessarily an incentive for students).</span>
<span id="cb12-2456"><a href="#cb12-2456"></a></span>
<span id="cb12-2457"><a href="#cb12-2457"></a><span class="fu">#### Auction Theory {.unnumbered}</span></span>
<span id="cb12-2458"><a href="#cb12-2458"></a></span>
<span id="cb12-2459"><a href="#cb12-2459"></a><span class="fu">##### Single-Item Auctions {#single-item-auctions .unnumbered}</span></span>
<span id="cb12-2460"><a href="#cb12-2460"></a></span>
<span id="cb12-2461"><a href="#cb12-2461"></a>The first problem within auction theory we will consider is the</span>
<span id="cb12-2462"><a href="#cb12-2462"></a>*single-item auction*. The premise of this problem is that there is a</span>
<span id="cb12-2463"><a href="#cb12-2463"></a>single item to sell, $n$ bidders (with unknown private valuations of the</span>
<span id="cb12-2464"><a href="#cb12-2464"></a>item $v_1$, <span class="sc">\.</span>.., $v_n$). The bidder's individual objective is to</span>
<span id="cb12-2465"><a href="#cb12-2465"></a>maximize utility: the value $v_i$ of the item subtracted by the price</span>
<span id="cb12-2466"><a href="#cb12-2466"></a>paid for the item. The auction procedure is standard in the sense that</span>
<span id="cb12-2467"><a href="#cb12-2467"></a>bids are solicited and the highest bid will win the auction. While the</span>
<span id="cb12-2468"><a href="#cb12-2468"></a>objective of the individual bidder is clear, there could be a plethora</span>
<span id="cb12-2469"><a href="#cb12-2469"></a>of different objectives for the auction as a whole. One option could be</span>
<span id="cb12-2470"><a href="#cb12-2470"></a>to maximize social surplus, meaning the goal is to maximize the value of</span>
<span id="cb12-2471"><a href="#cb12-2471"></a>the winner. Another objective could be to maximize seller profit which</span>
<span id="cb12-2472"><a href="#cb12-2472"></a>is the payment of the winner. For simplicity, we can focus on the first</span>
<span id="cb12-2473"><a href="#cb12-2473"></a>objective where the goal is to maximize social surplus. If we want to</span>
<span id="cb12-2474"><a href="#cb12-2474"></a>maximize social surplus it turns out that a great way to do this is the</span>
<span id="cb12-2475"><a href="#cb12-2475"></a>"second-price auction".</span>
<span id="cb12-2476"><a href="#cb12-2476"></a></span>
<span id="cb12-2477"><a href="#cb12-2477"></a><span class="fu">###### Maximizing Social Surplus {#maximizing-social-surplus .unnumbered}</span></span>
<span id="cb12-2478"><a href="#cb12-2478"></a></span>
<span id="cb12-2479"><a href="#cb12-2479"></a>In the second-price auction, we will operate under slightly different</span>
<span id="cb12-2480"><a href="#cb12-2480"></a>conditions. In the second-price auction we 1) solicit sealed bids, 2)</span>
<span id="cb12-2481"><a href="#cb12-2481"></a>have the winner be the highest bidder, and 3) charger winner the</span>
<span id="cb12-2482"><a href="#cb12-2482"></a>second-highest bid price. As an example, if the solicited bids are</span>
<span id="cb12-2483"><a href="#cb12-2483"></a>$b = (2, 6, 4, 1)$ the winner will be that who bid $6$, but will pay a</span>
<span id="cb12-2484"><a href="#cb12-2484"></a>price of $4$. From here, we can do some equilibrium analysis to try and</span>
<span id="cb12-2485"><a href="#cb12-2485"></a>learn what the optimal bidding strategy is for each bidder. Let the</span>
<span id="cb12-2486"><a href="#cb12-2486"></a>amount bidder $i$ bids to be $b_i$, so we have bids</span>
<span id="cb12-2487"><a href="#cb12-2487"></a>$b_1, b_2, ..., b_n$. How much should bidder $i$ bid? To analyze this,</span>
<span id="cb12-2488"><a href="#cb12-2488"></a>let us define $t_i = max_{j \neq i} b_j$ which represents the max of the</span>
<span id="cb12-2489"><a href="#cb12-2489"></a>bids that is not from bidder $i$. There are now two cases to consider:</span>
<span id="cb12-2490"><a href="#cb12-2490"></a>if $b_i$ <span class="sc">\&gt;</span> $t_i$ and if $b_i$ <span class="sc">\&lt;</span> $t_i$. In the first case the bidder</span>
<span id="cb12-2491"><a href="#cb12-2491"></a>$i$ wins, and if the bidder bid $b_i = v_i$, they are guaranteed to have</span>
<span id="cb12-2492"><a href="#cb12-2492"></a>a positive return on bid. In the other case, they lose the bid and the</span>
<span id="cb12-2493"><a href="#cb12-2493"></a>net loss is 0 because they don't have to pay. From this we can conclude</span>
<span id="cb12-2494"><a href="#cb12-2494"></a>that bidder $i$'s dominant strategy is to just bid $b_i = v_i$.</span>
<span id="cb12-2495"><a href="#cb12-2495"></a>Rigorously proving this is a little bit trickier, but it was shown from</span>
<span id="cb12-2496"><a href="#cb12-2496"></a>Vickrey in 1961 <span class="sc">\[</span>cite<span class="sc">\]</span> that truthful bidding is the dominant strategy</span>
<span id="cb12-2497"><a href="#cb12-2497"></a>in second-price auctions. A corollary of this is that we are maximizing</span>
<span id="cb12-2498"><a href="#cb12-2498"></a>social surplus since bids are values and the winner is the bidder with</span>
<span id="cb12-2499"><a href="#cb12-2499"></a>highest valuation.</span>
<span id="cb12-2500"><a href="#cb12-2500"></a></span>
<span id="cb12-2501"><a href="#cb12-2501"></a><span class="fu">###### Maximize Seller Profit {#maximize-seller-profit .unnumbered}</span></span>
<span id="cb12-2502"><a href="#cb12-2502"></a></span>
<span id="cb12-2503"><a href="#cb12-2503"></a>If we want to look at things from the perspective of a seller trying to</span>
<span id="cb12-2504"><a href="#cb12-2504"></a>maximize their profit we need to treat the bidder's bids as uniform</span>
<span id="cb12-2505"><a href="#cb12-2505"></a>random variables. Consider the example scenario where we have two</span>
<span id="cb12-2506"><a href="#cb12-2506"></a>bidders each bidding uniformly between 0 and 1. What is the seller's</span>
<span id="cb12-2507"><a href="#cb12-2507"></a>expected profit? (in this case profit and revenue for the seller are the</span>
<span id="cb12-2508"><a href="#cb12-2508"></a>same because we assume the seller throws away the item if it doesn't</span>
<span id="cb12-2509"><a href="#cb12-2509"></a>sell/has no valuation for it).</span>
<span id="cb12-2510"><a href="#cb12-2510"></a></span>
<span id="cb12-2511"><a href="#cb12-2511"></a>From there the question now becomes, can we get more expected profit</span>
<span id="cb12-2512"><a href="#cb12-2512"></a>from the seller's perspective? It turns out there is a design where we</span>
<span id="cb12-2513"><a href="#cb12-2513"></a>can add a reserve price of $r$ to the second-price auction. The way this</span>
<span id="cb12-2514"><a href="#cb12-2514"></a>works is we can 1) Insert seller-bid at $r$, 2) solicit bids, 3) pick</span>
<span id="cb12-2515"><a href="#cb12-2515"></a>the highest bidder, and 3) charge the 2nd-highest bid. In effect, this</span>
<span id="cb12-2516"><a href="#cb12-2516"></a>is just the second-price auction but with a bid from the seller as well,</span>
<span id="cb12-2517"><a href="#cb12-2517"></a>at a price of $r$. A lemma, that we won't prove here, is that the</span>
<span id="cb12-2518"><a href="#cb12-2518"></a>second-price auction with reserve price $r$ still has a dominant</span>
<span id="cb12-2519"><a href="#cb12-2519"></a>strategy of just being truthful.</span>
<span id="cb12-2520"><a href="#cb12-2520"></a></span>
<span id="cb12-2521"><a href="#cb12-2521"></a>Let's now consider what the profit of a second-price auction would be</span>
<span id="cb12-2522"><a href="#cb12-2522"></a>with two bidders that uniformly bid between 0 and 1 -- but this time we</span>
<span id="cb12-2523"><a href="#cb12-2523"></a>have a reserve price of $1/2$. To calculate the expected profit we break</span>
<span id="cb12-2524"><a href="#cb12-2524"></a>down the situation into 3 cases:</span>
<span id="cb12-2525"><a href="#cb12-2525"></a></span>
<span id="cb12-2526"><a href="#cb12-2526"></a><span class="ss">-   </span>Case 1:</span>
<span id="cb12-2527"><a href="#cb12-2527"></a>    $1/2 &gt; v_1 &gt; v_2 \rightarrow 1/4 \text{ probability} \rightarrow  E<span class="co">[</span><span class="ot">\text{profit}</span><span class="co">]</span> = 0$</span>
<span id="cb12-2528"><a href="#cb12-2528"></a></span>
<span id="cb12-2529"><a href="#cb12-2529"></a><span class="ss">-   </span>Case 2:</span>
<span id="cb12-2530"><a href="#cb12-2530"></a>    $v_1 &gt; v_2 &gt; 1/2 \rightarrow 1/4 \text{probability} \rightarrow E<span class="co">[</span><span class="ot">v2 | case 2</span><span class="co">]</span> = 2/3$</span>
<span id="cb12-2531"><a href="#cb12-2531"></a></span>
<span id="cb12-2532"><a href="#cb12-2532"></a><span class="ss">-   </span>Case 3:</span>
<span id="cb12-2533"><a href="#cb12-2533"></a>    $v_1 &gt; 1/2 &gt; v_2 \rightarrow 1/2 \text{ probability} \rightarrow 1/2$</span>
<span id="cb12-2534"><a href="#cb12-2534"></a></span>
<span id="cb12-2535"><a href="#cb12-2535"></a>Why is $E<span class="co">[</span><span class="ot">v2 | case 2</span><span class="co">]</span> = 2/3$? If $v_1$ and $v_2$ are greater than</span>
<span id="cb12-2536"><a href="#cb12-2536"></a>$1/2$, they are evenly spread across the interval, meaning the</span>
<span id="cb12-2537"><a href="#cb12-2537"></a>expectation will be 1/2 + 1/6 = 2/3. Adding up all these cases we get</span>
<span id="cb12-2538"><a href="#cb12-2538"></a>$E<span class="co">[</span><span class="ot">profit</span><span class="co">]</span> = 5/12$. It turns out that second-price auctions with reserve</span>
<span id="cb12-2539"><a href="#cb12-2539"></a>actually maximize profit in general (for symmetric bidders)!</span>
<span id="cb12-2540"><a href="#cb12-2540"></a></span>
<span id="cb12-2541"><a href="#cb12-2541"></a>In the previous section we conclude that second-price auctions with</span>
<span id="cb12-2542"><a href="#cb12-2542"></a>reserve maximize profit for the seller. In order to prove this, we now</span>
<span id="cb12-2543"><a href="#cb12-2543"></a>move to the more general topic of asking how should a monopolist divide</span>
<span id="cb12-2544"><a href="#cb12-2544"></a>good across separate markets. We can make the assumption that the demand</span>
<span id="cb12-2545"><a href="#cb12-2545"></a>model is a concave revenue $R(q)$ in quantity $q$. Under this</span>
<span id="cb12-2546"><a href="#cb12-2546"></a>assumption, we can just divide supply into $q = q_a + q_b$ such that</span>
<span id="cb12-2547"><a href="#cb12-2547"></a>$R'_a(q_a) = R'_b(q_b)$. The idea from here is a theorem from Myerson in</span>
<span id="cb12-2548"><a href="#cb12-2548"></a>1981 that states an optimal action maximizes \"marginal revenue\".</span>
<span id="cb12-2549"><a href="#cb12-2549"></a>Consider an example where we have two bidders bidding a uniform value</span>
<span id="cb12-2550"><a href="#cb12-2550"></a>between 0 and 1. Our revenue curve can now be derived from the offering</span>
<span id="cb12-2551"><a href="#cb12-2551"></a>price $V(q) = 1 - q$ like so: $R(q) = qV(q) = q - q^2$. Taking the</span>
<span id="cb12-2552"><a href="#cb12-2552"></a>derivative gives us the marginal revenue $R'(q) = 1-2q$. This means two</span>
<span id="cb12-2553"><a href="#cb12-2553"></a>things: 1) we want to sell to bidder $i$ with the highest $R'(q_i)$ and</span>
<span id="cb12-2554"><a href="#cb12-2554"></a>2) we want to sell to bidder $i$ with value at least $1/2$ (if we want a</span>
<span id="cb12-2555"><a href="#cb12-2555"></a>positive $R'(q_i)$. But this is just a second-price auction with reserve</span>
<span id="cb12-2556"><a href="#cb12-2556"></a>$1/2$! This means that for symmetric bidders, a second price with</span>
<span id="cb12-2557"><a href="#cb12-2557"></a>reserve is the optimal auction.</span>
<span id="cb12-2558"><a href="#cb12-2558"></a></span>
<span id="cb12-2559"><a href="#cb12-2559"></a><span class="fu">###### What good are auctions? {#what-good-are-auctions .unnumbered}</span></span>
<span id="cb12-2560"><a href="#cb12-2560"></a></span>
<span id="cb12-2561"><a href="#cb12-2561"></a>An interesting topic to discuss is what benefits auctions bring to the</span>
<span id="cb12-2562"><a href="#cb12-2562"></a>table as opposed to just standard pricing. Online auctions used to be a</span>
<span id="cb12-2563"><a href="#cb12-2563"></a>lot more popular in the early 2000s and have been completely replaced by</span>
<span id="cb12-2564"><a href="#cb12-2564"></a>standard online pricing, even on sites like e-bay. While auctions are</span>
<span id="cb12-2565"><a href="#cb12-2565"></a>slower and have added inherent complexities, they are actually optimal</span>
<span id="cb12-2566"><a href="#cb12-2566"></a>on paper. Standard pricing on the other is non-optimal; although it is</span>
<span id="cb12-2567"><a href="#cb12-2567"></a>fast and simpler for buyers. There is actually a way to quantify this:</span>
<span id="cb12-2568"><a href="#cb12-2568"></a>for pricing $k$ units, the loss is at most $1 / \sqrt{2\pi k}$ of</span>
<span id="cb12-2569"><a href="#cb12-2569"></a>optimal profit.</span>
<span id="cb12-2570"><a href="#cb12-2570"></a></span>
<span id="cb12-2571"><a href="#cb12-2571"></a>Let's consider applications in duopoly platform design. We know that the</span>
<span id="cb12-2572"><a href="#cb12-2572"></a>optimal auction is second-price with reserve, but what happens when we</span>
<span id="cb12-2573"><a href="#cb12-2573"></a>introduce competition between two auction platforms? Some important</span>
<span id="cb12-2574"><a href="#cb12-2574"></a>details related to the revenue of a second-price auction is that a</span>
<span id="cb12-2575"><a href="#cb12-2575"></a>second-price auction with no reserve and n bidders leads to larger</span>
<span id="cb12-2576"><a href="#cb12-2576"></a>revenue having an optimal reserve and n - 1 bidders</span>
<span id="cb12-2577"><a href="#cb12-2577"></a> <span class="co">[</span><span class="ot">@bulow-klemperer1996</span><span class="co">]</span>. Additionally, with an entry cost, no reserve is</span>
<span id="cb12-2578"><a href="#cb12-2578"></a>the optimal strategy for maximizing revenue  <span class="co">[</span><span class="ot">@mcafee-87</span><span class="co">]</span>. Let's</span>
<span id="cb12-2579"><a href="#cb12-2579"></a>consider an example of a competing auction system which is Google ads vs</span>
<span id="cb12-2580"><a href="#cb12-2580"></a>Bing ads. How should an advertiser divide the budget between Google and</span>
<span id="cb12-2581"><a href="#cb12-2581"></a>Bing? They should give the same budget to both companies. What happens</span>
<span id="cb12-2582"><a href="#cb12-2582"></a>if Bing raises their prices? Then, the advertising company moves more of</span>
<span id="cb12-2583"><a href="#cb12-2583"></a>its budget to Google from Bing.</span>
<span id="cb12-2584"><a href="#cb12-2584"></a></span>
<span id="cb12-2585"><a href="#cb12-2585"></a><span class="fu">#### Prior-Independent Auctions {#prior-independent-auctions .unnumbered}</span></span>
<span id="cb12-2586"><a href="#cb12-2586"></a></span>
<span id="cb12-2587"><a href="#cb12-2587"></a>The Bulow-Klemperer theorem demonstrates that increased competition can</span>
<span id="cb12-2588"><a href="#cb12-2588"></a>be more valuable than perfect knowledge of bidders' valuation</span>
<span id="cb12-2589"><a href="#cb12-2589"></a>distributions. This result provides insight into the potential of</span>
<span id="cb12-2590"><a href="#cb12-2590"></a>simple, prior-independent auctions to approach the performance of</span>
<span id="cb12-2591"><a href="#cb12-2591"></a>optimal auctions. The theorem states that for a single-item auction with</span>
<span id="cb12-2592"><a href="#cb12-2592"></a>bidders' valuations drawn independently from a regular distribution $F$:</span>
<span id="cb12-2593"><a href="#cb12-2593"></a></span>
<span id="cb12-2594"><a href="#cb12-2594"></a>::: {.callout-note title="theorem"}</span>
<span id="cb12-2595"><a href="#cb12-2595"></a>::: {#thm-bulow-klemperer}</span>
<span id="cb12-2596"><a href="#cb12-2596"></a>Let $F$ be a regular distribution and $n$ a positive integer. Then:</span>
<span id="cb12-2597"><a href="#cb12-2597"></a>$$E_{v_1,\ldots,v_{n+1} \sim F}<span class="co">[</span><span class="ot">\text{Rev(VA)}(n+1 \text{ bidders})</span><span class="co">]</span> \geq E_{v_1,\ldots,v_n \sim F}<span class="co">[</span><span class="ot">\text{Rev(OPT}_F)(n \text{ bidders})</span><span class="co">]</span>$$  {#eq-eq3.64}</span>
<span id="cb12-2598"><a href="#cb12-2598"></a>where VA denotes the Vickrey auction and $\text{OPT}_F$ denotes the</span>
<span id="cb12-2599"><a href="#cb12-2599"></a>optimal auction for $F$.</span>
<span id="cb12-2600"><a href="#cb12-2600"></a>:::</span>
<span id="cb12-2601"><a href="#cb12-2601"></a>:::</span>
<span id="cb12-2602"><a href="#cb12-2602"></a></span>
<span id="cb12-2603"><a href="#cb12-2603"></a>This shows that running a simple Vickrey auction with one extra bidder</span>
<span id="cb12-2604"><a href="#cb12-2604"></a>outperforms the revenue-optimal auction that requires precise knowledge</span>
<span id="cb12-2605"><a href="#cb12-2605"></a>of the distribution. It suggests that in practice, effort spent on</span>
<span id="cb12-2606"><a href="#cb12-2606"></a>recruiting additional bidders may be more fruitful than fine-tuning</span>
<span id="cb12-2607"><a href="#cb12-2607"></a>auction parameters.</span>
<span id="cb12-2608"><a href="#cb12-2608"></a></span>
<span id="cb12-2609"><a href="#cb12-2609"></a><span class="fu">##### The VCG Mechanism {#the-vcg-mechanism .unnumbered}</span></span>
<span id="cb12-2610"><a href="#cb12-2610"></a></span>
<span id="cb12-2611"><a href="#cb12-2611"></a>The VCG mechanism is a cornerstone of mechanism design theory, providing</span>
<span id="cb12-2612"><a href="#cb12-2612"></a>a general solution for welfare maximization in multi-parameter</span>
<span id="cb12-2613"><a href="#cb12-2613"></a>environments. The key result is:</span>
<span id="cb12-2614"><a href="#cb12-2614"></a></span>
<span id="cb12-2615"><a href="#cb12-2615"></a>::: {.callout-note title="theorem"}</span>
<span id="cb12-2616"><a href="#cb12-2616"></a>::: {#thm-VCG}</span>
<span id="cb12-2617"><a href="#cb12-2617"></a>[]{#thm:VCG label="thm:VCG"} In every general mechanism design</span>
<span id="cb12-2618"><a href="#cb12-2618"></a>environment, there is a dominant-strategy incentive-compatible (DSIC)</span>
<span id="cb12-2619"><a href="#cb12-2619"></a>welfare-maximizing mechanism.</span>
<span id="cb12-2620"><a href="#cb12-2620"></a>:::</span>
<span id="cb12-2621"><a href="#cb12-2621"></a>:::</span>
<span id="cb12-2622"><a href="#cb12-2622"></a></span>
<span id="cb12-2623"><a href="#cb12-2623"></a>The VCG mechanism operates as follows:</span>
<span id="cb12-2624"><a href="#cb12-2624"></a></span>
<span id="cb12-2625"><a href="#cb12-2625"></a><span class="ss">1.  </span>Given bids $b_1, \ldots, b_n$, where each $b_i$ is indexed by the</span>
<span id="cb12-2626"><a href="#cb12-2626"></a>    outcome set $\Omega$, the allocation rule is:</span>
<span id="cb12-2627"><a href="#cb12-2627"></a></span>
<span id="cb12-2628"><a href="#cb12-2628"></a>    $$x(b) = \arg \max_{\omega \in \Omega} \sum_{i=1}^n b_i(\omega)$$  {#eq-eq3.65}</span>
<span id="cb12-2629"><a href="#cb12-2629"></a></span>
<span id="cb12-2630"><a href="#cb12-2630"></a><span class="ss">2.  </span>The payment rule for each agent $i$ is:</span>
<span id="cb12-2631"><a href="#cb12-2631"></a></span>
<span id="cb12-2632"><a href="#cb12-2632"></a>    $$p_i(b) = \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) - \sum_{j \neq i} b_j(\omega^*)$$  {#eq-eq3.66}</span>
<span id="cb12-2633"><a href="#cb12-2633"></a></span>
<span id="cb12-2634"><a href="#cb12-2634"></a>    where $\omega^* = x(b)$ is the chosen outcome.</span>
<span id="cb12-2635"><a href="#cb12-2635"></a></span>
<span id="cb12-2636"><a href="#cb12-2636"></a>The key insight is to charge each agent its "externality" - the welfare</span>
<span id="cb12-2637"><a href="#cb12-2637"></a>loss inflicted on other agents by its presence. This payment rule,</span>
<span id="cb12-2638"><a href="#cb12-2638"></a>coupled with the welfare-maximizing allocation rule, yields a DSIC</span>
<span id="cb12-2639"><a href="#cb12-2639"></a>mechanism.</span>
<span id="cb12-2640"><a href="#cb12-2640"></a></span>
<span id="cb12-2641"><a href="#cb12-2641"></a>The VCG mechanism can be interpreted as having each agent pay its bid</span>
<span id="cb12-2642"><a href="#cb12-2642"></a>minus a \"rebate\" equal to the increase in welfare attributable to its</span>
<span id="cb12-2643"><a href="#cb12-2643"></a>presence:</span>
<span id="cb12-2644"><a href="#cb12-2644"></a></span>
<span id="cb12-2645"><a href="#cb12-2645"></a>$$p_i(b) = b_i(\omega^*) - \left[ \sum_{j=1}^n b_j(\omega^*) - \max_{\omega \in \Omega} \sum_{j \neq i} b_j(\omega) \right]$$  {#eq-eq3.67}</span>
<span id="cb12-2646"><a href="#cb12-2646"></a></span>
<span id="cb12-2647"><a href="#cb12-2647"></a>While the VCG mechanism provides a theoretical solution for DSIC</span>
<span id="cb12-2648"><a href="#cb12-2648"></a>welfare-maximization in general environments, it can be challenging to</span>
<span id="cb12-2649"><a href="#cb12-2649"></a>implement in practice due to computational and communication</span>
<span id="cb12-2650"><a href="#cb12-2650"></a>complexities.</span>
<span id="cb12-2651"><a href="#cb12-2651"></a></span>
<span id="cb12-2652"><a href="#cb12-2652"></a><span class="fu">##### Combinatorial Auctions {#combinatorial-auctions .unnumbered}</span></span>
<span id="cb12-2653"><a href="#cb12-2653"></a></span>
<span id="cb12-2654"><a href="#cb12-2654"></a>Combinatorial auctions are an important class of multi-parameter</span>
<span id="cb12-2655"><a href="#cb12-2655"></a>mechanism design problems, with applications ranging from spectrum</span>
<span id="cb12-2656"><a href="#cb12-2656"></a>auctions to airport slot allocation. In a combinatorial auction:</span>
<span id="cb12-2657"><a href="#cb12-2657"></a></span>
<span id="cb12-2658"><a href="#cb12-2658"></a><span class="ss">-   </span>There are $n$ bidders and a set $M$ of $m$ items.</span>
<span id="cb12-2659"><a href="#cb12-2659"></a></span>
<span id="cb12-2660"><a href="#cb12-2660"></a><span class="ss">-   </span>The outcome set $\Omega$ consists of allocations</span>
<span id="cb12-2661"><a href="#cb12-2661"></a>    $(S_1, \ldots, S_n)$, where $S_i$ is the bundle allocated to bidder</span>
<span id="cb12-2662"><a href="#cb12-2662"></a>    $i$.</span>
<span id="cb12-2663"><a href="#cb12-2663"></a></span>
<span id="cb12-2664"><a href="#cb12-2664"></a><span class="ss">-   </span>Each bidder $i$ has a private valuation $v_i(S)$ for each bundle</span>
<span id="cb12-2665"><a href="#cb12-2665"></a>    $S \subseteq M$.</span>
<span id="cb12-2666"><a href="#cb12-2666"></a></span>
<span id="cb12-2667"><a href="#cb12-2667"></a>While the VCG mechanism theoretically solves the welfare-maximization</span>
<span id="cb12-2668"><a href="#cb12-2668"></a>problem, combinatorial auctions face several major challenges in</span>
<span id="cb12-2669"><a href="#cb12-2669"></a>practice:</span>
<span id="cb12-2670"><a href="#cb12-2670"></a></span>
<span id="cb12-2671"><a href="#cb12-2671"></a><span class="ss">1.  </span>Preference Elicitation: Each bidder has $2^m - 1$ private</span>
<span id="cb12-2672"><a href="#cb12-2672"></a>    parameters, making direct revelation infeasible for even moderate</span>
<span id="cb12-2673"><a href="#cb12-2673"></a>    numbers of items. This necessitates the use of indirect mechanisms</span>
<span id="cb12-2674"><a href="#cb12-2674"></a>    that elicit information on a \"need-to-know\" basis.</span>
<span id="cb12-2675"><a href="#cb12-2675"></a></span>
<span id="cb12-2676"><a href="#cb12-2676"></a><span class="ss">2.  </span>Computational Complexity: Even when preference elicitation is not an</span>
<span id="cb12-2677"><a href="#cb12-2677"></a>    issue, welfare-maximization can be an intractable problem. In</span>
<span id="cb12-2678"><a href="#cb12-2678"></a>    practice, approximations are often used, hoping to achieve</span>
<span id="cb12-2679"><a href="#cb12-2679"></a>    reasonably good welfare.</span>
<span id="cb12-2680"><a href="#cb12-2680"></a></span>
<span id="cb12-2681"><a href="#cb12-2681"></a><span class="ss">3.  </span>VCG Limitations: The VCG mechanism can exhibit bad revenue and</span>
<span id="cb12-2682"><a href="#cb12-2682"></a>    incentive properties in combinatorial settings. For example, adding</span>
<span id="cb12-2683"><a href="#cb12-2683"></a>    bidders can sometimes decrease revenue to zero, and the mechanism</span>
<span id="cb12-2684"><a href="#cb12-2684"></a>    can be vulnerable to collusion and false-name bids.</span>
<span id="cb12-2685"><a href="#cb12-2685"></a></span>
<span id="cb12-2686"><a href="#cb12-2686"></a><span class="ss">4.  </span>Strategic Behavior in Iterative Auctions: Most practical</span>
<span id="cb12-2687"><a href="#cb12-2687"></a>    combinatorial auctions are iterative, comprising multiple rounds.</span>
<span id="cb12-2688"><a href="#cb12-2688"></a>    This introduces new opportunities for strategic behavior, such as</span>
<span id="cb12-2689"><a href="#cb12-2689"></a>    using bids to signal intentions to other bidders.</span>
<span id="cb12-2690"><a href="#cb12-2690"></a></span>
<span id="cb12-2691"><a href="#cb12-2691"></a>These challenges make combinatorial auctions a rich and complex area of</span>
<span id="cb12-2692"><a href="#cb12-2692"></a>study, requiring careful design to balance theoretical guarantees with</span>
<span id="cb12-2693"><a href="#cb12-2693"></a>practical considerations.</span>
<span id="cb12-2694"><a href="#cb12-2694"></a></span>
<span id="cb12-2695"><a href="#cb12-2695"></a><span class="fu">##### Spectrum Auctions {#spectrum-auctions .unnumbered}</span></span>
<span id="cb12-2696"><a href="#cb12-2696"></a></span>
<span id="cb12-2697"><a href="#cb12-2697"></a>Spectrum auctions represent a complex application of combinatorial</span>
<span id="cb12-2698"><a href="#cb12-2698"></a>auction theory. With n bidders and m non-identical items, each bidder</span>
<span id="cb12-2699"><a href="#cb12-2699"></a>has a private valuation for every possible bundle of items, making it</span>
<span id="cb12-2700"><a href="#cb12-2700"></a>impractical to directly elicit all preferences. This necessitates the</span>
<span id="cb12-2701"><a href="#cb12-2701"></a>use of indirect, iterative mechanisms that query bidders for valuation</span>
<span id="cb12-2702"><a href="#cb12-2702"></a>information on a "need-to-know" basis, sacrificing some of the desirable</span>
<span id="cb12-2703"><a href="#cb12-2703"></a>properties of direct mechanisms like dominant strategy incentive</span>
<span id="cb12-2704"><a href="#cb12-2704"></a>compatibility (DSIC) and full welfare maximization.</span>
<span id="cb12-2705"><a href="#cb12-2705"></a></span>
<span id="cb12-2706"><a href="#cb12-2706"></a>The fundamental challenge in spectrum auctions lies in the nature of the</span>
<span id="cb12-2707"><a href="#cb12-2707"></a>items being sold. There is a dichotomy between items that are</span>
<span id="cb12-2708"><a href="#cb12-2708"></a>substitutes (where $v(AB) \leq v(A) + v(B))$ and those that are</span>
<span id="cb12-2709"><a href="#cb12-2709"></a>complements (where $v(AB) &gt; v(A) + v(B))$. Substitute items, such as</span>
<span id="cb12-2710"><a href="#cb12-2710"></a>licenses for the same area with equal-sized frequency ranges, are</span>
<span id="cb12-2711"><a href="#cb12-2711"></a>generally easier to handle. When items are substitutes, welfare</span>
<span id="cb12-2712"><a href="#cb12-2712"></a>maximization is computationally tractable, and the VCG mechanism avoids</span>
<span id="cb12-2713"><a href="#cb12-2713"></a>many undesirable properties. However, complementary items, which arise</span>
<span id="cb12-2714"><a href="#cb12-2714"></a>naturally in spectrum auctions when bidders want adjacent licenses,</span>
<span id="cb12-2715"><a href="#cb12-2715"></a>present significant challenges.</span>
<span id="cb12-2716"><a href="#cb12-2716"></a></span>
<span id="cb12-2717"><a href="#cb12-2717"></a>Early attempts at spectrum auctions revealed the pitfalls of naive</span>
<span id="cb12-2718"><a href="#cb12-2718"></a>approaches. Sequential auctions, where items are sold one after another,</span>
<span id="cb12-2719"><a href="#cb12-2719"></a>proved problematic as demonstrated by a Swiss auction in 2000. Bidders</span>
<span id="cb12-2720"><a href="#cb12-2720"></a>struggled to bid intelligently without knowing future prices, leading to</span>
<span id="cb12-2721"><a href="#cb12-2721"></a>unpredictable outcomes and potential revenue loss. Similarly,</span>
<span id="cb12-2722"><a href="#cb12-2722"></a>simultaneous sealed-bid auctions, as used in New Zealand in 1990,</span>
<span id="cb12-2723"><a href="#cb12-2723"></a>created difficulties for bidders in coordinating their bids across</span>
<span id="cb12-2724"><a href="#cb12-2724"></a>multiple items, resulting in severely suboptimal outcomes.</span>
<span id="cb12-2725"><a href="#cb12-2725"></a></span>
<span id="cb12-2726"><a href="#cb12-2726"></a>The Simultaneous Ascending Auction (SAA) emerged as a solution to these</span>
<span id="cb12-2727"><a href="#cb12-2727"></a>issues and has formed the basis of most spectrum auctions over the past</span>
<span id="cb12-2728"><a href="#cb12-2728"></a>two decades. In an SAA, multiple items are auctioned simultaneously in</span>
<span id="cb12-2729"><a href="#cb12-2729"></a>rounds, with bidders placing bids on any subset of items subject to an</span>
<span id="cb12-2730"><a href="#cb12-2730"></a>activity rule. This format facilitates price discovery, allowing bidders</span>
<span id="cb12-2731"><a href="#cb12-2731"></a>to adjust their strategies as they learn about others' valuations. It</span>
<span id="cb12-2732"><a href="#cb12-2732"></a>also allows bidders to determine valuations on a need-to-know basis,</span>
<span id="cb12-2733"><a href="#cb12-2733"></a>reducing the cognitive burden compared to direct-revelation auctions.</span>
<span id="cb12-2734"><a href="#cb12-2734"></a></span>
<span id="cb12-2735"><a href="#cb12-2735"></a>Despite its advantages, the SAA is not without vulnerabilities. Demand</span>
<span id="cb12-2736"><a href="#cb12-2736"></a>reduction, where bidders strategically reduce their demand to lower</span>
<span id="cb12-2737"><a href="#cb12-2737"></a>prices, can lead to inefficient outcomes even when items are</span>
<span id="cb12-2738"><a href="#cb12-2738"></a>substitutes. The exposure problem arises with complementary items, where</span>
<span id="cb12-2739"><a href="#cb12-2739"></a>bidders risk winning only a subset of desired items at unfavorable</span>
<span id="cb12-2740"><a href="#cb12-2740"></a>prices. These issues highlight the ongoing challenges in designing</span>
<span id="cb12-2741"><a href="#cb12-2741"></a>effective spectrum auctions, balancing theoretical guarantees with</span>
<span id="cb12-2742"><a href="#cb12-2742"></a>practical considerations.</span>
<span id="cb12-2743"><a href="#cb12-2743"></a></span>
<span id="cb12-2744"><a href="#cb12-2744"></a><span class="fu">##### Case study: Classroom Peer Grading {#case-study-classroom-peer-grading .unnumbered}</span></span>
<span id="cb12-2745"><a href="#cb12-2745"></a></span>
<span id="cb12-2746"><a href="#cb12-2746"></a>This chapter discusses work by Jason Hartline, Yingkai Li, Liren Shan,</span>
<span id="cb12-2747"><a href="#cb12-2747"></a>and Yifan Wu at Northwestern University, where researchers examined</span>
<span id="cb12-2748"><a href="#cb12-2748"></a>mechanism design for the classroom, specifically in terms of the</span>
<span id="cb12-2749"><a href="#cb12-2749"></a>optimization of scoring rules. They explored peer grading in the</span>
<span id="cb12-2750"><a href="#cb12-2750"></a>classroom and how to construct a peer grading system that optimizes the</span>
<span id="cb12-2751"><a href="#cb12-2751"></a>objectives for each stakeholder in the system, including those being</span>
<span id="cb12-2752"><a href="#cb12-2752"></a>graded, the peer graders, the TAs of the class, and the professor.</span>
<span id="cb12-2753"><a href="#cb12-2753"></a></span>
<span id="cb12-2754"><a href="#cb12-2754"></a>Firstly, let's think of the classroom like a computer. We can think of</span>
<span id="cb12-2755"><a href="#cb12-2755"></a>students as local optimizers; their incentive is to minimize the amount</span>
<span id="cb12-2756"><a href="#cb12-2756"></a>of work they need to do and maximize the grades that they receive. The</span>
<span id="cb12-2757"><a href="#cb12-2757"></a>graders are imprecise operators, which means that there is some</span>
<span id="cb12-2758"><a href="#cb12-2758"></a>uncertainty in their ability to grade the work completed by the</span>
<span id="cb12-2759"><a href="#cb12-2759"></a>students. The syllabus can be thought of as the rules that map the</span>
<span id="cb12-2760"><a href="#cb12-2760"></a>actions of the students to the grade they end up receiving in the class.</span>
<span id="cb12-2761"><a href="#cb12-2761"></a>Our overall goals for this classroom based on these definitions is to</span>
<span id="cb12-2762"><a href="#cb12-2762"></a>minimize work, maximize learning, and fairly assess the students for the</span>
<span id="cb12-2763"><a href="#cb12-2763"></a>work that they do  <span class="co">[</span><span class="ot">@jasonH2020</span><span class="co">]</span>.</span>
<span id="cb12-2764"><a href="#cb12-2764"></a></span>
<span id="cb12-2765"><a href="#cb12-2765"></a>One basic question that we can examine, is what is the best syllabus</span>
<span id="cb12-2766"><a href="#cb12-2766"></a>that maximizes our objectives for our classroom design. Some components</span>
<span id="cb12-2767"><a href="#cb12-2767"></a>of this could include grading randomized exams, grading with partial</span>
<span id="cb12-2768"><a href="#cb12-2768"></a>credit, group projects, and finally, peer grading, which is the</span>
<span id="cb12-2769"><a href="#cb12-2769"></a>component that we will be taking a deeper dive into.</span>
<span id="cb12-2770"><a href="#cb12-2770"></a></span>
<span id="cb12-2771"><a href="#cb12-2771"></a>The general situation of the peer grading problem is that proper scoring</span>
<span id="cb12-2772"><a href="#cb12-2772"></a>rules make peer grades horrible  <span class="co">[</span><span class="ot">@jasonH2020</span><span class="co">]</span>. So we want to be able to</span>
<span id="cb12-2773"><a href="#cb12-2773"></a>optimize scoring rules and make sure that we are optimizing each</span>
<span id="cb12-2774"><a href="#cb12-2774"></a>component of the peer grading pipeline.</span>
<span id="cb12-2775"><a href="#cb12-2775"></a></span>
<span id="cb12-2776"><a href="#cb12-2776"></a>The main algorithms focused on in this peer grading design paper were</span>
<span id="cb12-2777"><a href="#cb12-2777"></a>matching peers and TAs to submissions and the grading of those</span>
<span id="cb12-2778"><a href="#cb12-2778"></a>submissions from the TAs and the peer reviews  <span class="co">[</span><span class="ot">@jasonH2020</span><span class="co">]</span>. There are</span>
<span id="cb12-2779"><a href="#cb12-2779"></a>quite a number of advantages to peer grading including that peers are</span>
<span id="cb12-2780"><a href="#cb12-2780"></a>able to learn from reviewing other people's work, it reduces the work</span>
<span id="cb12-2781"><a href="#cb12-2781"></a>for the teacher, and improves the turnaround time for assignment</span>
<span id="cb12-2782"><a href="#cb12-2782"></a>feedback (which are all part of our overarching goals for our mechanism</span>
<span id="cb12-2783"><a href="#cb12-2783"></a>design for the classroom). But, it is also important to acknowledge the</span>
<span id="cb12-2784"><a href="#cb12-2784"></a>potential disadvantages of the peer grading system: it is possible that</span>
<span id="cb12-2785"><a href="#cb12-2785"></a>the peer graders present inaccurate grades and there is student unrest.</span>
<span id="cb12-2786"><a href="#cb12-2786"></a>This presents us with a challenge: being able to incentivize accurate</span>
<span id="cb12-2787"><a href="#cb12-2787"></a>peer reviews.</span>
<span id="cb12-2788"><a href="#cb12-2788"></a></span>
<span id="cb12-2789"><a href="#cb12-2789"></a>One problem that we run into, when we use the proper scoring rule to</span>
<span id="cb12-2790"><a href="#cb12-2790"></a>score peer reviews, if the peer graders use the lazy peer strategy,</span>
<span id="cb12-2791"><a href="#cb12-2791"></a>which means that they always report 80$\%$ for their peer reviews, they</span>
<span id="cb12-2792"><a href="#cb12-2792"></a>get graded very well using the proper scoring rule algorithm. In fact,</span>
<span id="cb12-2793"><a href="#cb12-2793"></a>the proper scoring rule says that their peer review is 96$\%$ accurate</span>
<span id="cb12-2794"><a href="#cb12-2794"></a> <span class="co">[</span><span class="ot">@jasonH2023</span><span class="co">]</span>. So how do we incentivize effort in reviews from peer</span>
<span id="cb12-2795"><a href="#cb12-2795"></a>graders? We use a scoring rule that maximizes the difference in score</span>
<span id="cb12-2796"><a href="#cb12-2796"></a>between effort or no effort reviews as indicated by the peer reviewers</span>
<span id="cb12-2797"><a href="#cb12-2797"></a> <span class="co">[</span><span class="ot">@jasonH2023</span><span class="co">]</span>. So overall, the analysis of datasets leads to decision</span>
<span id="cb12-2798"><a href="#cb12-2798"></a>optimizations and, eventually, payoff from those decisions.</span>
<span id="cb12-2799"><a href="#cb12-2799"></a></span>
<span id="cb12-2800"><a href="#cb12-2800"></a>To conclude our mechanism design in the classroom discussion, we have</span>
<span id="cb12-2801"><a href="#cb12-2801"></a>two key takeaways: scoring rules are essential in being able to</span>
<span id="cb12-2802"><a href="#cb12-2802"></a>understand and analyze data thoroughly, and optimal scoring rules for</span>
<span id="cb12-2803"><a href="#cb12-2803"></a>binary effort allow us to understand the setting independent of the</span>
<span id="cb12-2804"><a href="#cb12-2804"></a>dataset  <span class="co">[</span><span class="ot">@jasonH2023</span><span class="co">]</span>.</span>
<span id="cb12-2805"><a href="#cb12-2805"></a></span>
<span id="cb12-2806"><a href="#cb12-2806"></a><span class="fu">#### Mutual Information Paradigm {#mutual-information-paradigm .unnumbered}</span></span>
<span id="cb12-2807"><a href="#cb12-2807"></a></span>
<span id="cb12-2808"><a href="#cb12-2808"></a>In this section we discuss an influential new framework for designing</span>
<span id="cb12-2809"><a href="#cb12-2809"></a>peer prediction mechanisms, the Mutual Information Paradigm (MIP)</span>
<span id="cb12-2810"><a href="#cb12-2810"></a>introduced by Kong and Schoenebeck <span class="co">[</span><span class="ot">@kongschoenebeck2019</span><span class="co">]</span>. Traditional</span>
<span id="cb12-2811"><a href="#cb12-2811"></a>peer prediction approaches typically rely on scoring rules and</span>
<span id="cb12-2812"><a href="#cb12-2812"></a>correlation between agents' signals. However, these methods often</span>
<span id="cb12-2813"><a href="#cb12-2813"></a>struggle with issues like uninformed equilibria, where agents can</span>
<span id="cb12-2814"><a href="#cb12-2814"></a>coordinate on uninformative strategies that yield higher payoffs than</span>
<span id="cb12-2815"><a href="#cb12-2815"></a>truth-telling. The core idea is to reward agents based on the mutual</span>
<span id="cb12-2816"><a href="#cb12-2816"></a>information between their report and the reports of other agents.</span>
<span id="cb12-2817"><a href="#cb12-2817"></a></span>
<span id="cb12-2818"><a href="#cb12-2818"></a>We consider a setting with $n$ agents, each possessing a private signal</span>
<span id="cb12-2819"><a href="#cb12-2819"></a>$\Psi_i$ drawn from some set $\Sigma$. The mechanism asks each agent to</span>
<span id="cb12-2820"><a href="#cb12-2820"></a>report their signal, which we denote as $\hat{\Psi}_i$. For each agent</span>
<span id="cb12-2821"><a href="#cb12-2821"></a>$i$, the mechanism randomly selects a reference agent $j \neq i$. Agent</span>
<span id="cb12-2822"><a href="#cb12-2822"></a>$i$'s payment is then calculated as: </span>
<span id="cb12-2823"><a href="#cb12-2823"></a>$$MI(\hat{\Psi}_i; \hat{\Psi}_j)$$   {#eq-eq3.68}</span>
<span id="cb12-2824"><a href="#cb12-2824"></a>where $MI$ is an information-monotone mutual information measure. An</span>
<span id="cb12-2825"><a href="#cb12-2825"></a>information-monotone $MI$ measure must satisfy the following properties:</span>
<span id="cb12-2826"><a href="#cb12-2826"></a></span>
<span id="cb12-2827"><a href="#cb12-2827"></a><span class="ss">-   </span>**Symmetry**: $MI(X; Y) = MI(Y; X)$.</span>
<span id="cb12-2828"><a href="#cb12-2828"></a></span>
<span id="cb12-2829"><a href="#cb12-2829"></a><span class="ss">-   </span>**Non-negativity**: $MI(X; Y) \geq 0$, with equality if and only if</span>
<span id="cb12-2830"><a href="#cb12-2830"></a>    $X$ and $Y$ are independent.</span>
<span id="cb12-2831"><a href="#cb12-2831"></a></span>
<span id="cb12-2832"><a href="#cb12-2832"></a><span class="ss">-   </span>**Data processing inequality**: For any transition probability $M$,</span>
<span id="cb12-2833"><a href="#cb12-2833"></a>    if $Y$ is independent of $M(X)$ conditioned on $X$, then</span>
<span id="cb12-2834"><a href="#cb12-2834"></a>    $MI(M(X); Y) \leq MI(X; Y)$.</span>
<span id="cb12-2835"><a href="#cb12-2835"></a></span>
<span id="cb12-2836"><a href="#cb12-2836"></a>Two important families of mutual information measures that satisfy these</span>
<span id="cb12-2837"><a href="#cb12-2837"></a>properties are $f$-mutual information and Bregman mutual information.</span>
<span id="cb12-2838"><a href="#cb12-2838"></a>The $f$-mutual information is defined as:</span>
<span id="cb12-2839"><a href="#cb12-2839"></a>$$MI_f(X; Y) = D_f(U_{X,Y}, V_{X,Y})$$  {#eq-eq3.69}</span>
<span id="cb12-2840"><a href="#cb12-2840"></a> where $D_f$ is an $f$-divergence,</span>
<span id="cb12-2841"><a href="#cb12-2841"></a>$U_{X,Y}$ is the joint distribution of $X$ and $Y$, and $V_{X,Y}$ is the</span>
<span id="cb12-2842"><a href="#cb12-2842"></a>product of their marginal distributions. The Bregman mutual information</span>
<span id="cb12-2843"><a href="#cb12-2843"></a>is defined as: $$BMI_{PS}(X; Y) = \mathbb{E}_{X} [D{PS}(U_{Y|X}, U_Y)]$$  {#eq-eq3.70}</span>
<span id="cb12-2844"><a href="#cb12-2844"></a>where $D_{PS}$ is a Bregman divergence based on a proper scoring rule</span>
<span id="cb12-2845"><a href="#cb12-2845"></a>$PS$, $U_{Y|X}$ is the conditional distribution of $Y$ given $X$, and</span>
<span id="cb12-2846"><a href="#cb12-2846"></a>$U_Y$ is the marginal distribution of $Y$.</span>
<span id="cb12-2847"><a href="#cb12-2847"></a></span>
<span id="cb12-2848"><a href="#cb12-2848"></a>The MIP framework can be applied in both single-question and</span>
<span id="cb12-2849"><a href="#cb12-2849"></a>multi-question settings. In the multi-question setting, the mechanism</span>
<span id="cb12-2850"><a href="#cb12-2850"></a>can estimate the mutual information empirically from multiple questions.</span>
<span id="cb12-2851"><a href="#cb12-2851"></a>In the single-question setting, additional techniques like asking for</span>
<span id="cb12-2852"><a href="#cb12-2852"></a>predictions about other agents' reports are used to estimate the mutual</span>
<span id="cb12-2853"><a href="#cb12-2853"></a>information.</span>
<span id="cb12-2854"><a href="#cb12-2854"></a></span>
<span id="cb12-2855"><a href="#cb12-2855"></a>A key theoretical result of the MIP framework is that when the chosen</span>
<span id="cb12-2856"><a href="#cb12-2856"></a>mutual information measure is strictly information-monotone with respect</span>
<span id="cb12-2857"><a href="#cb12-2857"></a>to agents' priors, the resulting mechanism is both dominantly truthful</span>
<span id="cb12-2858"><a href="#cb12-2858"></a>and strongly truthful. This means that truth-telling is a dominant</span>
<span id="cb12-2859"><a href="#cb12-2859"></a>strategy for each agent and that the truth-telling equilibrium yields</span>
<span id="cb12-2860"><a href="#cb12-2860"></a>strictly higher payoffs than any other non-permutation strategy profile.</span>
<span id="cb12-2861"><a href="#cb12-2861"></a></span>
<span id="cb12-2862"><a href="#cb12-2862"></a>As research continues to address practical implementation challenges of</span>
<span id="cb12-2863"><a href="#cb12-2863"></a>designing truthful mechanisms, MIP-based approaches have significant</span>
<span id="cb12-2864"><a href="#cb12-2864"></a>potential to improve preference elicitation and aggregation in</span>
<span id="cb12-2865"><a href="#cb12-2865"></a>real-world applications lacking verifiable ground truth.</span>
<span id="cb12-2866"><a href="#cb12-2866"></a></span>
<span id="cb12-2867"><a href="#cb12-2867"></a><span class="fu">#### Auction Theory 2 {.unnumbered}</span></span>
<span id="cb12-2868"><a href="#cb12-2868"></a></span>
<span id="cb12-2869"><a href="#cb12-2869"></a><span class="fu">##### Single-Item Auctions {#single-item-auctions-1 .unnumbered}</span></span>
<span id="cb12-2870"><a href="#cb12-2870"></a></span>
<span id="cb12-2871"><a href="#cb12-2871"></a>The first problem within auction we will consider is the *single-item</span>
<span id="cb12-2872"><a href="#cb12-2872"></a>auction*. In this problem setup, there is a single item to sell and $n$</span>
<span id="cb12-2873"><a href="#cb12-2873"></a>bidders each with unknown private valuations of the item</span>
<span id="cb12-2874"><a href="#cb12-2874"></a>$v_1, \ldots, v_n$,</span>
<span id="cb12-2875"><a href="#cb12-2875"></a></span>
<span id="cb12-2876"><a href="#cb12-2876"></a><span class="ot">[^1]: </span>Here, the "$\sup$\" operation refers to the *supremum*, which is</span>
<span id="cb12-2877"><a href="#cb12-2877"></a>    the largest value that a certain set can take on, whereas "$\inf$\"</span>
<span id="cb12-2878"><a href="#cb12-2878"></a>    would refer to the *infimum*, or the smallest value a certain set</span>
<span id="cb12-2879"><a href="#cb12-2879"></a>    can take on.</span>
<span id="cb12-2880"><a href="#cb12-2880"></a></span>
<span id="cb12-2881"><a href="#cb12-2881"></a><span class="fu">### Application: Guiding Human Demonstrations in Robotics</span></span>
<span id="cb12-2882"><a href="#cb12-2882"></a></span>
<span id="cb12-2883"><a href="#cb12-2883"></a>A strong approach to learning policies for robotic manipulation is</span>
<span id="cb12-2884"><a href="#cb12-2884"></a>imitation learning, the technique of learning behaviors from human</span>
<span id="cb12-2885"><a href="#cb12-2885"></a>demonstrations. In particular, interactive imitation learning allows a</span>
<span id="cb12-2886"><a href="#cb12-2886"></a>group of humans to contribute their own demonstrations for a task,</span>
<span id="cb12-2887"><a href="#cb12-2887"></a>allowing for scalable learning. However, not all groups of demonstrators</span>
<span id="cb12-2888"><a href="#cb12-2888"></a>are equally helpful for interactive imitation learning.</span>
<span id="cb12-2889"><a href="#cb12-2889"></a></span>
<span id="cb12-2890"><a href="#cb12-2890"></a>The ideal set of demonstrations for imitation learning would follow a</span>
<span id="cb12-2891"><a href="#cb12-2891"></a>single, optimal method for performing the task, which a robot could</span>
<span id="cb12-2892"><a href="#cb12-2892"></a>learn to mimic. Conversely, *multimodality*, the presence of multiple</span>
<span id="cb12-2893"><a href="#cb12-2893"></a>optimal methods in the demonstration set, is challenging for imitation</span>
<span id="cb12-2894"><a href="#cb12-2894"></a>learning since it has to learn from contradicting information for how to</span>
<span id="cb12-2895"><a href="#cb12-2895"></a>accomplish a task.</span>
<span id="cb12-2896"><a href="#cb12-2896"></a></span>
<span id="cb12-2897"><a href="#cb12-2897"></a>A common reason for multimodality is the fact that different people</span>
<span id="cb12-2898"><a href="#cb12-2898"></a>often subconsciously choose different paths for execution, as</span>
<span id="cb12-2899"><a href="#cb12-2899"></a>illustrated in @fig-multimodalexecution.</span>
<span id="cb12-2900"><a href="#cb12-2900"></a></span>
<span id="cb12-2901"><a href="#cb12-2901"></a>![Examples of two different ways to insert a nut onto a round peg. The</span>
<span id="cb12-2902"><a href="#cb12-2902"></a>orange demonstration picks up the nut from the hole while the blue</span>
<span id="cb12-2903"><a href="#cb12-2903"></a>demonstration picks up the nut from the side</span>
<span id="cb12-2904"><a href="#cb12-2904"></a><span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span>](Figures/multimodal_peg.png){#fig-multimodalexecution</span>
<span id="cb12-2905"><a href="#cb12-2905"></a>width="50%"}</span>
<span id="cb12-2906"><a href="#cb12-2906"></a></span>
<span id="cb12-2907"><a href="#cb12-2907"></a>Gandhi et al. <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span> identifies whether demonstrations</span>
<span id="cb12-2908"><a href="#cb12-2908"></a>are compatible with one another and offer an active elicitation</span>
<span id="cb12-2909"><a href="#cb12-2909"></a>interface to guide humans to provide better demonstrations in</span>
<span id="cb12-2910"><a href="#cb12-2910"></a>interactive imitation learning. Their key motivation is to allow</span>
<span id="cb12-2911"><a href="#cb12-2911"></a>multiple users to contribute demonstrations over the course of data</span>
<span id="cb12-2912"><a href="#cb12-2912"></a>collection by guiding users towards compatible demonstrations.</span>
<span id="cb12-2913"><a href="#cb12-2913"></a></span>
<span id="cb12-2914"><a href="#cb12-2914"></a>To identify whether a demonstration is "compatible" with a base policy</span>
<span id="cb12-2915"><a href="#cb12-2915"></a>trained with prior demonstrations, the researchers measure the</span>
<span id="cb12-2916"><a href="#cb12-2916"></a>*likelihood* of demonstrated actions under the base policy, and the</span>
<span id="cb12-2917"><a href="#cb12-2917"></a>*novelty* of the visited states. Intuitively, low likelihood and low</span>
<span id="cb12-2918"><a href="#cb12-2918"></a>novelty demonstrations should be excluded since they represent</span>
<span id="cb12-2919"><a href="#cb12-2919"></a>conflicting modes of behavior on states that the robot can already</span>
<span id="cb12-2920"><a href="#cb12-2920"></a>handle, and are therefore incompatible. This concept of compatibility is</span>
<span id="cb12-2921"><a href="#cb12-2921"></a>used for filtering a new set of demonstrations and actively eliciting</span>
<span id="cb12-2922"><a href="#cb12-2922"></a>compatible demonstrations.</span>
<span id="cb12-2923"><a href="#cb12-2923"></a></span>
<span id="cb12-2924"><a href="#cb12-2924"></a>In the following subsections, we describe the process of estimating</span>
<span id="cb12-2925"><a href="#cb12-2925"></a>compatibility and active elicitation in more detal.</span>
<span id="cb12-2926"><a href="#cb12-2926"></a></span>
<span id="cb12-2927"><a href="#cb12-2927"></a><span class="fu">#### Estimating Compatiblity {.unnumbered}</span></span>
<span id="cb12-2928"><a href="#cb12-2928"></a></span>
<span id="cb12-2929"><a href="#cb12-2929"></a>We want to define a compatibility measure $\mathcal{M}$, that estimates</span>
<span id="cb12-2930"><a href="#cb12-2930"></a>the performance of policy $\pi_{base}$ that is retrained on a union of</span>
<span id="cb12-2931"><a href="#cb12-2931"></a>$\mathcal{D}_{base}$, the known base dataset, and $\mathcal{D}_{new}$,</span>
<span id="cb12-2932"><a href="#cb12-2932"></a>the newly collected dataset. To define this compatibility measure in a</span>
<span id="cb12-2933"><a href="#cb12-2933"></a>way that is easy to compute, we can use two interpretable metrics:</span>
<span id="cb12-2934"><a href="#cb12-2934"></a>likelihood and novelty.</span>
<span id="cb12-2935"><a href="#cb12-2935"></a></span>
<span id="cb12-2936"><a href="#cb12-2936"></a>The likelihood of actions $a_{new}$ in $\mathcal{D}_{new}$ is measured</span>
<span id="cb12-2937"><a href="#cb12-2937"></a>as the negative mean squared error between actions predicted by the base</span>
<span id="cb12-2938"><a href="#cb12-2938"></a>policy and this proposed action:</span>
<span id="cb12-2939"><a href="#cb12-2939"></a></span>
<span id="cb12-2940"><a href="#cb12-2940"></a>$$\begin{aligned}</span>
<span id="cb12-2941"><a href="#cb12-2941"></a>    likelihood(s_{new}, a_{new}) = -\mathbb{E}<span class="co">[</span><span class="ot">|| \pi_{base}(s_{new}) - a_{new} ||^2_2</span><span class="co">]</span>.</span>
<span id="cb12-2942"><a href="#cb12-2942"></a>\end{aligned}$$  {#eq-eq3.61}</span>
<span id="cb12-2943"><a href="#cb12-2943"></a></span>
<span id="cb12-2944"><a href="#cb12-2944"></a>The novelty of the state $s_{new}$ in $\mathcal{D}_{new}$ is the</span>
<span id="cb12-2945"><a href="#cb12-2945"></a>standard deviation in the predicted actions under base policy:</span>
<span id="cb12-2946"><a href="#cb12-2946"></a></span>
<span id="cb12-2947"><a href="#cb12-2947"></a>$$\begin{aligned}</span>
<span id="cb12-2948"><a href="#cb12-2948"></a>    novelty(s_{new}) = \mathrm{Var}<span class="co">[</span><span class="ot">\pi_{base}(s_{new})</span><span class="co">]</span>.</span>
<span id="cb12-2949"><a href="#cb12-2949"></a>\end{aligned}$$  {#eq-eq3.62}</span>
<span id="cb12-2950"><a href="#cb12-2950"></a></span>
<span id="cb12-2951"><a href="#cb12-2951"></a>We can plot likelihood and novelty on a 2D plane, as shown in @fig-likelihood_novelty, and identify thresholds on</span>
<span id="cb12-2952"><a href="#cb12-2952"></a>likelihood and novelty, denoted as $\lambda$ and $\eta$ respectively.</span>
<span id="cb12-2953"><a href="#cb12-2953"></a>Intuitively, demonstrations with low likelihood in low novelty states</span>
<span id="cb12-2954"><a href="#cb12-2954"></a>should be excluded, because this indicates that there is a conflict</span>
<span id="cb12-2955"><a href="#cb12-2955"></a>between the base behavior and the new demonstration due to</span>
<span id="cb12-2956"><a href="#cb12-2956"></a>multimodality. Note that in high novelty states, the likelihood should</span>
<span id="cb12-2957"><a href="#cb12-2957"></a>be disregarded because the base policy does not have a concrete idea for</span>
<span id="cb12-2958"><a href="#cb12-2958"></a>how to handle these states anyways so more data is needed.</span>
<span id="cb12-2959"><a href="#cb12-2959"></a></span>
<span id="cb12-2960"><a href="#cb12-2960"></a>![Examples of plots of likelihood and novelty for compatible and</span>
<span id="cb12-2961"><a href="#cb12-2961"></a>incompatible operators</span>
<span id="cb12-2962"><a href="#cb12-2962"></a><span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span>](Figures/likelihood_novelty.png){#fig-likelihood_novelty</span>
<span id="cb12-2963"><a href="#cb12-2963"></a>width="80%"}</span>
<span id="cb12-2964"><a href="#cb12-2964"></a></span>
<span id="cb12-2965"><a href="#cb12-2965"></a>The final compatibility metric, parameterized by the likelihood and</span>
<span id="cb12-2966"><a href="#cb12-2966"></a>novelty thresholds $\lambda$ and $\eta$, is</span>
<span id="cb12-2967"><a href="#cb12-2967"></a>$\mathcal{M}(\mathcal{D}_{base}, (s_{new}, a_{new})) \in <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$,</span>
<span id="cb12-2968"><a href="#cb12-2968"></a>defined as:</span>
<span id="cb12-2969"><a href="#cb12-2969"></a></span>
<span id="cb12-2970"><a href="#cb12-2970"></a>$$\begin{aligned}</span>
<span id="cb12-2971"><a href="#cb12-2971"></a>    \mathcal{M} = \begin{cases} </span>
<span id="cb12-2972"><a href="#cb12-2972"></a>        1 - \min(\frac{\mathbb{E}<span class="co">[</span><span class="ot">|| \pi_{base}(s_{new}) - a_{new} ||^2_2</span><span class="co">]</span>}{\lambda}, 1) &amp; \text{ if } \text{novelty}(s_{new}) &lt; \eta <span class="sc">\\</span></span>
<span id="cb12-2973"><a href="#cb12-2973"></a>        1 &amp; \text{ otherwise }</span>
<span id="cb12-2974"><a href="#cb12-2974"></a>       \end{cases}.</span>
<span id="cb12-2975"><a href="#cb12-2975"></a>\end{aligned}$$  {#eq-eq3.63}</span>
<span id="cb12-2976"><a href="#cb12-2976"></a></span>
<span id="cb12-2977"><a href="#cb12-2977"></a>Note that $\lambda$ and $\eta$ need to be specified by hand. This is</span>
<span id="cb12-2978"><a href="#cb12-2978"></a>accomplished by assuming the ability to collect *a priori incompatible*</span>
<span id="cb12-2979"><a href="#cb12-2979"></a>demonstrations to identify reasonable thresholds that remove the most</span>
<span id="cb12-2980"><a href="#cb12-2980"></a>datapoints in the incompatible demonstrations while keeping the most</span>
<span id="cb12-2981"><a href="#cb12-2981"></a>datapoints in the compatible demonstrations.</span>
<span id="cb12-2982"><a href="#cb12-2982"></a></span>
<span id="cb12-2983"><a href="#cb12-2983"></a><span class="fu">#### Case Studies with Fixed Sets {.unnumbered}</span></span>
<span id="cb12-2984"><a href="#cb12-2984"></a></span>
<span id="cb12-2985"><a href="#cb12-2985"></a>The researchers evaluate the utility of the compatibility metric on</span>
<span id="cb12-2986"><a href="#cb12-2986"></a>three tasks: placing a square nut on a square peg, placing a round nut</span>
<span id="cb12-2987"><a href="#cb12-2987"></a>on a round peg, and opening a drawer and placing a hammer inside. For</span>
<span id="cb12-2988"><a href="#cb12-2988"></a>each task, they train a base policy using a "proficient" operator's</span>
<span id="cb12-2989"><a href="#cb12-2989"></a>demonstration while sampling trajectories from other operators for the</span>
<span id="cb12-2990"><a href="#cb12-2990"></a>new set.</span>
<span id="cb12-2991"><a href="#cb12-2991"></a></span>
<span id="cb12-2992"><a href="#cb12-2992"></a>The naive baseline is to use all datapoints while the</span>
<span id="cb12-2993"><a href="#cb12-2993"></a>$\mathcal{M}$-Filtered demonstrations use the compatibility metric to</span>
<span id="cb12-2994"><a href="#cb12-2994"></a>filter out incompatible demonstrations. The results are presented in</span>
<span id="cb12-2995"><a href="#cb12-2995"></a>@tbl-m_filter_table. As you can see, M-filtering results in</span>
<span id="cb12-2996"><a href="#cb12-2996"></a>equal or greater performance despite using less data than the naive</span>
<span id="cb12-2997"><a href="#cb12-2997"></a>baseline, demonstrating the effectiveness of compatibility-based</span>
<span id="cb12-2998"><a href="#cb12-2998"></a>filtering.</span>
<span id="cb12-2999"><a href="#cb12-2999"></a></span>
<span id="cb12-3000"><a href="#cb12-3000"></a>::: {#tbl-m_filter_table}</span>
<span id="cb12-3001"><a href="#cb12-3001"></a>  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------</span>
<span id="cb12-3002"><a href="#cb12-3002"></a><span class="in">                   **Square Nut**                            **Round Nut**                            **Hammer Placement**  </span></span>
<span id="cb12-3003"><a href="#cb12-3003"></a>  **Operator**         Naive        $\mathcal{M}$-Filtered       Naive       $\mathcal{M}$-Filtered          Naive           $\mathcal{M}$-Filtered</span>
<span id="cb12-3004"><a href="#cb12-3004"></a>  Base Operator      38.7 (2.1)               <span class="sc">\-</span>              13.3 (2.3)               <span class="sc">\-</span>                  24.7 (6.1)                  <span class="sc">\-</span></span>
<span id="cb12-3005"><a href="#cb12-3005"></a>  Operator 1         54.3 (1.5)           61.0 (4.4)          26.7 (11.7)         32.0 (12.2)              38.0 (2.0)              39.7 (4.6)</span>
<span id="cb12-3006"><a href="#cb12-3006"></a>  Operator 2         40.3 (5.1)           42.0 (2.0)          22.0 (7.2)           26.7 (5.0)              33.3 (3.1)              32.7 (6.4)</span>
<span id="cb12-3007"><a href="#cb12-3007"></a>  Operator 3         37.3 (2.1)           42.7 (0.6)          17.3 (4.6)          18.0 (13.9)              8.0 (0.0)               12.0 (0.0)</span>
<span id="cb12-3008"><a href="#cb12-3008"></a>  Operator 4         27.3 (3.5)           37.3 (2.1)           7.3 (4.6)           13.3 (1.2)              4.0 (0.0)               4.0 (0.0)</span>
<span id="cb12-3009"><a href="#cb12-3009"></a>  --------------- ---------------- ------------------------ --------------- ------------------------ ---------------------- ------------------------</span>
<span id="cb12-3010"><a href="#cb12-3010"></a></span>
<span id="cb12-3011"><a href="#cb12-3011"></a>  : Success rates (mean/std across 3 training runs) for policies trained</span>
<span id="cb12-3012"><a href="#cb12-3012"></a>  on $\mathcal{D}_{new}$ by using all the data (Naive) or filtering by</span>
<span id="cb12-3013"><a href="#cb12-3013"></a>  compatibility ($\mathcal{M}$-Filtered) <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span></span>
<span id="cb12-3014"><a href="#cb12-3014"></a>:::</span>
<span id="cb12-3015"><a href="#cb12-3015"></a></span>
<span id="cb12-3016"><a href="#cb12-3016"></a>![The phases of the active elicitation interface: (a) initial prompting,</span>
<span id="cb12-3017"><a href="#cb12-3017"></a>(b) demonstrations with live feedback, and (c) corrective feedback</span>
<span id="cb12-3018"><a href="#cb12-3018"></a><span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span>](Figures/active_elicitation.png){#fig-active_elicitation</span>
<span id="cb12-3019"><a href="#cb12-3019"></a>width="80%"}</span>
<span id="cb12-3020"><a href="#cb12-3020"></a></span>
<span id="cb12-3021"><a href="#cb12-3021"></a><span class="fu">#### Actively Eliciting Compatible Demonstrations {.unnumbered}</span></span>
<span id="cb12-3022"><a href="#cb12-3022"></a></span>
<span id="cb12-3023"><a href="#cb12-3023"></a>In the previous section, we assume access to a dataset that has already</span>
<span id="cb12-3024"><a href="#cb12-3024"></a>been collected, and we see how filtering out incompatible demonstrations</span>
<span id="cb12-3025"><a href="#cb12-3025"></a>helps improve performance. However, when collecting a new dataset, it</span>
<span id="cb12-3026"><a href="#cb12-3026"></a>would be better to ensure that operators collect compatible</span>
<span id="cb12-3027"><a href="#cb12-3027"></a>demonstrations from the start, allowing us to retain as much data as</span>
<span id="cb12-3028"><a href="#cb12-3028"></a>possible for training.</span>
<span id="cb12-3029"><a href="#cb12-3029"></a></span>
<span id="cb12-3030"><a href="#cb12-3030"></a>To actively elicit compatible demonstrations, the researchers set up a</span>
<span id="cb12-3031"><a href="#cb12-3031"></a>pipeline for live feedback and examples. At the start, operators are</span>
<span id="cb12-3032"><a href="#cb12-3032"></a>given a task specification and some episodes to practice using the</span>
<span id="cb12-3033"><a href="#cb12-3033"></a>robot. Then, the active elicitation process begins, as shown in @fig-active_elicitation. Each operator is shown some</span>
<span id="cb12-3034"><a href="#cb12-3034"></a>rollouts of the base policy to understand the style of the base</span>
<span id="cb12-3035"><a href="#cb12-3035"></a>operator. Next, the operator provides a demonstration similar to the</span>
<span id="cb12-3036"><a href="#cb12-3036"></a>ones they were shown. As they record their demonstrations, the interface</span>
<span id="cb12-3037"><a href="#cb12-3037"></a>provides online feedback, with green indicating compatible actions and</span>
<span id="cb12-3038"><a href="#cb12-3038"></a>red indicating incompatible actions. If the number of incompatible</span>
<span id="cb12-3039"><a href="#cb12-3039"></a>state-action pairs (ones where $\mathcal{M}$ is zero) exceeds 5% of the</span>
<span id="cb12-3040"><a href="#cb12-3040"></a>demonstration length, the demonstration is rejected. However, to provide</span>
<span id="cb12-3041"><a href="#cb12-3041"></a>corrective feedback, the interface shows the areas of the demonstration</span>
<span id="cb12-3042"><a href="#cb12-3042"></a>with the highest average incompatibility and also provides an expert</span>
<span id="cb12-3043"><a href="#cb12-3043"></a>demo that shows what should actually be done. Demonstrators can use this</span>
<span id="cb12-3044"><a href="#cb12-3044"></a>feedback to provide more compatible demonstrations moving forward.</span>
<span id="cb12-3045"><a href="#cb12-3045"></a></span>
<span id="cb12-3046"><a href="#cb12-3046"></a>This process helps improve the demonstration quality in both simulation</span>
<span id="cb12-3047"><a href="#cb12-3047"></a>and real experiments, as show in @tbl-active_elicitation_results. Specifically, on the real</span>
<span id="cb12-3048"><a href="#cb12-3048"></a>results, active elicitation outperformed the base policy by 25% and</span>
<span id="cb12-3049"><a href="#cb12-3049"></a>naive data collection by 55%. Overall, active elicitation is a powerful</span>
<span id="cb12-3050"><a href="#cb12-3050"></a>tool to ensure that data collected for imitation learning improves the</span>
<span id="cb12-3051"><a href="#cb12-3051"></a>quality of the learned policy.</span>
<span id="cb12-3052"><a href="#cb12-3052"></a></span>
<span id="cb12-3053"><a href="#cb12-3053"></a>::: {#tbl-active_elicitation_results}</span>
<span id="cb12-3054"><a href="#cb12-3054"></a>  **Task**                                            **Base**     **Naive**    **Naive + Filtered**   **Informed**</span>
<span id="cb12-3055"><a href="#cb12-3055"></a>  ------------------------------------------------- ------------ ------------- ---------------------- --------------</span>
<span id="cb12-3056"><a href="#cb12-3056"></a>  **Round Nut**                                      13.3 (2.3)    9.6 (4.6)         9.7 (4.2)          15.7 (6.0)</span>
<span id="cb12-3057"><a href="#cb12-3057"></a>  **Hammer Placement**                               24.7 (6.1)   20.8 (15.7)       22.0 (15.5)        31.8 (16.3)</span>
<span id="cb12-3058"><a href="#cb12-3058"></a>  **$\left[ \textup{Real} \right]$ Food Plating**       60.0      30.0 (17.3)            <span class="sc">\-</span>             85.0 (9.6)</span>
<span id="cb12-3059"><a href="#cb12-3059"></a></span>
<span id="cb12-3060"><a href="#cb12-3060"></a>  : Success rates (mean/std across users) for policies trained on</span>
<span id="cb12-3061"><a href="#cb12-3061"></a>  $\mathcal{D}_{new}$ by using all the data (Naive), filtering by</span>
<span id="cb12-3062"><a href="#cb12-3062"></a>  compatibility ($\mathcal{M}$-Filtered), or using informed</span>
<span id="cb12-3063"><a href="#cb12-3063"></a>  demonstration collection <span class="co">[</span><span class="ot">@gandhi2022eliciting</span><span class="co">]</span></span>
<span id="cb12-3064"><a href="#cb12-3064"></a>:::</span>
<span id="cb12-3065"><a href="#cb12-3065"></a></span>
<span id="cb12-3066"><a href="#cb12-3066"></a><span class="fu">#### Limitations and Future Work for Active Elicitation {.unnumbered}</span></span>
<span id="cb12-3067"><a href="#cb12-3067"></a></span>
<span id="cb12-3068"><a href="#cb12-3068"></a>A fundamental limitation of eliciting compatible demonstrations is the</span>
<span id="cb12-3069"><a href="#cb12-3069"></a>fact that the "base" demonstrator is considered the ground truth. When</span>
<span id="cb12-3070"><a href="#cb12-3070"></a>the base demonstrator specifies a preference, all other demonstrators</span>
<span id="cb12-3071"><a href="#cb12-3071"></a>must abide by it, even if they have strong preferences against it. For</span>
<span id="cb12-3072"><a href="#cb12-3072"></a>instance, when pouring milk and cereal into a bowl, different people</span>
<span id="cb12-3073"><a href="#cb12-3073"></a>have different preferences for what is the correct order, but active</span>
<span id="cb12-3074"><a href="#cb12-3074"></a>elicitation forces all demonstrators to follow the initial preference of</span>
<span id="cb12-3075"><a href="#cb12-3075"></a>the base operator. The researchers hope that future work can enable</span>
<span id="cb12-3076"><a href="#cb12-3076"></a>users to override the default demonstration set and follow a base</span>
<span id="cb12-3077"><a href="#cb12-3077"></a>behavior that better aligns with their preferences. This could enable</span>
<span id="cb12-3078"><a href="#cb12-3078"></a>multiple modes of behavior to be collected in data while only following</span>
<span id="cb12-3079"><a href="#cb12-3079"></a>a user's specified preference instead of attempting to collapse all</span>
<span id="cb12-3080"><a href="#cb12-3080"></a>modes into a single policy.</span>
<span id="cb12-3081"><a href="#cb12-3081"></a></span>
<span id="cb12-3082"><a href="#cb12-3082"></a>Looking forward, active elicitation provides a foundation for allowing</span>
<span id="cb12-3083"><a href="#cb12-3083"></a>robots to query humans about the type of data needed, enabling more</span>
<span id="cb12-3084"><a href="#cb12-3084"></a>efficient data collection through transparency.</span>
<span id="cb12-3085"><a href="#cb12-3085"></a></span>
<span id="cb12-3086"><a href="#cb12-3086"></a><span class="fu">### Conclusion</span></span>
<span id="cb12-3087"><a href="#cb12-3087"></a></span>
<span id="cb12-3088"><a href="#cb12-3088"></a>In summary, this chapter has explored the complexities and innovations</span>
<span id="cb12-3089"><a href="#cb12-3089"></a>in interactive learning as applied to large models within robotics. It</span>
<span id="cb12-3090"><a href="#cb12-3090"></a>begins by investigating pairwise comparisons and their role in</span>
<span id="cb12-3091"><a href="#cb12-3091"></a>efficiently learning linear reward functions from large datasets,</span>
<span id="cb12-3092"><a href="#cb12-3092"></a>overcoming limitations in supervised learning. When combined with active</span>
<span id="cb12-3093"><a href="#cb12-3093"></a>learning techniques, these comparisons supply timely, targeted, and</span>
<span id="cb12-3094"><a href="#cb12-3094"></a>context-appropriate feedback, enhancing performance in time-critical</span>
<span id="cb12-3095"><a href="#cb12-3095"></a>applications like exoskeleton adjustments during rehabilitation.</span>
<span id="cb12-3096"><a href="#cb12-3096"></a></span>
<span id="cb12-3097"><a href="#cb12-3097"></a>We then shift to imitation learning or inverse reward learning from</span>
<span id="cb12-3098"><a href="#cb12-3098"></a>demonstrations, emphasizing the difficulties introduced by multimodal</span>
<span id="cb12-3099"><a href="#cb12-3099"></a>demonstration sets. active elicitation approaches to compile compatible</span>
<span id="cb12-3100"><a href="#cb12-3100"></a>demonstrations, streamlining the learning process by guiding users to</span>
<span id="cb12-3101"><a href="#cb12-3101"></a>provide more valuable, steady examples are incredibly promising,</span>
<span id="cb12-3102"><a href="#cb12-3102"></a>however, to tackling this issue. This method shows promise in refining</span>
<span id="cb12-3103"><a href="#cb12-3103"></a>the interactive imitation learning data collection pipeline, enabling</span>
<span id="cb12-3104"><a href="#cb12-3104"></a>more capable and effective robotic training.</span>
<span id="cb12-3105"><a href="#cb12-3105"></a></span>
<span id="cb12-3106"><a href="#cb12-3106"></a>Additionally, the chapter examines the integration of foundation models</span>
<span id="cb12-3107"><a href="#cb12-3107"></a>into robotics, highlighting the transformative innovations of R3M and</span>
<span id="cb12-3108"><a href="#cb12-3108"></a>Voltron. R3M's pre-training on diverse human activities dramatically</span>
<span id="cb12-3109"><a href="#cb12-3109"></a>improves robotic manipulation with minimal supervision. Meanwhile,</span>
<span id="cb12-3110"><a href="#cb12-3110"></a>Voltron builds on these capabilities by incorporating language-driven</span>
<span id="cb12-3111"><a href="#cb12-3111"></a>representation learning for remarkably adaptable and nuanced robotic</span>
<span id="cb12-3112"><a href="#cb12-3112"></a>task performance. These models represent significant leaps in robotics</span>
<span id="cb12-3113"><a href="#cb12-3113"></a>while opening new frontiers for future research and applications.</span>
<span id="cb12-3114"><a href="#cb12-3114"></a></span>
<span id="cb12-3115"><a href="#cb12-3115"></a></span>
<span id="cb12-3116"><a href="#cb12-3116"></a>{{&lt; include psets/pset2.qmd &gt;}}</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/003-measure.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>