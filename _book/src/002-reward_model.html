<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Human Decision Making and Choice Models – Machine Learning from Human Preferences</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../src/003-measure.html" rel="next">
<link href="../index.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-d166b450ba5a8e9f7a0ab969bf6592c1.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-a0aefded8822f1bee14b20ac4fd2b1d6.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-5c897cb370a42f0721f6bac59365aff2.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../src/002-reward_model.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning from Human Preferences</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/sangttruong/mlhp" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../Machine-Learning-from-Human-Preferences.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/002-reward_model.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/003-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/004-optim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model-Free Preference Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/005-align.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Human Values and AI Alignment</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../src/006-conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>license.qmd</p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#sec-foundations" id="toc-sec-foundations" class="nav-link" data-scroll-target="#sec-foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</a>
  <ul class="collapse">
  <li><a href="#axiom-1-preference-models-model-choice" id="toc-axiom-1-preference-models-model-choice" class="nav-link" data-scroll-target="#axiom-1-preference-models-model-choice">Axiom 1: Preference models model choice</a></li>
  <li><a href="#axiom-2-preference-captures-decision-making" id="toc-axiom-2-preference-captures-decision-making" class="nav-link" data-scroll-target="#axiom-2-preference-captures-decision-making">Axiom 2: Preference captures decision-making</a></li>
  <li><a href="#axiom-3-preference-centers-around-utility" id="toc-axiom-3-preference-centers-around-utility" class="nav-link" data-scroll-target="#axiom-3-preference-centers-around-utility">Axiom 3: Preference centers around utility</a></li>
  </ul></li>
  <li><a href="#sec-models" id="toc-sec-models" class="nav-link" data-scroll-target="#sec-models"><span class="header-section-number">2.3</span> Models of Individual Choices</a>
  <ul class="collapse">
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection"><span class="header-section-number">2.3.1</span> Data Collection</a></li>
  <li><a href="#data-interpretation" id="toc-data-interpretation" class="nav-link" data-scroll-target="#data-interpretation"><span class="header-section-number">2.3.2</span> Data Interpretation</a></li>
  </ul></li>
  <li><a href="#sec-learning" id="toc-sec-learning" class="nav-link" data-scroll-target="#sec-learning"><span class="header-section-number">2.4</span> Parameter Learning</a>
  <ul class="collapse">
  <li><a href="#reward-learning-with-large-language-models" id="toc-reward-learning-with-large-language-models" class="nav-link" data-scroll-target="#reward-learning-with-large-language-models"><span class="header-section-number">2.4.1</span> Reward Learning with Large Language Models</a></li>
  <li><a href="#reward-learning-in-robotics" id="toc-reward-learning-in-robotics" class="nav-link" data-scroll-target="#reward-learning-in-robotics"><span class="header-section-number">2.4.2</span> Reward Learning in Robotics</a></li>
  <li><a href="#reward-learning-with-meta-learning" id="toc-reward-learning-with-meta-learning" class="nav-link" data-scroll-target="#reward-learning-with-meta-learning"><span class="header-section-number">2.4.3</span> Reward Learning with Meta Learning</a></li>
  <li><a href="#direct-preference-optimization" id="toc-direct-preference-optimization" class="nav-link" data-scroll-target="#direct-preference-optimization"><span class="header-section-number">2.4.4</span> Direct Preference Optimization</a></li>
  <li><a href="#model-design-consideration" id="toc-model-design-consideration" class="nav-link" data-scroll-target="#model-design-consideration"><span class="header-section-number">2.4.5</span> Model Design Consideration</a></li>
  </ul></li>
  <li><a href="#multimodal-preferences" id="toc-multimodal-preferences" class="nav-link" data-scroll-target="#multimodal-preferences"><span class="header-section-number">2.5</span> Multimodal Preferences</a></li>
  <li><a href="#social-choices" id="toc-social-choices" class="nav-link" data-scroll-target="#social-choices"><span class="header-section-number">2.6</span> Social Choices</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">2.7</span> Exercises</a>
  <ul class="collapse">
  <li><a href="#question-1-choice-modeling-15-points" id="toc-question-1-choice-modeling-15-points" class="nav-link" data-scroll-target="#question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</a></li>
  <li><a href="#question-2-revealed-and-stated-preferences-20-points" id="toc-question-2-revealed-and-stated-preferences-20-points" class="nav-link" data-scroll-target="#question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</a></li>
  <li><a href="#question-3-probabilistic-multi-modal-preferences-25-points" id="toc-question-3-probabilistic-multi-modal-preferences-25-points" class="nav-link" data-scroll-target="#question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</a></li>
  <li><a href="#question-4-direct-preference-optimization-40-points" id="toc-question-4-direct-preference-optimization-40-points" class="nav-link" data-scroll-target="#question-4-direct-preference-optimization-40-points">Question 4: Direct Preference Optimization (40 points)</a></li>
  </ul></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="ch-human-decision-making-choice-models" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Human Decision Making and Choice Models</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<iframe src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<iframe src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" style="width:45%; height:225px;">
</iframe>
<p><a href="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 1</a> <a href="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/" class="btn btn-outline-primary" role="button">Fullscreen Part 2</a></p>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Human preference modeling aims to capture humans’ decision making processes in a probabilistic framework. Many problems would benefit from a quantitative perspective, enabling an understanding of how humans engage with the world. While human decision-making is only somewhat understood, we can use real-world data representing the outcomes of decisions to align human-facing systems with user preferences. Through our exploration of human preference models, we will ground ourselves in building a health coaching system that can provide meal recommendations aligned with a user’s dietary needs and preferences. Examples of scenarios which can benefit from a model of how humans make choices include:</p>
<ol type="1">
<li><p><strong>Health coaching:</strong> Humans express their preferences every time they pick lunch for consumption. Humans may have several goals related to nutrition, such as weight loss and improving concentration. We can learn how a given individual or set of individuals prefer to eat to provide personalized recommendations to help them attain their goals. This chapter will use this use case to ground human preference modeling in a real-life application.</p></li>
<li><p><strong>Social media:</strong> Platforms have a far greater amount of content than one can consume in a lifetime, yet such products must aim to maximize user engagement. To accomplish this, we can learn what specific things people like to see in their feeds to optimize the value they gain out of their time on social media. For example, the video feed social media platform <a href="https://www.tiktok.com/">TikTok</a> has had viral adoption due to its notorious ability to personalize a feed for its users based on their preferences.</p></li>
<li><p><strong>Shopping:</strong> Retail corporations largely aim to maximize revenue by making it easy for people to make purchases. Recommendation systems on online shopping platforms provide a mechanism for curating specific items based on an individual’s previous purchases (or even browsing history) to make shoppers aware of items they may like and, therefore, purchase.</p></li>
</ol>
<div id="tbl-philosophy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-philosophy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Examples of machine learning tasks and their interpretation as modeling human preferences.
</figcaption>
<div aria-describedby="tbl-philosophy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 48%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Application</th>
<th style="text-align: left;">Human Preference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computer vision: train a neural network to predict bounding boxes delineating all instances of dogs in an image</td>
<td style="text-align: left;">This is how humans process images by identifying the position and geometry of the things we see in them</td>
</tr>
<tr class="even">
<td style="text-align: left;">Natural language processing: train a model to generate coherent text</td>
<td style="text-align: left;">Coherent text is itself a human-created and defined concept, and we prefer that any synthetically generated text matches that of humans</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Computer vision: train a diffusion model to generate realistic images of nature</td>
<td style="text-align: left;">Humans prefer that images accurately capture the world as observed by humans, and this generative model should reflect the details that comprise that preference</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In this chapter, we will explore how one can model human preferences, including different formulations of such models, how one can optimize these models given data, and considerations one must understand to create such systems. We note that the exact assumptions we make about human preferences in this chapter differentiate the <em>specific</em> human preference learning problem we are considering from the discriminative and generative tasks we describe in <a href="#tbl-philosophy" class="quarto-xref">Table&nbsp;<span>2.1</span></a>. We describe these assumptions in <a href="#sec-foundations" class="quarto-xref"><span>Section 2.2</span></a>.</p>
</section>
<section id="sec-foundations" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-foundations"><span class="header-section-number">2.2</span> Foundations of Preference Models</h2>
<p>We introduce a framework for discussing human preferences. The different methods to model these preferences <a href="#sec-models" class="quarto-xref"><span>Section 2.3</span></a> all build upon this framework.</p>
<section id="axiom-1-preference-models-model-choice" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-1-preference-models-model-choice">Axiom 1: Preference models model choice</h3>
<p>Human preference models model the preferred choice or choices amongst a set of options. In our health coaching example, this could be modeling which meal from a set of options a person will most likely choose. An alternative framework we will explore is ranking, in which we can model an ordering of given choices from most to least desirable. It is certainly possible that there is an infinite set of options (such as in a continuous action space); in this case, our model will have to reason about a discretized set of options and may fail to capture the full space of possibilities a human would choose from in the real world.</p>
<p>Choices are <em>collectively exhaustive</em>, <em>mutually exclusive</em>, and <em>finite</em>. Human preference models must enumerate an <em>action space</em>, or the set of all possible choices included in a human decision. As such, we must ensure that the choices we enumerate capture the entire domain (collectively exhaustive) but are indeed distinct (mutually exclusive) choices. In our health coaching example, a person either chooses to eat chicken or fish. Choosing one does not affect the other.</p>
<p>A discrete set of choices is a constraint we canonically impose to ensure we can tractably model preferences and aptly estimate the parameters of preference models. This is usually sufficiently expressive to create a powerful human preference model (for example, recent generative language models have vocabulary sizes of 40,000+ and can model nearly arbitrary language sequences <span class="citation" data-cites="Radford2018GPT">(<a href="#ref-Radford2018GPT" role="doc-biblioref">Radford et al. 2018</a>)</span>). While in theory, one can imagine a continuous domain for choices, a discrete set fits nicely with most decision-making processes humans face. While human thought is extremely nuanced, most thoughts are expressed as discrete words or discrete decisions in every step humans take in the world.</p>
</section>
<section id="axiom-2-preference-captures-decision-making" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-2-preference-captures-decision-making">Axiom 2: Preference captures decision-making</h3>
<p>There are certainly cases in which human preferences don’t reflect the human decision-making process, for example if there are external factors (social, political, economic) which govern a human’s choices, or if one is explicitly choosing to go against their preferences in the context of exploration. However, human preference models will always do their best to model the ultimate decision, and we assume that they are in some way accounting for these other factors (and any lack of such accounting will result in a biased model). Human preferences are generally classified into two categories:</p>
<ol type="1">
<li><p>Revealed preferences are those one can observe retroactively from existing data. The implicit decision-making knowledge can be captured via learnable parameters and their usage in models which represent relationships between input decision attributes that may have little human interpretability, but enable powerful models of human preference. For health coaching, we may have information about which foods an individual has chosen previously in different contexts, allowing us to build a model from their decisions. Such data may be easier to acquire and can reflect real-world outcomes (since they are, at least theoretically, inherently based on human preferences). However, if we fail to capture sufficient context in such data, human preference models may not sufficiently capture human preferences.</p></li>
<li><p>Stated preferences are those individuals explicitly indicate in potentially experimental conditions. The explicit knowledge may be leveraged by including inductive biases during modeling (for example, the context used in a model) which are reasonable assumptions for how a human would consider a set of options.This may include controlled experiments or studies. This may be harder to obtain and somewhat biased, as they can be hypothetical or only accurately reflect a piece of the overall context of a decision. However, they enable greater control of the decision-making process.</p></li>
</ol>
<section id="human-rationality" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="human-rationality">Human Rationality</h4>
<p>Modeling decision-making must also take into account the rational and irrational behaviour of humans. Therefore we consider <em>rationality assumptions</em> as a fundamental aspect of understanding how individuals make decisions. These assumptions provide a framework for predicting and modeling human behavior by outlining the principles that guide decision-making processes <span class="citation" data-cites="keisler2003common">(<a href="#ref-keisler2003common" role="doc-biblioref">Keisler and Lee 2003</a>)</span>.</p>
<p>Perfect rationality posits that individuals always make decisions that maximize their utility. It assumes that individuals have complete information and the cognitive ability to process this information to make optimal choices <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. This assumption is often used in economic models to predict how rational agents would behave under ideal conditions. However, numerous studies have shown that this assumption frequently fails to describe actual human behavior, as individuals do not always act in ways that maximize their utility due to various constraints and biases <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. Bounded rationality, on the other hand, acknowledges that individuals operate within the limits of their information and cognitive capabilities. Decisions are made using heuristics or rules of thumb rather than through exhaustive analysis, reflecting the practical constraints of real-world decision-making <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. This concept, introduced by Herbert Simon, recognizes the limitations of human cognitive processing and the impact of these limitations on decision-making. Simon’s theory suggests that instead of optimizing, individuals satisfy, seeking solutions or decisions that are “good enough” under the circumstances <span class="citation" data-cites="simon1972theories">(<a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>. Noisy rationality assumes that decisions are influenced by random noise, resulting in probabilistic choice behavior. This means that while individuals aim to maximize their utility, random factors can lead to deviations from perfectly rational choices. This approach is useful for modeling behavior in situations where decisions are not entirely deterministic and are subject to variability <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>. This probabilistic approach aligns with findings from behavioral economics and psychology, which indicate that human decision-making is often inconsistent and influenced by various random factors <span class="citation" data-cites="miljkovic2005rational">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>)</span>.</p>
<p>Understanding rationality assumptions is crucial for modeling and predicting human behavior in various decision-making scenarios. These assumptions provide the foundation for developing models that can simulate and analyze how individuals interact with one another and their environment. By incorporating different types of rationality, researchers can create more accurate and realistic models that reflect the complexities of human decision-making. This comprehensive approach enhances the predictive power of models and improves the understanding of human behavior in economic and social contexts <span class="citation" data-cites="miljkovic2005rational simon1972theories">(<a href="#ref-miljkovic2005rational" role="doc-biblioref">Miljkovic 2005</a>; <a href="#ref-simon1972theories" role="doc-biblioref">Simon 1972</a>)</span>.</p>
<p>Luce’s axiom of choice <span class="citation" data-cites="Luce1977">(<a href="#ref-Luce1977" role="doc-biblioref">Luce 1977</a>)</span> and Boltzmann’s Rationality provide a probabilistic framework for modeling noisily-rational human behavior. Luce’s axiom of choice addresses the likelihood of a human selecting an option <span class="math inline">\(o\)</span> from a set <span class="math inline">\(O\)</span>. Desirability is represented by a value function <span class="math inline">\(v : O \rightarrow \mathbb{R}^+\)</span>, with the selection probability calculated as <span class="math inline">\(P(o) = \frac{v(o)}{\sum_{o' \in O} v(o')}\)</span>. Assuming there is an underlying reward for each option <span class="math inline">\(R(o) \in \mathbb{R}\)</span> such that <span class="math inline">\(v(o) = e^{R(o)}\)</span>, we get <span class="math inline">\(P(o) = \frac{e^{R(o)}}{\sum_{\bar{o} \in \mathcal{O}} e^{R(\bar{o})}}\)</span>. Essentially, “A human will act out a trajectory with a probability proportional to the exponentiated return they receive for the trajectory.” This probabilistic approach challenges the traditional assumption of perfect economic rationality, where individuals always make decisions that maximize their utility. When choices involve trajectories <span class="math inline">\(\xi \in \Xi\)</span> (sequences of actions), the Boltzmann model <span class="citation" data-cites="VonNeumannMorgenstern1945">(<a href="#ref-VonNeumannMorgenstern1945" role="doc-biblioref">Neumann and Morgenstern 1945</a>)</span> is used. Here, the reward <span class="math inline">\(R\)</span> is typically a function of a feature vector <span class="math inline">\(\phi : \Xi \rightarrow \mathbb{R}^k\)</span>, and the probability density is given by <span class="math inline">\(p(\xi) = \frac{e^{R(\phi(\xi))}}{\int_{\Xi} e^{R(\phi(\bar{\xi}))} d\bar{\xi}}\)</span>. Boltzmann Rationality serves a critical role in human preferences and decision-making. It captures the probabilistic nature of human choices, recognizing that decisions are often noisy and influenced by various factors. This model is instrumental in preference modeling, accommodating human preferences’ inherent variability and uncertainty.</p>
<p>However, the Luce choice axiom and Boltzmann Rationality encounter a known issue called the “duplicates problem,” where there is no concept of similar actions (e.g., choosing between using a car or a train for transportation, with no particular preference). The probability of making the decision is 50% for either option. However, if we now have 100 cars, under Luce/Boltzmann, we would have a 99% probability of choosing a car, which is unrealistic.</p>
<p>To address this issue, various extensions have been proposed. One such extension is the attribute rule, which interprets options as bundles of attributes. In this rule, attributes <span class="math inline">\(X\)</span> are associated with options, and they have desirability values <span class="math inline">\(w(x)\)</span>. An attribute intensity function <span class="math inline">\(s(x, o)\)</span> indicates the degree to which an attribute is expressed in an option. The probability of choosing option <span class="math inline">\(o\)</span> is calculated as:</p>
<p><span class="math display">\[P(o) = \sum_{x \in \mathcal{X}_o} \frac{w(x)}{\sum_{\bar{x} \in \mathcal{X}_o} w(\bar{x})} \cdot \frac{s(x, o)}{\sum_{\tilde{o} \in \mathcal{O}} s(x, \bar{o})}\]</span></p>
<p>This equation describes a two-step process where an attribute <span class="math inline">\(x \in X_O\)</span> is first chosen according to a Luce-like rule and then an option <span class="math inline">\(o \in O\)</span> with that attribute is selected using another Luce-like rule. This approach handles duplicates gracefully by effectively creating a two-layer hierarchy in choosing an option.</p>
<p>Boltzmann Rationality finds practical applications in various fields, particularly in reinforcement learning, where it models decision-making in uncertain environments. It also applies to trajectory selection, where the probability of a sequence of actions (trajectory) is proportional to the exponential return. These applications enhance the accuracy of models that interact with or predict human behavior, making Boltzmann Rationality a vital component of the models of interaction.</p>
<p>We next explore a case study to deepen our understanding of rationality: Limiting Errors due to Similar Selection (LESS) <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. LESS takes inspiration from the attribute rule and extends it to continuous trajectories <span class="citation" data-cites="2001.04465">(<a href="#ref-2001.04465" role="doc-biblioref">Bobu et al. 2020</a>)</span>. The key insight is that instead of creating “attributes”, which group together similar discrete options, it introduces a similarity metric on the space of continuous actions, thereby creating similar groupings on trajectories.</p>
<p>First, discussing the distinction between trajectory and feature space is important. The LESS similarity metric could be defined in trajectory space, where the trajectory is some theoretical notion of all states and actions one passes through over time. However, it is instead defined on the measured feature vector <span class="math inline">\(\phi(\xi)\)</span> associated with the agent’s trajectory <span class="math inline">\(\xi\)</span>. Why? In practice, one can never measure the exact trajectory with perfect fidelity. The feature vector will almost necessarily map in a one-to-many fashion with trajectories. Formally, let <span class="math inline">\(\phi \in \Phi\)</span> be the set of all possible feature vectors <span class="math inline">\(\xi \in \Xi\)</span> the set of all trajectories. The set of feature vectors belonging to a set of trajectories <span class="math inline">\(\Xi' \subseteq \Xi\)</span> is <span class="math inline">\(\Phi_{\Xi'}\)</span>. We begin with equation (4) and substitute our similarity metric on feature vectors of trajectories.</p>
<p><span class="math display">\[\begin{aligned}
    P(\xi) = \frac{e^{R(\phi(\xi))}}{\sum_{\bar{\phi} \in \Phi_{\Xi}} e^{R(\hat{\phi})}} \cdot \frac{s(\phi(\xi), \bar{\xi})}{\sum_{\hat{\xi} \in \Xi} s(\phi(\xi), \bar{\xi})}
\end{aligned}\]</span></p>
<p>In this formulation, the first half of the product is simply Boltzmann equation. The probability of choosing trajectory <span class="math inline">\(\xi\)</span> is proportional to the exponentiated reward for the agent’s measured trajectory <span class="math inline">\(\phi(\xi)\)</span>, normalized by the sum of all rewards over all possible measured trajectories. The second half of the product is a normalization factor based on how similar the current trajectory is to other trajectories in feature space. We can define the similarity function as an indicator function, where <span class="math inline">\(s(x, \xi) = 1\)</span> only if <span class="math inline">\(x = \phi(\xi)\)</span>. That means that multiple trajectories with the same feature vector will effectively be considered a single option. Thus, we achieve the “bundling” of trajectories, in the same way that the attribute rule bundled options under different attributes.</p>
<p>However, setting the similarity metric as an indicator function isn’t sufficiently flexible. We want a proper metric that acts more as a continuous distance over the feature space. We instead define <span class="math inline">\(s\)</span> to be a <em>soft similarity metric</em> <span class="math inline">\(s : \Phi \times \Xi \rightarrow \mathbb{R}^+\)</span>. It has the following properties:</p>
<ol type="1">
<li><p><span class="math inline">\(s(\phi(\xi), \xi) = \max_{x \in \phi, \bar{\xi} \in \Xi} s(x, \hat{\xi})) \forall (\xi \in \Xi)\)</span></p></li>
<li><p>Symmetric: <span class="math inline">\(s(\phi(\xi), \bar{\xi}) = s(\phi(\bar{\xi}), \xi)\)</span></p></li>
<li><p>Positive Semidefinite: <span class="math inline">\(s(x, \xi) \geq 0\)</span></p></li>
</ol>
<p>Using this redefined similarity metric <span class="math inline">\(s\)</span>, we extend (5) to be a probability density on the continuous trajectory space <span class="math inline">\(\mathcal{E}\)</span>, as in (3).</p>
<p><span class="math display">\[p(\hat{\xi}) = \frac{\frac{e^{R(\phi(\xi))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}}{\int_{\Xi}\frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\hat{\xi}), \bar{\xi}) d\bar{\xi}}d\hat{\xi}} \propto \frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}\]</span></p>
<p>Under this formulation, the likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories. This de-weights similar trajectories, which is the desired effect for our LESS model of human decision-making. This means, though, that the “trajectory bundle” of similar trajectories still has a reasonable probability of being chosen.</p>
</section>
</section>
<section id="axiom-3-preference-centers-around-utility" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="axiom-3-preference-centers-around-utility">Axiom 3: Preference centers around utility</h3>
<p>Human preference models are centered around the notion of utility, which can mean a reward one attains after expressing one’s preference over options.[^1] In health coaching, the utility, as a function of the health choices users make, may be satiety, latent promotion of overall health, or even a quantitative extension of life. Of course, humans don’t necessarily use an explicit measure of utility — frequently humans use qualitative factors such as emotion or external influence to make decision. However, we assume that the underlying utility mechanism of a human preference model still captures the final decision output from a human.</p>
<p>Utility can be interpreted as a scalar quantity representing the benefit or value an individual attains from selecting a given choice. Each choice has an associated utility. Human preference models capture both the utility of a choice (e.g.&nbsp;we model the utility value as a function of attributes of a given choice) and how the utilities interact to make a decision.[^2] We use the notation <span class="math inline">\(U_i\)</span> as the utility corresponding to choice <span class="math inline">\(i\)</span>.</p>
<ol type="1">
<li><p><strong>The utility of a choice is a stochastic function of the choice’s attributes.</strong> We will henceforth define utility as follows <span class="math inline">\(U_i = H_i(z_i)\)</span> where <span class="math inline">\(z_i\)</span> is a variable describing the attributes of choice <span class="math inline">\(i\)</span> and <span class="math inline">\(H_i\)</span> is the stochastic function defining this choice’s utility. As a simple example, we can use a 1-D linear stochastic function to define <span class="math inline">\(H_i\)</span>: <span class="math inline">\(U_i = H_i(z_i) = \beta z_i + \epsilon_i\)</span>, where <span class="math inline">\(\beta\)</span> is a parameter of the model and <span class="math inline">\(\epsilon_i\)</span> is an unobserved factor for choice <span class="math inline">\(i\)</span>. Generally, we assume that the <span class="math inline">\(\epsilon_i\)</span> factor is a random variable following a specified distribution, such as a standard normal distribution. The attributes we use to represent a choice (a single scalar value <span class="math inline">\(z_i\)</span> in this example) is a critical design decision in defining the human preference model. These attributes define the context our model has in representing the human behavior we wish to capture, when choice <span class="math inline">\(i\)</span> is made. In our health coaching example, we may hope to provide the best possible diet recommendations for an individual. However, if our vector representation <span class="math inline">\(z_i\)</span> of their choice <span class="math inline">\(i\)</span> does not include vital information, such as allergy risks associated to the choice or ingredients which make up the choice, our model may not have enough information to properly capture the human preference.</p></li>
<li><p><strong>The preferred choice is that whose corresponding utility is the largest.</strong> Given that we model utility as the underlying benefit or value a human derives from choosing a given option, intuitively, we expect a human to choose the option with the largest utility. In our example of health coaching, if we model utility as the expected increase in lifespan, we will surely opt for the choice that maximizes this notion of utility. In our example, since <span class="math inline">\(U_1 &gt; U_2\)</span>, our model indicates that a user would opt for the burrito.</p></li>
<li><p><strong>Relativity of Utility.</strong> Given the two previously defined characteristics of utility, we observe that only the relative difference in utility matters. Even if <span class="math inline">\(U_1 = 0.001\)</span> and <span class="math inline">\(U_2 = 0.0005\)</span>, the model indicates the same outcome: a user prefers option <span class="math inline">\(1\)</span>. As such, even the scale of the utilities is irrelevant within a given set of human preference data for a given individual. In our example, we can scale the value of <span class="math inline">\(\beta\)</span> without changing the overall outcome so long as we do not change the sign. The scale of utilities <em>is</em> important when comparing human preferences across datasets, or comparing the same model across different humans; since utility may be defined differently in various datasets, perhaps their exact values are not aligned in a manner which allows one to robustly compare preferences between them. A common practice to address this consideration is to standardize the utilities in each dataset based on its variance in the observed data. Furthermore, a human preference model may generate different scales of utilities across different humans (based on the inputs and representation of the human). In this case, one can standardize the utilities for each individual based on the observed variance for that human. As we can see, the relativity of utility can be both powerful (enabling us to create flexible models and efficiently optimize them) and limiting (requiring us to perform mitigations when translating models across datasets or individuals. Still, we find the notion of utility necessary to model human preferences as it provides a quantitative value we can use to model human decisions.</p></li>
</ol>
<p>As a concrete model of meal recommendation in health coaching, let us suppose that we have three choices:</p>
<ol type="1">
<li><p>A burrito with rice, beans, and cheese.</p></li>
<li><p>French fries covered in mayonnaise.</p></li>
<li><p>A rice bowl with beans and chicken.</p></li>
</ol>
<p>If we design <span class="math inline">\(z_i\)</span> to be 1D, for example:</p>
<ol type="1">
<li><p><span class="math inline">\(z_1 = 1\)</span> for the burrito since this is a somewhat balanced meal that may help prolong the lifespan, which a user prefers.</p></li>
<li><p><span class="math inline">\(z_2 = -1\)</span> since this is unhealthy due to being deep fried, including saturated fats, and potentially reducing lifespan.</p></li>
<li><p><span class="math inline">\(z_3 = 1\)</span> since a rice bowl is another healthy meal.</p></li>
</ol>
<p>After observing the choices of a user who likes to eat healthily, we might learn that <span class="math inline">\(\beta = 1\)</span> is the best parameter for this model, and maybe we assume that <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>. Then, this model implies that <span class="math inline">\(U_1 = 1 \cdot 1 + 0.03 = 1.03\)</span>, <span class="math inline">\(U_2 = 1 \cdot -1 + (-0.07) = -1.07\)</span>, <span class="math inline">\(U_3 = 1 \cdot 1 + (0.02) = 1.02\)</span>, which means that the user, for whom <span class="math inline">\(\beta = 1\)</span> is the learned parameter, they would prefer the first meal, with the third meal as a close second option.</p>
<p>If we design <span class="math inline">\(z_i\)</span> to be 3D, to indicate the carbohydrate, protein, and fat content of each meal, then for example:</p>
<ol type="1">
<li><p><span class="math inline">\(z_1 = (1, 1, 0.1)\)</span> for the burrito</p></li>
<li><p><span class="math inline">\(z_2 = (1, 0, 1)\)</span> for the fries</p></li>
<li><p><span class="math inline">\(z_3 = (1, 1, 0.2)\)</span> for the rice bowl.</p></li>
</ol>
<p>After observing the choices of a user who likes to eat healthy, we might learn that <span class="math inline">\(\beta = (1, 1, -1)\)</span> is the best parameter for this model, and maybe we assume that <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>. Then, this model implies that <span class="math inline">\(U_1 = (1, 1, 0.1) \cdot (1, 1, -1) + 0.01 = 1.91\)</span>, <span class="math inline">\(U_2 = (1, 0, 1) \cdot (1, 1, -1) + 0.03 = 0.03\)</span>, <span class="math inline">\(U_3 = (1, 1, 0.2) \cdot (1, 1, -1) - 0.07 = 1.73\)</span>, which means that the user prefers meals 1 and 3, which again have the best utility, but in this multi-dimensional representation of <span class="math inline">\(z_i\)</span>, we start understanding how the two preferred meals are related (low fat and high protein).</p>
</section>
</section>
<section id="sec-models" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-models"><span class="header-section-number">2.3</span> Models of Individual Choices</h2>
<p>After exploring motivations for preference learning and the framework we use to characterize human preferences to enable modeling, we now expand on the common probabilistic methods used to model human preference tasks. We will instantiate these models for our real-world health coaching application throughout as a pedagogical example. Specifically, we can define the following domain for meal choices: <span class="math inline">\(z_i, \beta \in \mathbb{Z}^3\)</span>, where <span class="math inline">\(z_i\)</span> defines the representation of a meal option with the three dimensions representing the carbohydrate, protein, and lipid macronutrient content of the meal, respectively, all measured in grams. <span class="math inline">\(\beta\)</span> is a parameter of the model. This simple representation will allow us to consider how different probabilistic frameworks for human preferences can model a user’s meal preferences. The information representation we instantiate here can accommodate scalar and high-dimensional vectors. While we use a mixture of integer and real-valued vectors in this simple example, we refer the reader to code in the practicum section for an example where vectors are all real-valued. If we let <span class="math inline">\(z = [20, 15, 3]\)</span> and <span class="math inline">\(\beta = [0.2, 1, -3]\)</span>. This corresponds to a meal with 20g carbohydrates, 15g protein, and 3g lipids. In the following sections, we discover how to learn the parameter <span class="math inline">\(\beta\)</span> and how to predict <span class="math inline">\(y\)</span> for this meal, which indicates whether the user chooses it or refuses it.</p>
<section id="data-collection" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="data-collection"><span class="header-section-number">2.3.1</span> Data Collection</h3>
<section id="pairwise-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="pairwise-sampling">Pairwise Sampling</h5>
<p>In pairwise sampling, participants compare two options simultaneously to determine which is preferred. The goal is to understand relative preferences between pairs of items. This method is frequently used in preference and choice studies to gather detailed preference data. Two key models used in pairwise sampling are the Thurstonian and Bradley-Terry models <span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>. The Thurstonian model assumes each item <span class="math inline">\(i\)</span> has a true score <span class="math inline">\(u_i\)</span> following a normal distribution. The difference <span class="math inline">\(d_{ij} = u_i - u_j\)</span> is also normally distributed. The probability that item <span class="math inline">\(i\)</span> is preferred over item <span class="math inline">\(j\)</span> is given by <span class="math inline">\(P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)\)</span>, where <span class="math inline">\(\Phi\)</span> is the cumulative normal distribution function. The denominator <span class="math inline">\(\sqrt{2\sigma^2}\)</span> is the standard deviation of the difference <span class="math inline">\(d_{ij} = u_i - u_j\)</span> when <span class="math inline">\(u_i\)</span> and <span class="math inline">\(u_j\)</span> are normally distributed with variance <span class="math inline">\(\sigma^2\)</span><span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>. The Bradley-Terry model defines the probability of preference based on latent scores <span class="math inline">\(\beta_i\)</span> and <span class="math inline">\(\beta_j\)</span>. The probability that item <span class="math inline">\(i\)</span> is preferred over item <span class="math inline">\(j\)</span> is <span class="math inline">\(P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}\)</span>. This model is used to estimate relative strengths or preferences based on latent scores. <span class="citation" data-cites="cattelan2012">(<a href="#ref-cattelan2012" role="doc-biblioref">Cattelan 2012</a>)</span>.</p>
</section>
<section id="rank-order-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="rank-order-sampling">Rank-Order Sampling</h5>
<p>Rank-order sampling methods enable analysis of human preferences by asking participants to rank a set of items from most to least preferred. This approach is widely used in voting systems, market research, and psychological studies to understand the overall preference ordering among a set of items. Rank-order sampling offers comprehensive preference data, capturing detailed information about the relative ranking of multiple items. This richness makes them suitable for various applications, including market research, voting systems, sports competitions, and recommender systems. However, these models can be more complex and time-consuming for participants compared to pairwise comparisons, and they impose a higher cognitive load, especially with large sets of items. Additionally, participants may show inconsistencies when ranking many items <span class="citation" data-cites="ragain2019">(<a href="#ref-ragain2019" role="doc-biblioref">Ragain and Ugander 2019</a>)</span>.</p>
</section>
<section id="rating-scale-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="rating-scale-sampling">Rating-Scale Sampling</h5>
<p>Rating-scale sampling is a method in which participants rate items on a numerical scale to measure the intensity of preference or attitude towards items. These models are commonly used in surveys, product reviews, and psychological assessments to gather detailed information on how participants feel about various subjects. The Likert scale is a widely used rating-scale model. In this approach, participants rate items on a fixed-point scale, typically ranging from 1 to 5 or 1 to 7, to measure levels of agreement or satisfaction. For instance, a Likert scale might ask participants to rate their agreement with statements such as “Strongly Disagree” to “Strongly Agree” <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. This method is prevalent in survey research, customer satisfaction studies, and attitude measurement. Another key model is the continuous rating scale, where participants mark a point on a continuous line to indicate their preference or attitude. This provides a more nuanced measure compared to discrete scales. For example, participants might indicate their satisfaction on a line ranging from “Very Unsatisfied” to “Very Satisfied” <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. This model is used in detailed feedback mechanisms, user experience studies, and fine-grained preference measurements.</p>
<p>Rating-scale sampling offers several advantages. They are simple for participants to understand and use, provide rich data on the intensity of preferences, and are flexible enough for various types of measurements (e.g., agreement, satisfaction). Moreover, the data collected can be easily analyzed using standard statistical methods <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>.</p>
<p>Applications include data collection on opinions, attitudes, and behaviors; in product reviews to measure customer satisfaction and product quality; in psychological assessments to evaluate mental states, personality traits, and attitudes; and in user experience studies to understand user satisfaction and usability of products <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>. However, rating-scale sampling methods also have limitations. Ratings can be influenced by personal biases and interpretations of scales, leading to subjectivity. There is a central tendency bias, where participants may avoid extreme ratings, resulting in a clustering of responses around the middle. Different participants might interpret scale points differently, and fixed-point scales may not capture the full nuance of participants’ preferences or attitudes <span class="citation" data-cites="harpe2015">(<a href="#ref-harpe2015" role="doc-biblioref">Harpe 2015</a>)</span>.</p>
</section>
<section id="best-worst-scaling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="best-worst-scaling">Best-Worst Scaling</h5>
<p>Best-Worst Scaling (BWS) is a powerful method for understanding preferences and the relative importance of different items. In BWS, participants are presented with a set of items and are asked to identify the most and least preferred options. This method helps to gather detailed preference data, providing more nuanced insights than traditional ranking or rating systems. The primary objective of BWS is to discern the relative importance or preference of items within a set, making it widely applicable in various fields such as market research, health economics, and social sciences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>A key method within BWS is MaxDiff Analysis, which involves presenting participants with sets of items and asking them to select the best and worst options. This approach yields richer data by identifying extremes in preferences, offering a clearer picture of the relative importance of each item. For instance, in a product development context, MaxDiff Analysis can help identify the most and least important features according to consumer preferences <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>The advantages of Best-Worst Scaling are significant. It provides rich data on the relative importance of items, helps clarify preferences, reduces biases found in traditional rating scales, and results in utility scores that are easy to interpret. BWS is particularly useful in market research for understanding consumer preferences, in health economics for evaluating patient treatment preferences, in social sciences for studying the importance of social issues, and in product development for identifying key features driving consumer choices <span class="citation" data-cites="campbell2015">(<a href="#ref-campbell2015" role="doc-biblioref">Campbell and Erdem 2015</a>)</span>.</p>
<p>However, BWS also has limitations, including increased complexity and cognitive load for participants compared to simpler rating scales, potential scale interpretation differences among participants, and design challenges to avoid biases. Additionally, differences in how participants interpret the scale can introduce variability, and the design of BWS studies requires careful consideration to avoid biases, such as the order effect or the context in which items are presented.</p>
</section>
<section id="multiple-choice-sampling" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="multiple-choice-sampling">Multiple-Choice Sampling</h5>
<p>Multiple-choice sampling models are widely used in various fields such as voting systems, surveys, and market research to understand the preferred choice among a set of alternatives. These models involve participants selecting one option from a set of alternatives, providing insights into the most favored options.</p>
<p>Multiple-choice sampling methods offer several advantages. They are simple for participants to understand and reflect on realistic decision-making scenarios where individuals choose one option from many. These models are versatile and can be applied in various applications, from voting to market research, providing clear preferences directly from the participants’ choices. It is particularly useful in complex choice scenarios such as mode of transportation, where choices are not independent <span class="citation" data-cites="bolt2009">(<a href="#ref-bolt2009" role="doc-biblioref">Bolt and Wollack 2009</a>)</span>.</p>
<p>However, multiple-choice sampling also has limitations. It often relies on simplistic assumptions such as the independence of irrelevant alternatives (IIA), which may not always hold true. Additionally, these models can place a cognitive load on participants, especially if the number of choices is large, leading to decision fatigue. This method may also fail to capture the variation in preferences among different individuals, as it typically records only the most preferred choice without accounting for the relative importance of other options.</p>
</section>
</section>
<section id="data-interpretation" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="data-interpretation"><span class="header-section-number">2.3.2</span> Data Interpretation</h3>
<section id="binary-choice-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="binary-choice-model">Binary Choice Model</h5>
<p>is centered around one specific user option. The model predicts, for that option, after observing user choices in the past, whether that option will be chosen or not. Specifically, if we are looking at a certain choice, we use binary variable <span class="math inline">\(y \in \{0, 1\}\)</span> to represent whether that choice will be picked or not by the user in the next phase of selection. Since <span class="math inline">\(\mathbb{P}(y = 0) = 1 - \mathbb{P}(y = 1)\)</span>, we only need to model <span class="math inline">\(\mathbb{P}(y = 1)\)</span> which we will denote as <span class="math inline">\(P\)</span>.</p>
<p>We can use a linear model represented by the parameter <span class="math inline">\(\beta\)</span> we have already defined. Since utility is a stochastic function of the choice attributes, we will represent our utility as <span class="math inline">\(U = \beta^\top z + \epsilon\)</span>. We can formally model <span class="math inline">\(y\)</span> as a function of the utility of the positive choice: <span class="math inline">\(y = \mathbb{I}[U&gt;0]\)</span>.</p>
<p>We explore two cases based on the choice of distribution for the unobserved random variable <span class="math inline">\(\epsilon\)</span>. If <span class="math inline">\(\epsilon \sim \text{Logistic}\)</span>, then <span class="math inline">\(\mathbb{P}(\epsilon &lt; a) = \frac{1}{1 + \exp^{-a}}\)</span>. The probability <span class="math inline">\(P\)</span> can be modeled as: <span class="math display">\[\begin{aligned}
    P &amp; = \mathbb{P}(U &gt; 0) = \mathbb{P}(\beta^\top z + \epsilon &gt; 0) = \mathbb{P}( \epsilon &gt; -\beta^\top z) = 1 - \mathbb{P}( \epsilon &lt; -\beta^\top z) = 1 - \frac{1}{1 + \exp^{\beta^\top z}} \\
    &amp; = \frac{1 + \exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} - \frac{1}{1 + \exp^{\beta^\top z}} = \frac{\exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} = \frac{1}{1 + \exp^{-\beta^\top z}}
\end{aligned}\]</span></p>
<p>In the health coaching example, using this logistic model, we can compute the probability that an individual would choose this meal over no meal: <span class="math inline">\(P = \frac{1}{1 + \exp^{-(4 + 15 - 9)}} = 0.99995\)</span>. Therefore, the model predicts a high probability that the user would choose the meal over the no-meal option.</p>
<p>On the other hand, if <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>, then <span class="math inline">\(\mathbb{P}(\epsilon &lt; a) = \Phi(a)\)</span>, where <span class="math inline">\(\Phi(a)\)</span> is the cumulative distribution function of the standard normal distribution. The probability <span class="math inline">\(P\)</span> is modeled as:</p>
<p><span class="math display">\[P = \mathbb{P}(U &gt; 0) = \mathbb{P}(\beta^\top z + \epsilon &gt; 0) = \mathbb{P}( \epsilon &gt; -\beta^\top z) = \mathbb{P}( \epsilon &lt; \beta^\top z) = \Phi(\beta^\top z)\]</span></p>
<p>In the same health coaching example, we can compute the probability that an individual would choose this meal over no meal: <span class="math inline">\(\Phi(4 + 15 - 9) = 1\)</span>. This model also predicts that the user will most likely take the meal!</p>
</section>
<section id="bradley-terry-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="bradley-terry-model">Bradley-Terry Model</h5>
<p>The Bradley-Terry (BT) model introduces a framework to model the utility of choice <em>over all others</em> (a multipronged prediction of overall choices, not just a binary prediction over one choice), given their attribute vectors <span class="citation" data-cites="bradley-terry-model">(<a href="#ref-bradley-terry-model" role="doc-biblioref">Bradley and Terry 1952b</a>)</span>. Given information about all available operations, this is a general yet powerful method for modeling human preferences. The core idea in this model is to compare utilities of all items at once to model the probability of a user’s actions and, therefore, their preferences. In the BT model, we have a discrete set of <span class="math inline">\(J\)</span> choices <span class="math inline">\(i \in \{1, 2, \dots, J\}\)</span>, each with an attribute representation <span class="math inline">\(z_i \in \mathbb{Z}^n\)</span> (where <span class="math inline">\(n\)</span> is the dimensionality of the representation). Each choice can also have its unique random noise variable representing the unobserved factor, although we can also choose to have all choices’ unobserved factors follow the same distribution (e.g.&nbsp;independent and identically distributed, or iid).</p>
<p>We keep the assumption from previous sections that the utility <span class="math inline">\(U_i\)</span> of choice <span class="math inline">\(i\)</span> is also a linear stochastic function where the noise is sampled from the specified distribution: <span class="math inline">\(U_i = \beta^\top z_i + \epsilon_i\)</span>. The noise is represented as an extreme value distribution, although we can choose alternatives such as a multivariate Gaussian distribution: <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \Sigma)\)</span>. If <span class="math inline">\(\Sigma\)</span> is not a diagonal matrix, we effectively model correlations in the noise across choices, enabling us to avoid the iid assumption if necessary. In the case of the extreme value distribution, we model the probability of a user preferring choice <span class="math inline">\(i\)</span>, which we denote as <span class="math inline">\(P_i\)</span> as <span class="math inline">\(P_i = \exp(\beta^\top z_i)/Z\)</span> where <span class="math inline">\(Z = \sum_{j = 1}^{J} \exp(\beta^\top z_j)\)</span>.</p>
<p>We revisit the health coaching example. Denote two choices, where <span class="math inline">\(z_1 = [20, 15, 3]\)</span> is the choice from the previous example. Still, we now have a second choice <span class="math inline">\(z_2 = [60, 20, 7]\)</span> (which seems to be a very carbohydrate-heavy meal and potentially a larger meal overall). We will also assume we choose an extreme value distribution to model the unobserved factors, which are sampled i.i.d. Then, we have <span class="math inline">\(\beta^\top z_1 = 10\)</span> and <span class="math inline">\(\beta^\top z_2 = 11 \Rightarrow P_1 = \frac{1}{1 + \exp(1)} = 0.2689\)</span>. Since there are only two choices, the probabilities <span class="math inline">\(P_1\)</span> and <span class="math inline">\(P_2\)</span> must sum to <span class="math inline">\(1\)</span>. Therefore, we can calculate <span class="math inline">\(P_2\)</span> as <span class="math inline">\(P_2 = 1 - P_1 = 1 - 0.2689 \approx 0.7311\)</span>. Our model predicts that choice 2 is more favorable between these two options.</p>
</section>
<section id="ordered-preferences-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="ordered-preferences-model">Ordered Preferences Model</h5>
<p>In all previous examples, we have assumed that we have no information on any explicit ordering of the available options a human can choose from: all choices were treated as independent by the model. The model aims to capture how an individual chooses between them. However, in many cases, we may introduce an inductive bias based on information about the options. For example, in a study for stated preferences, a user may be able to choose from intricately dependent options such as very poor, poor, fair, good, and great. In this case, it can be useful to include this bias in our model to represent a human’s decision-making process better. For such cases, instead of comparing choices against alternatives, we can focus on a single example and use additional parameters to define classification criteria based on the utility determined by the model. Formally, let us suppose we have a single example with attributes <span class="math inline">\(z_i\)</span>, and wish to know which of <span class="math inline">\(J\)</span> predefined options an individual will choose from. We can define <span class="math inline">\(J - 1\)</span> parameters, which act as thresholds on the utility computed by <span class="math inline">\(U_i = H(z_i)\)</span> to classify the predicted choice between these options. For example, if there are 3 predefined options, we can define parameters <span class="math inline">\(a, b \in \mathbb{R}\)</span> such that <span class="math display">\[y_i = \begin{cases}
      1 &amp; U &lt; a \\
      2 &amp; a \le U &lt; b \\
      3 &amp; \text{else}
   \end{cases}\]</span></p>
<p><strong>1. Logistic Distribution</strong></p>
<p>From a probabilistic perspective, we can use our cumulative distributions as before to model the probability that a person will choose a given option. Continuing with our linear utility function <span class="math inline">\(U_i = \beta^\top z_i + \epsilon_i\)</span>, we can start with the setting that we assume unobserved factors follow a logistic distribution and focus on the first case: <span class="math display">\[\mathbb{P}(y_i = 1) = \mathbb{P}(U &lt; a) = \mathbb{P}(\beta^\top z + \epsilon &lt; a )  = \mathbb{P}( \epsilon &lt; a - \beta^\top z)  = \frac{1}{1 + \exp(\beta^\top z - a)}\]</span></p>
<p>Extending this method to the second case, where we estimate the probability of the utility falling within a specific interval: <span class="math display">\[\begin{aligned}
    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le U &lt; b) = \mathbb{P}(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \mathbb{P}( \epsilon &lt; a - \beta^\top z) ) \\
    &amp; = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \frac{1}{1 + \exp(\beta^\top z - a)}  ) = \frac{1}{1 + \exp(\beta^\top z - b)}  - \frac{1}{1 + \exp(a - \beta^\top z)}  ) \\
\end{aligned}\]</span></p>
<p>The final case follows the form of the inverse of the first case:</p>
<p><span class="math display">\[\mathbb{P}(y_i = 3) = \mathbb{P}(U &gt; b) = \mathbb{P}(\beta^\top z + \epsilon &gt; b ) = \mathbb{P}( \epsilon &gt; b - \beta^\top z) = 1 - \mathbb{P}( \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}\]</span></p>
<p><strong>2. Normal Distribution</strong></p>
<p>In the case of modeling unobserved factors with a standard normal distribution, we have: <span class="math display">\[\begin{split}
    \mathbb{P}(y_i = 1) &amp; = \mathbb{P}(U &lt; a) = \mathbb{P}(\beta^\top z + \epsilon &lt; a ) = \mathbb{P}( \epsilon &lt; a - \beta^\top z) = \Phi(a - \beta^\top z) \\
    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le U &lt; b)
    = \mathbb{P}(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \Phi(b - \beta^\top z) - \Phi(a - \beta^\top z) \\
    \mathbb{P}(y_i = 3) &amp; = \mathbb{P}(U &gt; b)
    = 1 - \Phi(b - \beta^\top z)
\end{split}\]</span></p>
<p>In our health coaching example, the derivation above yields three exact expressions for computing the probability of choosing each of our meals. Each computation involves the normal cumulative distribution function as seen for the binary choice model with standard normal for <span class="math inline">\(\epsilon\)</span> after parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are learned <a href="#sec-learning" class="quarto-xref"><span>Section 2.4</span></a>.</p>
</section>
<section id="plackett-luce-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="plackett-luce-model">Plackett-Luce Model</h5>
<p>In other cases, we may need an even more general framework combining elements of the BT model and ordered preferences. Specifically, we can model an open-ended ranking of the available options in a similar probabilistic framework. To do so, we can leverage the Plackett-Luce (PL) Model, in which we jointly model the full sequence of choice ordering. <span class="citation" data-cites="plackett_luce">(<a href="#ref-plackett_luce" role="doc-biblioref">Plackett 1975</a>)</span></p>
<p>The general form models the joint distribution as the product of conditional probabilities, where each is conditioned on the preceding ranking terms. Given an ordering of <span class="math inline">\(J\)</span> choices <span class="math inline">\(\{Y_1, Y_2, \dots, Y_J\}\)</span> where <span class="math inline">\(Y_1\)</span> is the first selection, <span class="math inline">\(Y_2\)</span> is the second, and so on, we decompose the joint probability into its respective conditionals. To compute the conditional probabilities, we can use the same method as the BT model, using a softmax to produce valid conditional distributions for each element of the sequence: <span class="math display">\[\mathbb{P}(Y_1, Y_2, \dots, Y_J) = \mathbb{P}(Y_1) \cdot \mathbb{P}(Y_2 | Y_1) \cdot \dots \cdot \mathbb{P}(Y_J | Y_1, Y_2, \dots Y_{J - 1}) = \prod_{i = 1}^J \frac{\exp(\beta^\top z_i)}{\sum_{j \ge i} \exp(\beta^\top z_j)}\]</span></p>
<p>An interesting property of the PL Model is that in the naive case of only ordering a single choice, it is equivalent to the pairwise preference formulation of the BT model.</p>
<p><strong>Exercise (Health coaching example)</strong>: In our application, if we have <span class="math inline">\(3\)</span> choices (burrito (B), fries (F), rice bowl (R)), we can let <span class="math inline">\(Y_1, Y_2, Y_3\)</span> be variables to which we assign meals in a one-to-one manner to establish a ranking.</p>
<ol type="1">
<li><p>One of the possible ranking assignments is <span class="math inline">\(Y_1=B, Y_2=F, Y_3=R\)</span>. How many assignments are there in all, and what are they explicitly?</p></li>
<li><p>What would one expect the sign to be, out of <span class="math inline">\(\{\leq, \geq, =\}\)</span> in the following expression? (Hint: healthier meals should be placed earlier in the ranking.) <span class="math display">\[\mathbb{P}(Y_1=F, Y_2=R, Y_3=B) \ \ \_\_\ \ \mathbb{P}(Y_1=R, Y_2=B, Y_3=F)\]</span></p></li>
</ol>
</section>
<section id="ideal-point-model" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="ideal-point-model">Ideal Point Model</h5>
<p>An observation one can make is that we have strictly used linear functions to represent the utility. However, in the case of vector representations of choice attributes and the individual, one can exploit vector geometry to compute this utility value. The Ideal Point Model does this by using distance functions to compute utility for individual-choice pairs <span class="citation" data-cites="huber1976ideal">(<a href="#ref-huber1976ideal" role="doc-biblioref">Huber 1976</a>)</span>. Formally, with our vector representation <span class="math inline">\(z_i\)</span> of choice <span class="math inline">\(i\)</span> and a vector <span class="math inline">\(\textbf{v}_n\)</span> representing an individual <span class="math inline">\(n\)</span>, we can use a distance function to model a stochastic utility function, keeping the notion of unobserved factors following a specified distribution: <span class="math inline">\(U_{n, i} = \texttt{dist}(z_i, \textbf{v}_n) + \epsilon_{n, i}\)</span>. We continue with our framework of a human’s preference following the choice corresponding to the maximum utility: <span class="math inline">\(y_{n, i} = \mathbb{I}[U_{n, i} &gt; U_{n, j}\ \forall i \ne j]\)</span>. The intuition supporting this type of model is that vectors exist in a shared <span class="math inline">\(n\)</span>-dimensional space, and as such we can use geometry to match choices whose representations are closest to that of a given individual.</p>
<p>An observation with this model type is that it can often result in faster learning compared to non-geometric approaches <span class="citation" data-cites="ideal_point tatli2022distancepreferences">(<a href="#ref-ideal_point" role="doc-biblioref">Jamieson and Nowak 2011</a>; <a href="#ref-tatli2022distancepreferences" role="doc-biblioref">Tatli, Nowak, and Vinayak 2022</a>)</span>. However, it carries the added burden of having to specify a distance metric. Certain distance metrics, such as Euclidian distance or inner product, can easily be biased by the scale of vectors. A distance measure such as cosine similarity, which compensates for scale by normalizing the inner product of two vectors by the product of their magnitudes, can mitigate this bias yet may discard valuable information encoded by the length of the vectors. Beyond the distance metric alone, this model places a strong inductive bias that the individual and choice representations all share a common embedding space. In some contexts, this can be a robust bias to add to the model <span class="citation" data-cites="idealpoints">(<a href="#ref-idealpoints" role="doc-biblioref">Greiner 2005</a>)</span>, but it is a key factor one must take into account before employing such a model, and is a key design choice for modeling.</p>
<p><strong>Health coaching example</strong>: vector representations may indeed be useful as an individual’s representation can capture the macronutrient proportions and volumes they wish to consume, enabling a distance metric such as inner product to be a powerful tool. This model also starts capturing user properties (e.g.&nbsp;a user may be more into working out, another into lowering anxiety and another into gaining weight) and implicitly the commonalities between user characteristics start being captured, akin to a recommendation system <span class="citation" data-cites="recommender_systems">(<a href="#ref-recommender_systems" role="doc-biblioref">Roy and Dutta 2022</a>)</span>. However, in other domains and formulations, where perhaps user profiles are not as explicit, this may certainly hinder performance and make learning human preferences difficult.</p>
</section>
</section>
</section>
<section id="sec-learning" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-learning"><span class="header-section-number">2.4</span> Parameter Learning</h2>
<p>With an understanding of the various techniques we can use to model human preferences, we can now create robust models which utilize context attributes about the options an individual has in front of them and model their choices. However, these models on their own are powerless; their parameters are initialized randomly and we must fit the models to the actual human choice data!</p>
<p>Each of the models we have studied contain distinct parameters which aim to capture human preferences; for example <span class="math inline">\(\beta\)</span> is a parameter vector containing variables which represent a linear function to compute utility given a choice’s attributes. We can also choose to represent stochastic utility functions or embedding functions for Ideal Point Models as neural networks. But how can we compute the optimal values of these parameters?</p>
<p>In this section, we give the reader an overview of the different methods available to tune human preference model parameters using given data. We refer the reader to <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span> for first-principle derivations of these methods and a deeper dive into their theoretical properties (convergence, generalization, data-hungriness, etc.).</p>
<p>A common and powerful approach for computing the parameters of a model is maximum likelihood estimation <span class="citation" data-cites="book_estimation_casella book_estimation_bock">(<a href="#ref-book_estimation_casella" role="doc-biblioref">Casella and Berger 1990</a>; <a href="#ref-book_estimation_bock" role="doc-biblioref">Bock et al. 2015</a>)</span>. The likelihood of a model is the probability of the observed data given the model parameters; intuitively we wish to maximize this likelihood, as that would mean that our model associates observed human preferences in the data with high probability. We can formally define the likelihood for a model with parameters <span class="math inline">\(\beta\)</span> and a given data point <span class="math inline">\((z_i, y_i)\)</span> as: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>Assuming our data is independent and identically distributed (iid), the likelihood over the entire dataset is the joint probability of all observed data as defined by the model: <span class="math display">\[\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)\]</span></p>
<p>In our very first example of binary choice with logistic noise, this was simply the model’s probability of the observed preference value: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-\beta^\top z}}\]</span></p>
<p>In the same case with noise following a standard normal distribution, this took the form: <span class="math display">\[\mathcal{L}(z_i, y_i; \beta) = \Phi(\beta^\top z)\]</span></p>
<p>Fortunately, in these cases, there are straightforward methods for parameter estimation: logistic regression and probit regression (binary or multinomial, depending on the model), respectively. We can use ordinal regression to estimate the model’s parameters for our ordered preference model.</p>
<p>Generally, the objective function commonly found in parameter learning can be optimized with stochastic gradient descent (SGD) <span class="citation" data-cites="gradient_descent">(<a href="#ref-gradient_descent" role="doc-biblioref">Ruder 2016</a>)</span>. We can define an objective function as the likelihood to maximize this objective. Since SGD minimizes a given objective, we must negate the likelihood, which ensures that a converged solution maximizes the likelihood. SGD operates by computing the gradient of the objective with respect to the parameters of the model, which provides a signal of the direction in which the parameters must move to <em>maximize</em> the objective. Then, SGD makes an update step by subtracting this gradient from the parameters (most often with a scale factor called a <em>learning rate</em>), to move the parameters in a direction which <em>minimizes</em> the objective. When the objective is the negative likelihood (or sometimes negative log-likelihood for convenience or tractability), the result is an increase in the overall likelihood.</p>
<p>In the case of logistic and Gaussian models, SGD may yield a challenging optimization problem as its stochasticity can lead to noisy updates, for example, if certain examples or batches of examples are biased. Mitigations include batched SGD, in which multiple samples are randomly sampled from the dataset at each iteration, learning rates, which reduce the impact of noisy gradient updates, and momentum and higher-order optimizers which reduce noise by using movering averages of gradients or provide better estimates of the best direction in which to update the gradients. Some models, such as those that use neural networks, may, in fact, be intractable to estimate without a method such as SGD (or its momentum-based derivatives). For example, neural networks with many layers, non-linearities, and parameters can only be efficiently computed with gradient-based methods.</p>
<section id="reward-learning-with-large-language-models" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="reward-learning-with-large-language-models"><span class="header-section-number">2.4.1</span> Reward Learning with Large Language Models</h3>
<p>Taking a step away from explicitly modeling human bias and preference, we consider applying a deep learning approach to state-of-the-art language models. We begin by introducing the concepts of <em>foundation models</em> and <em>alignment</em>. A foundation model <span class="citation" data-cites="Liang2021">(<a href="#ref-Liang2021" role="doc-biblioref">Bommasani et al. 2021</a>)</span> in machine learning typically refers to a large and pre-trained neural network model that serves as the basis for various downstream tasks. In natural language processing, models like GPT-3, Llama, and BERT are considered foundation models. They are pre-trained on a massive corpus of text data, learning to understand language and context, and are capable of various language-related tasks such as text classification, language generation, and question answering. Foundation models are important because they alleviate the need to train massive neural networks from scratch, a compute and data expensive endeavor. However, a raw foundation model, trained on a pretraining objective such as a language modeling objective, is not useful on its own. It must be aligned to respond correctly based on human preferences.</p>
<p>In short, alignment for foundation models is the process by which model behavior is aligned with human values, ethics, and societal norms. Large Language Models (LLMs) are a foundation model for natural language processing. They are trained using a next-word prediction objective, allowing them to generate coherent language. A simple way to align a Large Language Model is to train it to follow instructions in a supervised way, using instruction-response pairs curated by hand. However, this limits the upper limit of LLM performance to the performance of the annotators’ writing abilities. This type of annotation is also expensive.</p>
<p>An alternative, more promising approach is to train LLMs using reinforcement learning, potentially enabling them to surpass human-level performance. The main challenge with this method lies in defining an explicit reward function for generating free-form text. To address this, a reward model (RM) can be trained based on human preferences, providing a mechanism to score the quality of the generated text. This approach, known as Reinforcement Learning from Human Feedback (RLHF), leverages human feedback to guide model training, allowing LLMs to better align with human expectations while continuously improving performance.</p>
<div id="fig-rm-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/arch.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rm-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: Overall architecture of a reward model based on LLM
</figcaption>
</figure>
</div>
<p>The Llama2 reward model <span class="citation" data-cites="2307.09288">(<a href="#ref-2307.09288" role="doc-biblioref">Touvron et al. 2023</a>)</span> is initialized from the pretrained Llama2 LLM. In the LLM, the last layer is a mapping <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^V\)</span>, where <span class="math inline">\(D\)</span> is the embedding dimension from the transformer decoder stack and <span class="math inline">\(V\)</span> is the vocabulary size. To get the RM, we replace that last layer with a randomly initialized scalar head that maps <span class="math inline">\(L: \mathbb{R}^D \rightarrow \mathbb{R}^1\)</span>. It’s important to initialize the RM from the LLM it’s meant to evaluate. This is because:</p>
<ol type="1">
<li><p>The RM will have the same “knowledge” as the LLM. This is particularly useful if evaluating things like “does the LLM know when it doesn’t know?”. However, in cases where the RM is simply evaluating helpfulness or factuality, it may be useful to have the RM know more.</p></li>
<li><p>The RM is on distribution for the LLM - it is initialized in a way where it semantically understands the LLM’s outputs.</p></li>
</ol>
<p>An RM is trained with paired preferences, following the format: <span class="math display">\[\begin{aligned}
    \langle prompt\_history, response\_accepted, response\_rejected \rangle
\end{aligned}\]</span> Prompt_history is a multiturn history of user prompts and model generations, response_accepted is the preferred final model generation by an annotator, and response_rejected is the unpreferred response. The RM is trained with a binary ranking loss with an optional margin term m(r), shown in equation (7). There is also often a small regularization term added to center the score distribution on 0. <span class="math display">\[\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))\]</span> The margin term increases the distance in scores specifically for preference pairs annotators rate as easier to separate.</p>
<div id="tbl-margin_nums" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.2: Two variants of preference rating based margin with different magnitude.
</figcaption>
<div aria-describedby="tbl-margin_nums-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td></td>
<td style="text-align: center;">Significantly</td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Slightly</td>
<td style="text-align: center;">Negligibly</td>
</tr>
<tr class="even">
<td></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Better</td>
<td style="text-align: center;">Better / Unsure</td>
</tr>
<tr class="odd">
<td>Margin Small</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2/3</td>
<td style="text-align: center;">1/3</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td>Margin Large</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-margin-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/margin-2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-margin-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Reward model score distribution shift caused by incorporating preference rating based margin in ranking loss. With the margin term, we observe a binary split pattern in reward distribution, especially for a larger margin.
</figcaption>
</figure>
</div>
<p>It may seem confusing how the margins were chosen. It’s primarily because the sigmoid function, which is used to normalize the raw reward model score, flattens out beyond the range of <span class="math inline">\([-4, 4]\)</span>. Thus, the maximum possible margin is eight.</p>
<p>When training or using a reward model, watching for the following is important:</p>
<ol type="1">
<li><p><strong>LLM Distribution Shift</strong>: With each finetune of the LLM, the RM should be updated through a collection of fresh human preferences using generations from the new LLM. This ensures that the RM stays aligned with the current distribution of the LLM and avoids drifting off-distribution.</p></li>
<li><p><strong>RM and LLM are coupled</strong>: An RM is generally optimized to distinguish human preferences more efficiently within the specific distribution of the LLM to be optimized. However, this specialization poses a challenge: such an RM will underperform when dealing with generations not aligned with this specific LLM distribution, such as generations from a completely different LLM.</p></li>
<li><p><strong>Training Sensitivities of RMs</strong>: Training RMs can be unstable and prone to overfitting, especially with multiple training epochs. It’s generally advisable to limit the number of epochs during RM training to avoid this issue.</p></li>
</ol>
<p>The industry has centered around optimizing for two primary qualities in LLMs: helpfulness and harmlessness (safety). There are also other axes such as factuality, reasoning, tool use, code, multilingual, and more, but these are out of scope for us. In the Llama2 paper, preference data was collected from humans for each quality, with separate guidelines. This presents a challenge for co-optimizing the final LLM towards both goals.</p>
<p>Two main approaches can be taken for Reinforcement Learning from Human Feedback (RLHF) in this context:</p>
<ol type="1">
<li><p>Train a unified reward model that integrates both datasets.</p></li>
<li><p>Train two separate reward models, one for each quality, and optimize the LLM toward both.</p></li>
</ol>
<p>Option 1 is difficult because of the tension between helpfulness and harmlessness. They trade off against each other, confusing an RM trained on both. The chosen solution was option 2, where two RMs are used to train the LLM in a piecewise fashion. The helpfulness RM is used as the primary optimization term, while the harmlessness RM acts as a penalty term, driving the behavior of the LLM away from unsafe territory only when the LLM veers beyond a certain threshold. This is formalized as follows, where <span class="math inline">\(R_s\)</span>, <span class="math inline">\(R_h\)</span>, and <span class="math inline">\(R_c\)</span> are the safety, helpfulness, and combined reward, respectively. <span class="math inline">\(g\)</span> and <span class="math inline">\(p\)</span> are the model generation and the user prompt: <span class="math display">\[\begin{aligned}
    R_c(g \mid p) =
    \begin{cases}
        R_s(g \mid p) &amp; \text{if } \text{is\_safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 \\
        R_h(g \mid p) &amp; \text{otherwise}
    \end{cases}
\end{aligned}\]</span></p>
<p>There are several open issues with reward models alluded to in the paper. For example, how best to collect human feedback? Training annotators and making sure they do the correct thing is hard. What should the guidelines be? Another question is whether RMs can be made robust to adversarial prompts. Last but not least, do RMs have well-calibrated scores? This matters for RLHF - pure preference accuracy isn’t enough.</p>
</section>
<section id="reward-learning-in-robotics" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="reward-learning-in-robotics"><span class="header-section-number">2.4.2</span> Reward Learning in Robotics</h3>
<p>To help set up our basic reward learning problem, consider a user and a robot. The user’s preferences or goals can be represented by an internal reward function, R(<span class="math inline">\(\xi\)</span>), which the robot needs to learn. Since the reward function isn’t explicit, there are a variety of ways that the robot can learn this reward function, which we will discuss in the next section. An example method of learning a reward function from human data is using pairwise comparison. Consider the robot example from section one, but now, the robot shows the human two possible trajectories <span class="math inline">\(\xi_A\)</span> and <span class="math inline">\(\xi_B\)</span> as depicted in the diagram below.</p>
<div id="fig-reward-robot-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robots.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-reward-robot-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Two different trajectories taken by a robot to prompt user ranking.
</figcaption>
</figure>
</div>
<p>The user is show both the trajectories above and asked to rank which one is better. Based on iterations of multiple trajectories and ranking, the robot is able to learn the user’s internal reward function. There quite a lot of ways that models can learn a reward function from human data. Here’s a list <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> of some of them:</p>
<ol type="1">
<li><p>Pairwise comparison: This is the method that we saw illustrated in the previous example. The robot is able to learn based on a comparison ranking provided by the user.</p></li>
<li><p>Expert demonstrations: Experts perform the task and the robot learns the optimal reward function from these demonstrations.</p></li>
<li><p>Sub-optimal demonstrations: The robot is provided with demonstrations that are not quite as good as the expert demonstrations but it is still able to learn a noisy reward function from the demonstrations.</p></li>
<li><p>Physical Corrections: While the robot is performing the task, at each point in its trajectory (or at an arbitrary point in its trajectory) its arm is corrected to a more suitable position. Based on these corrections, the robot is able to learn the reward function.</p></li>
<li><p>Ranking: This method is similar to pairwise comparison but involves more trajectories than 2. All the trajectories may have subtle differences from each other, but these differences help provide insight to the model.</p></li>
<li><p>Trajectory Assessment: Given a single trajectory, the user rates how close it is to optimal, typically using a ranking scale.</p>
<p>Each of these methods allows the robot to refine its understanding of the user’s reward function, but their effectiveness can vary depending on the application. For instance, expert demonstrations tend to produce more reliable results but may not always be feasible in everyday tasks. Pairwise comparison and ranking methods offer more flexibility but might require a higher number of iterations.</p></li>
</ol>
</section>
<section id="reward-learning-with-meta-learning" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="reward-learning-with-meta-learning"><span class="header-section-number">2.4.3</span> Reward Learning with Meta Learning</h3>
<p>Learning a reward function from human preferences is an intricate and complicated task. At its core, this task is about designing algorithms that can capture what humans value based on their elicited preferences. However, due to the nuanced and multifaceted nature of human desires, learning reward functions from human can be a difficult task. Therefore, meta-learning rewards may be considered to facilitate the reward learning processes. Meta-learning, often referred to as “learning to learn,” aims to design models that can adapt to new tasks with minimal additional efforts. We discuss paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> in <a href="#sec-few-shot" class="quarto-xref"><span>Section 2.4.3.1</span></a> showing how meta-learning can be leveraged for few-shot preference learning, where a system can quickly adapt to a new task after only a few queries to pairwise preferences from human.</p>
<p>Moving beyond the concept of learning from pairwise preferences, in <a href="#sec-watch" class="quarto-xref"><span>Section 2.4.3.2</span></a> we discuss a different approach where meta-learning intersects with both demonstrations and rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span>. This paper considers the use of both demonstrations and rewards elicited from human that guide the learning process.</p>
<p>In the regular learning setting, a model is fitted to a dataset with certain learning algorithm. The learning algorithm, for example, can be the minimization of a loss function. To formulate the “regular” learning procedure, let’s denote the training dataset as <span class="math inline">\(D\)</span>, and the test dataset as <span class="math inline">\(S\)</span>. Given a model parameterized by <span class="math inline">\(\theta\)</span>; training loss function <span class="math inline">\(L(\theta, D)\)</span>; and test loss function <span class="math inline">\(L(\theta, S)\)</span>, we can formulate a process of “regular” machine learning process as <span class="math display">\[\begin{aligned}
    \theta^\star = \arg\min_\theta\quad L(\theta, D).
\end{aligned}\]</span> Note that the minimization of the training loss function is essentially <em>one</em> possible learning algorithm. For example, instead of minimizing the loss function, one may do gradient descent with model regularization on the loss function, where the final solution may not be the one that actually minimizes the loss function. As a result, we may want to be more general and more abstract for the moment, and denote the learning algorithm as <span class="math inline">\(\mathcal{A}\)</span>. Thus, we can write <span class="math display">\[\begin{aligned}
    \theta^\star = \mathcal{A}(D),
\end{aligned}\]</span> i.e., the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> takes in a training dataset and outputs a model parameter <span class="math inline">\(\theta^\star\)</span>. Then, the performance of the model is evaluated by the test loss <span class="math inline">\(L(\mathcal{A}(D), S)\)</span>. As we can see, in the regime of “regular” learning, the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> is pre-defined and fixed.</p>
<p>Meta-learning, or learning-to-learn, essentially asks the question of whether one can <em>learn</em> the learning algorithm <span class="math inline">\(\mathcal{A}\)</span> from prior tasks, such that the modal can adapt to a new task more quickly/proficiently. For example, different human languages share similar ideas, and therefore a human expert who has learned many languages should be able to learn a new language easier than an average person. In other words, the human expert should have learned how to learn new languages more quickly based on their past experiences on learning languages.</p>
<p>To mathematically formulate meta-learning, we consider a family of learning algorithms <span class="math inline">\(\mathcal{A}_\omega\)</span> parameterized by <span class="math inline">\(\omega\)</span>. The “prior” tasks are represented by a set of meta-training datasets <span class="math inline">\(\{(D_i, S_i)\}_{i=1}^N\)</span> consists of <span class="math inline">\(N\)</span> pairs of training dataset <span class="math inline">\(D_i\)</span> and test dataset <span class="math inline">\(S_i\)</span>. As we noted before, a learning algorithm <span class="math inline">\(\mathcal{A}_\omega\)</span> takes in a training dataset, and outputs a model, i.e., <span class="math display">\[\begin{aligned}
    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).
\end{aligned}\]</span></p>
<p>Therefore, the <strong>meta-learning objective</strong> is <span class="math display">\[\begin{aligned}
    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).
\end{aligned}\]</span> The above optimization problem gives a solution <span class="math inline">\(\omega^\star\)</span> which we use as the meta-parameter. Then, when a new task comes with a new training dataset <span class="math inline">\(D_{new}\)</span>, we can simply apply <span class="math inline">\(\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})\)</span> to obtain the adapted model <span class="math inline">\(\theta^\star_{new}\)</span>. Note that we usually assume the meta-training datasets <span class="math inline">\(D_i, S_i\)</span> and the new dataset <span class="math inline">\(D_{new}\)</span> share the same underlying structure, or they come from the same distribution of datasets.</p>
<p>One of the most popular meta-learning method is Model-Agnosic Meta-Learning (MAML) <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. In MAML, the meta-parameter <span class="math inline">\(\omega\)</span> shares the same space as the model parameter <span class="math inline">\(\theta\)</span>. At its core, in MAML the learning algorithm is defined to be <span class="math display">\[\begin{aligned}
    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha\)</span> is the step size. As we can see, in fact <span class="math inline">\(\omega\)</span> is defined as the initialization of fine-tuning <span class="math inline">\(\theta\)</span>. With a good <span class="math inline">\(\omega\)</span> learned, the model can adapt to a new task very quickly. In general, meta-learning can be summarized as follows: Given data from prior tasks, learn to solve a new task more quickly/proficiently. Given the general nature of meta-learning, one may be curious about whether preference learning can be benefited from meta-learning, which we discuss in the following section.</p>
<section id="sec-few-shot" class="level4" data-number="2.4.3.1">
<h4 data-number="2.4.3.1" class="anchored" data-anchor-id="sec-few-shot"><span class="header-section-number">2.4.3.1</span> Few-Shot Preference Learning for Reinforcement Learning</h4>
<p>Reinforcement learning (RL) in robotics often stumbles when it comes to devising reward functions aligning with human intentions. Preference-based RL algorithms aim to solve this by learning from human feedback, but this often demands a <em>highly impractical number of queries</em> or leads to oversimplified reward functions that don’t hold up in real-world tasks.</p>
<p>To address the impractical requirement of human queries, as we discussed in the previous section, one may apply meta-learning so that the RL agent can adapt to new tasks with fewer human queries. <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> proposes to pre-training models on previous tasks with the meta-learning method MAML <span class="citation" data-cites="finn2017model">(<a href="#ref-finn2017model" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>, and then the meta-trained model can adapt to new tasks with fewer queries.</p>
<p>We consider Reinforcement Learning (RL) settings where a state is denoted as <span class="math inline">\(s\in S\)</span>, and action is denoted as <span class="math inline">\(a\in A\)</span>, for state space <span class="math inline">\(S\)</span> and action space <span class="math inline">\(A\)</span>. The reward function <span class="math inline">\(r:S\times A \to \mathbb{R}\)</span> is unknown and need to be learned from eliciting human preferences. There are multiple tasks, where each task has its own reward function and transition probabilities. The reward model is parameterized by <span class="math inline">\(\psi\)</span>. We denote <span class="math inline">\(\hat{r}_\psi(s,a)\)</span> to be a learned estimate of an unknown ground-truth reward function <span class="math inline">\(r(s,a)\)</span>, parameterized by <span class="math inline">\(\psi\)</span>. Accordingly, a reward model determines a RL policy <span class="math inline">\(\phi\)</span> by maximizing the accumulated rewards. The preferences is learned via pairwise comparison of trajectory segments <span class="math display">\[\begin{aligned}
    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})
\end{aligned}\]</span> of <span class="math inline">\(k\)</span> states and actions.</p>
<p>For each pre-training task, there is a dataset <span class="math inline">\(D\)</span> consists of labeled queries <span class="math inline">\((\sigma_1, \sigma_2, y)\)</span> where <span class="math inline">\(y\in \{0, 1\}\)</span> is the label representing which trajectory is preferred. Therefore, a loss function <span class="math inline">\(L(\psi, D)\)</span> captures how well the reward model characterizes the preferences in dataset <span class="math inline">\(D\)</span>. In <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> they the preference predictor over segments using the Bradley-Terry model of paired comparisons <span class="citation" data-cites="bradley1952rank">(<a href="#ref-bradley1952rank" role="doc-biblioref">Bradley and Terry 1952a</a>)</span>, i.e., <span class="math display">\[\begin{aligned}
    P[\sigma_1 \succ \sigma_2 ] = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.
\end{aligned}\]</span> Then, the loss function is essentially a binary cross-entropy which the reward model <span class="math inline">\(\psi\)</span> aims to minimize, i.e., <span class="math display">\[\begin{aligned}
    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left[ y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right].
\end{aligned}\]</span></p>
<section id="method-component-1-pre-training-with-meta-learning" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-1-pre-training-with-meta-learning">Method Component 1: Pre-Training with Meta Learning</h5>
<p>To efficiently approximate the reward function <span class="math inline">\(r_\text{new}\)</span> for a new task with minimal queries, as described in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, we aim to utilize a pre-trained reward function <span class="math inline">\(\hat{r}_\psi\)</span> that can be quickly fine-tuned using just a few preference comparisons. By pre-training on data from prior tasks, we can leverage the common structure across tasks to speed up the adaptation process. Although any meta-learning method is compatible, <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> opt for Model Agnostic Meta-Learning (MAML) due to its simplicity. Therefore, the pre-training update for the reward model <span class="math inline">\(\psi\)</span> is <span class="math display">\[\begin{aligned}
    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),
\end{aligned}\]</span> where <span class="math inline">\(\alpha, \beta\)</span> are the inner and outer learning rate, respectively. We note that data <span class="math inline">\(\{D_i\}_i\)</span> of labeled preferences queries for prior tasks can come from offline datasets, simulated policies, or actual humans.</p>
</section>
<section id="method-component-2-few-shot-adaptation" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="method-component-2-few-shot-adaptation">Method Component 2: Few-Shot Adaptation</h5>
<p>With the aforementioned pre-training with meta learning, the meta-learned reward model can then be used for few-shot preference based RL during an online adaptation phase. The core procedure of the few-shot adaption is descibed as below</p>
<ol type="1">
<li><p>Given a pre-trained reward model <span class="math inline">\(\psi\)</span></p></li>
<li><p>For time step <span class="math inline">\(t=1, 2, \dots\)</span></p>
<ol type="1">
<li><p>Find pairs of trajectories <span class="math inline">\((\sigma_1, \sigma_2)\)</span> with preference uncertainty based on <span class="math inline">\(\psi\)</span>.</p></li>
<li><p>Query human preference <span class="math inline">\(y\)</span> and forms a new dataset <span class="math inline">\(D_{new}\)</span></p></li>
<li><p>Update the reward model by <span class="math inline">\(\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})\)</span></p></li>
<li><p>Update the policy with the new reward model <span class="math inline">\(\psi'\)</span></p></li>
</ol></li>
</ol>
<p>As mentioned in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>, uncertain queries are selected using the disagreement of an ensemble of reward functions over the preference predictors. Specifically, comparisons that maximize <span class="math inline">\(\texttt{std}(P[\sigma_1 \succ \sigma_2])\)</span> are selected each time feedback is collected.</p>
<p>The whole pipeline of the method is outlined in <a href="#fig-few-1" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>.</p>
<div id="fig-few-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/overview-few.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: An overview of the proposed method in <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span>. <strong>Pre-training (left):</strong> In the pre-training phase, trajectory segment comparisons are generated using data from previously learned tasks. Then, they are used to train a reward model. <strong>Online-Adaptation (Right)</strong>: After pre-training the reward model, it is adapted to new data from human feedback. The adapted reward model is then used to train a policy for a new task in a closed loop manner.
</figcaption>
</figure>
</div>
<p>We present one set of experiment from the paper, as it illustrates the effectiveness of the proposed method in a straightforward way. The experiment test the propoesed method on the Meta-World benchmark <span class="citation" data-cites="yu2020meta">(<a href="#ref-yu2020meta" role="doc-biblioref">Yu et al. 2020</a>)</span>. Three baselines are compared with the proposed method:</p>
<ol type="1">
<li><p>SAC: The Soft-Actor Critic RL algorithm trained from ground truth rewards. This represents the standard best possible method given the ground-truth reward.</p></li>
<li><p>PEBBLE: The PEBBLE algorithm <span class="citation" data-cites="lee2021pebble">(<a href="#ref-lee2021pebble" role="doc-biblioref">Lee, Smith, and Abbeel 2021</a>)</span>. It does not use information from pripor tasks.</p></li>
<li><p>Init: This method initialize the reward model with the pretained weights from meta learning. However, instead of adapting the reward model to the new task, it performs standard updates as in PEBBLE.</p></li>
</ol>
<p>The results are shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>, where we can see that the proposed methord outperforms all of the baselines.</p>
<div id="fig-few-exp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/few-exp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-few-exp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Results on MetaWorld tasks. The title of each subplot indicates the task and number of artificial feedback queries used in training. Results for each method are shown across five seeds.
</figcaption>
</figure>
</div>
<p>This paper <span class="citation" data-cites="hejna2023few">(<a href="#ref-hejna2023few" role="doc-biblioref">Hejna III and Sadigh 2023</a>)</span> shows that meta reward learning indeed reduce the number of queries of human preferences. However, as mentioned in the paper, there are still some drawbacks, as shown in the following.</p>
<p>Many of the queries the model pick for human preference elicitation are actually almost identical to human. After all, the model would pick the most uncertain pair of trajectories for human preference queries, and similar trajectories are for sure having high uncertainty in their preference. This suggest the need of new ways for designing the query selection strategy.</p>
<p>Moreover, despite the improved query complexity, it still needs an impractical amount of queries. As shown in <a href="#fig-few-exp" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>, the “sweep into” task still needs 2500 human queries for it to work properly, which is still not ideal for what we want them to be.</p>
<p>In addition, it is mentioned in the paper that the proposed method may be even worse than training from scratch, if the new task is too out-of-distribution. Certainly, since meta-learning assumes in-distribution tasks, we cannot expect the proposed method to be good for out-of-distribution task. It is thus an interesting future direction to investigate whether one can design a method that automatically balance between using the prior information or training from scratch.</p>
</section>
</section>
<section id="sec-watch" class="level4" data-number="2.4.3.2">
<h4 data-number="2.4.3.2" class="anchored" data-anchor-id="sec-watch"><span class="header-section-number">2.4.3.2</span> Watch Try Learn</h4>
<p>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards <span class="citation" data-cites="zhou2019watch">(<a href="#ref-zhou2019watch" role="doc-biblioref">Zhou et al. 2019</a>)</span> asks the question “How can we efficiently learn both from expert demonstrations and from trials where we only get <strong>binary</strong> feedback from a human". Why do we care about this question? In the context of robotics, a very compelling answer is the <em>cost of data-collection</em>. In a hypothetical world in which we have a vast number of <strong>expert demonstrations</strong> of robots accomplishing a large number of diverse tasks, we don’t necessarily need to worry about learning from trials or from humans. We could simply learn a very capable imitation agent to perform any task. Natural Language Processing could be seen as living in this world, because internet-scale data is available. <strong>Robots, however, are expensive</strong>, so people generally don’t have access to them, and therefore cannot use them to produce information to imitate. Similarly, <strong>human time is expensive</strong>, so even for large organizations that do have access to a lot of robots, it’s still hard to collect a lot of expert demonstrations.</p>
<p>The largest available collection of robotics datasets today is Open X-Embodiment (<span class="citation" data-cites="padalkar2023open">(<a href="#ref-padalkar2023open" role="doc-biblioref">Padalkar et al. 2023</a>)</span>), which consists of around 1M episodes from more than 300 different scenes. Even such large datastes are not enough to learn generally-capable robotic policies from imitation learning alone.</p>
<div id="fig-open-x-embodiment" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/open_x_embodiment.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-open-x-embodiment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Visualization of the Open X-Embodiment dataset collection. Even this large-scale dataset for robot learning is not yet enough to learn generally-capable robotic policies.
</figcaption>
</figure>
</div>
<p><strong>Main insight:</strong> binary feedback is much cheaper to obtain than expert demonstrations! Instead of hiring people to act as robot operators to tell the robot exactly what to do, if there was a way of having many robots trying things in parallel, we can have humans watch videos of what the robots did and then give a success classification of whether the robot accomplished the goal. This is a much cheaper form of human supervision because the human labels don’t necessarily need to be given in real time, so one human labeler can label many trajectories in parallel, and the human doesn’t need to be a skilled robot operator.</p>
<p>Concretely, this paper seeks to learn new tasks with the following general problem setting:</p>
<ol type="1">
<li><p>We only get 1 expert demonstration of the target task</p></li>
<li><p>After seeing the expert demonstration, we have robots try to solve the task 1 or more times.</p></li>
<li><p>The user (or some pre-defined reward function) annotates each trial as success/failure.</p></li>
<li><p>The agent learns from both the demos and the annotated trials to perform well on the target task.</p></li>
</ol>
<p>Note that this work falls under the <strong>meta-learning</strong> umbrella, because we are learning an algorithm for quickly learning new tasks given new observations (demos, trials, and success labels.)</p>
<p>The <strong>main contribution</strong> of this paper is a meta-learning algorithm for incorporating demonstrations and binary feedback from trials to solve new tasks.</p>
<p>Meta-Learning deals with efficient learning of new tasks. In the context of robotics or reinforcement learning in general, <strong>how do we define tasks</strong>? We will use the Markov decision process (<strong>MDP</strong>) formalism. A task <span class="math inline">\(T_i\)</span> is described with the tuple <span class="math inline">\(\{S, A, r_i, P_i\}\)</span>.</p>
<ol type="1">
<li><p><span class="math inline">\(S\)</span> represents the <em>state-space</em> of the task, or all possible states the agent could find itself in. This work uses image-observations, so <span class="math inline">\(S\)</span> is the space of all possible RGB images.</p></li>
<li><p><span class="math inline">\(A\)</span> is the action space, meaning the set of all possible actions the agent could take. In robotics there are many ways of representing action spaces, and this work considers end-effector positions, rotations, and opening.</p></li>
<li><p><span class="math inline">\(r_i\)</span> is the reward function for the task, with function signature <span class="math inline">\(r_i : S \times A \to \mathbb{R}\)</span>. This work assumes all reward functions are binary.</p></li>
<li><p><span class="math inline">\(P_i\)</span> is the transition dynamics function. It’s a function that maps state-action pairs to probability distributions over next states.</p></li>
</ol>
<p>Notice that <span class="math inline">\(S\)</span> and <span class="math inline">\(A\)</span> are shared across tasks. Transition dynamics functions are normally also shared between tasks because they represent the laws of physics. However, this work considers environments with different objects, so they don’t share the dynamics function. Given this definition for tasks, they assume that the tasks from the data that they get come from some unknown task-generating distribution <span class="math inline">\(p(T)\)</span>.</p>
<p>Let’s give a more precise definition of the problem statement considered by <strong>Watch, Try, Learn</strong>. As the paper name suggests, there are 3 phases for the problem statement.</p>
<p><strong>Watch:</strong> During the <em>watch</em> phase, we give the agent <span class="math inline">\(K\)</span> demonstrations of the target tasks. This paper considers the case where <span class="math inline">\(K\)</span> always equals 1, and all demonstrations are successful. That is, each demonstration consists of a trajectory <span class="math inline">\(\{(s_0, a_0), \ldots, (s_H, a_H)\}\)</span> where <span class="math inline">\(H\)</span> is the task horizon, and the final state is always successful, that is <span class="math inline">\(r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0\)</span> for every <span class="math inline">\(j \neq H\)</span>.</p>
<p>Importantly, these demonstrations alone might not be sufficient for <strong>full task specification</strong>. As an example, consider a demonstration in which an apple is moved to the right, next to a pan. Seeing this demonstration alone, the task could be always moving the apple to the right, or it could be always moving the apple next to the pan, irrespective of where the pan is. The expected output after the Watch phase is a policy capable of gathering information about a task, given demonstrations.</p>
<p><strong>Try:</strong> In the Try phase, we use the agent learned during the Watch phase to attempt the task for <span class="math inline">\(L\)</span> trials. As specified earlier, this paper considers the casae where <span class="math inline">\(L\)</span> always equals 1. After the agent completes the trials, humans (or pre-programmed reward functions) provide one binary reward for each trial, indicating whether the trial was successful. The expected output of this phase is <span class="math inline">\(L\)</span> trajectories and corresponding feedback that hopefully <em>disambiguate</em> the task.</p>
<p><strong>Learn:</strong> After completing the trials, the agent must learn from both the original expert demonstrations and the trials, and become capable of solving the target task.</p>
<p><strong>Given Data:</strong> To train agents that can Watch, Try, and Learn, we are given a dataset of expert demonstrations containing multiple demos for each task, and the dataset contains hundreds of tasks. Importantly, <strong>no online interaction</strong> is needed for training, and this method trains only with <strong>supervised learning</strong> and no reinforcement learning.</p>
<p>This section describes exactly how this paper trains an agent from the given expert demonstrations, and how to incorporate the trials and human feedback into the loop.</p>
<p><strong>Training to Watch:</strong> We now describe the algorithm to obtain an agent conditioned on the given expert demonstration. In particular, what we want to obtain out of the Watch phase is a policy conditioned on a set of expert demonstrations. Formally, we want to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>.</p>
<p>The way we can obtain this policy is through <strong>meta-imitation learning</strong>. Given the demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span> for task <span class="math inline">\(i\)</span>, we sample another <em>different</em> demonstration coming from the same task <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. The key insight here is that <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span> is an example of <strong>optimal behavior</strong> given the demonstrations. Therefore, to obtain <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{d_{i,k}\})\)</span>, we simply regress the policy to imitate actions taken on <span class="math inline">\(\textbf{d}_i^{\text{test}}\)</span>. Concretely, we train policy parameters <span class="math inline">\(\theta\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}\}) \big]\)</span></p>
<p>This corresponds to doing imitation learning by minimizing the negative log-likelihood of the test trajectory actions, conditioning the policy on the entire demo set. However, how is the conditioning on the demo set achieved?</p>
<div id="fig-watch-try-learn-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: Vision-based policy architecture that conditions on a set of demonstrations.
</figcaption>
</figure>
</div>
<p><a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>2.7</span></a> visualizes how Watch Try Learn deals with conditioning on demonstrations. In addition to using features obtained from the images of the current state, the architecture uses features from frames sampled (in order) from the demonstration episodes, which are concatenated together.</p>
<p><strong>Trying:</strong> On the <strong>Try</strong> phase, when the agent is given a set of demonstrations <span class="math inline">\(\{\textbf{d}_{i,k}\}\)</span>, we deploy the policy <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span> to collect <span class="math inline">\(L\)</span> trials. There is no training involved in the Try phase, we simply condition the policy on the given demonstrations</p>
<p><strong>Training to Learn:</strong> During the Watch phase the objective was to train a policy conditioned on demonstrations <span class="math inline">\(\pi_\theta^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\})\)</span>. The authors of Watch, Try, Learn use a similar strategy as the Watch phase for the Learn phase. We now want to train a policy that is conditioned on the demonstrations, as well as the trials and binary feedback. That is, we want to learn <span class="math inline">\(\pi_\phi^{\text{watch}}(a | s, \{\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}\})\)</span>. To train the policy, we again use meta-imitation learning where we additionally sample yet another trajectory from the same task. Concretely, we train policy parameters <span class="math inline">\(\phi\)</span> to minimize the following loss:</p>
<p><span class="math inline">\(\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[
- \log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}\}) \big]\)</span></p>
<p>The conditioning on both the demo episodes and the trial episodes is achieved in the exact same way as in the Watch phase, and is visualized in <a href="#fig-watch-try-learn-arch" class="quarto-xref">Figure&nbsp;<span>2.7</span></a>. The architecture is simply adjusted to be able to take in more images fro mthe trial episodes.</p>
<p>In this section, we describe the evaluation suite for the paper, including the simulation benchmark used, the baselines considered, and the results.</p>
<p><strong>Gripper environment setup:</strong></p>
<div id="fig-envs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-envs.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-envs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: Visualization of different tasks from the simulated benchmark for Watch Try Learn.
</figcaption>
</figure>
</div>
<p><a href="#fig-envs" class="quarto-xref">Figure&nbsp;<span>2.8</span></a> illustrates the different task families considered in the simulated Gripper environment. Button Pressing, Grasping, Pushing, and Pick and Place. For each task family, the environment supports hundreds of different tasks by changing the objects in the scene and the objectives (e.g.&nbsp;which object to pick and where to place). For each task in each task family, a handful of expert demonstrations are given in a demonstrations dataset. As mentioned previously, the environment gives the agent image observations, and take in actions as end-effector (gripper) positions, angles, and opening.</p>
<p><strong>Baselines:</strong> The following three baselines are considered:</p>
<ol type="1">
<li><p><strong>Behavior Cloning</strong>: simple imitation learning based on maximum log-likelihood training using data from all tasks.</p></li>
<li><p><strong>Meta-imitation learning</strong>: This baseline corresponds to simply running the policy from the Watch step, without using any trial data. That is, we only condition on the set of expert demonstrations, but no online trials.</p></li>
<li><p><strong>Behavior Cloning + SAC</strong>: Pre-train a policy with Behavior Cloning on all data, and follow that with Reinforcement Learning fine-tuning for the specific target task, using the maximum-entropy algorithm SAC (<span class="citation" data-cites="haarnoja2018soft">(<a href="#ref-haarnoja2018soft" role="doc-biblioref">Haarnoja et al. 2018</a>)</span>).</p></li>
</ol>
<div id="fig-watch-try-learn-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/watch-try-learn-results.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watch-try-learn-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: Results for Watch Try Learn on the gripper control environment, and comparisons with baselines.
</figcaption>
</figure>
</div>
<div id="tbl-watch-try-learn-table" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.3: Average success rates over all tasks.
</figcaption>
<div aria-describedby="tbl-watch-try-learn-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><strong>METHOD</strong></th>
<th style="text-align: center;"><strong>SUCCESS RATE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BC</td>
<td style="text-align: center;">.09 <span class="math inline">\(\pm\)</span> .01</td>
</tr>
<tr class="even">
<td style="text-align: left;">MIL</td>
<td style="text-align: center;">.30 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">WTL, 1 TRIAL (OURS)</td>
<td style="text-align: center;">.42 <span class="math inline">\(\pm\)</span> .02</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>RL FINE-TUNING WITH SAC</strong></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 1500 TRIALS</td>
<td style="text-align: center;">.11 <span class="math inline">\(\pm\)</span> .07</td>
</tr>
<tr class="even">
<td style="text-align: left;">BC + SAC, 2000 TRIALS</td>
<td style="text-align: center;">.29 <span class="math inline">\(\pm\)</span> .10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BC + SAC, 2500 TRIALS</td>
<td style="text-align: center;">.39 <span class="math inline">\(\pm\)</span> .11</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#fig-watch-try-learn-results" class="quarto-xref">Figure&nbsp;<span>2.9</span></a> shows average success rates for Watch Try Learn compared to baselines. Watch Try Learn significantly outperforms baselines on every task family. In particular, it is far superior to Behavior Cloning, which is a very weak baseline, and it significantly surpasses Meta-Imitation Learning on 3 out of 4 task families. <a href="#tbl-watch-try-learn-table" class="quarto-xref">Table&nbsp;<span>2.3</span></a> includes comparison with BC fine-tuned with Reinforcement Learning. Even after 2500 online trials, SAC is not able to obtain the success rate that Watch Try Learn achieves after only 1 trial. Overall, Watch Try Learn exhibits very significant performance gains over prior methods.</p>
</section>
</section>
<section id="direct-preference-optimization" class="level3" data-number="2.4.4">
<h3 data-number="2.4.4" class="anchored" data-anchor-id="direct-preference-optimization"><span class="header-section-number">2.4.4</span> Direct Preference Optimization</h3>
<p>A modern method for estimating the parameters of a human preference model is direct preference optimization <span class="citation" data-cites="rafailov2023direct">(<a href="#ref-rafailov2023direct" role="doc-biblioref">Rafailov et al. 2023</a>)</span>, which is used in the context of aligning language models to human preferences. A recent approach <span class="citation" data-cites="christiano2023deep">(<a href="#ref-christiano2023deep" role="doc-biblioref">Christiano et al. 2023</a>)</span> first trains a reward model that captures human preferences and then uses proximal policy optimization to train a language model-based policy to reflect those learned preferences. Direct Preference Optimization (DPO), on the other hand, removes the need for a reward model by directly using the model likelihood of two outcomes (a preferred or highly-ranked sequence and an unpreferred or low-ranked sequence) to capture the preference represented in the data. DPO provides a simpler framework than its reinforcement learning approach and results in comparable performance with improved stability. Furthermore, it obviates the need to train a reward model, instead using a language model policy and human preference dataset to align the policy directly to human preferences.</p>
</section>
<section id="model-design-consideration" class="level3" data-number="2.4.5">
<h3 data-number="2.4.5" class="anchored" data-anchor-id="model-design-consideration"><span class="header-section-number">2.4.5</span> Model Design Consideration</h3>
<p>When designing models and learning their parameters, one must account for important tradeoffs when designing and optimizing a model to learn human preferences.</p>
<p><strong>Bias vs.&nbsp;Variance Trade-off.</strong> In modeling human preferences, we aim to ensure that predicted utilities accurately reflect overall human preferences. One key challenge is managing the bias and variance trade-off.</p>
<p>Bias refers to assumptions made during model design and training that can skew predictions. For example, in Ideal Point Models, we make the assumption that the representations we use for individuals and choices are aligned in the embedding space, and that this representation is sufficient to capture human preferences using distance metrics. However, there are myriad cases in which this may break down, for example if the two sets of vectors follow different distributions each with their own unique biases. If the representations do not come from the same domain, one may have little visibility into how a distance metric computes the final utility value for a choice for a given individual. Some ways to mitigate bias in human preference models include increasing the number of parameters in a model (allowing for better learning of patterns in the data) or removing inductive biases based on our assumptions of the underlying data.</p>
<p>On the other hand, variance refers to the model’s sensitivity to small changes in the input, which leads to significant changes in the outp ut. This phenomenon is often termed ‘overfitting’ or ‘overparameterization.’ This behavior can occur in models that have many parameters, and learn correlations in the data that do not contribute to learning human preferences, but are artifacts of noise in the dataset that one should ultimately ignore. One can address variance in models by reducing the number of parameters or incorporating biases in the model based on factors we can assume about the data.</p>
<p><strong>Model Scope.</strong> One important consideration unique to human preference models is that we wish to model individual preferences, and we may choose to do so at arbitrary granularity. For example, we can fit models to a specific individual or even multiple models for an individual, each for different purposes or contexts. On the other end of the spectrum, we may create a model to capture human preferences across large populations or the world.</p>
<p>Individual models may certainly prove to be more powerful, as they do not need to generalize across multiple individuals and can dedicate all of their parameters to learning the preferences of a single user. In the context of human behavior, this can be a significant advantage as any two individuals can be arbitrarily different or even opposite in their preferences. On the other hand, models fit only one person can tremendously overfit to the training distribution and capture noise in the data, which is not truly representative of human preferences.</p>
<p>On the end of the spectrum, models fit to the entire world may be inadequate to model human preferences for arbitrary individuals, especially those whose data it has not been fit to. As such, models may underfit the given training distribution. These models aim to generalize to many people but may fail to capture the nuances of individual preferences, especially for those whose data is not represented in the training set. As a result, they may not perform well for arbitrary individuals within the target population</p>
<p>Choosing the appropriate scope for a model is crucial. ne must balance the trade-off between overfitting to noise in highly granular models and underfitting in broader models that may not capture individual nuances.</p>
</section>
</section>
<section id="multimodal-preferences" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="multimodal-preferences"><span class="header-section-number">2.5</span> Multimodal Preferences</h2>
<p>One of the core assumptions about learning a reward function is that it is unimodal, meaning that it consists of data from one person with a certain set of preferences or a group of people with similar preferences. However, the model of unimodality often oversimplifies human preferences and their often conflicting nature. To accurately capture all the nuances of human preference, we examine a multi-modal distribution with some baseline assumptions. Consider a scenario where we, as regular drivers, make a left-hand turn at an intersection <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. What would we do if we saw a car speeding down the road approaching us? The figure below describes some options. Following a timid driving pattern, some vehicles would stop to let the other car go, preventing a collision. Other vehicles would be more aggressive and try to make the turn before colliding with the oncoming vehicle. Given the data of one of these driving patterns, our model (our autonomous vehicle) can make an appropriate decision. However, what if our model was given data from both aggressive and timid drivers, and we don’t know which data corresponds to which type of driver? If we applied standard learning based on comparison techniques, we see, as illustrated by the figure below, that the car would have an accident trying to find a policy close enough to both driving patterns.</p>
<div id="fig-driving-patt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-patt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-patt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> shows the possibilities of 2 different driving patterns when a car is taking a left-hand turn at an intersection and sees another car approaching head-on.
</figcaption>
</figure>
</div>
<div id="fig-driving-coll" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/driving-coll.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-driving-coll-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: The figure <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span> depicts the resultant collision when we try to find a policy close enough to both the driving patterns.
</figcaption>
</figure>
</div>
<p>As illustrated by the driving example, we see that multi-modality for our reward function is extremely important and, in some cases, if it is not considered, can lead to fatal decisions <span class="citation" data-cites="myers2021learning">(<a href="#ref-myers2021learning" role="doc-biblioref">Myers et al. 2021</a>)</span>. But why can’t we label the groups, which would be the timid and aggressive drivers in the driving case, and then learn separate reward functions for each driver? The first problem with this approach is that it is inefficient and time-consuming to separate the data into groups because we would have to cluster and label the data. Secondly, it would not be accurate just to split the data because a more timid driver can be aggressive when they are in a hurry.</p>
<p>To formulate this problem of learning reward functions and mixing coefficients from ranking queries in a fully observable deterministic dynamical system, we begin by describing the system as a trajectory <span class="math inline">\(\xi = (s_0, a_0, ..., s_T, a_T)\)</span>, where the sequence of states and actions represents the system’s evolution over time. Assume there are <span class="math inline">\(M\)</span> different reward functions, each representing an expert’s preferences. Using the linearity assumption in reward learning, we model each expert’s reward function as a linear combination of features in a known, fixed feature space <span class="math inline">\(\phi(\xi)\)</span>. The reward for the <span class="math inline">\(m\)</span>-th expert is given by: <span class="math display">\[R_m(\xi) = \omega^T_m \phi(\xi),\]</span> where <span class="math inline">\(\omega_m\)</span> is a vector of parameters corresponding to the <span class="math inline">\(m\)</span>-th expert’s preferences. There exists an unknown distribution over the reward parameters and we can represent this distribution with mixing coefficients <span class="math inline">\(\alpha_m\)</span> such that <span class="math inline">\(\sum_M^{m = 1} \alpha_m = 1\)</span>. Our goal is to learn reward functions and mixing coefficients using ranking queries.</p>
<p>To define our problem, let’s consider a robot who performs the following trajectories and asks a user to rank all the trajectories.</p>
<div id="fig-robot-traj" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/robot-traj.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robot-traj-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: The figure <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span> depicts a few different trajectories for an example multi-modal ranking scenario.
</figcaption>
</figure>
</div>
<p>The robot will be given back a set of trajectory rankings, coming from M humans and the objective is to learn the underlying reward function. We can represent the response of the ranking query as <span class="math inline">\(x = (\xi_{a_1},\ ...\ ,\xi_{a_K})\)</span> where <span class="math inline">\(a_1\)</span> is the index of the expert’s top choice, <span class="math inline">\(a_2\)</span> is the index of the expert’s second choice, ... and so on. With the response <span class="math inline">\(x\)</span>, we generate a probability distribution with the softmax rule <span class="citation" data-cites="myers2022learning">(<a href="#ref-myers2022learning" role="doc-biblioref">Myers et al. 2022</a>)</span>: <span class="math inline">\(Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}\)</span>. where <span class="math inline">\(R_m(\xi_{a_i})\)</span> denotes the reward assigned by the <span class="math inline">\(m\)</span>-th expert to trajectory <span class="math inline">\(\xi_{a_i}\)</span>. Then, we randomly sample our probability distribution to pick our top choice. From the remaining trajectories, we noisily choose from our distribution to rank our second-best option. We repeat this process until we have ranked all our trajectories. This follows what is known as the Plackett-Luce Ranking Model.</p>
<p>Given knowledge of the true reward function weights <span class="math inline">\(\omega_m\)</span> and mixing coefficients <span class="math inline">\(\alpha_m\)</span>, we have the following joint mass over observations x from a query Q: <span class="math inline">\(Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}\)</span>.</p>
<p>With the above formulation of the joint mass distribution over observation and queries, we can now formulate an objective. Specifically, it is to present users with the best set of queries that learn reward weights, <span class="math inline">\(\omega\)</span>, and mixing coefficient, <span class="math inline">\(\alpha\)</span>, based upon user rankings of preferred query responses. By learning these parameters, we can have an accurate estimation of the joint mass distribution of the observations.</p>
<p>To learn these parameters, we use a Bayesian learning framework. The goal will be to learn the reward weights, <span class="math inline">\(\omega_m\)</span>, and all mixing coefficients <span class="math inline">\(\alpha_m\)</span>. Thus, define the parameters to be <span class="math inline">\(\theta = \{\omega, \alpha\}\)</span>. We start by simplifying the posterior over the parameters.</p>
<p><span class="math display">\[\begin{aligned}
\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) \\
&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) \\
&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})
\end{aligned}\]</span></p>
<p>Note that the first proportionality term is directly from Bayes rule (removing normalization constant). The first equation comes directly from the assumption that the queries at timestamp <span class="math inline">\(t\)</span> are conditionally independent of the parameters given previous queries &amp; rankings. This assumption is reasonable because the previous queries &amp; rankings ideally give all the information to inform the choice of the next set of. The last proportionality term comes from the assumption that the ranked queries are conditionally independent given the parameters</p>
<p>The prior distribution is dependent on use case. For example, in the user studies conducted by the authors to verify this method, they use a standard Gaussian for the reward weights and the mixing coefficients to be uniform on a <span class="math inline">\(M - 1\)</span> simplex to ensure that they add up to 1. Then we can use maximum likelihood estimation to compute the parameters with the simplified posterior.</p>
</section>
<section id="social-choices" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="social-choices"><span class="header-section-number">2.6</span> Social Choices</h2>
<p>Game theory provides a mathematical framework for analyzing strategic interactions among rational agents. These models help in understanding and predicting human behavior by considering multiple criteria and the associated trade-offs. They enhance the understanding of preferences across multiple criteria and allow for richer and more accurate feedback through structured comparisons. Game-theory framings capture the complexity of preferences and interactions in decision-making processes <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The most popular form of preference elicitation involves pairwise comparisons. Users are asked to choose between two options, such as product A or product B. This method is used in various applications like search engines, recommender systems, and interactive robotics. Key concepts include the Von Neumann Winner and the Blackwell Winner. The Von Neumann Winner refers to a distribution over objects that beats or ties every other object in the collection under the expected utility assumption. The Blackwell Winner generalizes the Von Neumann Winner for multi-criteria problems using a target set for acceptable payoff vectors <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>Game-theory framings provide a framework for preference learning along multiple criteria. These models use tools from vector-valued payoffs in game theory, with Blackwell’s approach being a key concept. This approach allows for a more comprehensive understanding of preferences by considering multiple criteria simultaneously <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>In game-theory framings, pairwise preferences are modeled as random variables. Comparisons between objects along different criteria are captured in a preference tensor <span class="math inline">\(P\)</span>. This tensor models the probability that one object is preferred over another along a specific criterion, allowing for a detailed understanding of preferences across multiple dimensions <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The preference tensor <span class="math inline">\(P\)</span> captures object comparisons along different criteria. It is defined as: <span class="math display">\[P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)\]</span> where <span class="math inline">\(P(i_2, i_1; j) = 1 - P(i_1, i_2; j)\)</span>. These values are aggregated to form an overall preference matrix <span class="math inline">\(P_{ov}\)</span> <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The Blackwell Winner is defined using a target set <span class="math inline">\(S\)</span> of acceptable score vectors. The goal is to find a distribution <span class="math inline">\(\pi^*\)</span> such that <span class="math inline">\(P(\pi^*, \pi) \in S\)</span> for all <span class="math inline">\(\pi\)</span>. This method minimizes the maximum distance to the target set, providing a robust solution to multi-criteria preference problems <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
<p>The optimization problem for finding the Blackwell Winner is defined as: <span class="math display">\[\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left[ \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right]\]</span> where <span class="math inline">\(\rho(u, v) = \|u - v\|\)</span>. This measures the distance to the target set, ensuring that the selected distribution is as close as possible to the ideal preference vector <span class="citation" data-cites="bhatia2020preference">(<a href="#ref-bhatia2020preference" role="doc-biblioref">Bhatia et al. 2020</a>)</span>.</p>
</section>
<section id="exercises" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.7</span> Exercises</h2>
<section id="question-1-choice-modeling-15-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-1-choice-modeling-15-points">Question 1: Choice Modeling (15 points)</h3>
<p>In Chapter 2, we discussed discrete choice modeling in the context of utility being a linear function. Suppose we are deciding between <span class="math inline">\(N\)</span> choices and that the utility of each choice is given by <span class="math inline">\(U_i=\beta_i\mathbf{x}+\epsilon_i\)</span> for <span class="math inline">\(i=1, 2, \cdots, N\)</span>. We view <span class="math inline">\(\mathbf{x}\)</span> as the data point that is being conditioned on for deciding which choice to select, and <span class="math inline">\(\beta_i\)</span> as the weights driving the linear utility model. The noise <span class="math inline">\(\epsilon_i\)</span> is i.i.d. sampled from a type of extreme value distribution called the <em>Gumbel</em> distribution. The standard Gumbel distribution is given by the density function <span class="math inline">\(f(x)=e^{-(x+e^{-x})}\)</span> and cumulative distribution function <span class="math inline">\(F(x)=e^{-e^{-x}}.\)</span> Fix <span class="math inline">\(i\)</span>. Our objective is to calculate <span class="math inline">\(\Pr(U_i\,\, \text{has max utility})\)</span>.</p>
<ol type="a">
<li><p><strong>(Written, 2 points)</strong>. To start, set <span class="math inline">\(U_i=t\)</span> and compute <span class="math inline">\(\Pr(U_j&lt;t)\)</span> for <span class="math inline">\(j\neq i\)</span> in terms of <span class="math inline">\(F\)</span>. Use this probability to derive an integral for <span class="math inline">\(\Pr(U_i\,\,  \text{has max utility})\)</span> over <span class="math inline">\(t\)</span> in terms of <span class="math inline">\(f\)</span> and <span class="math inline">\(F\)</span>.</p>
<p>Example of solution environment.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Compute the integral derived in part (a) with the appropriate <span class="math inline">\(u\)</span>-substitution. Show your work. You should arrive at multi-class logistic regression in the end!</p></li>
</ol>
<p>Next, you will implement logistic regression to predict preferred prompt completions. We will use the preference dataset from <a href="https://huggingface.co/datasets/allenai/reward-bench">RewardBench</a>. Notice the provided <code>data/chosen_embeddings.pt</code> and <code>data/rejected_embeddings.pt</code> files. These files were constructed by feeding the prompt alongside the chosen/rejected responses through Llama3-8B-Instruct and selecting the last token’s final hidden embedding. Let <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> be two hidden embeddings with <span class="math inline">\(e_1\succ e_2\)</span>. We assume weights <span class="math inline">\(w\)</span> exist for which the Bradley-Terry reward of an embedding <span class="math inline">\(e\)</span> can be modeled as <span class="math inline">\(r=w\cdot e\)</span>. In this setting, the probability of <span class="math inline">\(e_1\succ e_2\)</span> is <span class="math display">\[\frac{e^{w\cdot e_1}}{e^{w\cdot e_1}+e^{w\cdot e_2}}=\frac{1}{1+e^{w\cdot(e_2-e_1)}}=\sigma(w\cdot(e_1-e_2)).\]</span> Hence, we can view maximum likelihood across the preference dataset with this model as logistic regression on <span class="math inline">\(e_1-e_2\)</span> without a bias term and all labels being <span class="math inline">\(1\)</span>.</p>
<p>In biasless logistic regression, we are given a dataset <span class="math inline">\(X\)</span> with <span class="math inline">\(N\)</span> rows of datapoints and <span class="math inline">\(D\)</span> features per datapoint. The weights of the model are parametrized by <span class="math inline">\(\theta\)</span>, a <span class="math inline">\(D\)</span>-dimensional column vector. Given binary labels <span class="math inline">\(y\)</span> of shape <span class="math inline">\(N\)</span> by <span class="math inline">\(1\)</span>, the binary cross-entropy loss is <span class="math display">\[J(\theta)=-\frac{1}{N}(y^T\log(\sigma(X\theta)) + (1-y)^T\log(1-\sigma(X\theta)))\]</span> where <span class="math inline">\(\sigma\)</span> is the sigmoid function and is applied element-wise along with <span class="math inline">\(\log\)</span>. The gradient of loss is <span class="math display">\[\nabla_\theta J(\theta)=\frac{1}{N}X^T(\sigma(X\theta)-y).\]</span></p>
<ol type="1">
<li><p><strong>(Coding, 3 points)</strong>. Open the file <code>logistic_regression/logistic_regression.py</code>. Implement the function <code>train</code> in the biasless case.</p></li>
<li><p><strong>(Coding, 2 points)</strong>. Implement the function <code>predict_probs</code>.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Open the notebook <code>rewardbench_preferences.ipynb</code> and run all the cells. Make sure to tune the <code>learning_rate</code> and <code>num_iterations</code>. Report your final expected accuracy on the training and validation sets. How close are the two expected accuracies? You should be able to achieve <span class="math inline">\(\approx 90\%\)</span> expected accuracy on validation. You may add loss reporting to the <code>train</code> function to verify your model is improving over time.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="4a7c4d31" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3"></a></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">class</span> LogisticRegression:</span>
<span id="cb1-5"><a href="#cb1-5"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span>  <span class="co"># Initialized during training</span></span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, learning_rate, num_iterations):</span>
<span id="cb1-9"><a href="#cb1-9"></a>        <span class="co">"""</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">        Train the logistic regression model using gradient descent (no bias).</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">        Each gradient update should be with respect to the entire dataset X.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">        Parameters:</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">        - X (torch.Tensor): Training data of shape (n_samples, n_features).</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">        - y (torch.Tensor): Target labels of shape (n_samples,).</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="co">        """</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb1-18"><a href="#cb1-18"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a>        <span class="co"># Initialize weights without the bias term</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>        <span class="va">self</span>.weights <span class="op">=</span> torch.zeros(n_features)</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb1-23"><a href="#cb1-23"></a>            <span class="co"># YOUR CODE HERE (~4-5 lines)</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>                <span class="cf">pass</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-26"><a href="#cb1-26"></a></span>
<span id="cb1-27"><a href="#cb1-27"></a>    <span class="kw">def</span> predict_probs(<span class="va">self</span>, X):</span>
<span id="cb1-28"><a href="#cb1-28"></a>        <span class="co">"""</span></span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="co">        Predict probabilities for samples in X (no bias).</span></span>
<span id="cb1-30"><a href="#cb1-30"></a></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="co">        Parameters:</span></span>
<span id="cb1-32"><a href="#cb1-32"></a><span class="co">        - X (torch.Tensor): Input data of shape (n_samples, n_features).</span></span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="co">        Returns:</span></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="co">        - y_probs (torch.Tensor): Predicted probabilities.</span></span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="co">        """</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        y_probs <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>        <span class="co"># YOUR CODE HERE (~2-3 lines)</span></span>
<span id="cb1-40"><a href="#cb1-40"></a>        <span class="cf">pass</span></span>
<span id="cb1-41"><a href="#cb1-41"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>        <span class="cf">return</span> y_probs</span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb1-47"><a href="#cb1-47"></a>    <span class="co"># %%</span></span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="co"># Load in Llama3 embeddings of prompt + completions on RewardBench</span></span>
<span id="cb1-49"><a href="#cb1-49"></a>    chosen_embeddings <span class="op">=</span> torch.load(<span class="st">'data/chosen_embeddings.pt'</span>)</span>
<span id="cb1-50"><a href="#cb1-50"></a>    rejected_embeddings <span class="op">=</span> torch.load(<span class="st">'data/rejected_embeddings.pt'</span>)</span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>    <span class="co"># Subtract the embeddings according to the Bradley-Terry reward model setup presented in the problem </span></span>
<span id="cb1-53"><a href="#cb1-53"></a>    X <span class="op">=</span> (chosen_embeddings <span class="op">-</span> rejected_embeddings).to(torch.<span class="bu">float</span>)</span>
<span id="cb1-54"><a href="#cb1-54"></a>    y <span class="op">=</span> torch.ones(X.shape[<span class="dv">0</span>])</span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>    <span class="co"># Split dataset 80/20 into training and validation sets</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>    X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)  </span>
<span id="cb1-58"><a href="#cb1-58"></a></span>
<span id="cb1-59"><a href="#cb1-59"></a>    <span class="bu">print</span>(<span class="st">"Training set size:"</span>, X_train.shape)</span>
<span id="cb1-60"><a href="#cb1-60"></a>    <span class="bu">print</span>(<span class="st">"Validation set size:"</span>, X_val.shape)</span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a>    model <span class="op">=</span> LogisticRegression()</span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a>    <span class="co"># Tune the learning_rate and num_iterations until you achieve expected validation accuracy of at least 90%</span></span>
<span id="cb1-65"><a href="#cb1-65"></a>    learning_rate <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-66"><a href="#cb1-66"></a>    num_iterations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>    model.train(X_train, y_train, learning_rate<span class="op">=</span>learning_rate, num_iterations<span class="op">=</span>num_iterations)</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a>    y_train_probs <span class="op">=</span> model.predict_probs(X_train)</span>
<span id="cb1-71"><a href="#cb1-71"></a>    <span class="bu">print</span>(<span class="ss">f"Expected Train Accuracy: </span><span class="sc">{</span>y_train_probs<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-72"><a href="#cb1-72"></a></span>
<span id="cb1-73"><a href="#cb1-73"></a>    y_val_probs <span class="op">=</span> model.predict_probs(X_val)</span>
<span id="cb1-74"><a href="#cb1-74"></a>    <span class="bu">print</span>(<span class="ss">f"Expected Validation Accuracy: </span><span class="sc">{</span>y_val_probs<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Should reach at least 90%</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-2-revealed-and-stated-preferences-20-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-2-revealed-and-stated-preferences-20-points">Question 2: Revealed and Stated Preferences (20 points)</h3>
<p>Alice and Bob are running for president. For <span class="math inline">\(R\)</span> voters, we have access to their revealed candidate preferences through some means (e.g., social media, blogs, event history). Assume there is an underlying probability <span class="math inline">\(z\)</span> of voting for Alice among the population that is unknown. The aim of this question is to estimate <span class="math inline">\(z\)</span> through <em>maximum likelihood estimation</em> by also incorporating stated preferences. In this scenario, we collect stated preferences through surveys. When surveyed, voters tend to be more likely to vote for Alice with probability <span class="math inline">\(\frac{z+1}{2}\)</span> for reasons of “political correctness.”</p>
<ol type="a">
<li><p><strong>(Written, 5 points)</strong>. Suppose there are <span class="math inline">\(R_A\)</span> revealed preferences for Alice, <span class="math inline">\(R_B\)</span> revealed preferences for Bob, <span class="math inline">\(S_A\)</span> stated preferences for Alice, and <span class="math inline">\(S_B\)</span> stated preferences for Bob. Note <span class="math inline">\(R=R_A+R_B\)</span>. Compute the log-likelihood of observing such preferences in terms of <span class="math inline">\(z, R_A, R_B, S_A, S_B\)</span>.</p></li>
<li><p><strong>(Coding, 1 point)</strong>. Implement the short function <code>stated_prob</code> in the file <code>voting/simulation.py</code>.</p></li>
<li><p><strong>(Coding, 5 points)</strong>. Implement the class <code>VotingSimulation</code>.</p></li>
<li><p><strong>(Coding, 7 points)</strong>. Implement your derived expression from part (a) in the <code>log_likelihoods</code> function.</p></li>
<li><p><strong>(Written, 2 points)</strong>. Finally, implement the <code>average_mae_mle</code> method that will allow us to visualize the mean absolute error (MAE) of our maximum likelihood estimate <span class="math inline">\(\hat{z}\)</span> (i.e., <span class="math inline">\(|\hat{z}-z|\)</span>) as the number of voters surveyed increases. Open <code>voting/visualize_sim.ipynb</code> and run the cells to get a plot of MAE vs.&nbsp;voters surveyed averaged across <span class="math inline">\(100\)</span> simulations. Attach the plot to this question and briefly explain what you notice.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="7d38b47c" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> random</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb2-5"><a href="#cb2-5"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">def</span> stated_prob(z_values):</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="co">"""</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co">    Computes the probability of stated preferences based on z values.</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="co">    </span></span>
<span id="cb2-12"><a href="#cb2-12"></a><span class="co">    Args:</span></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co">        z_values (torch.Tensor): The z value(s), where z represents the true probability of voting for Alice.</span></span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co">    Returns:</span></span>
<span id="cb2-16"><a href="#cb2-16"></a><span class="co">        torch.Tensor: Probability for stated preferences, derived from z values.</span></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="co">    """</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>    <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb2-19"><a href="#cb2-19"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-20"><a href="#cb2-20"></a></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="kw">class</span> VotingSimulation:</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="co">"""</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co">    A class to simulate the voting process where revealed and stated preferences are generated.</span></span>
<span id="cb2-24"><a href="#cb2-24"></a><span class="co">    </span></span>
<span id="cb2-25"><a href="#cb2-25"></a><span class="co">    Attributes:</span></span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="co">        R (int): Number of revealed preferences.</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="co">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb2-28"><a href="#cb2-28"></a><span class="co">        revealed_preferences (torch.Tensor): Simulated revealed preferences of R voters using Bernoulli distribution.</span></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">                                             Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb2-30"><a href="#cb2-30"></a><span class="co">        stated_preferences (torch.Tensor): Simulated stated preferences, initialized as an empty tensor.</span></span>
<span id="cb2-31"><a href="#cb2-31"></a><span class="co">                                           Takes on 1 for Alice, and 0 for Bob.</span></span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="co">    """</span></span>
<span id="cb2-33"><a href="#cb2-33"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, R, z):</span>
<span id="cb2-34"><a href="#cb2-34"></a>        <span class="va">self</span>.R <span class="op">=</span> R</span>
<span id="cb2-35"><a href="#cb2-35"></a>        <span class="va">self</span>.z <span class="op">=</span> z</span>
<span id="cb2-36"><a href="#cb2-36"></a>        <span class="va">self</span>.revealed_preferences <span class="op">=</span> <span class="va">None</span> <span class="co"># YOUR CODE HERE (~1 line)</span></span>
<span id="cb2-37"><a href="#cb2-37"></a>        <span class="va">self</span>.stated_preferences <span class="op">=</span> torch.tensor([])</span>
<span id="cb2-38"><a href="#cb2-38"></a></span>
<span id="cb2-39"><a href="#cb2-39"></a>    <span class="kw">def</span> add_survey(<span class="va">self</span>):</span>
<span id="cb2-40"><a href="#cb2-40"></a>        <span class="co">"""</span></span>
<span id="cb2-41"><a href="#cb2-41"></a><span class="co">        Simulates an additional stated preference based on stated_prob and adds it to the list.</span></span>
<span id="cb2-42"><a href="#cb2-42"></a><span class="co">        This updates the self.stated_preferences tensor by concatenating on a new simulated survey result.</span></span>
<span id="cb2-43"><a href="#cb2-43"></a><span class="co">        """</span></span>
<span id="cb2-44"><a href="#cb2-44"></a>        <span class="co"># YOUR CODE HERE (~3 lines)</span></span>
<span id="cb2-45"><a href="#cb2-45"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-46"><a href="#cb2-46"></a></span>
<span id="cb2-47"><a href="#cb2-47"></a><span class="kw">def</span> log_likelihoods(revealed_preferences, stated_preferences, z_values):</span>
<span id="cb2-48"><a href="#cb2-48"></a>    <span class="co">"""</span></span>
<span id="cb2-49"><a href="#cb2-49"></a><span class="co">    Computes the log likelihoods across both revealed and stated preferences.</span></span>
<span id="cb2-50"><a href="#cb2-50"></a><span class="co">    Use your answer in part (a) to help.</span></span>
<span id="cb2-51"><a href="#cb2-51"></a><span class="co">    </span></span>
<span id="cb2-52"><a href="#cb2-52"></a><span class="co">    Args:</span></span>
<span id="cb2-53"><a href="#cb2-53"></a><span class="co">        revealed_preferences (torch.Tensor): Tensor containing revealed preferences (0 or 1).</span></span>
<span id="cb2-54"><a href="#cb2-54"></a><span class="co">        stated_preferences (torch.Tensor): Tensor containing stated preferences (0 or 1).</span></span>
<span id="cb2-55"><a href="#cb2-55"></a><span class="co">        z_values (torch.Tensor): Tensor of underlying z values to calculate likelihood for.</span></span>
<span id="cb2-56"><a href="#cb2-56"></a></span>
<span id="cb2-57"><a href="#cb2-57"></a><span class="co">    Returns:</span></span>
<span id="cb2-58"><a href="#cb2-58"></a><span class="co">        torch.Tensor: Log likelihood for each z value.</span></span>
<span id="cb2-59"><a href="#cb2-59"></a><span class="co">    """</span></span>
<span id="cb2-60"><a href="#cb2-60"></a>    <span class="co"># YOUR CODE HERE (~10-16 lines)</span></span>
<span id="cb2-61"><a href="#cb2-61"></a>    <span class="cf">pass</span></span>
<span id="cb2-62"><a href="#cb2-62"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb2-63"><a href="#cb2-63"></a></span>
<span id="cb2-64"><a href="#cb2-64"></a><span class="kw">def</span> average_mae_mle(R, z, survey_count, num_sims, z_sweep):</span>
<span id="cb2-65"><a href="#cb2-65"></a>    <span class="co">"""</span></span>
<span id="cb2-66"><a href="#cb2-66"></a><span class="co">    Runs multiple simulations to compute the average mean absolute error (MAE) of Maximum Likelihood Estimation (MLE) </span></span>
<span id="cb2-67"><a href="#cb2-67"></a><span class="co">    for z after increasing number of surveys.</span></span>
<span id="cb2-68"><a href="#cb2-68"></a><span class="co">    </span></span>
<span id="cb2-69"><a href="#cb2-69"></a><span class="co">    Args:</span></span>
<span id="cb2-70"><a href="#cb2-70"></a><span class="co">        R (int): Number of revealed preferences.</span></span>
<span id="cb2-71"><a href="#cb2-71"></a><span class="co">        z (float): The true probability of voting for Alice.</span></span>
<span id="cb2-72"><a href="#cb2-72"></a><span class="co">        survey_count (int): Number of additional surveys to perform.</span></span>
<span id="cb2-73"><a href="#cb2-73"></a><span class="co">        num_sims (int): Number of simulation runs to average over.</span></span>
<span id="cb2-74"><a href="#cb2-74"></a><span class="co">        z_sweep (torch.Tensor): Range of z values to consider for maximum likelihood estimation.</span></span>
<span id="cb2-75"><a href="#cb2-75"></a></span>
<span id="cb2-76"><a href="#cb2-76"></a><span class="co">    Returns:</span></span>
<span id="cb2-77"><a href="#cb2-77"></a><span class="co">        torch.Tensor: Tensor of mean absolute errors averaged over simulations.</span></span>
<span id="cb2-78"><a href="#cb2-78"></a><span class="co">                      Should have shape (survey_count, )</span></span>
<span id="cb2-79"><a href="#cb2-79"></a><span class="co">    """</span></span>
<span id="cb2-80"><a href="#cb2-80"></a>    all_errors <span class="op">=</span> []</span>
<span id="cb2-81"><a href="#cb2-81"></a>    <span class="cf">for</span> _ <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_sims)):</span>
<span id="cb2-82"><a href="#cb2-82"></a>        errors <span class="op">=</span> []</span>
<span id="cb2-83"><a href="#cb2-83"></a>        vote_simulator <span class="op">=</span> VotingSimulation(R<span class="op">=</span>R, z<span class="op">=</span>z)</span>
<span id="cb2-84"><a href="#cb2-84"></a></span>
<span id="cb2-85"><a href="#cb2-85"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(survey_count):</span>
<span id="cb2-86"><a href="#cb2-86"></a>            revealed_preferences <span class="op">=</span> vote_simulator.revealed_preferences</span>
<span id="cb2-87"><a href="#cb2-87"></a>            stated_preferences <span class="op">=</span> vote_simulator.stated_preferences</span>
<span id="cb2-88"><a href="#cb2-88"></a></span>
<span id="cb2-89"><a href="#cb2-89"></a>            <span class="co"># YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb2-90"><a href="#cb2-90"></a>            <span class="cf">pass</span> <span class="co"># Compute log_likelihoods across z_sweep. Argmax to find MLE for z. </span></span>
<span id="cb2-91"><a href="#cb2-91"></a>                 <span class="co"># Append the absolute error to errors and add a survey to the simulator.</span></span>
<span id="cb2-92"><a href="#cb2-92"></a>            <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb2-93"><a href="#cb2-93"></a></span>
<span id="cb2-94"><a href="#cb2-94"></a>        errors_tensor <span class="op">=</span> torch.stack(errors) </span>
<span id="cb2-95"><a href="#cb2-95"></a>        all_errors.append(errors_tensor)</span>
<span id="cb2-96"><a href="#cb2-96"></a></span>
<span id="cb2-97"><a href="#cb2-97"></a>    <span class="co"># Calculate the average error across simulations </span></span>
<span id="cb2-98"><a href="#cb2-98"></a>    mean_errors <span class="op">=</span> torch.stack(all_errors).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-99"><a href="#cb2-99"></a>    <span class="cf">return</span> mean_errors</span>
<span id="cb2-100"><a href="#cb2-100"></a></span>
<span id="cb2-101"><a href="#cb2-101"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb2-102"><a href="#cb2-102"></a>    <span class="co"># DO NOT CHANGE!</span></span>
<span id="cb2-103"><a href="#cb2-103"></a>    max_surveys <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb2-104"><a href="#cb2-104"></a>    z <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb2-105"><a href="#cb2-105"></a>    R <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-106"><a href="#cb2-106"></a>    num_sims <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-107"><a href="#cb2-107"></a>    z_sweep <span class="op">=</span> torch.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">981</span>)</span>
<span id="cb2-108"><a href="#cb2-108"></a></span>
<span id="cb2-109"><a href="#cb2-109"></a>    <span class="co"># Compute and plot the errors. Attach this plot to part (d).</span></span>
<span id="cb2-110"><a href="#cb2-110"></a>    mean_errors <span class="op">=</span> average_mae_mle(R, z, max_surveys, num_sims, z_sweep)</span>
<span id="cb2-111"><a href="#cb2-111"></a>    plt.plot(mean_errors)</span>
<span id="cb2-112"><a href="#cb2-112"></a></span>
<span id="cb2-113"><a href="#cb2-113"></a>    plt.xlabel(<span class="st">'Surveys Conducted'</span>)</span>
<span id="cb2-114"><a href="#cb2-114"></a>    plt.ylabel(<span class="st">'Average Error'</span>)</span>
<span id="cb2-115"><a href="#cb2-115"></a>    plt.title(<span class="ss">f'MLE MAE Error (z=</span><span class="sc">{</span>z<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>num_sims<span class="sc">}</span><span class="ss"> simulations)'</span>)</span>
<span id="cb2-116"><a href="#cb2-116"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-3-probabilistic-multi-modal-preferences-25-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-3-probabilistic-multi-modal-preferences-25-points">Question 3: Probabilistic Multi-modal Preferences (25 points)</h3>
<p>Suppose you are part of the ML team on the movie streaming site CardinalStreams. After taking CS329H, you collect a movie preferences dataset with <span class="math inline">\(30000\)</span> examples of the form <span class="math inline">\((m_1, m_2, \text{user id})\)</span> where <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span> are movies with <span class="math inline">\(m_1\succ m_2\)</span>. The preferences come from <span class="math inline">\(600\)</span> distinct users with <span class="math inline">\(50\)</span> examples per user. Each movie has a <span class="math inline">\(10\)</span>-dimensional feature vector <span class="math inline">\(m\)</span>, and each user has a <span class="math inline">\(10\)</span>-dimensional weight vector <span class="math inline">\(u\)</span>. Given movie features <span class="math inline">\(m_1, m_2\)</span> and user weights <span class="math inline">\(u\)</span>, the user’s preference between the movies is given by a Bradley-Terry reward model, i.e., <span class="math display">\[P(m_1\succ m_2)=\frac{e^{u\cdot m_1}}{e^{u\cdot m_1} + e^{u\cdot m_2}}=\frac{1}{1+e^{u\cdot (m_2-m_1)}}=\sigma(u\cdot (m_1-m_2)).\]</span></p>
<p>You realize that trying to estimate the weights for each user with only <span class="math inline">\(50\)</span> examples will not work due to the lack of data. Instead, you choose to drop the user IDs column and shuffle the dataset in order to take a <em>multi-modal preferences</em> approach. For simplicity, you assume a model where a proportion <span class="math inline">\(p\)</span> of the users have weights <span class="math inline">\(w_1\)</span> and the other <span class="math inline">\(1-p\)</span> have weights <span class="math inline">\(w_2\)</span>. In this setting, each user belongs to one of two groups: users with weights <span class="math inline">\(w_1\)</span> are part of Group 1, and users with weights <span class="math inline">\(w_2\)</span> are part of Group 2.</p>
<ol type="a">
<li><p><strong>(Written, 3 points)</strong>. For a datapoint <span class="math inline">\((m_1, m_2)\)</span> with label <span class="math inline">\(m_1\succ m_2\)</span>, compute the data likelihood <span class="math inline">\(P(m_1\succ m_2 | p, w_1, w_2)\)</span> assuming <span class="math inline">\(p, w_1, w_2\)</span> are given.</p></li>
<li><p><strong>(Written, 3 points)</strong>. As a follow up, use the likelihood to simplify the posterior distribution of <span class="math inline">\(p, w_1, w_2\)</span> after updating on <span class="math inline">\((m_1, m_2)\)</span> leaving terms for the priors unchanged.</p></li>
<li><p><strong>(Written, 4 points)</strong>. Assume priors <span class="math inline">\(p\sim B(1, 1)\)</span>, <span class="math inline">\(w_1\sim\mathcal{N}(0, \mathbf{I})\)</span>, and <span class="math inline">\(w_2\sim\mathcal{N}(0, \mathbf{I})\)</span> where <span class="math inline">\(B\)</span> represents the Beta distribution and <span class="math inline">\(\mathcal{N}\)</span> represents the normal distribution. You will notice that the posterior from part (b) has no simple closed-form. As a result, we must resort to <em>Markov Chain Monte Carlo (MCMC)</em> approaches to sample from the posterior. These approaches allow sampling from highly complex distributions by constructing a Markov chain <span class="math inline">\(\{x_t\}_{t=1}^\infty\)</span> so that <span class="math inline">\(\lim_{t\to\infty}x_t\)</span> act as desired samples from the target distribution. You can think of a Markov chain as a sequence with the special property that <span class="math inline">\(x_{t+1}\)</span> only depends on <span class="math inline">\(x_t\)</span> for all <span class="math inline">\(t\ge 1\)</span>.</p>
<p>The most basic version of MCMC is known as Metropolis-Hastings. Assume <span class="math inline">\(\pi\)</span> is the target distribution we wish to sample from where <span class="math inline">\(\pi(z)\)</span> represents the probability density at point <span class="math inline">\(z\)</span>. Metropolis-Hastings constructs the approximating Markov chain <span class="math inline">\(x_t\)</span> as follows: a proposal <span class="math inline">\(P\)</span> for <span class="math inline">\(x_{t+1}\)</span> is made via sampling from a chosen distribution <span class="math inline">\(Q(\,\cdot\,| x_t)\)</span> (e.g., adding Gaussian noise). The acceptance probability of the proposal is given by <span class="math display">\[A= \min \left( 1, \frac{\pi(P)Q(x_t | P)}{\pi(x_t)Q(P | x_t)} \right).\]</span> That is, <span class="math display">\[x_{t+1}=\begin{cases}
P &amp; \text{with probability } A, \\
x_t &amp; \text{with probability } 1 - A.
\end{cases}\]</span> To extract our samples from <span class="math inline">\(\pi\)</span>, we run the Markov chain for <span class="math inline">\(N\)</span> timesteps and disregard the first <span class="math inline">\(T&lt;N\)</span> timesteps in what is called the <em>burn-in or mixing time</em> (i.e., our final samples are <span class="math inline">\(x_{T+1}, x_{T+2},\cdots, x_{N}\)</span>). The mixing time is needed to ensure that the Markov chain elements are representative of the distribution <span class="math inline">\(\pi\)</span> – initial elements of the chain will not be a good approximation of <span class="math inline">\(\pi\)</span> and depend more on the choice of initialization <span class="math inline">\(x_1\)</span>.</p>
<p>To build some intuition, suppose we have a biased coin that turns heads with probability <span class="math inline">\(p_{\text{heads}}\)</span>. We observe <span class="math inline">\(12\)</span> coin flips to have <span class="math inline">\(9\)</span> heads and <span class="math inline">\(3\)</span> tails. If our prior for <span class="math inline">\(p_{\text{heads}}\)</span> was <span class="math inline">\(B(1, 1)\)</span>, then our posterior will be <span class="math inline">\(B(1+9, 1+3)=B(10, 4)\)</span>. The Bayesian update is given by</p>
<p><span class="math display">\[\begin{aligned}
    P(p_{\text{heads}}|9\text{ heads}, 3\text{ tails})&amp;=\frac{P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})B(1, 1)(p_{\text{heads}})}{\int_0^1 P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})B(1, 1)(p_{\text{heads}}) dp_{\text{heads}}}\\
    &amp;=\frac{P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})}{\int_0^1 P(9\text{ heads}, 3\text{ tails} | p_{\text{heads}})  dp_{\text{heads}}}.
\end{aligned}\]</span></p>
<p><strong>Find the acceptance probablity</strong> <span class="math inline">\(A\)</span> in the setting of the biased coin assuming the proposal distribution <span class="math inline">\(Q(\cdot|x_t)=x_t+N(0,\sigma)\)</span> for given <span class="math inline">\(\sigma\)</span>. Notice that this choice of <span class="math inline">\(Q\)</span> is symmetric, i.e., <span class="math inline">\(Q(x_t|P)=Q(P|x_t)\)</span>. In addition, you will realize that is unnecessary to compute the normalizing constant of the Bayesian update (i.e., the integral in the denominator) which is why MCMC is commonly used to sample from posteriors!</p></li>
<li><p><strong>(Written + Coding, 6 points)</strong>. Implement Metropolis-Hastings to sample from the posterior distribution of the biased coin in <code>multimodal_preferences/biased_coin.py</code>. Attach a histogram of your MCMC samples overlayed on top of the true posterior <span class="math inline">\(B(10, 4)\)</span> by running <code>python biased_coin.py</code>.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="6e274b6d" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="kw">def</span> likelihood(p: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-6"><a href="#cb3-6"></a>    <span class="co">"""</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">    Computes the likelihood of 9 heads and 3 tails assuming p_heads is p.</span></span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">    Args:</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">    p (float): A value between 0 and 1 representing the probability of heads.</span></span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    Returns:</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    float: The likelihood value at p_heads=p. Return 0 if p is outside the range [0, 1].</span></span>
<span id="cb3-14"><a href="#cb3-14"></a><span class="co">    """</span></span>
<span id="cb3-15"><a href="#cb3-15"></a>    <span class="co"># YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb3-16"><a href="#cb3-16"></a>    <span class="cf">pass</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-18"><a href="#cb3-18"></a></span>
<span id="cb3-19"><a href="#cb3-19"></a></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="kw">def</span> propose(x_current: <span class="bu">float</span>, sigma: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-21"><a href="#cb3-21"></a>    <span class="co">"""</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="co">    Proposes a new sample from the proposal distribution Q.</span></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">    Here, Q is a normal distribution centered at x_current with standard deviation sigma.</span></span>
<span id="cb3-24"><a href="#cb3-24"></a></span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="co">    Args:</span></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">    sigma (float): Standard deviation of the normal proposal distribution.</span></span>
<span id="cb3-28"><a href="#cb3-28"></a></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co">    Returns:</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">    float: The proposed new sample.</span></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">    """</span></span>
<span id="cb3-32"><a href="#cb3-32"></a>    <span class="co"># YOUR CODE HERE (~1-3 lines)</span></span>
<span id="cb3-33"><a href="#cb3-33"></a>    <span class="cf">pass</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-35"><a href="#cb3-35"></a></span>
<span id="cb3-36"><a href="#cb3-36"></a></span>
<span id="cb3-37"><a href="#cb3-37"></a><span class="kw">def</span> acceptance_probability(x_current: <span class="bu">float</span>, x_proposed: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-38"><a href="#cb3-38"></a>    <span class="co">"""</span></span>
<span id="cb3-39"><a href="#cb3-39"></a><span class="co">    Computes the acceptance probability A for the proposed sample.</span></span>
<span id="cb3-40"><a href="#cb3-40"></a><span class="co">    Since the proposal distribution is symmetric, Q cancels out.</span></span>
<span id="cb3-41"><a href="#cb3-41"></a></span>
<span id="cb3-42"><a href="#cb3-42"></a><span class="co">    Args:</span></span>
<span id="cb3-43"><a href="#cb3-43"></a><span class="co">    x_current (float): The current value in the Markov chain.</span></span>
<span id="cb3-44"><a href="#cb3-44"></a><span class="co">    x_proposed (float): The proposed new value.</span></span>
<span id="cb3-45"><a href="#cb3-45"></a></span>
<span id="cb3-46"><a href="#cb3-46"></a><span class="co">    Returns:</span></span>
<span id="cb3-47"><a href="#cb3-47"></a><span class="co">    float: The acceptance probability</span></span>
<span id="cb3-48"><a href="#cb3-48"></a><span class="co">    """</span></span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb3-50"><a href="#cb3-50"></a>    <span class="cf">pass</span></span>
<span id="cb3-51"><a href="#cb3-51"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-52"><a href="#cb3-52"></a></span>
<span id="cb3-53"><a href="#cb3-53"></a></span>
<span id="cb3-54"><a href="#cb3-54"></a><span class="kw">def</span> metropolis_hastings(N: <span class="bu">int</span>, T: <span class="bu">int</span>, x_init: <span class="bu">float</span>, sigma: <span class="bu">float</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-55"><a href="#cb3-55"></a>    <span class="co">"""</span></span>
<span id="cb3-56"><a href="#cb3-56"></a><span class="co">    Runs the Metropolis-Hastings algorithm to sample from a posterior distribution.</span></span>
<span id="cb3-57"><a href="#cb3-57"></a></span>
<span id="cb3-58"><a href="#cb3-58"></a><span class="co">    Args:</span></span>
<span id="cb3-59"><a href="#cb3-59"></a><span class="co">    N (int): Total number of iterations.</span></span>
<span id="cb3-60"><a href="#cb3-60"></a><span class="co">    T (int): Burn-in period (number of initial samples to discard).</span></span>
<span id="cb3-61"><a href="#cb3-61"></a><span class="co">    x_init (float): Initial value of the chain.</span></span>
<span id="cb3-62"><a href="#cb3-62"></a><span class="co">    sigma (float): Standard deviation of the proposal distribution.</span></span>
<span id="cb3-63"><a href="#cb3-63"></a></span>
<span id="cb3-64"><a href="#cb3-64"></a><span class="co">    Returns:</span></span>
<span id="cb3-65"><a href="#cb3-65"></a><span class="co">    list: Samples collected after the burn-in period.</span></span>
<span id="cb3-66"><a href="#cb3-66"></a><span class="co">    """</span></span>
<span id="cb3-67"><a href="#cb3-67"></a>    samples <span class="op">=</span> []</span>
<span id="cb3-68"><a href="#cb3-68"></a>    x_current <span class="op">=</span> x_init</span>
<span id="cb3-69"><a href="#cb3-69"></a></span>
<span id="cb3-70"><a href="#cb3-70"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb3-71"><a href="#cb3-71"></a>        <span class="co"># YOUR CODE HERE (~7-10 lines)</span></span>
<span id="cb3-72"><a href="#cb3-72"></a>        <span class="co"># Use the propose and acceptance_probability functions to get x_{t+1} and store it in samples after the burn-in period T</span></span>
<span id="cb3-73"><a href="#cb3-73"></a>        <span class="cf">pass</span></span>
<span id="cb3-74"><a href="#cb3-74"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb3-75"><a href="#cb3-75"></a></span>
<span id="cb3-76"><a href="#cb3-76"></a>    <span class="cf">return</span> samples</span>
<span id="cb3-77"><a href="#cb3-77"></a></span>
<span id="cb3-78"><a href="#cb3-78"></a></span>
<span id="cb3-79"><a href="#cb3-79"></a><span class="kw">def</span> plot_results(samples: np.ndarray) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-80"><a href="#cb3-80"></a>    <span class="co">"""</span></span>
<span id="cb3-81"><a href="#cb3-81"></a><span class="co">    Plots the histogram of MCMC samples along with the true Beta(10, 4) PDF.</span></span>
<span id="cb3-82"><a href="#cb3-82"></a></span>
<span id="cb3-83"><a href="#cb3-83"></a><span class="co">    Args:</span></span>
<span id="cb3-84"><a href="#cb3-84"></a><span class="co">    samples (np.ndarray): Array of samples collected from the Metropolis-Hastings algorithm.</span></span>
<span id="cb3-85"><a href="#cb3-85"></a></span>
<span id="cb3-86"><a href="#cb3-86"></a><span class="co">    Returns:</span></span>
<span id="cb3-87"><a href="#cb3-87"></a><span class="co">    None</span></span>
<span id="cb3-88"><a href="#cb3-88"></a><span class="co">    """</span></span>
<span id="cb3-89"><a href="#cb3-89"></a>    <span class="co"># Histogram of the samples from the Metropolis-Hastings algorithm</span></span>
<span id="cb3-90"><a href="#cb3-90"></a>    plt.hist(samples, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">"MCMC Samples"</span>)</span>
<span id="cb3-91"><a href="#cb3-91"></a></span>
<span id="cb3-92"><a href="#cb3-92"></a>    <span class="co"># True Beta(10, 4) distribution for comparison</span></span>
<span id="cb3-93"><a href="#cb3-93"></a>    p <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb3-94"><a href="#cb3-94"></a>    beta_pdf <span class="op">=</span> beta.pdf(p, <span class="dv">10</span>, <span class="dv">4</span>)</span>
<span id="cb3-95"><a href="#cb3-95"></a>    plt.plot(p, beta_pdf, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">"Beta(10, 4) PDF"</span>)</span>
<span id="cb3-96"><a href="#cb3-96"></a></span>
<span id="cb3-97"><a href="#cb3-97"></a>    plt.xlabel(<span class="st">"p_heads"</span>)</span>
<span id="cb3-98"><a href="#cb3-98"></a>    plt.ylabel(<span class="st">"Density"</span>)</span>
<span id="cb3-99"><a href="#cb3-99"></a>    plt.title(<span class="st">"Metropolis-Hastings Sampling of Biased Coin Posterior"</span>)</span>
<span id="cb3-100"><a href="#cb3-100"></a>    plt.legend()</span>
<span id="cb3-101"><a href="#cb3-101"></a>    plt.show()</span>
<span id="cb3-102"><a href="#cb3-102"></a></span>
<span id="cb3-103"><a href="#cb3-103"></a></span>
<span id="cb3-104"><a href="#cb3-104"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb3-105"><a href="#cb3-105"></a>    <span class="co"># MCMC Parameters (DO NOT CHANGE!)</span></span>
<span id="cb3-106"><a href="#cb3-106"></a>    N <span class="op">=</span> <span class="dv">50000</span>  <span class="co"># Total number of iterations</span></span>
<span id="cb3-107"><a href="#cb3-107"></a>    T <span class="op">=</span> <span class="dv">10000</span>  <span class="co"># Burn-in period to discard</span></span>
<span id="cb3-108"><a href="#cb3-108"></a>    x_init <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Initial guess for p_heads</span></span>
<span id="cb3-109"><a href="#cb3-109"></a>    sigma <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># Standard deviation of the proposal distribution</span></span>
<span id="cb3-110"><a href="#cb3-110"></a></span>
<span id="cb3-111"><a href="#cb3-111"></a>    <span class="co"># Run Metropolis-Hastings and plot the results</span></span>
<span id="cb3-112"><a href="#cb3-112"></a>    samples <span class="op">=</span> metropolis_hastings(N, T, x_init, sigma)</span>
<span id="cb3-113"><a href="#cb3-113"></a>    plot_results(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<ol start="5" type="a">
<li><strong>(Coding, 9 points)</strong>. Implement Metropolis-Hastings in the movie setting inside<br>
<code>multimodal_preferences/movie_metropolis.py</code>. The movie dataset we use for grading will not be provided. However, randomly constructed datasets can be used to test your implementation by running <code>python movie_metropolis.py</code>. You should be able to achieve a <span class="math inline">\(90\%\)</span> success rate with most <code>fraction_accepted</code> values above <span class="math inline">\(0.1\)</span>. Success is measured by thresholded closeness of predicted parameters to true parameters. You may notice occasional failures that occur due to lack of convergence which we will account for in grading.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="6d3c98a6" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> torch.distributions <span class="im">as</span> dist</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> math</span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a><span class="kw">def</span> make_data(</span>
<span id="cb4-8"><a href="#cb4-8"></a>    true_p: torch.Tensor, true_weights_1: torch.Tensor, true_weights_2: torch.Tensor, num_movies: <span class="bu">int</span>, feature_dim: <span class="bu">int</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor]:</span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="co">"""</span></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co">    Generates a synthetic movie dataset according to the CardinalStreams model.</span></span>
<span id="cb4-12"><a href="#cb4-12"></a></span>
<span id="cb4-13"><a href="#cb4-13"></a><span class="co">    Args:</span></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co">        true_p (torch.Tensor): Probability of coming from Group 1.</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="co">        true_weights_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb4-16"><a href="#cb4-16"></a><span class="co">        true_weights_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb4-17"><a href="#cb4-17"></a></span>
<span id="cb4-18"><a href="#cb4-18"></a><span class="co">    Returns:</span></span>
<span id="cb4-19"><a href="#cb4-19"></a><span class="co">        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the dataset and labels.</span></span>
<span id="cb4-20"><a href="#cb4-20"></a><span class="co">    """</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>    <span class="co"># Create movie features</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    first_movie_features <span class="op">=</span> torch.randn((num_movies, feature_dim))</span>
<span id="cb4-23"><a href="#cb4-23"></a>    second_movie_features <span class="op">=</span> torch.randn((num_movies, feature_dim))</span>
<span id="cb4-24"><a href="#cb4-24"></a></span>
<span id="cb4-25"><a href="#cb4-25"></a>    <span class="co"># Only care about difference of features for Bradley-Terry</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>    dataset <span class="op">=</span> first_movie_features <span class="op">-</span> second_movie_features</span>
<span id="cb4-27"><a href="#cb4-27"></a></span>
<span id="cb4-28"><a href="#cb4-28"></a>    <span class="co"># Get probabilities that first movie is preferred assuming Group 1 or Group 2</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>    weight_1_probs <span class="op">=</span> torch.sigmoid(dataset <span class="op">@</span> true_weights_1)</span>
<span id="cb4-30"><a href="#cb4-30"></a>    weight_2_probs <span class="op">=</span> torch.sigmoid(dataset <span class="op">@</span> true_weights_2)</span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>    <span class="co"># Probability that first movie is preferred overall can be viewed as sum of conditioning on Group 1 and Group 2</span></span>
<span id="cb4-33"><a href="#cb4-33"></a>    first_movie_preferred_probs <span class="op">=</span> (</span>
<span id="cb4-34"><a href="#cb4-34"></a>        true_p <span class="op">*</span> weight_1_probs <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> true_p) <span class="op">*</span> weight_2_probs</span>
<span id="cb4-35"><a href="#cb4-35"></a>    )</span>
<span id="cb4-36"><a href="#cb4-36"></a>    labels <span class="op">=</span> dist.Bernoulli(first_movie_preferred_probs).sample()</span>
<span id="cb4-37"><a href="#cb4-37"></a>    <span class="cf">return</span> dataset, labels</span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="kw">def</span> compute_likelihoods(</span>
<span id="cb4-41"><a href="#cb4-41"></a>    dataset: torch.Tensor,</span>
<span id="cb4-42"><a href="#cb4-42"></a>    labels: torch.Tensor,</span>
<span id="cb4-43"><a href="#cb4-43"></a>    p: torch.Tensor,</span>
<span id="cb4-44"><a href="#cb4-44"></a>    w_1: torch.Tensor,</span>
<span id="cb4-45"><a href="#cb4-45"></a>    w_2: torch.Tensor,</span>
<span id="cb4-46"><a href="#cb4-46"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-47"><a href="#cb4-47"></a>    <span class="co">"""</span></span>
<span id="cb4-48"><a href="#cb4-48"></a><span class="co">    Computes the likelihood of each datapoint. Use your calculation from part (a) to help.</span></span>
<span id="cb4-49"><a href="#cb4-49"></a></span>
<span id="cb4-50"><a href="#cb4-50"></a><span class="co">    Args:</span></span>
<span id="cb4-51"><a href="#cb4-51"></a><span class="co">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb4-52"><a href="#cb4-52"></a><span class="co">        labels (torch.Tensor): The labels where 1 indicates the first movie is preferred, and 0 indicates preference of the second movie.</span></span>
<span id="cb4-53"><a href="#cb4-53"></a><span class="co">        p (torch.Tensor): The probability of coming from Group 1.</span></span>
<span id="cb4-54"><a href="#cb4-54"></a><span class="co">        w_1 (torch.Tensor): Weights for Group 1.</span></span>
<span id="cb4-55"><a href="#cb4-55"></a><span class="co">        w_2 (torch.Tensor): Weights for Group 2.</span></span>
<span id="cb4-56"><a href="#cb4-56"></a></span>
<span id="cb4-57"><a href="#cb4-57"></a><span class="co">    Returns:</span></span>
<span id="cb4-58"><a href="#cb4-58"></a><span class="co">        torch.Tensor: The likelihoods for each datapoint. Should have shape (dataset.shape[0], )</span></span>
<span id="cb4-59"><a href="#cb4-59"></a><span class="co">    """</span></span>
<span id="cb4-60"><a href="#cb4-60"></a>    <span class="co"># YOUR CODE HERE (~6-8 lines)</span></span>
<span id="cb4-61"><a href="#cb4-61"></a>    <span class="cf">pass</span></span>
<span id="cb4-62"><a href="#cb4-62"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-63"><a href="#cb4-63"></a></span>
<span id="cb4-64"><a href="#cb4-64"></a><span class="kw">def</span> compute_prior_density(</span>
<span id="cb4-65"><a href="#cb4-65"></a>    p: torch.Tensor, w_1: torch.Tensor, w_2: torch.Tensor</span>
<span id="cb4-66"><a href="#cb4-66"></a>) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-67"><a href="#cb4-67"></a>    <span class="co">"""</span></span>
<span id="cb4-68"><a href="#cb4-68"></a><span class="co">    Computes the prior density of the parameters.</span></span>
<span id="cb4-69"><a href="#cb4-69"></a></span>
<span id="cb4-70"><a href="#cb4-70"></a><span class="co">    Args:</span></span>
<span id="cb4-71"><a href="#cb4-71"></a><span class="co">        p (torch.Tensor): The probability of preferring model 1.</span></span>
<span id="cb4-72"><a href="#cb4-72"></a><span class="co">        w_1 (torch.Tensor): Weights for model 1.</span></span>
<span id="cb4-73"><a href="#cb4-73"></a><span class="co">        w_2 (torch.Tensor): Weights for model 2.</span></span>
<span id="cb4-74"><a href="#cb4-74"></a></span>
<span id="cb4-75"><a href="#cb4-75"></a><span class="co">    Returns:</span></span>
<span id="cb4-76"><a href="#cb4-76"></a><span class="co">        torch.Tensor: The prior densities of p, w_1, and w_2.</span></span>
<span id="cb4-77"><a href="#cb4-77"></a><span class="co">    """</span></span>
<span id="cb4-78"><a href="#cb4-78"></a>    <span class="co"># Adjusts p to stay in the range [0.3, 0.7] to prevent multiple equilibria issues at p=0 and p=1</span></span>
<span id="cb4-79"><a href="#cb4-79"></a>    p_prob <span class="op">=</span> torch.tensor([<span class="fl">2.5</span>]) <span class="cf">if</span> <span class="fl">0.3</span> <span class="op">&lt;=</span> p <span class="op">&lt;=</span> <span class="fl">0.7</span> <span class="cf">else</span> torch.tensor([<span class="fl">0.0</span>])</span>
<span id="cb4-80"><a href="#cb4-80"></a></span>
<span id="cb4-81"><a href="#cb4-81"></a>    <span class="kw">def</span> normal_pdf(x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb4-82"><a href="#cb4-82"></a>        <span class="co">"""Computes the PDF of the standard normal distribution at x."""</span></span>
<span id="cb4-83"><a href="#cb4-83"></a>        <span class="cf">return</span> (<span class="fl">1.0</span> <span class="op">/</span> torch.sqrt(torch.tensor(<span class="dv">2</span> <span class="op">*</span> math.pi))) <span class="op">*</span> torch.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb4-84"><a href="#cb4-84"></a></span>
<span id="cb4-85"><a href="#cb4-85"></a>    weights_1_prob <span class="op">=</span> normal_pdf(w_1)</span>
<span id="cb4-86"><a href="#cb4-86"></a>    weights_2_prob <span class="op">=</span> normal_pdf(w_2)</span>
<span id="cb4-87"><a href="#cb4-87"></a></span>
<span id="cb4-88"><a href="#cb4-88"></a>    <span class="co"># Concatenate the densities</span></span>
<span id="cb4-89"><a href="#cb4-89"></a>    concatenated_prob <span class="op">=</span> torch.cat([p_prob, weights_1_prob, weights_2_prob])</span>
<span id="cb4-90"><a href="#cb4-90"></a>    <span class="cf">return</span> concatenated_prob</span>
<span id="cb4-91"><a href="#cb4-91"></a></span>
<span id="cb4-92"><a href="#cb4-92"></a></span>
<span id="cb4-93"><a href="#cb4-93"></a><span class="kw">def</span> metropolis_hastings(</span>
<span id="cb4-94"><a href="#cb4-94"></a>    dataset: torch.Tensor,</span>
<span id="cb4-95"><a href="#cb4-95"></a>    labels: torch.Tensor,</span>
<span id="cb4-96"><a href="#cb4-96"></a>    sigma: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.01</span>,</span>
<span id="cb4-97"><a href="#cb4-97"></a>    num_iters: <span class="bu">int</span> <span class="op">=</span> <span class="dv">30000</span>,</span>
<span id="cb4-98"><a href="#cb4-98"></a>    burn_in: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20000</span>,</span>
<span id="cb4-99"><a href="#cb4-99"></a>) <span class="op">-&gt;</span> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, <span class="bu">float</span>]:</span>
<span id="cb4-100"><a href="#cb4-100"></a>    <span class="co">"""</span></span>
<span id="cb4-101"><a href="#cb4-101"></a><span class="co">    Performs the Metropolis-Hastings algorithm to sample from the posterior distribution.</span></span>
<span id="cb4-102"><a href="#cb4-102"></a><span class="co">    DO NOT CHANGE THE DEFAULT VALUES!</span></span>
<span id="cb4-103"><a href="#cb4-103"></a></span>
<span id="cb4-104"><a href="#cb4-104"></a><span class="co">    Args:</span></span>
<span id="cb4-105"><a href="#cb4-105"></a><span class="co">        dataset (torch.Tensor): The dataset of differences between movie features.</span></span>
<span id="cb4-106"><a href="#cb4-106"></a><span class="co">        labels (torch.Tensor): The labels indicating which movie is preferred.</span></span>
<span id="cb4-107"><a href="#cb4-107"></a><span class="co">        sigma (float, optional): Standard deviation for proposal distribution.</span></span>
<span id="cb4-108"><a href="#cb4-108"></a><span class="co">            Defaults to 0.01.</span></span>
<span id="cb4-109"><a href="#cb4-109"></a><span class="co">        num_iters (int, optional): Total number of iterations. Defaults to 30000.</span></span>
<span id="cb4-110"><a href="#cb4-110"></a><span class="co">        burn_in (int, optional): Number of iterations to discard as burn-in.</span></span>
<span id="cb4-111"><a href="#cb4-111"></a><span class="co">            Defaults to 20000.</span></span>
<span id="cb4-112"><a href="#cb4-112"></a></span>
<span id="cb4-113"><a href="#cb4-113"></a><span class="co">    Returns:</span></span>
<span id="cb4-114"><a href="#cb4-114"></a><span class="co">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]: Samples of p,</span></span>
<span id="cb4-115"><a href="#cb4-115"></a><span class="co">        w_1, w_2, and the fraction of accepted proposals.</span></span>
<span id="cb4-116"><a href="#cb4-116"></a><span class="co">    """</span></span>
<span id="cb4-117"><a href="#cb4-117"></a>    feature_dim <span class="op">=</span> dataset.shape[<span class="dv">1</span>]</span>
<span id="cb4-118"><a href="#cb4-118"></a></span>
<span id="cb4-119"><a href="#cb4-119"></a>    <span class="co"># Initialize random starting parameters by sampling priors</span></span>
<span id="cb4-120"><a href="#cb4-120"></a>    curr_p <span class="op">=</span> <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> torch.rand(<span class="dv">1</span>)</span>
<span id="cb4-121"><a href="#cb4-121"></a>    curr_w_1 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-122"><a href="#cb4-122"></a>    curr_w_2 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-123"><a href="#cb4-123"></a></span>
<span id="cb4-124"><a href="#cb4-124"></a>    <span class="co"># Keep track of samples and total number of accepted proposals</span></span>
<span id="cb4-125"><a href="#cb4-125"></a>    p_samples <span class="op">=</span> []</span>
<span id="cb4-126"><a href="#cb4-126"></a>    w_1_samples <span class="op">=</span> []</span>
<span id="cb4-127"><a href="#cb4-127"></a>    w_2_samples <span class="op">=</span> []</span>
<span id="cb4-128"><a href="#cb4-128"></a>    accept_count <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb4-129"><a href="#cb4-129"></a></span>
<span id="cb4-130"><a href="#cb4-130"></a>    <span class="cf">for</span> T <span class="kw">in</span> tqdm(<span class="bu">range</span>(num_iters)):</span>
<span id="cb4-131"><a href="#cb4-131"></a>        <span class="co"># YOUR CODE HERE (~3 lines)</span></span>
<span id="cb4-132"><a href="#cb4-132"></a>        <span class="cf">pass</span> <span class="co"># Sample proposals for p, w_1, w_2</span></span>
<span id="cb4-133"><a href="#cb4-133"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-134"><a href="#cb4-134"></a></span>
<span id="cb4-135"><a href="#cb4-135"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-136"><a href="#cb4-136"></a>        <span class="cf">pass</span> <span class="co"># Compute likehoods and prior densities on both the proposed and current samples</span></span>
<span id="cb4-137"><a href="#cb4-137"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-138"><a href="#cb4-138"></a></span>
<span id="cb4-139"><a href="#cb4-139"></a>        <span class="co"># YOUR CODE HERE (~2-4 lines)</span></span>
<span id="cb4-140"><a href="#cb4-140"></a>        <span class="cf">pass</span> <span class="co"># Obtain the ratios of the likelihoods and prior densities between the proposed and current samples </span></span>
<span id="cb4-141"><a href="#cb4-141"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-142"><a href="#cb4-142"></a></span>
<span id="cb4-143"><a href="#cb4-143"></a>        <span class="co"># YOUR CODE HERE (~1-2 lines)</span></span>
<span id="cb4-144"><a href="#cb4-144"></a>        <span class="cf">pass</span> <span class="co"># Multiply all ratios (both likelihoods and prior densities) and use this to calculate the acceptance probability of the proposal</span></span>
<span id="cb4-145"><a href="#cb4-145"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb4-146"><a href="#cb4-146"></a></span>
<span id="cb4-147"><a href="#cb4-147"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-148"><a href="#cb4-148"></a>        <span class="cf">pass</span> <span class="co"># Sample randomness to determine whether the proposal should be accepted to update curr_p, curr_w_1, curr_w_2, and accept_count</span></span>
<span id="cb4-149"><a href="#cb4-149"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-150"><a href="#cb4-150"></a></span>
<span id="cb4-151"><a href="#cb4-151"></a>        <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb4-152"><a href="#cb4-152"></a>        <span class="cf">pass</span> <span class="co"># Update p_samples, w_1_samples, w_2_samples if we have passed the burn in period T</span></span>
<span id="cb4-153"><a href="#cb4-153"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE </span></span>
<span id="cb4-154"><a href="#cb4-154"></a></span>
<span id="cb4-155"><a href="#cb4-155"></a>    fraction_accepted <span class="op">=</span> accept_count <span class="op">/</span> num_iters</span>
<span id="cb4-156"><a href="#cb4-156"></a>    <span class="bu">print</span>(<span class="ss">f"Fraction of accepted proposals: </span><span class="sc">{</span>fraction_accepted<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-157"><a href="#cb4-157"></a>    <span class="cf">return</span> (</span>
<span id="cb4-158"><a href="#cb4-158"></a>        torch.stack(p_samples),</span>
<span id="cb4-159"><a href="#cb4-159"></a>        torch.stack(w_1_samples),</span>
<span id="cb4-160"><a href="#cb4-160"></a>        torch.stack(w_2_samples),</span>
<span id="cb4-161"><a href="#cb4-161"></a>        fraction_accepted,</span>
<span id="cb4-162"><a href="#cb4-162"></a>    )</span>
<span id="cb4-163"><a href="#cb4-163"></a></span>
<span id="cb4-164"><a href="#cb4-164"></a></span>
<span id="cb4-165"><a href="#cb4-165"></a><span class="kw">def</span> evaluate_metropolis(num_sims: <span class="bu">int</span>, num_movies: <span class="bu">int</span>, feature_dim: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-166"><a href="#cb4-166"></a>    <span class="co">"""</span></span>
<span id="cb4-167"><a href="#cb4-167"></a><span class="co">    Runs the Metropolis-Hastings algorithm N times and compare estimated parameters</span></span>
<span id="cb4-168"><a href="#cb4-168"></a><span class="co">    with true parameters to obtain success rate. You should attain a success rate of around 90%. </span></span>
<span id="cb4-169"><a href="#cb4-169"></a></span>
<span id="cb4-170"><a href="#cb4-170"></a><span class="co">    Note that there are two successful equilibria to converge to. They are true_weights_1 and true_weights_2 with probabilities</span></span>
<span id="cb4-171"><a href="#cb4-171"></a><span class="co">    p and 1-p in addition to true_weights_2 and true_weights_1 with probabilities 1-p and p. This is why even though it may appear your</span></span>
<span id="cb4-172"><a href="#cb4-172"></a><span class="co">    predicted parameters don't match the true parameters, they are in fact equivalent. </span></span>
<span id="cb4-173"><a href="#cb4-173"></a></span>
<span id="cb4-174"><a href="#cb4-174"></a><span class="co">    Args:</span></span>
<span id="cb4-175"><a href="#cb4-175"></a><span class="co">        num_sims (int): Number of simulations to run.</span></span>
<span id="cb4-176"><a href="#cb4-176"></a></span>
<span id="cb4-177"><a href="#cb4-177"></a><span class="co">    Returns:</span></span>
<span id="cb4-178"><a href="#cb4-178"></a><span class="co">        None</span></span>
<span id="cb4-179"><a href="#cb4-179"></a><span class="co">    """</span></span>
<span id="cb4-180"><a href="#cb4-180"></a>    </span>
<span id="cb4-181"><a href="#cb4-181"></a>    success_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-182"><a href="#cb4-182"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_sims):</span>
<span id="cb4-183"><a href="#cb4-183"></a>        <span class="co"># Sample random ground truth parameters</span></span>
<span id="cb4-184"><a href="#cb4-184"></a>        true_p <span class="op">=</span> <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.4</span> <span class="op">*</span> torch.rand(<span class="dv">1</span>)</span>
<span id="cb4-185"><a href="#cb4-185"></a>        true_weights_1 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-186"><a href="#cb4-186"></a>        true_weights_2 <span class="op">=</span> torch.randn(feature_dim)</span>
<span id="cb4-187"><a href="#cb4-187"></a></span>
<span id="cb4-188"><a href="#cb4-188"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">---- MCMC Simulation ----"</span>)</span>
<span id="cb4-189"><a href="#cb4-189"></a>        <span class="bu">print</span>(<span class="st">"True parameters:"</span>, true_p, true_weights_1, true_weights_2)</span>
<span id="cb4-190"><a href="#cb4-190"></a></span>
<span id="cb4-191"><a href="#cb4-191"></a>        dataset, labels <span class="op">=</span> make_data(true_p, true_weights_1, true_weights_2, num_movies, feature_dim)</span>
<span id="cb4-192"><a href="#cb4-192"></a>        p_samples, w_1_samples, w_2_samples, _ <span class="op">=</span> metropolis_hastings(dataset, labels)</span>
<span id="cb4-193"><a href="#cb4-193"></a></span>
<span id="cb4-194"><a href="#cb4-194"></a>        p_pred <span class="op">=</span> p_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-195"><a href="#cb4-195"></a>        w_1_pred <span class="op">=</span> w_1_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-196"><a href="#cb4-196"></a>        w_2_pred <span class="op">=</span> w_2_samples.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-197"><a href="#cb4-197"></a></span>
<span id="cb4-198"><a href="#cb4-198"></a>        <span class="bu">print</span>(<span class="st">"Predicted parameters:"</span>, p_pred, w_1_pred, w_2_pred)</span>
<span id="cb4-199"><a href="#cb4-199"></a></span>
<span id="cb4-200"><a href="#cb4-200"></a>        <span class="co"># Do casework on two equilibria cases to check for success</span></span>
<span id="cb4-201"><a href="#cb4-201"></a>        p_diff_case_1 <span class="op">=</span> torch.<span class="bu">abs</span>(p_pred <span class="op">-</span> true_p)</span>
<span id="cb4-202"><a href="#cb4-202"></a>        p_diff_case_2 <span class="op">=</span> torch.<span class="bu">abs</span>(p_pred <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> true_p))</span>
<span id="cb4-203"><a href="#cb4-203"></a></span>
<span id="cb4-204"><a href="#cb4-204"></a>        w_1_diff_case_1 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_1_pred <span class="op">-</span> true_weights_1))</span>
<span id="cb4-205"><a href="#cb4-205"></a>        w_1_diff_case_2 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_1_pred <span class="op">-</span> true_weights_2))</span>
<span id="cb4-206"><a href="#cb4-206"></a></span>
<span id="cb4-207"><a href="#cb4-207"></a>        w_2_diff_case_1 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_2_pred <span class="op">-</span> true_weights_2))</span>
<span id="cb4-208"><a href="#cb4-208"></a>        w_2_diff_case_2 <span class="op">=</span> torch.<span class="bu">max</span>(torch.<span class="bu">abs</span>(w_2_pred <span class="op">-</span> true_weights_1))</span>
<span id="cb4-209"><a href="#cb4-209"></a></span>
<span id="cb4-210"><a href="#cb4-210"></a>        pass_case_1 <span class="op">=</span> (</span>
<span id="cb4-211"><a href="#cb4-211"></a>            p_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.1</span> <span class="kw">and</span> w_1_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.5</span> <span class="kw">and</span> w_2_diff_case_1 <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb4-212"><a href="#cb4-212"></a>        )</span>
<span id="cb4-213"><a href="#cb4-213"></a>        pass_case_2 <span class="op">=</span> (</span>
<span id="cb4-214"><a href="#cb4-214"></a>            p_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.1</span> <span class="kw">and</span> w_1_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.5</span> <span class="kw">and</span> w_2_diff_case_2 <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb4-215"><a href="#cb4-215"></a>        )</span>
<span id="cb4-216"><a href="#cb4-216"></a>        passes <span class="op">=</span> pass_case_1 <span class="kw">or</span> pass_case_2</span>
<span id="cb4-217"><a href="#cb4-217"></a></span>
<span id="cb4-218"><a href="#cb4-218"></a>        <span class="bu">print</span>(<span class="ss">f'Result: </span><span class="sc">{</span><span class="st">"Success"</span> <span class="cf">if</span> passes <span class="cf">else</span> <span class="st">"FAILED"</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-219"><a href="#cb4-219"></a>        <span class="cf">if</span> passes:</span>
<span id="cb4-220"><a href="#cb4-220"></a>            success_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-221"><a href="#cb4-221"></a>    <span class="bu">print</span>(<span class="ss">f'Success rate: </span><span class="sc">{</span>success_count <span class="op">/</span> num_sims<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-222"><a href="#cb4-222"></a></span>
<span id="cb4-223"><a href="#cb4-223"></a></span>
<span id="cb4-224"><a href="#cb4-224"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-225"><a href="#cb4-225"></a>    evaluate_metropolis(num_sims<span class="op">=</span><span class="dv">10</span>, num_movies<span class="op">=</span><span class="dv">30000</span>, feature_dim<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="question-4-direct-preference-optimization-40-points" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="question-4-direct-preference-optimization-40-points">Question 4: Direct Preference Optimization (40 points)</h3>
<p>Note this question requires a GPU which is provided for free on Google Colab (T4 instance) or through the course cloud credits provided on Ed.<br>
Direct Preference Optimization (DPO) allows for policy alignment on a preference dataset without the need to train a separate reward model. The preference dataset is constructed by sampling generations <span class="math inline">\((y_1, y_2)\sim \pi_{\text{ref}}(\cdot\mid x)\)</span> where <span class="math inline">\(\pi_\text{ref}\)</span> is the base policy to be aligned, and <span class="math inline">\(x\)</span> comes from a set of previously collected prompts. The pairs of generations are then labeled by an annotator for which of the generations is preferred. Denote the preference dataset by <span class="math inline">\(\mathcal{D}=\left\{\left(x^{(i)}, y_+^{(i)}, y_-^{(i)}\right)\right\}_{i=1}^N\)</span>, where <span class="math inline">\(y_+\)</span> and <span class="math inline">\(y_-\)</span> are the preferred and non-preferred generations, respectively. DPO aims to solve the following: <span class="math display">\[\hat{\pi}=\arg \min_{\pi\in\Pi}\mathbb{E}_{(x, y_+, y_-)\sim\mathcal{D}}\left[-\log\sigma\left(
\beta\log\left(\frac{\pi(y_+ | x)}{\pi_{\text{ref}}(y_+ | x)}\right)-\beta\log\left(\frac{\pi(y_- | x)}{\pi_{\text{ref}}(y_- | x)}\right)\right)\right]\]</span> where <span class="math inline">\(\Pi\)</span> is the space of possible polices <span class="math inline">\(\pi\)</span> can take on. <span class="math inline">\(\pi\)</span> is typically parametrized.</p>
<ol type="a">
<li><p><strong>(Written, 6 points)</strong>. Consider the setting where <span class="math inline">\(\pi_{\text{ref}}\)</span> has no conditioning features and randomly outputs one of two possible values, <span class="math inline">\(\mathbf{A}\)</span> or <span class="math inline">\(\mathbf{B}\)</span> (also known as the “Bandit” setting). Suppose that <span class="math inline">\(\pi_{\text{ref}}(\mathbf{A})=p_0\)</span> and <span class="math inline">\(\pi_{\text{ref}}(\mathbf{B})=1-p_0\)</span>. Furthermore, assume that the preference dataset <span class="math inline">\(\mathcal{D}\)</span> is infinitely large, sampled from <span class="math inline">\(\pi_{\text{ref}}\)</span>, and that the preferred response is selected through a Bradley-Terry reward model where <span class="math inline">\(\mathbf{A}\)</span> has reward score <span class="math inline">\(r_A\)</span> and <span class="math inline">\(\mathbf{B}\)</span> has reward score <span class="math inline">\(r_B\)</span>. Set <span class="math inline">\(\Pi=\{\pi_p\mid 0&lt;p&lt;1\}\)</span> where <span class="math inline">\(\pi_p\)</span> is the policy defined by <span class="math inline">\(\pi_p(\mathbf{A})=p\)</span> and <span class="math inline">\(\pi_p(\mathbf{B})=1-p\)</span>. The DPO objective is to compute: <span class="math display">\[\pi_{\hat{p}}=\arg \min_{\pi_p\in \Pi} f(p, p_0, \beta, r_A, r_B),\]</span> for a function <span class="math inline">\(f\)</span>. Find <span class="math inline">\(f\)</span> by explicitly computing the relevant expectation.</p></li>
<li><p><strong>(Written, 8 points)</strong>. Assume that a solution to the optimization problem in part (a) exists. Find an expression for <span class="math inline">\(\hat{p}\)</span>. (Hint: Make sure to know your sigmoid derivative properties! Everything should simplify nicely. You may use the <a href="https://en.wikipedia.org/wiki/Logit"><em>logit function</em></a> denoted by <span class="math inline">\(\sigma^{-1}\)</span> in your final expression.)</p></li>
<li><p><strong>(Written, 3 points)</strong>. Show that <span class="math inline">\(\lim_{\beta\to\infty}\hat{p}=p_0.\)</span> (Very) briefly explain why this makes sense intuitively based on the role of <span class="math inline">\(\beta\)</span> in KL-constrained reward optimization (we suggest two sentences).</p></li>
<li><p><strong>(Written, 3 points)</strong>. Assume <span class="math inline">\(r_A=r_B\)</span> and <span class="math inline">\(\beta&gt;0\)</span>. Notice that <span class="math inline">\(\hat{p}=p_0\)</span>. Briefly explain why this makes sense intuitively (we suggest two sentences).</p></li>
</ol>
<p>Next, you will fine-tune the lightweight 2 billion parameter Gemma 2 model on the DPO objective. We will use the instruction fine-tuned variant of the model (i.e., designed for chat-based interactions).</p>
<ol type="1">
<li><p><strong>(Coding, 4 points)</strong>. Open the <code>dpo/dpo.ipynb</code> file of the PSET’s codebase. Execute the first few cells of the notebook until you see the <code>sample_chat_tokens</code> and their IDs printed out. The next cell requires you to implement the <code>get_response_idxs</code> function in <code>dpo/dpo.py</code>.</p>
<p>To implement it, you must find the indices of the first and last token of the model’s response in <code>sample_chat_tokens</code>. In the notebook’s example, this corresponds to the tokens “As” and “.”</p></li>
<li><p><strong>(Coding, 4 points)</strong>. The following cell asks you to implement the <code>get_response_next_token_probs</code> function. The next token logits for each token of the chat prompt are provided. Pass them through the softmax function and appropriately index the next token IDs.</p>
<pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user
Where are you?&lt;end_of_turn&gt;
&lt;start_of_turn&gt;model
I am here.&lt;end_of_turn&gt;</code></pre>
<p>In the example above, we look for the next-token probabilities of “I”, “am”, “here”, and “.” To do so, you must extract the logits for “\n”, “I”, “am”, and “here” because the probability of generating a given token comes from the prediction of the token before. Use the return value of <code>get_response_idxs</code> as anchor points for indexing. Be careful of off-by-one indexing mistakes!</p></li>
<li><p><strong>(Coding, 6 points)</strong>. The training and reference LLM policies are loaded for you. We load the training policy in with LoRA for computational efficiency during fine-tuning in the next part. Implement <code>compute_dpo_objective</code> with the objective provided in the theory portion for your favorite positive value of <span class="math inline">\(\beta\)</span>. Does <span class="math inline">\(\beta\)</span> affect the loss printed out? Why or why not? You do not need to write why in your submission, but this line of thinking will help debug any issues with your DPO loss function.</p></li>
<li><p><strong>(Written + Coding, 6 points)</strong>. Finally, you will fine-tune the Gemma model on the DPO loss function with batch size (and dataset size) of <span class="math inline">\(1\)</span> by implementing <code>finetune</code>. The prompt and completions are provided in the notebook. The optimizer, <span class="math inline">\(\beta\)</span>, and the number of fine-tuning steps have also been provided. Make sure to use <code>torch.no_grad()</code> on the reference model to prevent unnecessary gradients!</p>
<p>Report the proportion of “because of” occurences before and after fine-tuning. Additionally, include a plot of the DPO loss curve.</p></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="6d898a48" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM, set_seed</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>set_seed(<span class="dv">42</span>) <span class="co"># DO NOT CHANGE THE SEED</span></span>
<span id="cb6-6"><a href="#cb6-6"></a></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="kw">def</span> get_response_idxs(tokenizer, chat_token_ids):</span>
<span id="cb6-8"><a href="#cb6-8"></a>    <span class="co">"""</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">    Finds the start and end indices of the response in the tokenized chat.</span></span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co">    Args:</span></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co">    chat_token_ids (list[int]): The token IDs representing the chat conversation.</span></span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="co">    Returns:</span></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="co">    tuple: A tuple (response_start_idx, response_end_idx), both of which are nonnegative integers.</span></span>
<span id="cb6-17"><a href="#cb6-17"></a><span class="co">    """</span></span>
<span id="cb6-18"><a href="#cb6-18"></a></span>
<span id="cb6-19"><a href="#cb6-19"></a>    start_of_turn_id <span class="op">=</span> tokenizer.convert_tokens_to_ids(<span class="st">"&lt;start_of_turn&gt;"</span>)</span>
<span id="cb6-20"><a href="#cb6-20"></a>    end_of_turn_id <span class="op">=</span> tokenizer.convert_tokens_to_ids(<span class="st">"&lt;end_of_turn&gt;"</span>)</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>    response_start_idx <span class="op">=</span> <span class="va">None</span> <span class="co"># Nonnegative integer</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>    response_end_idx <span class="op">=</span> <span class="va">None</span> <span class="co"># Nonnegative integer</span></span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="co"># YOUR CODE HERE (~3-5 lines)</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>    <span class="cf">pass</span></span>
<span id="cb6-27"><a href="#cb6-27"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>    <span class="cf">return</span> response_start_idx, response_end_idx</span>
<span id="cb6-30"><a href="#cb6-30"></a></span>
<span id="cb6-31"><a href="#cb6-31"></a><span class="kw">def</span> get_response_next_token_probs(tokenizer, model, chat_token_ids):</span>
<span id="cb6-32"><a href="#cb6-32"></a>    <span class="co">"""</span></span>
<span id="cb6-33"><a href="#cb6-33"></a><span class="co">    Computes the next token probabilities for the response in a chat.</span></span>
<span id="cb6-34"><a href="#cb6-34"></a></span>
<span id="cb6-35"><a href="#cb6-35"></a><span class="co">    Args:</span></span>
<span id="cb6-36"><a href="#cb6-36"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-37"><a href="#cb6-37"></a><span class="co">    model: The language model used to generate the logits.</span></span>
<span id="cb6-38"><a href="#cb6-38"></a><span class="co">    chat_token_ids (list[int]): The token IDs representing the chat conversation.</span></span>
<span id="cb6-39"><a href="#cb6-39"></a></span>
<span id="cb6-40"><a href="#cb6-40"></a><span class="co">    Returns:</span></span>
<span id="cb6-41"><a href="#cb6-41"></a><span class="co">    torch.Tensor: A 1D tensor containing the probabilities of the tokens in the response found by appropriately indexing</span></span>
<span id="cb6-42"><a href="#cb6-42"></a><span class="co">                  the next token probabilities of the preceding token.</span></span>
<span id="cb6-43"><a href="#cb6-43"></a><span class="co">    """</span></span>
<span id="cb6-44"><a href="#cb6-44"></a></span>
<span id="cb6-45"><a href="#cb6-45"></a>    response_start_idx, response_end_idx <span class="op">=</span> get_response_idxs(tokenizer, chat_token_ids)</span>
<span id="cb6-46"><a href="#cb6-46"></a>    chat_token_ids_tensor <span class="op">=</span> torch.tensor([chat_token_ids]).to(model.device)</span>
<span id="cb6-47"><a href="#cb6-47"></a>    logits <span class="op">=</span> model(chat_token_ids_tensor).logits[<span class="dv">0</span>, :, :] <span class="co"># shape (len(chat_token_ids), vocabulary_size)</span></span>
<span id="cb6-48"><a href="#cb6-48"></a></span>
<span id="cb6-49"><a href="#cb6-49"></a>    next_token_probs <span class="op">=</span> <span class="va">None</span> <span class="co"># Should be a 1D-tensor</span></span>
<span id="cb6-50"><a href="#cb6-50"></a></span>
<span id="cb6-51"><a href="#cb6-51"></a>    <span class="co"># YOUR CODE HERE (~3-5 lines)</span></span>
<span id="cb6-52"><a href="#cb6-52"></a>    <span class="cf">pass</span></span>
<span id="cb6-53"><a href="#cb6-53"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-54"><a href="#cb6-54"></a></span>
<span id="cb6-55"><a href="#cb6-55"></a>    <span class="cf">return</span> next_token_probs</span>
<span id="cb6-56"><a href="#cb6-56"></a></span>
<span id="cb6-57"><a href="#cb6-57"></a><span class="kw">def</span> compute_dpo_objective(preferred_train_probs, nonpreferred_train_probs, preferred_ref_probs, nonpreferred_ref_probs, beta):</span>
<span id="cb6-58"><a href="#cb6-58"></a>    <span class="co">"""</span></span>
<span id="cb6-59"><a href="#cb6-59"></a><span class="co">    Computes the Direct Preference Optimization (DPO) objective for training.</span></span>
<span id="cb6-60"><a href="#cb6-60"></a></span>
<span id="cb6-61"><a href="#cb6-61"></a><span class="co">    Args:</span></span>
<span id="cb6-62"><a href="#cb6-62"></a><span class="co">    preferred_train_probs (torch.Tensor): Token probabilities for the preferred chat sequence from the training model.</span></span>
<span id="cb6-63"><a href="#cb6-63"></a><span class="co">    nonpreferred_train_probs (torch.Tensor): Token probabilities for the non-preferred chat sequence from the training model.</span></span>
<span id="cb6-64"><a href="#cb6-64"></a><span class="co">    preferred_ref_probs (torch.Tensor): Token probabilities for the preferred chat sequence from the reference model.</span></span>
<span id="cb6-65"><a href="#cb6-65"></a><span class="co">    nonpreferred_ref_probs (torch.Tensor): Token probabilities for the non-preferred chat sequence from the reference model.</span></span>
<span id="cb6-66"><a href="#cb6-66"></a><span class="co">    beta (float): Controls the KL strength of staying close to the reference model.</span></span>
<span id="cb6-67"><a href="#cb6-67"></a></span>
<span id="cb6-68"><a href="#cb6-68"></a><span class="co">    Returns:</span></span>
<span id="cb6-69"><a href="#cb6-69"></a><span class="co">    torch.Tensor: The computed DPO objective, which is a float.</span></span>
<span id="cb6-70"><a href="#cb6-70"></a><span class="co">    """</span></span>
<span id="cb6-71"><a href="#cb6-71"></a></span>
<span id="cb6-72"><a href="#cb6-72"></a>    dpo_obj <span class="op">=</span> <span class="va">None</span> <span class="co"># Float value</span></span>
<span id="cb6-73"><a href="#cb6-73"></a>    </span>
<span id="cb6-74"><a href="#cb6-74"></a>    <span class="co"># YOUR CODE HERE (~4-6 lines)</span></span>
<span id="cb6-75"><a href="#cb6-75"></a>    <span class="cf">pass</span></span>
<span id="cb6-76"><a href="#cb6-76"></a>    <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-77"><a href="#cb6-77"></a></span>
<span id="cb6-78"><a href="#cb6-78"></a>    <span class="cf">return</span> dpo_obj</span>
<span id="cb6-79"><a href="#cb6-79"></a></span>
<span id="cb6-80"><a href="#cb6-80"></a><span class="kw">def</span> finetune(tokenizer, optimizer, train_model, ref_model, preferred_chat_ids, nonpreferred_chat_ids, num_gradient_steps, beta):</span>
<span id="cb6-81"><a href="#cb6-81"></a>    <span class="co">"""</span></span>
<span id="cb6-82"><a href="#cb6-82"></a><span class="co">    Fine-tunes the training model using DPO. Make sure to disable gradients on the reference model!</span></span>
<span id="cb6-83"><a href="#cb6-83"></a></span>
<span id="cb6-84"><a href="#cb6-84"></a><span class="co">    Args:</span></span>
<span id="cb6-85"><a href="#cb6-85"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-86"><a href="#cb6-86"></a><span class="co">    optimizer: The optimizer for updating the training model's parameters.</span></span>
<span id="cb6-87"><a href="#cb6-87"></a><span class="co">    train_model: The model being fine-tuned.</span></span>
<span id="cb6-88"><a href="#cb6-88"></a><span class="co">    ref_model: The reference model.</span></span>
<span id="cb6-89"><a href="#cb6-89"></a><span class="co">    preferred_chat_ids (list[int]): The token IDs representing the preferred chat sequence.</span></span>
<span id="cb6-90"><a href="#cb6-90"></a><span class="co">    nonpreferred_chat_ids (list[int]): The token IDs representing the non-preferred chat sequence.</span></span>
<span id="cb6-91"><a href="#cb6-91"></a><span class="co">    num_gradient_steps (int): The number of gradient updates to perform.</span></span>
<span id="cb6-92"><a href="#cb6-92"></a><span class="co">    beta (float): A parameter used in computing the DPO objective.</span></span>
<span id="cb6-93"><a href="#cb6-93"></a></span>
<span id="cb6-94"><a href="#cb6-94"></a><span class="co">    Returns:</span></span>
<span id="cb6-95"><a href="#cb6-95"></a><span class="co">    None</span></span>
<span id="cb6-96"><a href="#cb6-96"></a><span class="co">    """</span></span>
<span id="cb6-97"><a href="#cb6-97"></a></span>
<span id="cb6-98"><a href="#cb6-98"></a>    <span class="bu">print</span>(<span class="st">'Fine-tuning...'</span>)</span>
<span id="cb6-99"><a href="#cb6-99"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_gradient_steps):</span>
<span id="cb6-100"><a href="#cb6-100"></a>        <span class="co"># YOUR CODE HERE (~9-12 lines)</span></span>
<span id="cb6-101"><a href="#cb6-101"></a>        <span class="cf">pass</span></span>
<span id="cb6-102"><a href="#cb6-102"></a>        <span class="co"># </span><span class="re">END</span><span class="co"> OF YOUR CODE</span></span>
<span id="cb6-103"><a href="#cb6-103"></a>    <span class="bu">print</span>(<span class="st">"Fine-tuning complete!"</span>)</span>
<span id="cb6-104"><a href="#cb6-104"></a></span>
<span id="cb6-105"><a href="#cb6-105"></a><span class="co"># DO NOT CHANGE!</span></span>
<span id="cb6-106"><a href="#cb6-106"></a><span class="kw">def</span> sample_model(tokenizer, model, prompt, N<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-107"><a href="#cb6-107"></a>    <span class="co">"""</span></span>
<span id="cb6-108"><a href="#cb6-108"></a><span class="co">    Samples N different completions from the model based on the given prompt.</span></span>
<span id="cb6-109"><a href="#cb6-109"></a></span>
<span id="cb6-110"><a href="#cb6-110"></a><span class="co">    Args:</span></span>
<span id="cb6-111"><a href="#cb6-111"></a><span class="co">    tokenizer: The tokenizer object used to encode/decode text.</span></span>
<span id="cb6-112"><a href="#cb6-112"></a><span class="co">    model: The language model used for generation.</span></span>
<span id="cb6-113"><a href="#cb6-113"></a><span class="co">    prompt (str): The input prompt for which completions will be generated.</span></span>
<span id="cb6-114"><a href="#cb6-114"></a><span class="co">    N (int): The number of completions to generate.</span></span>
<span id="cb6-115"><a href="#cb6-115"></a></span>
<span id="cb6-116"><a href="#cb6-116"></a><span class="co">    Returns:</span></span>
<span id="cb6-117"><a href="#cb6-117"></a><span class="co">    list[str]: A list of N generated completions.</span></span>
<span id="cb6-118"><a href="#cb6-118"></a><span class="co">    """</span></span>
<span id="cb6-119"><a href="#cb6-119"></a></span>
<span id="cb6-120"><a href="#cb6-120"></a>    chat <span class="op">=</span> [{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}]</span>
<span id="cb6-121"><a href="#cb6-121"></a>    chat_tokens <span class="op">=</span> tokenizer.apply_chat_template(chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-122"><a href="#cb6-122"></a></span>
<span id="cb6-123"><a href="#cb6-123"></a>    <span class="co"># Generate N different responses</span></span>
<span id="cb6-124"><a href="#cb6-124"></a>    outputs <span class="op">=</span> model.generate(</span>
<span id="cb6-125"><a href="#cb6-125"></a>        torch.tensor([chat_tokens], device<span class="op">=</span>model.device),</span>
<span id="cb6-126"><a href="#cb6-126"></a>        num_return_sequences<span class="op">=</span>N,</span>
<span id="cb6-127"><a href="#cb6-127"></a>        max_new_tokens<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb6-128"><a href="#cb6-128"></a>        temperature<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb6-129"><a href="#cb6-129"></a>        top_k<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb6-130"><a href="#cb6-130"></a>        top_p<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb6-131"><a href="#cb6-131"></a>        do_sample<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-132"><a href="#cb6-132"></a>    )</span>
<span id="cb6-133"><a href="#cb6-133"></a></span>
<span id="cb6-134"><a href="#cb6-134"></a>    <span class="kw">def</span> extract_response(decoded_text):</span>
<span id="cb6-135"><a href="#cb6-135"></a>        <span class="cf">return</span> decoded_text.rsplit(<span class="st">'model</span><span class="ch">\n</span><span class="st">'</span>, <span class="dv">1</span>)[<span class="op">-</span><span class="dv">1</span>][:<span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb6-136"><a href="#cb6-136"></a></span>
<span id="cb6-137"><a href="#cb6-137"></a>    responses <span class="op">=</span> [extract_response(tokenizer.decode(output, skip_special_tokens<span class="op">=</span><span class="va">True</span>)) <span class="cf">for</span> output <span class="kw">in</span> outputs]</span>
<span id="cb6-138"><a href="#cb6-138"></a>    <span class="cf">return</span> responses</span>
<span id="cb6-139"><a href="#cb6-139"></a></span>
<span id="cb6-140"><a href="#cb6-140"></a><span class="co"># DO NOT CHANGE!</span></span>
<span id="cb6-141"><a href="#cb6-141"></a><span class="kw">def</span> fraction_responses_with_because_of(responses):</span>
<span id="cb6-142"><a href="#cb6-142"></a>    <span class="co">"""</span></span>
<span id="cb6-143"><a href="#cb6-143"></a><span class="co">    Calculates the fraction of responses that start with a specific match string.</span></span>
<span id="cb6-144"><a href="#cb6-144"></a></span>
<span id="cb6-145"><a href="#cb6-145"></a><span class="co">    Args:</span></span>
<span id="cb6-146"><a href="#cb6-146"></a><span class="co">    responses (list[str]): A list of model-generated responses.</span></span>
<span id="cb6-147"><a href="#cb6-147"></a></span>
<span id="cb6-148"><a href="#cb6-148"></a><span class="co">    Returns:</span></span>
<span id="cb6-149"><a href="#cb6-149"></a><span class="co">    float: The fraction of responses that start with the phrase "The sky appears blue because of".</span></span>
<span id="cb6-150"><a href="#cb6-150"></a><span class="co">    """</span></span>
<span id="cb6-151"><a href="#cb6-151"></a></span>
<span id="cb6-152"><a href="#cb6-152"></a>    match_str <span class="op">=</span> <span class="st">"The sky appears blue because of"</span></span>
<span id="cb6-153"><a href="#cb6-153"></a>    match_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a>    <span class="cf">for</span> response <span class="kw">in</span> responses:</span>
<span id="cb6-156"><a href="#cb6-156"></a>        <span class="cf">if</span> response.startswith(match_str):</span>
<span id="cb6-157"><a href="#cb6-157"></a>            match_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-158"><a href="#cb6-158"></a></span>
<span id="cb6-159"><a href="#cb6-159"></a>    <span class="cf">return</span> match_count <span class="op">/</span> <span class="bu">len</span>(responses)</span>
<span id="cb6-160"><a href="#cb6-160"></a></span>
<span id="cb6-161"><a href="#cb6-161"></a></span>
<span id="cb6-162"><a href="#cb6-162"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb6-163"><a href="#cb6-163"></a>    model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-164"><a href="#cb6-164"></a>        <span class="st">"google/gemma-2-2b-it"</span>,</span>
<span id="cb6-165"><a href="#cb6-165"></a>        torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb6-166"><a href="#cb6-166"></a>        device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb6-167"><a href="#cb6-167"></a>    )</span>
<span id="cb6-168"><a href="#cb6-168"></a>    tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"google/gemma-2-2b-it"</span>)</span>
<span id="cb6-169"><a href="#cb6-169"></a></span>
<span id="cb6-170"><a href="#cb6-170"></a>    sample_prompt <span class="op">=</span> <span class="st">"How is it going?"</span></span>
<span id="cb6-171"><a href="#cb6-171"></a>    sample_completion <span class="op">=</span> <span class="st">"As an AI, I don't have feelings or experiences like humans do, so I don't have a </span><span class="ch">\"</span><span class="st">going</span><span class="ch">\"</span><span class="st"> in the same way."</span></span>
<span id="cb6-172"><a href="#cb6-172"></a></span>
<span id="cb6-173"><a href="#cb6-173"></a>    sample_chat <span class="op">=</span> [</span>
<span id="cb6-174"><a href="#cb6-174"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: sample_prompt},</span>
<span id="cb6-175"><a href="#cb6-175"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: sample_completion}</span>
<span id="cb6-176"><a href="#cb6-176"></a>    ]</span>
<span id="cb6-177"><a href="#cb6-177"></a></span>
<span id="cb6-178"><a href="#cb6-178"></a>    sample_chat_tokens <span class="op">=</span> tokenizer.apply_chat_template(sample_chat, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-179"><a href="#cb6-179"></a>    sample_chat_token_ids <span class="op">=</span> tokenizer.apply_chat_template(sample_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-180"><a href="#cb6-180"></a></span>
<span id="cb6-181"><a href="#cb6-181"></a>    <span class="bu">print</span>(<span class="st">"Chat tokens:"</span>)</span>
<span id="cb6-182"><a href="#cb6-182"></a>    <span class="bu">print</span>(sample_chat_tokens)</span>
<span id="cb6-183"><a href="#cb6-183"></a></span>
<span id="cb6-184"><a href="#cb6-184"></a>    <span class="bu">print</span>(<span class="st">"Chat token IDs:"</span>)</span>
<span id="cb6-185"><a href="#cb6-185"></a>    <span class="bu">print</span>(sample_chat_token_ids)</span>
<span id="cb6-186"><a href="#cb6-186"></a></span>
<span id="cb6-187"><a href="#cb6-187"></a>    response_start_idx, response_end_idx <span class="op">=</span> get_response_idxs(tokenizer, sample_chat_token_ids)</span>
<span id="cb6-188"><a href="#cb6-188"></a>    <span class="bu">print</span>(<span class="ss">f"Response tokens index in sample_chat_tokens range from </span><span class="sc">{</span>response_start_idx<span class="sc">}</span><span class="ss"> to </span><span class="sc">{</span>response_end_idx<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb6-189"><a href="#cb6-189"></a></span>
<span id="cb6-190"><a href="#cb6-190"></a>    first_response_token_id <span class="op">=</span> sample_chat_token_ids[response_start_idx]</span>
<span id="cb6-191"><a href="#cb6-191"></a>    last_response_token_id <span class="op">=</span> sample_chat_token_ids[response_end_idx]</span>
<span id="cb6-192"><a href="#cb6-192"></a>    <span class="bu">print</span>(<span class="ss">f'First response token is "</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(first_response_token_id)<span class="sc">}</span><span class="ss">" with ID </span><span class="sc">{</span>first_response_token_id<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-193"><a href="#cb6-193"></a>    <span class="bu">print</span>(<span class="ss">f'Last response token is "</span><span class="sc">{</span>tokenizer<span class="sc">.</span>decode(last_response_token_id)<span class="sc">}</span><span class="ss">" with ID </span><span class="sc">{</span>last_response_token_id<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-194"><a href="#cb6-194"></a></span>
<span id="cb6-195"><a href="#cb6-195"></a>    <span class="co"># Make sure your code passes this test!</span></span>
<span id="cb6-196"><a href="#cb6-196"></a>    <span class="cf">assert</span> tokenizer.decode(first_response_token_id) <span class="op">==</span> <span class="st">"As"</span> <span class="kw">and</span> tokenizer.decode(last_response_token_id) <span class="op">==</span> <span class="st">"."</span></span>
<span id="cb6-197"><a href="#cb6-197"></a></span>
<span id="cb6-198"><a href="#cb6-198"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-199"><a href="#cb6-199"></a>        next_token_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, model, sample_chat_token_ids)</span>
<span id="cb6-200"><a href="#cb6-200"></a>    <span class="bu">print</span>(<span class="ss">f'Next token probabilities: </span><span class="sc">{</span>next_token_probs<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-201"><a href="#cb6-201"></a></span>
<span id="cb6-202"><a href="#cb6-202"></a>    <span class="co"># Make sure your code passes this test!</span></span>
<span id="cb6-203"><a href="#cb6-203"></a>    <span class="cf">assert</span> next_token_probs.mean() <span class="op">&gt;</span> <span class="fl">0.7</span></span>
<span id="cb6-204"><a href="#cb6-204"></a></span>
<span id="cb6-205"><a href="#cb6-205"></a>    train_model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb6-206"><a href="#cb6-206"></a>        <span class="st">"google/gemma-2-2b-it"</span>,</span>
<span id="cb6-207"><a href="#cb6-207"></a>        torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb6-208"><a href="#cb6-208"></a>        device_map<span class="op">=</span><span class="st">'auto'</span></span>
<span id="cb6-209"><a href="#cb6-209"></a>    )</span>
<span id="cb6-210"><a href="#cb6-210"></a>    lora_config <span class="op">=</span> LoraConfig()</span>
<span id="cb6-211"><a href="#cb6-211"></a>    train_model <span class="op">=</span> get_peft_model(train_model, lora_config)</span>
<span id="cb6-212"><a href="#cb6-212"></a>    train_model.train()</span>
<span id="cb6-213"><a href="#cb6-213"></a></span>
<span id="cb6-214"><a href="#cb6-214"></a>    ref_model <span class="op">=</span> model</span>
<span id="cb6-215"><a href="#cb6-215"></a>    ref_model.train()</span>
<span id="cb6-216"><a href="#cb6-216"></a>    <span class="bu">print</span>(<span class="st">'Loaded models!'</span>)</span>
<span id="cb6-217"><a href="#cb6-217"></a></span>
<span id="cb6-218"><a href="#cb6-218"></a>    <span class="co"># The model's response to the prompt usually includes the words "due to" - we want to change that to "because of" using DPO!</span></span>
<span id="cb6-219"><a href="#cb6-219"></a>    prompt <span class="op">=</span> <span class="st">"Explain why the sky is blue in one sentence."</span></span>
<span id="cb6-220"><a href="#cb6-220"></a>    preferred_completion <span class="op">=</span> <span class="st">"The sky appears blue because of"</span></span>
<span id="cb6-221"><a href="#cb6-221"></a>    nonpreferred_completion <span class="op">=</span> <span class="st">"The sky appears blue due to"</span></span>
<span id="cb6-222"><a href="#cb6-222"></a></span>
<span id="cb6-223"><a href="#cb6-223"></a>    preferred_chat <span class="op">=</span> [</span>
<span id="cb6-224"><a href="#cb6-224"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt},</span>
<span id="cb6-225"><a href="#cb6-225"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: preferred_completion}</span>
<span id="cb6-226"><a href="#cb6-226"></a>    ]</span>
<span id="cb6-227"><a href="#cb6-227"></a></span>
<span id="cb6-228"><a href="#cb6-228"></a>    nonpreferred_chat <span class="op">=</span> [</span>
<span id="cb6-229"><a href="#cb6-229"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt},</span>
<span id="cb6-230"><a href="#cb6-230"></a>        {<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: nonpreferred_completion}</span>
<span id="cb6-231"><a href="#cb6-231"></a>    ]</span>
<span id="cb6-232"><a href="#cb6-232"></a></span>
<span id="cb6-233"><a href="#cb6-233"></a>    preferred_chat_ids <span class="op">=</span> tokenizer.apply_chat_template(preferred_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-234"><a href="#cb6-234"></a>    nonpreferred_chat_ids <span class="op">=</span> tokenizer.apply_chat_template(nonpreferred_chat, tokenize<span class="op">=</span><span class="va">True</span>, add_generation_prompt<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-235"><a href="#cb6-235"></a></span>
<span id="cb6-236"><a href="#cb6-236"></a>    preferred_train_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, train_model, preferred_chat_ids)</span>
<span id="cb6-237"><a href="#cb6-237"></a>    nonpreferred_train_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, train_model, nonpreferred_chat_ids)</span>
<span id="cb6-238"><a href="#cb6-238"></a></span>
<span id="cb6-239"><a href="#cb6-239"></a>    <span class="co"># Gradients are not needed for the reference model since we will not be optimizing with respect to it</span></span>
<span id="cb6-240"><a href="#cb6-240"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-241"><a href="#cb6-241"></a>        preferred_ref_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, ref_model, preferred_chat_ids)</span>
<span id="cb6-242"><a href="#cb6-242"></a>        nonpreferred_ref_probs <span class="op">=</span> get_response_next_token_probs(tokenizer, ref_model, nonpreferred_chat_ids)</span>
<span id="cb6-243"><a href="#cb6-243"></a></span>
<span id="cb6-244"><a href="#cb6-244"></a>    your_favorite_beta <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># Feel free to play with beta here. Does anything change?</span></span>
<span id="cb6-245"><a href="#cb6-245"></a>    dpo_obj <span class="op">=</span> compute_dpo_objective(preferred_train_probs, nonpreferred_train_probs, preferred_ref_probs, nonpreferred_ref_probs, beta<span class="op">=</span>your_favorite_beta)</span>
<span id="cb6-246"><a href="#cb6-246"></a>    <span class="bu">print</span>(dpo_obj)</span>
<span id="cb6-247"><a href="#cb6-247"></a></span>
<span id="cb6-248"><a href="#cb6-248"></a>    prior_responses <span class="op">=</span> sample_model(tokenizer, train_model, prompt)</span>
<span id="cb6-249"><a href="#cb6-249"></a>    <span class="bu">print</span>(<span class="st">'Sampled responses before fine-tuning:</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(prior_responses[:<span class="dv">10</span>]))</span>
<span id="cb6-250"><a href="#cb6-250"></a>    <span class="bu">print</span>(<span class="ss">f'Fraction responses with because of: </span><span class="sc">{</span>fraction_responses_with_because_of(prior_responses)<span class="sc">}</span><span class="ss">'</span>) <span class="co"># should start close to 0</span></span>
<span id="cb6-251"><a href="#cb6-251"></a></span>
<span id="cb6-252"><a href="#cb6-252"></a>    <span class="co"># DO NOT CHANGE THESE VALUES</span></span>
<span id="cb6-253"><a href="#cb6-253"></a>    num_gradient_steps <span class="op">=</span> <span class="dv">150</span> </span>
<span id="cb6-254"><a href="#cb6-254"></a>    learning_rate <span class="op">=</span> <span class="fl">2e-6</span></span>
<span id="cb6-255"><a href="#cb6-255"></a>    beta <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-256"><a href="#cb6-256"></a>    optimizer <span class="op">=</span> torch.optim.Adam(train_model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb6-257"><a href="#cb6-257"></a></span>
<span id="cb6-258"><a href="#cb6-258"></a>    finetune(tokenizer, optimizer, train_model, ref_model, preferred_chat_ids, nonpreferred_chat_ids, num_gradient_steps, beta)</span>
<span id="cb6-259"><a href="#cb6-259"></a></span>
<span id="cb6-260"><a href="#cb6-260"></a>    <span class="co"># Save GPU memory</span></span>
<span id="cb6-261"><a href="#cb6-261"></a>    <span class="kw">del</span> ref_model</span>
<span id="cb6-262"><a href="#cb6-262"></a>    <span class="kw">del</span> model</span>
<span id="cb6-263"><a href="#cb6-263"></a></span>
<span id="cb6-264"><a href="#cb6-264"></a>    post_tuning_responses <span class="op">=</span> sample_model(tokenizer, train_model, prompt)</span>
<span id="cb6-265"><a href="#cb6-265"></a>    <span class="bu">print</span>(<span class="st">'Sampled responses after fine-tuning:</span><span class="ch">\n</span><span class="st">'</span> <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(post_tuning_responses[:<span class="dv">10</span>]))</span>
<span id="cb6-266"><a href="#cb6-266"></a>    <span class="bu">print</span>(<span class="ss">f'Fraction responses with because of: </span><span class="sc">{</span>fraction_responses_with_because_of(post_tuning_responses)<span class="sc">}</span><span class="ss">'</span>) <span class="co"># should be more than half</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>


<!-- -->

</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bhatia2020preference" class="csl-entry" role="listitem">
Bhatia, Kush, Ashwin Pananjady, Peter L. Bartlett, Anca D. Dragan, and Martin J. Wainwright. 2020. <span>“Preference Learning Along Multiple Criteria: A Game-Theoretic Perspective.”</span> <em>Neural Information Processing Systems</em> 34 (1): 1–12.
</div>
<div id="ref-2001.04465" class="csl-entry" role="listitem">
Bobu, Andreea, Dexter R. R. Scobee, Jaime F. Fisac, S. Shankar Sastry, and Anca D. Dragan. 2020. <span>“LESS Is More: Rethinking Probabilistic Models of Human Behavior.”</span> <a href="https://doi.org/10.1145/3319502.3374811">https://doi.org/10.1145/3319502.3374811</a>.
</div>
<div id="ref-book_estimation_bock" class="csl-entry" role="listitem">
Bock, Hans Georg, Thomas Carraro, Willi Jäger, Stefan Körkel, Rolf Rannacher, and Johannes P. Schlöder. 2015. <em>Model Based Parameter Estimation: Theory and Applications</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:60333071">https://api.semanticscholar.org/CorpusID:60333071</a>.
</div>
<div id="ref-bolt2009" class="csl-entry" role="listitem">
Bolt, Daniel M., and James A. Wollack. 2009. <span>“Application of a Multidimensional Nested Logit Model to Multiple-Choice Test Items.”</span> <em>Journal of Educational Measurement</em> 46 (3): 181–98. <a href="https://doi.org/10.1111/j.1745-3984.2009.00081.x">https://doi.org/10.1111/j.1745-3984.2009.00081.x</a>.
</div>
<div id="ref-Liang2021" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span>
</div>
<div id="ref-bradley1952rank" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E Terry. 1952a. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-bradley-terry-model" class="csl-entry" role="listitem">
Bradley, Ralph Allan, and Milton E. Terry. 1952b. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45. <a href="http://www.jstor.org/stable/2334029">http://www.jstor.org/stable/2334029</a>.
</div>
<div id="ref-campbell2015" class="csl-entry" role="listitem">
Campbell, Danny, and Seda Erdem. 2015. <span>“Position Bias in Best-Worst Scaling Surveys: A Case Study on Trust in Institutions.”</span> <em>American Journal of Agricultural Economics</em> 97 (2): 526–45. <a href="https://doi.org/10.1093/ajae/aau112">https://doi.org/10.1093/ajae/aau112</a>.
</div>
<div id="ref-book_estimation_casella" class="csl-entry" role="listitem">
Casella, George, and Roger L. Berger. 1990. <em>Statistical Inference</em>. Springer. <a href="https://api.semanticscholar.org/CorpusID:125727004">https://api.semanticscholar.org/CorpusID:125727004</a>.
</div>
<div id="ref-cattelan2012" class="csl-entry" role="listitem">
Cattelan, Manuela. 2012. <span>“Models for Paired Comparison Data: A Review with Emphasis on Dependent Data.”</span> <em>Statistical Science</em> 27 (3): 412–33. <a href="https://doi.org/10.1214/12-STS396">https://doi.org/10.1214/12-STS396</a>.
</div>
<div id="ref-christiano2023deep" class="csl-entry" role="listitem">
Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. <span>“Deep Reinforcement Learning from Human Preferences.”</span> <a href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a>.
</div>
<div id="ref-finn2017model" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.”</span> In <em>International Conference on Machine Learning</em>, 1126–35. PMLR.
</div>
<div id="ref-idealpoints" class="csl-entry" role="listitem">
Greiner, James. 2005. <span>“Ideal Points.”</span> Harvard IQSS Blog. <a href="https://blogs.iq.harvard.edu/ideal_points_1">https://blogs.iq.harvard.edu/ideal_points_1</a>.
</div>
<div id="ref-haarnoja2018soft" class="csl-entry" role="listitem">
Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. <span>“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</span> In <em>International Conference on Machine Learning</em>, 1861–70. PMLR.
</div>
<div id="ref-harpe2015" class="csl-entry" role="listitem">
Harpe, Spencer E. 2015. <span>“How to Analyze Likert and Other Rating Scale Data.”</span> <em>Currents in Pharmacy Teaching and Learning</em> 7 (5): 836–50. <a href="http://dx.doi.org/10.1016/j.cptl.2015.08.001">http://dx.doi.org/10.1016/j.cptl.2015.08.001</a>.
</div>
<div id="ref-hejna2023few" class="csl-entry" role="listitem">
Hejna III, Donald Joseph, and Dorsa Sadigh. 2023. <span>“Few-Shot Preference Learning for Human-in-the-Loop Rl.”</span> In <em>Conference on Robot Learning</em>, 2014–25. PMLR.
</div>
<div id="ref-huber1976ideal" class="csl-entry" role="listitem">
Huber, Joel. 1976. <span>“Ideal Point Models of Preference.”</span> In <em>Advances in Consumer Research</em>, 03:138–42. Association for Consumer Research.
</div>
<div id="ref-ideal_point" class="csl-entry" role="listitem">
Jamieson, Kevin G, and Robert Nowak. 2011. <span>“Active Ranking Using Pairwise Comparisons.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q. Weinberger. Vol. 24. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2011/file/6c14da109e294d1e8155be8aa4b1ce8e-Paper.pdf</a>.
</div>
<div id="ref-keisler2003common" class="csl-entry" role="listitem">
Keisler, H. Jerome, and Byung Soo Lee. 2003. <span>“Common Assumption of Rationality.”</span> <em>Economic Theory Journal</em> 30 (2): 123–45.
</div>
<div id="ref-lee2021pebble" class="csl-entry" role="listitem">
Lee, Kimin, Laura Smith, and Pieter Abbeel. 2021. <span>“Pebble: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-Training.”</span> <em>arXiv Preprint arXiv:2106.05091</em>.
</div>
<div id="ref-Luce1977" class="csl-entry" role="listitem">
Luce, R.Duncan. 1977. <span>“The Choice Axiom After Twenty Years.”</span> <em>Journal of Mathematical Psychology</em> 15 (3): 215–33. <a href="https://doi.org/10.1016/0022-2496(77)90032-3">https://doi.org/10.1016/0022-2496(77)90032-3</a>.
</div>
<div id="ref-miljkovic2005rational" class="csl-entry" role="listitem">
Miljkovic, Dragan. 2005. <span>“Rational Choice and Irrational Individuals or Simply an Irrational Theory: A Critical Review of the Hypothesis of Perfect Rationality.”</span> <em>The Journal of Socio-Economics</em> 34 (5): 621–34. <a href="https://doi.org/10.1016/j.socec.2003.12.031">https://doi.org/10.1016/j.socec.2003.12.031</a>.
</div>
<div id="ref-myers2022learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Biyik, Nima Anari, and Dorsa Sadigh. 2022. <span>“Learning Multimodal Rewards from Rankings.”</span> In <em>Conference on Robot Learning</em>, 342–52. PMLR.
</div>
<div id="ref-myers2021learning" class="csl-entry" role="listitem">
Myers, Vivek, Erdem Bıyık, Nima Anari, and Dorsa Sadigh. 2021. <span>“Learning Multimodal Rewards from Rankings.”</span> <a href="https://arxiv.org/abs/2109.12750">https://arxiv.org/abs/2109.12750</a>.
</div>
<div id="ref-VonNeumannMorgenstern1945" class="csl-entry" role="listitem">
Neumann, John Von, and Oskar Morgenstern. 1945. <em>Theory of Games and Economic Behavior</em>. Princeton, NJ: Princeton University Press.
</div>
<div id="ref-padalkar2023open" class="csl-entry" role="listitem">
Padalkar, Abhishek, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, et al. 2023. <span>“Open x-Embodiment: Robotic Learning Datasets and RT-x Models.”</span> <em>arXiv Preprint arXiv:2310.08864</em>.
</div>
<div id="ref-plackett_luce" class="csl-entry" role="listitem">
Plackett, R. L. 1975. <span>“The Analysis of Permutations.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 24 (2): 193–202. <a href="http://www.jstor.org/stable/2346567">http://www.jstor.org/stable/2346567</a>.
</div>
<div id="ref-Radford2018GPT" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. <span>“Improving Language Understanding by Generative Pre-Training.”</span> San Francisco, CA, USA.
</div>
<div id="ref-rafailov2023direct" class="csl-entry" role="listitem">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>.
</div>
<div id="ref-ragain2019" class="csl-entry" role="listitem">
Ragain, Stephen, and Johan Ugander. 2019. <span>“Choosing to Rank.”</span> <em>arXiv Preprint arXiv:1809.05139</em>. <a href="https://arxiv.org/abs/1809.05139">https://arxiv.org/abs/1809.05139</a>.
</div>
<div id="ref-recommender_systems" class="csl-entry" role="listitem">
Roy, Deepjyoti, and Mala Dutta. 2022. <span>“A Systematic Review and Research Perspective on Recommender Systems.”</span> <em>Journal of Big Data</em> 9: 1–36. <a href="https://api.semanticscholar.org/CorpusID:248508374">https://api.semanticscholar.org/CorpusID:248508374</a>.
</div>
<div id="ref-gradient_descent" class="csl-entry" role="listitem">
Ruder, Sebastian. 2016. <span>“An Overview of Gradient Descent Optimization Algorithms.”</span> <em>ArXiv</em> abs/1609.04747. <a href="https://api.semanticscholar.org/CorpusID:17485266">https://api.semanticscholar.org/CorpusID:17485266</a>.
</div>
<div id="ref-simon1972theories" class="csl-entry" role="listitem">
Simon, Herbert A. 1972. <span>“Theories of Bounded Rationality.”</span> In <em>Decision and Organization</em>, edited by C. B. McGuire and Roy Radner, 161–76. North-Holland Publishing Company.
</div>
<div id="ref-tatli2022distancepreferences" class="csl-entry" role="listitem">
Tatli, Gokcan, Rob Nowak, and Ramya Korlakai Vinayak. 2022. <span>“Learning Preference Distributions from Distance Measurements.”</span> In <em>2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)</em>, 1–8. <a href="https://doi.org/10.1109/Allerton49937.2022.9929404">https://doi.org/10.1109/Allerton49937.2022.9929404</a>.
</div>
<div id="ref-2307.09288" class="csl-entry" role="listitem">
Touvron, Hugo et al. 2023. <span>“Llama 2: Open Foundation and Fine-Tuned Chat Models.”</span> <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>.
</div>
<div id="ref-yu2020meta" class="csl-entry" role="listitem">
Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. 2020. <span>“Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.”</span> In <em>Conference on Robot Learning</em>, 1094–1100. PMLR.
</div>
<div id="ref-zhou2019watch" class="csl-entry" role="listitem">
Zhou, Allan, Eric Jang, Daniel Kappler, Alex Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, and Chelsea Finn. 2019. <span>“Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards.”</span> In <em>International Conference on Learning Representations</em>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../src/003-measure.html" class="pagination-link" aria-label="Model-Based Preference Optimization">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Model-Based Preference Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu"># Human Decision Making and Choice Models {#ch-human-decision-making-choice-models}</span></span>
<span id="cb7-2"><a href="#cb7-2"></a></span>
<span id="cb7-3"><a href="#cb7-3"></a>::: {.content-visible when-format="html"}</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>&lt;iframe</span>
<span id="cb7-6"><a href="#cb7-6"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/"</span>
<span id="cb7-7"><a href="#cb7-7"></a>  style="width:45%; height:225px;"</span>
<span id="cb7-8"><a href="#cb7-8"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb7-9"><a href="#cb7-9"></a>&lt;iframe</span>
<span id="cb7-10"><a href="#cb7-10"></a>  src="https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/"</span>
<span id="cb7-11"><a href="#cb7-11"></a>  style="width:45%; height:225px;"</span>
<span id="cb7-12"><a href="#cb7-12"></a>&gt;&lt;/iframe&gt;</span>
<span id="cb7-13"><a href="#cb7-13"></a><span class="co">[</span><span class="ot">Fullscreen Part 1</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.1.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co">[</span><span class="ot">Fullscreen Part 2</span><span class="co">](https://web.stanford.edu/class/cs329h/slides/2.2.choice_models/#/)</span>{.btn .btn-outline-primary .btn role="button"}</span>
<span id="cb7-15"><a href="#cb7-15"></a></span>
<span id="cb7-16"><a href="#cb7-16"></a>:::</span>
<span id="cb7-17"><a href="#cb7-17"></a></span>
<span id="cb7-18"><a href="#cb7-18"></a><span class="fu">## Introduction</span></span>
<span id="cb7-19"><a href="#cb7-19"></a></span>
<span id="cb7-20"><a href="#cb7-20"></a>Human preference modeling aims to capture humans' decision making</span>
<span id="cb7-21"><a href="#cb7-21"></a>processes in a probabilistic framework. Many problems would benefit from</span>
<span id="cb7-22"><a href="#cb7-22"></a>a quantitative perspective, enabling an understanding of how humans</span>
<span id="cb7-23"><a href="#cb7-23"></a>engage with the world. While human decision-making is only somewhat</span>
<span id="cb7-24"><a href="#cb7-24"></a>understood, we can use real-world data representing the outcomes of</span>
<span id="cb7-25"><a href="#cb7-25"></a>decisions to align human-facing systems with user preferences. Through</span>
<span id="cb7-26"><a href="#cb7-26"></a>our exploration of human preference models, we will ground ourselves in</span>
<span id="cb7-27"><a href="#cb7-27"></a>building a health coaching system that can provide meal recommendations</span>
<span id="cb7-28"><a href="#cb7-28"></a>aligned with a user's dietary needs and preferences. Examples of</span>
<span id="cb7-29"><a href="#cb7-29"></a>scenarios which can benefit from a model of how humans make choices</span>
<span id="cb7-30"><a href="#cb7-30"></a>include:</span>
<span id="cb7-31"><a href="#cb7-31"></a></span>
<span id="cb7-32"><a href="#cb7-32"></a><span class="ss">1.  </span>**Health coaching:** Humans express their preferences every time</span>
<span id="cb7-33"><a href="#cb7-33"></a>    they pick lunch for consumption. Humans may have several goals</span>
<span id="cb7-34"><a href="#cb7-34"></a>    related to nutrition, such as weight loss and improving</span>
<span id="cb7-35"><a href="#cb7-35"></a>    concentration. We can learn how a given individual or set of</span>
<span id="cb7-36"><a href="#cb7-36"></a>    individuals prefer to eat to provide personalized recommendations to</span>
<span id="cb7-37"><a href="#cb7-37"></a>    help them attain their goals. This chapter will use this use case to</span>
<span id="cb7-38"><a href="#cb7-38"></a>    ground human preference modeling in a real-life application.</span>
<span id="cb7-39"><a href="#cb7-39"></a></span>
<span id="cb7-40"><a href="#cb7-40"></a><span class="ss">2.  </span>**Social media:** Platforms have a far greater amount of content</span>
<span id="cb7-41"><a href="#cb7-41"></a>    than one can consume in a lifetime, yet such products must aim to</span>
<span id="cb7-42"><a href="#cb7-42"></a>    maximize user engagement. To accomplish this, we can learn what</span>
<span id="cb7-43"><a href="#cb7-43"></a>    specific things people like to see in their feeds to optimize the</span>
<span id="cb7-44"><a href="#cb7-44"></a>    value they gain out of their time on social media. For example, the</span>
<span id="cb7-45"><a href="#cb7-45"></a>    video feed social media platform <span class="co">[</span><span class="ot">TikTok</span><span class="co">](https://www.tiktok.com/)</span></span>
<span id="cb7-46"><a href="#cb7-46"></a>    has had viral adoption due to its notorious ability to personalize a</span>
<span id="cb7-47"><a href="#cb7-47"></a>    feed for its users based on their preferences.</span>
<span id="cb7-48"><a href="#cb7-48"></a></span>
<span id="cb7-49"><a href="#cb7-49"></a><span class="ss">3.  </span>**Shopping:** Retail corporations largely aim to maximize revenue by</span>
<span id="cb7-50"><a href="#cb7-50"></a>    making it easy for people to make purchases. Recommendation systems</span>
<span id="cb7-51"><a href="#cb7-51"></a>    on online shopping platforms provide a mechanism for curating</span>
<span id="cb7-52"><a href="#cb7-52"></a>    specific items based on an individual's previous purchases (or even</span>
<span id="cb7-53"><a href="#cb7-53"></a>    browsing history) to make shoppers aware of items they may like and,</span>
<span id="cb7-54"><a href="#cb7-54"></a>    therefore, purchase.</span>
<span id="cb7-55"><a href="#cb7-55"></a></span>
<span id="cb7-56"><a href="#cb7-56"></a>::: {#tbl-philosophy}</span>
<span id="cb7-57"><a href="#cb7-57"></a>  -----------------------------------------------------------------------</span>
<span id="cb7-58"><a href="#cb7-58"></a>  Application                         Human Preference</span>
<span id="cb7-59"><a href="#cb7-59"></a>  ----------------------------------- -----------------------------------</span>
<span id="cb7-60"><a href="#cb7-60"></a>  Computer vision: train a neural     This is how humans process images</span>
<span id="cb7-61"><a href="#cb7-61"></a>  network to predict bounding boxes   by identifying the position and</span>
<span id="cb7-62"><a href="#cb7-62"></a>  delineating all instances of dogs   geometry of the things we see in</span>
<span id="cb7-63"><a href="#cb7-63"></a>  in an image                         them</span>
<span id="cb7-64"><a href="#cb7-64"></a></span>
<span id="cb7-65"><a href="#cb7-65"></a>  Natural language processing: train  Coherent text is itself a</span>
<span id="cb7-66"><a href="#cb7-66"></a>  a model to generate coherent text   human-created and defined concept,</span>
<span id="cb7-67"><a href="#cb7-67"></a>                                      and we prefer that any</span>
<span id="cb7-68"><a href="#cb7-68"></a>                                      synthetically generated text</span>
<span id="cb7-69"><a href="#cb7-69"></a>                                      matches that of humans</span>
<span id="cb7-70"><a href="#cb7-70"></a></span>
<span id="cb7-71"><a href="#cb7-71"></a>  Computer vision: train a diffusion  Humans prefer that images</span>
<span id="cb7-72"><a href="#cb7-72"></a>  model to generate realistic images  accurately capture the world as</span>
<span id="cb7-73"><a href="#cb7-73"></a>  of nature                           observed by humans, and this</span>
<span id="cb7-74"><a href="#cb7-74"></a>                                      generative model should reflect the</span>
<span id="cb7-75"><a href="#cb7-75"></a>                                      details that comprise that</span>
<span id="cb7-76"><a href="#cb7-76"></a>                                      preference</span>
<span id="cb7-77"><a href="#cb7-77"></a>  -----------------------------------------------------------------------</span>
<span id="cb7-78"><a href="#cb7-78"></a></span>
<span id="cb7-79"><a href="#cb7-79"></a>  : Examples of machine learning tasks and their interpretation as</span>
<span id="cb7-80"><a href="#cb7-80"></a>  modeling human preferences.</span>
<span id="cb7-81"><a href="#cb7-81"></a>:::</span>
<span id="cb7-82"><a href="#cb7-82"></a></span>
<span id="cb7-83"><a href="#cb7-83"></a>In this chapter, we will explore how one can model human preferences,</span>
<span id="cb7-84"><a href="#cb7-84"></a>including different formulations of such models, how one can optimize</span>
<span id="cb7-85"><a href="#cb7-85"></a>these models given data, and considerations one must understand to</span>
<span id="cb7-86"><a href="#cb7-86"></a>create such systems. We note that the exact assumptions we make about</span>
<span id="cb7-87"><a href="#cb7-87"></a>human preferences in this chapter differentiate the *specific* human</span>
<span id="cb7-88"><a href="#cb7-88"></a>preference learning problem we are considering from the discriminative</span>
<span id="cb7-89"><a href="#cb7-89"></a>and generative tasks we describe in @tbl-philosophy.</span>
<span id="cb7-90"><a href="#cb7-90"></a>We describe these assumptions in @sec-foundations.</span>
<span id="cb7-91"><a href="#cb7-91"></a></span>
<span id="cb7-92"><a href="#cb7-92"></a><span class="fu">## Foundations of Preference Models {#sec-foundations}</span></span>
<span id="cb7-93"><a href="#cb7-93"></a></span>
<span id="cb7-94"><a href="#cb7-94"></a>We introduce a framework for discussing human preferences. The different</span>
<span id="cb7-95"><a href="#cb7-95"></a>methods to model these preferences @sec-models all</span>
<span id="cb7-96"><a href="#cb7-96"></a>build upon this framework.</span>
<span id="cb7-97"><a href="#cb7-97"></a></span>
<span id="cb7-98"><a href="#cb7-98"></a><span class="fu">### Axiom 1: Preference models model choice {#axiom-1-preference-models-model-choice .unnumbered}</span></span>
<span id="cb7-99"><a href="#cb7-99"></a></span>
<span id="cb7-100"><a href="#cb7-100"></a>Human preference models model the preferred choice or choices amongst a</span>
<span id="cb7-101"><a href="#cb7-101"></a>set of options. In our health coaching example, this could be modeling</span>
<span id="cb7-102"><a href="#cb7-102"></a>which meal from a set of options a person will most likely choose. An</span>
<span id="cb7-103"><a href="#cb7-103"></a>alternative framework we will explore is ranking, in which we can model</span>
<span id="cb7-104"><a href="#cb7-104"></a>an ordering of given choices from most to least desirable. It is</span>
<span id="cb7-105"><a href="#cb7-105"></a>certainly possible that there is an infinite set of options (such as in</span>
<span id="cb7-106"><a href="#cb7-106"></a>a continuous action space); in this case, our model will have to reason</span>
<span id="cb7-107"><a href="#cb7-107"></a>about a discretized set of options and may fail to capture the full</span>
<span id="cb7-108"><a href="#cb7-108"></a>space of possibilities a human would choose from in the real world.</span>
<span id="cb7-109"><a href="#cb7-109"></a></span>
<span id="cb7-110"><a href="#cb7-110"></a>Choices are *collectively exhaustive*, *mutually exclusive*, and</span>
<span id="cb7-111"><a href="#cb7-111"></a>*finite*. Human preference models must enumerate an *action space*, or</span>
<span id="cb7-112"><a href="#cb7-112"></a>the set of all possible choices included in a human decision. As such,</span>
<span id="cb7-113"><a href="#cb7-113"></a>we must ensure that the choices we enumerate capture the entire domain</span>
<span id="cb7-114"><a href="#cb7-114"></a>(collectively exhaustive) but are indeed distinct (mutually exclusive)</span>
<span id="cb7-115"><a href="#cb7-115"></a>choices. In our health coaching example, a person either chooses to eat</span>
<span id="cb7-116"><a href="#cb7-116"></a>chicken or fish. Choosing one does not affect the other.</span>
<span id="cb7-117"><a href="#cb7-117"></a></span>
<span id="cb7-118"><a href="#cb7-118"></a>A discrete set of choices is a constraint we canonically impose to</span>
<span id="cb7-119"><a href="#cb7-119"></a>ensure we can tractably model preferences and aptly estimate the</span>
<span id="cb7-120"><a href="#cb7-120"></a>parameters of preference models. This is usually sufficiently expressive</span>
<span id="cb7-121"><a href="#cb7-121"></a>to create a powerful human preference model (for example, recent</span>
<span id="cb7-122"><a href="#cb7-122"></a>generative language models have vocabulary sizes of 40,000+ and can</span>
<span id="cb7-123"><a href="#cb7-123"></a>model nearly arbitrary language sequences <span class="co">[</span><span class="ot">@Radford2018GPT</span><span class="co">]</span>). While in</span>
<span id="cb7-124"><a href="#cb7-124"></a>theory, one can imagine a continuous domain for choices, a discrete set</span>
<span id="cb7-125"><a href="#cb7-125"></a>fits nicely with most decision-making processes humans face. While human</span>
<span id="cb7-126"><a href="#cb7-126"></a>thought is extremely nuanced, most thoughts are expressed as discrete</span>
<span id="cb7-127"><a href="#cb7-127"></a>words or discrete decisions in every step humans take in the world.</span>
<span id="cb7-128"><a href="#cb7-128"></a></span>
<span id="cb7-129"><a href="#cb7-129"></a><span class="fu">### Axiom 2: Preference captures decision-making {#axiom-2-preference-captures-decision-making .unnumbered}</span></span>
<span id="cb7-130"><a href="#cb7-130"></a></span>
<span id="cb7-131"><a href="#cb7-131"></a>There are certainly cases in which human preferences don't reflect the</span>
<span id="cb7-132"><a href="#cb7-132"></a>human decision-making process, for example if there are external factors</span>
<span id="cb7-133"><a href="#cb7-133"></a>(social, political, economic) which govern a human's choices, or if one</span>
<span id="cb7-134"><a href="#cb7-134"></a>is explicitly choosing to go against their preferences in the context of</span>
<span id="cb7-135"><a href="#cb7-135"></a>exploration. However, human preference models will always do their best</span>
<span id="cb7-136"><a href="#cb7-136"></a>to model the ultimate decision, and we assume that they are in some way</span>
<span id="cb7-137"><a href="#cb7-137"></a>accounting for these other factors (and any lack of such accounting will</span>
<span id="cb7-138"><a href="#cb7-138"></a>result in a biased model). Human preferences are generally classified</span>
<span id="cb7-139"><a href="#cb7-139"></a>into two categories:</span>
<span id="cb7-140"><a href="#cb7-140"></a></span>
<span id="cb7-141"><a href="#cb7-141"></a><span class="ss">1.  </span>Revealed preferences are those one can observe retroactively from</span>
<span id="cb7-142"><a href="#cb7-142"></a>    existing data. The implicit decision-making knowledge can be</span>
<span id="cb7-143"><a href="#cb7-143"></a>    captured via learnable parameters and their usage in models which</span>
<span id="cb7-144"><a href="#cb7-144"></a>    represent relationships between input decision attributes that may</span>
<span id="cb7-145"><a href="#cb7-145"></a>    have little human interpretability, but enable powerful models of</span>
<span id="cb7-146"><a href="#cb7-146"></a>    human preference. For health coaching, we may have information about</span>
<span id="cb7-147"><a href="#cb7-147"></a>    which foods an individual has chosen previously in different</span>
<span id="cb7-148"><a href="#cb7-148"></a>    contexts, allowing us to build a model from their decisions. Such</span>
<span id="cb7-149"><a href="#cb7-149"></a>    data may be easier to acquire and can reflect real-world outcomes</span>
<span id="cb7-150"><a href="#cb7-150"></a>    (since they are, at least theoretically, inherently based on human</span>
<span id="cb7-151"><a href="#cb7-151"></a>    preferences). However, if we fail to capture sufficient context in</span>
<span id="cb7-152"><a href="#cb7-152"></a>    such data, human preference models may not sufficiently capture</span>
<span id="cb7-153"><a href="#cb7-153"></a>    human preferences.</span>
<span id="cb7-154"><a href="#cb7-154"></a></span>
<span id="cb7-155"><a href="#cb7-155"></a><span class="ss">2.  </span>Stated preferences are those individuals explicitly indicate in</span>
<span id="cb7-156"><a href="#cb7-156"></a>    potentially experimental conditions. The explicit knowledge may be</span>
<span id="cb7-157"><a href="#cb7-157"></a>    leveraged by including inductive biases during modeling (for</span>
<span id="cb7-158"><a href="#cb7-158"></a>    example, the context used in a model) which are reasonable</span>
<span id="cb7-159"><a href="#cb7-159"></a>    assumptions for how a human would consider a set of options.This may</span>
<span id="cb7-160"><a href="#cb7-160"></a>    include controlled experiments or studies. This may be harder to</span>
<span id="cb7-161"><a href="#cb7-161"></a>    obtain and somewhat biased, as they can be hypothetical or only</span>
<span id="cb7-162"><a href="#cb7-162"></a>    accurately reflect a piece of the overall context of a decision.</span>
<span id="cb7-163"><a href="#cb7-163"></a>    However, they enable greater control of the decision-making process.</span>
<span id="cb7-164"><a href="#cb7-164"></a></span>
<span id="cb7-165"><a href="#cb7-165"></a><span class="fu">#### Human Rationality {#human-rationality .unnumbered}</span></span>
<span id="cb7-166"><a href="#cb7-166"></a></span>
<span id="cb7-167"><a href="#cb7-167"></a>Modeling decision-making must also take into account the rational and</span>
<span id="cb7-168"><a href="#cb7-168"></a>irrational behaviour of humans. Therefore we consider *rationality</span>
<span id="cb7-169"><a href="#cb7-169"></a>assumptions* as a fundamental aspect of understanding how individuals</span>
<span id="cb7-170"><a href="#cb7-170"></a>make decisions. These assumptions provide a framework for predicting and</span>
<span id="cb7-171"><a href="#cb7-171"></a>modeling human behavior by outlining the principles that guide</span>
<span id="cb7-172"><a href="#cb7-172"></a>decision-making processes <span class="co">[</span><span class="ot">@keisler2003common</span><span class="co">]</span>.</span>
<span id="cb7-173"><a href="#cb7-173"></a></span>
<span id="cb7-174"><a href="#cb7-174"></a>Perfect rationality posits that individuals always make decisions that</span>
<span id="cb7-175"><a href="#cb7-175"></a>maximize their utility. It assumes that individuals have complete</span>
<span id="cb7-176"><a href="#cb7-176"></a>information and the cognitive ability to process this information to</span>
<span id="cb7-177"><a href="#cb7-177"></a>make optimal choices <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. This assumption is often</span>
<span id="cb7-178"><a href="#cb7-178"></a>used in economic models to predict how rational agents would behave</span>
<span id="cb7-179"><a href="#cb7-179"></a>under ideal conditions. However, numerous studies have shown that this</span>
<span id="cb7-180"><a href="#cb7-180"></a>assumption frequently fails to describe actual human behavior, as</span>
<span id="cb7-181"><a href="#cb7-181"></a>individuals do not always act in ways that maximize their utility due to</span>
<span id="cb7-182"><a href="#cb7-182"></a>various constraints and biases <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. Bounded</span>
<span id="cb7-183"><a href="#cb7-183"></a>rationality, on the other hand, acknowledges that individuals operate</span>
<span id="cb7-184"><a href="#cb7-184"></a>within the limits of their information and cognitive capabilities.</span>
<span id="cb7-185"><a href="#cb7-185"></a>Decisions are made using heuristics or rules of thumb rather than</span>
<span id="cb7-186"><a href="#cb7-186"></a>through exhaustive analysis, reflecting the practical constraints of</span>
<span id="cb7-187"><a href="#cb7-187"></a>real-world decision-making <span class="co">[</span><span class="ot">@simon1972theories</span><span class="co">]</span>. This concept,</span>
<span id="cb7-188"><a href="#cb7-188"></a>introduced by Herbert Simon, recognizes the limitations of human</span>
<span id="cb7-189"><a href="#cb7-189"></a>cognitive processing and the impact of these limitations on</span>
<span id="cb7-190"><a href="#cb7-190"></a>decision-making. Simon's theory suggests that instead of optimizing,</span>
<span id="cb7-191"><a href="#cb7-191"></a>individuals satisfy, seeking solutions or decisions that are "good</span>
<span id="cb7-192"><a href="#cb7-192"></a>enough" under the circumstances <span class="co">[</span><span class="ot">@simon1972theories</span><span class="co">]</span>. Noisy rationality</span>
<span id="cb7-193"><a href="#cb7-193"></a>assumes that decisions are influenced by random noise, resulting in</span>
<span id="cb7-194"><a href="#cb7-194"></a>probabilistic choice behavior. This means that while individuals aim to</span>
<span id="cb7-195"><a href="#cb7-195"></a>maximize their utility, random factors can lead to deviations from</span>
<span id="cb7-196"><a href="#cb7-196"></a>perfectly rational choices. This approach is useful for modeling</span>
<span id="cb7-197"><a href="#cb7-197"></a>behavior in situations where decisions are not entirely deterministic</span>
<span id="cb7-198"><a href="#cb7-198"></a>and are subject to variability <span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>. This</span>
<span id="cb7-199"><a href="#cb7-199"></a>probabilistic approach aligns with findings from behavioral economics</span>
<span id="cb7-200"><a href="#cb7-200"></a>and psychology, which indicate that human decision-making is often</span>
<span id="cb7-201"><a href="#cb7-201"></a>inconsistent and influenced by various random factors</span>
<span id="cb7-202"><a href="#cb7-202"></a><span class="co">[</span><span class="ot">@miljkovic2005rational</span><span class="co">]</span>.</span>
<span id="cb7-203"><a href="#cb7-203"></a></span>
<span id="cb7-204"><a href="#cb7-204"></a>Understanding rationality assumptions is crucial for modeling and</span>
<span id="cb7-205"><a href="#cb7-205"></a>predicting human behavior in various decision-making scenarios. These</span>
<span id="cb7-206"><a href="#cb7-206"></a>assumptions provide the foundation for developing models that can</span>
<span id="cb7-207"><a href="#cb7-207"></a>simulate and analyze how individuals interact with one another and their</span>
<span id="cb7-208"><a href="#cb7-208"></a>environment. By incorporating different types of rationality,</span>
<span id="cb7-209"><a href="#cb7-209"></a>researchers can create more accurate and realistic models that reflect</span>
<span id="cb7-210"><a href="#cb7-210"></a>the complexities of human decision-making. This comprehensive approach</span>
<span id="cb7-211"><a href="#cb7-211"></a>enhances the predictive power of models and improves the understanding</span>
<span id="cb7-212"><a href="#cb7-212"></a>of human behavior in economic and social contexts</span>
<span id="cb7-213"><a href="#cb7-213"></a><span class="co">[</span><span class="ot">@miljkovic2005rational; @simon1972theories</span><span class="co">]</span>.</span>
<span id="cb7-214"><a href="#cb7-214"></a></span>
<span id="cb7-215"><a href="#cb7-215"></a>Luce's axiom of choice <span class="co">[</span><span class="ot">@Luce1977</span><span class="co">]</span> and Boltzmann's Rationality provide a</span>
<span id="cb7-216"><a href="#cb7-216"></a>probabilistic framework for modeling noisily-rational human behavior.</span>
<span id="cb7-217"><a href="#cb7-217"></a>Luce's axiom of choice addresses the likelihood of a human selecting an</span>
<span id="cb7-218"><a href="#cb7-218"></a>option $o$ from a set $O$. Desirability is represented by a value</span>
<span id="cb7-219"><a href="#cb7-219"></a>function $v : O \rightarrow \mathbb{R}^+$, with the selection probability</span>
<span id="cb7-220"><a href="#cb7-220"></a>calculated as $P(o) = \frac{v(o)}{\sum_{o' \in O} v(o')}$. Assuming</span>
<span id="cb7-221"><a href="#cb7-221"></a>there is an underlying reward for each option $R(o) \in \mathbb{R}$ such that</span>
<span id="cb7-222"><a href="#cb7-222"></a>$v(o) = e^{R(o)}$, we get</span>
<span id="cb7-223"><a href="#cb7-223"></a>$P(o) = \frac{e^{R(o)}}{\sum_{\bar{o} \in \mathcal{O}} e^{R(\bar{o})}}$.</span>
<span id="cb7-224"><a href="#cb7-224"></a>Essentially, "A human will act out a trajectory with a probability</span>
<span id="cb7-225"><a href="#cb7-225"></a>proportional to the exponentiated return they receive for the</span>
<span id="cb7-226"><a href="#cb7-226"></a>trajectory." This probabilistic approach challenges the traditional</span>
<span id="cb7-227"><a href="#cb7-227"></a>assumption of perfect economic rationality, where individuals always</span>
<span id="cb7-228"><a href="#cb7-228"></a>make decisions that maximize their utility. When choices involve</span>
<span id="cb7-229"><a href="#cb7-229"></a>trajectories $\xi \in \Xi$ (sequences of actions), the Boltzmann model</span>
<span id="cb7-230"><a href="#cb7-230"></a><span class="co">[</span><span class="ot">@VonNeumannMorgenstern1945</span><span class="co">]</span> is used. Here, the reward $R$ is typically</span>
<span id="cb7-231"><a href="#cb7-231"></a>a function of a feature vector $\phi : \Xi \rightarrow \mathbb{R}^k$, and the</span>
<span id="cb7-232"><a href="#cb7-232"></a>probability density is given by</span>
<span id="cb7-233"><a href="#cb7-233"></a>$p(\xi) = \frac{e^{R(\phi(\xi))}}{\int_{\Xi} e^{R(\phi(\bar{\xi}))} d\bar{\xi}}$.</span>
<span id="cb7-234"><a href="#cb7-234"></a>Boltzmann Rationality serves a critical role in human preferences and</span>
<span id="cb7-235"><a href="#cb7-235"></a>decision-making. It captures the probabilistic nature of human choices,</span>
<span id="cb7-236"><a href="#cb7-236"></a>recognizing that decisions are often noisy and influenced by various</span>
<span id="cb7-237"><a href="#cb7-237"></a>factors. This model is instrumental in preference modeling,</span>
<span id="cb7-238"><a href="#cb7-238"></a>accommodating human preferences' inherent variability and uncertainty.</span>
<span id="cb7-239"><a href="#cb7-239"></a></span>
<span id="cb7-240"><a href="#cb7-240"></a>However, the Luce choice axiom and Boltzmann Rationality encounter a</span>
<span id="cb7-241"><a href="#cb7-241"></a>known issue called the "duplicates problem," where there is no concept</span>
<span id="cb7-242"><a href="#cb7-242"></a>of similar actions (e.g., choosing between using a car or a train for</span>
<span id="cb7-243"><a href="#cb7-243"></a>transportation, with no particular preference). The probability of</span>
<span id="cb7-244"><a href="#cb7-244"></a>making the decision is 50% for either option. However, if we now have</span>
<span id="cb7-245"><a href="#cb7-245"></a>100 cars, under Luce/Boltzmann, we would have a 99% probability of</span>
<span id="cb7-246"><a href="#cb7-246"></a>choosing a car, which is unrealistic.</span>
<span id="cb7-247"><a href="#cb7-247"></a></span>
<span id="cb7-248"><a href="#cb7-248"></a>To address this issue, various extensions have been proposed. One such</span>
<span id="cb7-249"><a href="#cb7-249"></a>extension is the attribute rule, which interprets options as bundles of</span>
<span id="cb7-250"><a href="#cb7-250"></a>attributes. In this rule, attributes $X$ are associated with options,</span>
<span id="cb7-251"><a href="#cb7-251"></a>and they have desirability values $w(x)$. An attribute intensity</span>
<span id="cb7-252"><a href="#cb7-252"></a>function $s(x, o)$ indicates the degree to which an attribute is</span>
<span id="cb7-253"><a href="#cb7-253"></a>expressed in an option. The probability of choosing option $o$ is</span>
<span id="cb7-254"><a href="#cb7-254"></a>calculated as:</span>
<span id="cb7-255"><a href="#cb7-255"></a></span>
<span id="cb7-256"><a href="#cb7-256"></a>$$P(o) = \sum_{x \in \mathcal{X}_o} \frac{w(x)}{\sum_{\bar{x} \in \mathcal{X}_o} w(\bar{x})} \cdot \frac{s(x, o)}{\sum_{\tilde{o} \in \mathcal{O}} s(x, \bar{o})}$$</span>
<span id="cb7-257"><a href="#cb7-257"></a></span>
<span id="cb7-258"><a href="#cb7-258"></a>This equation describes a two-step process where an attribute</span>
<span id="cb7-259"><a href="#cb7-259"></a>$x \in X_O$ is first chosen according to a Luce-like rule and then an</span>
<span id="cb7-260"><a href="#cb7-260"></a>option $o \in O$ with that attribute is selected using another Luce-like</span>
<span id="cb7-261"><a href="#cb7-261"></a>rule. This approach handles duplicates gracefully by effectively</span>
<span id="cb7-262"><a href="#cb7-262"></a>creating a two-layer hierarchy in choosing an option.</span>
<span id="cb7-263"><a href="#cb7-263"></a></span>
<span id="cb7-264"><a href="#cb7-264"></a>Boltzmann Rationality finds practical applications in various fields,</span>
<span id="cb7-265"><a href="#cb7-265"></a>particularly in reinforcement learning, where it models decision-making</span>
<span id="cb7-266"><a href="#cb7-266"></a>in uncertain environments. It also applies to trajectory selection,</span>
<span id="cb7-267"><a href="#cb7-267"></a>where the probability of a sequence of actions (trajectory) is</span>
<span id="cb7-268"><a href="#cb7-268"></a>proportional to the exponential return. These applications enhance the</span>
<span id="cb7-269"><a href="#cb7-269"></a>accuracy of models that interact with or predict human behavior, making</span>
<span id="cb7-270"><a href="#cb7-270"></a>Boltzmann Rationality a vital component of the models of interaction.</span>
<span id="cb7-271"><a href="#cb7-271"></a></span>
<span id="cb7-272"><a href="#cb7-272"></a>We next explore a case study to deepen our understanding of rationality:</span>
<span id="cb7-273"><a href="#cb7-273"></a>Limiting Errors due to Similar Selection (LESS) <span class="co">[</span><span class="ot">@2001.04465</span><span class="co">]</span>. LESS</span>
<span id="cb7-274"><a href="#cb7-274"></a>takes inspiration from the attribute rule and extends it to continuous</span>
<span id="cb7-275"><a href="#cb7-275"></a>trajectories <span class="co">[</span><span class="ot">@2001.04465</span><span class="co">]</span>. The key insight is that instead of creating</span>
<span id="cb7-276"><a href="#cb7-276"></a>"attributes", which group together similar discrete options, it</span>
<span id="cb7-277"><a href="#cb7-277"></a>introduces a similarity metric on the space of continuous actions,</span>
<span id="cb7-278"><a href="#cb7-278"></a>thereby creating similar groupings on trajectories.</span>
<span id="cb7-279"><a href="#cb7-279"></a></span>
<span id="cb7-280"><a href="#cb7-280"></a>First, discussing the distinction between trajectory and feature space</span>
<span id="cb7-281"><a href="#cb7-281"></a>is important. The LESS similarity metric could be defined in trajectory</span>
<span id="cb7-282"><a href="#cb7-282"></a>space, where the trajectory is some theoretical notion of all states and</span>
<span id="cb7-283"><a href="#cb7-283"></a>actions one passes through over time. However, it is instead defined on</span>
<span id="cb7-284"><a href="#cb7-284"></a>the measured feature vector $\phi(\xi)$ associated with the agent's</span>
<span id="cb7-285"><a href="#cb7-285"></a>trajectory $\xi$. Why? In practice, one can never measure the exact</span>
<span id="cb7-286"><a href="#cb7-286"></a>trajectory with perfect fidelity. The feature vector will almost</span>
<span id="cb7-287"><a href="#cb7-287"></a>necessarily map in a one-to-many fashion with trajectories. Formally,</span>
<span id="cb7-288"><a href="#cb7-288"></a>let $\phi \in \Phi$ be the set of all possible feature vectors</span>
<span id="cb7-289"><a href="#cb7-289"></a>$\xi \in \Xi$ the set of all trajectories. The set of feature vectors</span>
<span id="cb7-290"><a href="#cb7-290"></a>belonging to a set of trajectories $\Xi' \subseteq \Xi$ is</span>
<span id="cb7-291"><a href="#cb7-291"></a>$\Phi_{\Xi'}$. We begin with equation (4) and substitute our similarity</span>
<span id="cb7-292"><a href="#cb7-292"></a>metric on feature vectors of trajectories.</span>
<span id="cb7-293"><a href="#cb7-293"></a></span>
<span id="cb7-294"><a href="#cb7-294"></a>$$\begin{aligned}</span>
<span id="cb7-295"><a href="#cb7-295"></a>    P(\xi) = \frac{e^{R(\phi(\xi))}}{\sum_{\bar{\phi} \in \Phi_{\Xi}} e^{R(\hat{\phi})}} \cdot \frac{s(\phi(\xi), \bar{\xi})}{\sum_{\hat{\xi} \in \Xi} s(\phi(\xi), \bar{\xi})}</span>
<span id="cb7-296"><a href="#cb7-296"></a>\end{aligned}$$</span>
<span id="cb7-297"><a href="#cb7-297"></a></span>
<span id="cb7-298"><a href="#cb7-298"></a>In this formulation, the first half of the product is simply Boltzmann</span>
<span id="cb7-299"><a href="#cb7-299"></a>equation. The probability of choosing trajectory $\xi$ is proportional</span>
<span id="cb7-300"><a href="#cb7-300"></a>to the exponentiated reward for the agent's measured trajectory</span>
<span id="cb7-301"><a href="#cb7-301"></a>$\phi(\xi)$, normalized by the sum of all rewards over all possible</span>
<span id="cb7-302"><a href="#cb7-302"></a>measured trajectories. The second half of the product is a normalization</span>
<span id="cb7-303"><a href="#cb7-303"></a>factor based on how similar the current trajectory is to other</span>
<span id="cb7-304"><a href="#cb7-304"></a>trajectories in feature space. We can define the similarity function as</span>
<span id="cb7-305"><a href="#cb7-305"></a>an indicator function, where $s(x, \xi) = 1$ only if $x = \phi(\xi)$.</span>
<span id="cb7-306"><a href="#cb7-306"></a>That means that multiple trajectories with the same feature vector will</span>
<span id="cb7-307"><a href="#cb7-307"></a>effectively be considered a single option. Thus, we achieve the</span>
<span id="cb7-308"><a href="#cb7-308"></a>"bundling" of trajectories, in the same way that the attribute rule</span>
<span id="cb7-309"><a href="#cb7-309"></a>bundled options under different attributes.</span>
<span id="cb7-310"><a href="#cb7-310"></a></span>
<span id="cb7-311"><a href="#cb7-311"></a>However, setting the similarity metric as an indicator function isn't</span>
<span id="cb7-312"><a href="#cb7-312"></a>sufficiently flexible. We want a proper metric that acts more as a</span>
<span id="cb7-313"><a href="#cb7-313"></a>continuous distance over the feature space. We instead define $s$ to be</span>
<span id="cb7-314"><a href="#cb7-314"></a>a *soft similarity metric* $s : \Phi \times \Xi \rightarrow \mathbb{R}^+$. It</span>
<span id="cb7-315"><a href="#cb7-315"></a>has the following properties:</span>
<span id="cb7-316"><a href="#cb7-316"></a></span>
<span id="cb7-317"><a href="#cb7-317"></a><span class="ss">1.  </span>$s(\phi(\xi), \xi) = \max_{x \in \phi, \bar{\xi} \in \Xi} s(x, \hat{\xi})) \forall (\xi \in \Xi)$</span>
<span id="cb7-318"><a href="#cb7-318"></a></span>
<span id="cb7-319"><a href="#cb7-319"></a><span class="ss">2.  </span>Symmetric: $s(\phi(\xi), \bar{\xi}) = s(\phi(\bar{\xi}), \xi)$</span>
<span id="cb7-320"><a href="#cb7-320"></a></span>
<span id="cb7-321"><a href="#cb7-321"></a><span class="ss">3.  </span>Positive Semidefinite: $s(x, \xi) \geq 0$</span>
<span id="cb7-322"><a href="#cb7-322"></a></span>
<span id="cb7-323"><a href="#cb7-323"></a>Using this redefined similarity metric $s$, we extend (5) to be a</span>
<span id="cb7-324"><a href="#cb7-324"></a>probability density on the continuous trajectory space $\mathcal{E}$, as</span>
<span id="cb7-325"><a href="#cb7-325"></a>in (3).</span>
<span id="cb7-326"><a href="#cb7-326"></a></span>
<span id="cb7-327"><a href="#cb7-327"></a>$$p(\hat{\xi}) = \frac{\frac{e^{R(\phi(\xi))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}}{\int_{\Xi}\frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\hat{\xi}), \bar{\xi}) d\bar{\xi}}d\hat{\xi}} \propto \frac{e^{R(\phi(\hat{\xi}))}}{\int_{\Xi} s(\phi(\xi), \bar{\xi}) d\bar{\xi}}$$</span>
<span id="cb7-328"><a href="#cb7-328"></a></span>
<span id="cb7-329"><a href="#cb7-329"></a>Under this formulation, the likelihood of selecting a trajectory is</span>
<span id="cb7-330"><a href="#cb7-330"></a>inversely proportional to its feature-space similarity with other</span>
<span id="cb7-331"><a href="#cb7-331"></a>trajectories. This de-weights similar trajectories, which is the desired</span>
<span id="cb7-332"><a href="#cb7-332"></a>effect for our LESS model of human decision-making. This means, though,</span>
<span id="cb7-333"><a href="#cb7-333"></a>that the "trajectory bundle" of similar trajectories still has a</span>
<span id="cb7-334"><a href="#cb7-334"></a>reasonable probability of being chosen.</span>
<span id="cb7-335"><a href="#cb7-335"></a></span>
<span id="cb7-336"><a href="#cb7-336"></a><span class="fu">### Axiom 3: Preference centers around utility {#axiom-3-preference-centers-around-utility .unnumbered}</span></span>
<span id="cb7-337"><a href="#cb7-337"></a></span>
<span id="cb7-338"><a href="#cb7-338"></a>Human preference models are centered around the notion of utility, which</span>
<span id="cb7-339"><a href="#cb7-339"></a>can mean a reward one attains after expressing one's preference over</span>
<span id="cb7-340"><a href="#cb7-340"></a>options.<span class="ot">[^1]</span> In health coaching, the utility, as a function of the</span>
<span id="cb7-341"><a href="#cb7-341"></a>health choices users make, may be satiety, latent promotion of overall</span>
<span id="cb7-342"><a href="#cb7-342"></a>health, or even a quantitative extension of life. Of course, humans</span>
<span id="cb7-343"><a href="#cb7-343"></a>don't necessarily use an explicit measure of utility --- frequently</span>
<span id="cb7-344"><a href="#cb7-344"></a>humans use qualitative factors such as emotion or external influence to</span>
<span id="cb7-345"><a href="#cb7-345"></a>make decision. However, we assume that the underlying utility mechanism</span>
<span id="cb7-346"><a href="#cb7-346"></a>of a human preference model still captures the final decision output</span>
<span id="cb7-347"><a href="#cb7-347"></a>from a human.</span>
<span id="cb7-348"><a href="#cb7-348"></a></span>
<span id="cb7-349"><a href="#cb7-349"></a>Utility can be interpreted as a scalar quantity representing the benefit</span>
<span id="cb7-350"><a href="#cb7-350"></a>or value an individual attains from selecting a given choice. Each</span>
<span id="cb7-351"><a href="#cb7-351"></a>choice has an associated utility. Human preference models capture both</span>
<span id="cb7-352"><a href="#cb7-352"></a>the utility of a choice (e.g. we model the utility value as a function</span>
<span id="cb7-353"><a href="#cb7-353"></a>of attributes of a given choice) and how the utilities interact to make</span>
<span id="cb7-354"><a href="#cb7-354"></a>a decision.<span class="ot">[^2]</span> We use the notation $U_i$ as the utility corresponding</span>
<span id="cb7-355"><a href="#cb7-355"></a>to choice $i$.</span>
<span id="cb7-356"><a href="#cb7-356"></a></span>
<span id="cb7-357"><a href="#cb7-357"></a><span class="ss">1.  </span>**The utility of a choice is a stochastic function of the choice's</span>
<span id="cb7-358"><a href="#cb7-358"></a>    attributes.** We will henceforth define utility as follows</span>
<span id="cb7-359"><a href="#cb7-359"></a>    $U_i = H_i(z_i)$ where $z_i$ is a variable describing the attributes</span>
<span id="cb7-360"><a href="#cb7-360"></a>    of choice $i$ and $H_i$ is the stochastic function defining this</span>
<span id="cb7-361"><a href="#cb7-361"></a>    choice's utility. As a simple example, we can use a 1-D linear</span>
<span id="cb7-362"><a href="#cb7-362"></a>    stochastic function to define $H_i$:</span>
<span id="cb7-363"><a href="#cb7-363"></a>    $U_i = H_i(z_i) = \beta z_i + \epsilon_i$, where $\beta$ is a</span>
<span id="cb7-364"><a href="#cb7-364"></a>    parameter of the model and $\epsilon_i$ is an unobserved factor for</span>
<span id="cb7-365"><a href="#cb7-365"></a>    choice $i$. Generally, we assume that the $\epsilon_i$ factor is a</span>
<span id="cb7-366"><a href="#cb7-366"></a>    random variable following a specified distribution, such as a</span>
<span id="cb7-367"><a href="#cb7-367"></a>    standard normal distribution. The attributes we use to represent a</span>
<span id="cb7-368"><a href="#cb7-368"></a>    choice (a single scalar value $z_i$ in this example) is a critical</span>
<span id="cb7-369"><a href="#cb7-369"></a>    design decision in defining the human preference model. These</span>
<span id="cb7-370"><a href="#cb7-370"></a>    attributes define the context our model has in representing the</span>
<span id="cb7-371"><a href="#cb7-371"></a>    human behavior we wish to capture, when choice $i$ is made. In our</span>
<span id="cb7-372"><a href="#cb7-372"></a>    health coaching example, we may hope to provide the best possible</span>
<span id="cb7-373"><a href="#cb7-373"></a>    diet recommendations for an individual. However, if our vector</span>
<span id="cb7-374"><a href="#cb7-374"></a>    representation $z_i$ of their choice $i$ does not include vital</span>
<span id="cb7-375"><a href="#cb7-375"></a>    information, such as allergy risks associated to the choice or</span>
<span id="cb7-376"><a href="#cb7-376"></a>    ingredients which make up the choice, our model may not have enough</span>
<span id="cb7-377"><a href="#cb7-377"></a>    information to properly capture the human preference.</span>
<span id="cb7-378"><a href="#cb7-378"></a></span>
<span id="cb7-379"><a href="#cb7-379"></a><span class="ss">2.  </span>**The preferred choice is that whose corresponding utility is the</span>
<span id="cb7-380"><a href="#cb7-380"></a>    largest.** Given that we model utility as the underlying benefit or</span>
<span id="cb7-381"><a href="#cb7-381"></a>    value a human derives from choosing a given option, intuitively, we</span>
<span id="cb7-382"><a href="#cb7-382"></a>    expect a human to choose the option with the largest utility. In our</span>
<span id="cb7-383"><a href="#cb7-383"></a>    example of health coaching, if we model utility as the expected</span>
<span id="cb7-384"><a href="#cb7-384"></a>    increase in lifespan, we will surely opt for the choice that</span>
<span id="cb7-385"><a href="#cb7-385"></a>    maximizes this notion of utility. In our example, since $U_1 &gt; U_2$,</span>
<span id="cb7-386"><a href="#cb7-386"></a>    our model indicates that a user would opt for the burrito.</span>
<span id="cb7-387"><a href="#cb7-387"></a></span>
<span id="cb7-388"><a href="#cb7-388"></a><span class="ss">3.  </span>**Relativity of Utility.** Given the two previously defined</span>
<span id="cb7-389"><a href="#cb7-389"></a>    characteristics of utility, we observe that only the relative</span>
<span id="cb7-390"><a href="#cb7-390"></a>    difference in utility matters. Even if $U_1 = 0.001$ and</span>
<span id="cb7-391"><a href="#cb7-391"></a>    $U_2 = 0.0005$, the model indicates the same outcome: a user prefers</span>
<span id="cb7-392"><a href="#cb7-392"></a>    option $1$. As such, even the scale of the utilities is irrelevant</span>
<span id="cb7-393"><a href="#cb7-393"></a>    within a given set of human preference data for a given individual.</span>
<span id="cb7-394"><a href="#cb7-394"></a>    In our example, we can scale the value of $\beta$ without changing</span>
<span id="cb7-395"><a href="#cb7-395"></a>    the overall outcome so long as we do not change the sign. The scale</span>
<span id="cb7-396"><a href="#cb7-396"></a>    of utilities *is* important when comparing human preferences across</span>
<span id="cb7-397"><a href="#cb7-397"></a>    datasets, or comparing the same model across different humans; since</span>
<span id="cb7-398"><a href="#cb7-398"></a>    utility may be defined differently in various datasets, perhaps</span>
<span id="cb7-399"><a href="#cb7-399"></a>    their exact values are not aligned in a manner which allows one to</span>
<span id="cb7-400"><a href="#cb7-400"></a>    robustly compare preferences between them. A common practice to</span>
<span id="cb7-401"><a href="#cb7-401"></a>    address this consideration is to standardize the utilities in each</span>
<span id="cb7-402"><a href="#cb7-402"></a>    dataset based on its variance in the observed data. Furthermore, a</span>
<span id="cb7-403"><a href="#cb7-403"></a>    human preference model may generate different scales of utilities</span>
<span id="cb7-404"><a href="#cb7-404"></a>    across different humans (based on the inputs and representation of</span>
<span id="cb7-405"><a href="#cb7-405"></a>    the human). In this case, one can standardize the utilities for each</span>
<span id="cb7-406"><a href="#cb7-406"></a>    individual based on the observed variance for that human. As we can</span>
<span id="cb7-407"><a href="#cb7-407"></a>    see, the relativity of utility can be both powerful (enabling us to</span>
<span id="cb7-408"><a href="#cb7-408"></a>    create flexible models and efficiently optimize them) and limiting</span>
<span id="cb7-409"><a href="#cb7-409"></a>    (requiring us to perform mitigations when translating models across</span>
<span id="cb7-410"><a href="#cb7-410"></a>    datasets or individuals. Still, we find the notion of utility</span>
<span id="cb7-411"><a href="#cb7-411"></a>    necessary to model human preferences as it provides a quantitative</span>
<span id="cb7-412"><a href="#cb7-412"></a>    value we can use to model human decisions.</span>
<span id="cb7-413"><a href="#cb7-413"></a></span>
<span id="cb7-414"><a href="#cb7-414"></a>As a concrete model of meal recommendation in health coaching, let us</span>
<span id="cb7-415"><a href="#cb7-415"></a>suppose that we have three choices:</span>
<span id="cb7-416"><a href="#cb7-416"></a></span>
<span id="cb7-417"><a href="#cb7-417"></a><span class="ss">1.  </span>A burrito with rice, beans, and cheese.</span>
<span id="cb7-418"><a href="#cb7-418"></a></span>
<span id="cb7-419"><a href="#cb7-419"></a><span class="ss">2.  </span>French fries covered in mayonnaise.</span>
<span id="cb7-420"><a href="#cb7-420"></a></span>
<span id="cb7-421"><a href="#cb7-421"></a><span class="ss">3.  </span>A rice bowl with beans and chicken.</span>
<span id="cb7-422"><a href="#cb7-422"></a></span>
<span id="cb7-423"><a href="#cb7-423"></a>If we design $z_i$ to be 1D, for example:</span>
<span id="cb7-424"><a href="#cb7-424"></a></span>
<span id="cb7-425"><a href="#cb7-425"></a><span class="ss">1.  </span>$z_1 = 1$ for the burrito since this is a somewhat balanced meal</span>
<span id="cb7-426"><a href="#cb7-426"></a>    that may help prolong the lifespan, which a user prefers.</span>
<span id="cb7-427"><a href="#cb7-427"></a></span>
<span id="cb7-428"><a href="#cb7-428"></a><span class="ss">2.  </span>$z_2 = -1$ since this is unhealthy due to being deep fried,</span>
<span id="cb7-429"><a href="#cb7-429"></a>    including saturated fats, and potentially reducing lifespan.</span>
<span id="cb7-430"><a href="#cb7-430"></a></span>
<span id="cb7-431"><a href="#cb7-431"></a><span class="ss">3.  </span>$z_3 = 1$ since a rice bowl is another healthy meal.</span>
<span id="cb7-432"><a href="#cb7-432"></a></span>
<span id="cb7-433"><a href="#cb7-433"></a>After observing the choices of a user who likes to eat healthily, we</span>
<span id="cb7-434"><a href="#cb7-434"></a>might learn that $\beta = 1$ is the best parameter for this model, and</span>
<span id="cb7-435"><a href="#cb7-435"></a>maybe we assume that $\epsilon \sim \mathcal{N}(0, 1)$. Then, this model implies</span>
<span id="cb7-436"><a href="#cb7-436"></a>that $U_1 = 1 \cdot 1 + 0.03 = 1.03$,</span>
<span id="cb7-437"><a href="#cb7-437"></a>$U_2 = 1 \cdot -1 + (-0.07) = -1.07$, $U_3 = 1 \cdot 1 + (0.02) = 1.02$,</span>
<span id="cb7-438"><a href="#cb7-438"></a>which means that the user, for whom $\beta = 1$ is the learned</span>
<span id="cb7-439"><a href="#cb7-439"></a>parameter, they would prefer the first meal, with the third meal as a</span>
<span id="cb7-440"><a href="#cb7-440"></a>close second option.</span>
<span id="cb7-441"><a href="#cb7-441"></a></span>
<span id="cb7-442"><a href="#cb7-442"></a>If we design $z_i$ to be 3D, to indicate the carbohydrate, protein, and</span>
<span id="cb7-443"><a href="#cb7-443"></a>fat content of each meal, then for example:</span>
<span id="cb7-444"><a href="#cb7-444"></a></span>
<span id="cb7-445"><a href="#cb7-445"></a><span class="ss">1.  </span>$z_1 = (1, 1, 0.1)$ for the burrito</span>
<span id="cb7-446"><a href="#cb7-446"></a></span>
<span id="cb7-447"><a href="#cb7-447"></a><span class="ss">2.  </span>$z_2 = (1, 0, 1)$ for the fries</span>
<span id="cb7-448"><a href="#cb7-448"></a></span>
<span id="cb7-449"><a href="#cb7-449"></a><span class="ss">3.  </span>$z_3 = (1, 1, 0.2)$ for the rice bowl.</span>
<span id="cb7-450"><a href="#cb7-450"></a></span>
<span id="cb7-451"><a href="#cb7-451"></a>After observing the choices of a user who likes to eat healthy, we might</span>
<span id="cb7-452"><a href="#cb7-452"></a>learn that $\beta = (1, 1, -1)$ is the best parameter for this model,</span>
<span id="cb7-453"><a href="#cb7-453"></a>and maybe we assume that $\epsilon \sim \mathcal{N}(0, 1)$. Then, this model</span>
<span id="cb7-454"><a href="#cb7-454"></a>implies that $U_1 = (1, 1, 0.1) \cdot (1, 1, -1) + 0.01 = 1.91$,</span>
<span id="cb7-455"><a href="#cb7-455"></a>$U_2 = (1, 0, 1) \cdot (1, 1, -1) + 0.03 = 0.03$,</span>
<span id="cb7-456"><a href="#cb7-456"></a>$U_3 = (1, 1, 0.2) \cdot (1, 1, -1) - 0.07 = 1.73$, which means that the</span>
<span id="cb7-457"><a href="#cb7-457"></a>user prefers meals 1 and 3, which again have the best utility, but in</span>
<span id="cb7-458"><a href="#cb7-458"></a>this multi-dimensional representation of $z_i$, we start understanding</span>
<span id="cb7-459"><a href="#cb7-459"></a>how the two preferred meals are related (low fat and high protein).</span>
<span id="cb7-460"><a href="#cb7-460"></a></span>
<span id="cb7-461"><a href="#cb7-461"></a><span class="fu">## Models of Individual Choices {#sec-models}</span></span>
<span id="cb7-462"><a href="#cb7-462"></a></span>
<span id="cb7-463"><a href="#cb7-463"></a>After exploring motivations for preference learning and the framework we</span>
<span id="cb7-464"><a href="#cb7-464"></a>use to characterize human preferences to enable modeling, we now expand</span>
<span id="cb7-465"><a href="#cb7-465"></a>on the common probabilistic methods used to model human preference</span>
<span id="cb7-466"><a href="#cb7-466"></a>tasks. We will instantiate these models for our real-world health</span>
<span id="cb7-467"><a href="#cb7-467"></a>coaching application throughout as a pedagogical example. Specifically,</span>
<span id="cb7-468"><a href="#cb7-468"></a>we can define the following domain for meal choices:</span>
<span id="cb7-469"><a href="#cb7-469"></a>$z_i, \beta \in \mathbb{Z}^3$, where $z_i$ defines the representation of a</span>
<span id="cb7-470"><a href="#cb7-470"></a>meal option with the three dimensions representing the carbohydrate,</span>
<span id="cb7-471"><a href="#cb7-471"></a>protein, and lipid macronutrient content of the meal, respectively, all</span>
<span id="cb7-472"><a href="#cb7-472"></a>measured in grams. $\beta$ is a parameter of the model. This simple</span>
<span id="cb7-473"><a href="#cb7-473"></a>representation will allow us to consider how different probabilistic</span>
<span id="cb7-474"><a href="#cb7-474"></a>frameworks for human preferences can model a user's meal preferences.</span>
<span id="cb7-475"><a href="#cb7-475"></a>The information representation we instantiate here can accommodate</span>
<span id="cb7-476"><a href="#cb7-476"></a>scalar and high-dimensional vectors. While we use a mixture of integer</span>
<span id="cb7-477"><a href="#cb7-477"></a>and real-valued vectors in this simple example, we refer the reader to</span>
<span id="cb7-478"><a href="#cb7-478"></a>code in the practicum section for an example where vectors are all</span>
<span id="cb7-479"><a href="#cb7-479"></a>real-valued. If we let $z = <span class="co">[</span><span class="ot">20, 15, 3</span><span class="co">]</span>$ and $\beta = <span class="co">[</span><span class="ot">0.2, 1, -3</span><span class="co">]</span>$.</span>
<span id="cb7-480"><a href="#cb7-480"></a>This corresponds to a meal with 20g carbohydrates, 15g protein, and 3g</span>
<span id="cb7-481"><a href="#cb7-481"></a>lipids. In the following sections, we discover how to learn the</span>
<span id="cb7-482"><a href="#cb7-482"></a>parameter $\beta$ and how to predict $y$ for this meal, which indicates</span>
<span id="cb7-483"><a href="#cb7-483"></a>whether the user chooses it or refuses it.</span>
<span id="cb7-484"><a href="#cb7-484"></a></span>
<span id="cb7-485"><a href="#cb7-485"></a><span class="fu">### Data Collection</span></span>
<span id="cb7-486"><a href="#cb7-486"></a></span>
<span id="cb7-487"><a href="#cb7-487"></a><span class="fu">##### Pairwise Sampling {#pairwise-sampling .unnumbered}</span></span>
<span id="cb7-488"><a href="#cb7-488"></a></span>
<span id="cb7-489"><a href="#cb7-489"></a>In pairwise sampling, participants compare two options simultaneously to</span>
<span id="cb7-490"><a href="#cb7-490"></a>determine which is preferred. The goal is to understand relative</span>
<span id="cb7-491"><a href="#cb7-491"></a>preferences between pairs of items. This method is frequently used in</span>
<span id="cb7-492"><a href="#cb7-492"></a>preference and choice studies to gather detailed preference data. Two</span>
<span id="cb7-493"><a href="#cb7-493"></a>key models used in pairwise sampling are the Thurstonian and</span>
<span id="cb7-494"><a href="#cb7-494"></a>Bradley-Terry models <span class="co">[</span><span class="ot">@cattelan2012</span><span class="co">]</span>. The Thurstonian model assumes each</span>
<span id="cb7-495"><a href="#cb7-495"></a>item $i$ has a true score $u_i$ following a normal distribution. The</span>
<span id="cb7-496"><a href="#cb7-496"></a>difference $d_{ij} = u_i - u_j$ is also normally distributed. The</span>
<span id="cb7-497"><a href="#cb7-497"></a>probability that item $i$ is preferred over item $j$ is given by</span>
<span id="cb7-498"><a href="#cb7-498"></a>$P(i \succ j) = \Phi \left( \frac{u_i - u_j}{\sqrt{2\sigma^2}} \right)$,</span>
<span id="cb7-499"><a href="#cb7-499"></a>where $\Phi$ is the cumulative normal distribution function. The</span>
<span id="cb7-500"><a href="#cb7-500"></a>denominator $\sqrt{2\sigma^2}$ is the standard deviation of the</span>
<span id="cb7-501"><a href="#cb7-501"></a>difference $d_{ij} = u_i - u_j$ when $u_i$ and $u_j$ are normally</span>
<span id="cb7-502"><a href="#cb7-502"></a>distributed with variance $\sigma^2$<span class="co">[</span><span class="ot">@cattelan2012</span><span class="co">]</span>. The Bradley-Terry</span>
<span id="cb7-503"><a href="#cb7-503"></a>model defines the probability of preference based on latent scores</span>
<span id="cb7-504"><a href="#cb7-504"></a>$\beta_i$ and $\beta_j$. The probability that item $i$ is preferred over</span>
<span id="cb7-505"><a href="#cb7-505"></a>item $j$ is</span>
<span id="cb7-506"><a href="#cb7-506"></a>$P(i \succ j) = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$. This</span>
<span id="cb7-507"><a href="#cb7-507"></a>model is used to estimate relative strengths or preferences based on</span>
<span id="cb7-508"><a href="#cb7-508"></a>latent scores. <span class="co">[</span><span class="ot">@cattelan2012</span><span class="co">]</span>.</span>
<span id="cb7-509"><a href="#cb7-509"></a></span>
<span id="cb7-510"><a href="#cb7-510"></a><span class="fu">##### Rank-Order Sampling {#rank-order-sampling .unnumbered}</span></span>
<span id="cb7-511"><a href="#cb7-511"></a></span>
<span id="cb7-512"><a href="#cb7-512"></a>Rank-order sampling methods enable analysis of human preferences by</span>
<span id="cb7-513"><a href="#cb7-513"></a>asking participants to rank a set of items from most to least preferred.</span>
<span id="cb7-514"><a href="#cb7-514"></a>This approach is widely used in voting systems, market research, and</span>
<span id="cb7-515"><a href="#cb7-515"></a>psychological studies to understand the overall preference ordering</span>
<span id="cb7-516"><a href="#cb7-516"></a>among a set of items. Rank-order sampling offers comprehensive</span>
<span id="cb7-517"><a href="#cb7-517"></a>preference data, capturing detailed information about the relative</span>
<span id="cb7-518"><a href="#cb7-518"></a>ranking of multiple items. This richness makes them suitable for various</span>
<span id="cb7-519"><a href="#cb7-519"></a>applications, including market research, voting systems, sports</span>
<span id="cb7-520"><a href="#cb7-520"></a>competitions, and recommender systems. However, these models can be more</span>
<span id="cb7-521"><a href="#cb7-521"></a>complex and time-consuming for participants compared to pairwise</span>
<span id="cb7-522"><a href="#cb7-522"></a>comparisons, and they impose a higher cognitive load, especially with</span>
<span id="cb7-523"><a href="#cb7-523"></a>large sets of items. Additionally, participants may show inconsistencies</span>
<span id="cb7-524"><a href="#cb7-524"></a>when ranking many items <span class="co">[</span><span class="ot">@ragain2019</span><span class="co">]</span>.</span>
<span id="cb7-525"><a href="#cb7-525"></a></span>
<span id="cb7-526"><a href="#cb7-526"></a><span class="fu">##### Rating-Scale Sampling {#rating-scale-sampling .unnumbered}</span></span>
<span id="cb7-527"><a href="#cb7-527"></a></span>
<span id="cb7-528"><a href="#cb7-528"></a>Rating-scale sampling is a method in which participants rate items on a</span>
<span id="cb7-529"><a href="#cb7-529"></a>numerical scale to measure the intensity of preference or attitude</span>
<span id="cb7-530"><a href="#cb7-530"></a>towards items. These models are commonly used in surveys, product</span>
<span id="cb7-531"><a href="#cb7-531"></a>reviews, and psychological assessments to gather detailed information on</span>
<span id="cb7-532"><a href="#cb7-532"></a>how participants feel about various subjects. The Likert scale is a</span>
<span id="cb7-533"><a href="#cb7-533"></a>widely used rating-scale model. In this approach, participants rate</span>
<span id="cb7-534"><a href="#cb7-534"></a>items on a fixed-point scale, typically ranging from 1 to 5 or 1 to 7,</span>
<span id="cb7-535"><a href="#cb7-535"></a>to measure levels of agreement or satisfaction. For instance, a Likert</span>
<span id="cb7-536"><a href="#cb7-536"></a>scale might ask participants to rate their agreement with statements</span>
<span id="cb7-537"><a href="#cb7-537"></a>such as "Strongly Disagree" to "Strongly Agree" <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>. This</span>
<span id="cb7-538"><a href="#cb7-538"></a>method is prevalent in survey research, customer satisfaction studies,</span>
<span id="cb7-539"><a href="#cb7-539"></a>and attitude measurement. Another key model is the continuous rating</span>
<span id="cb7-540"><a href="#cb7-540"></a>scale, where participants mark a point on a continuous line to indicate</span>
<span id="cb7-541"><a href="#cb7-541"></a>their preference or attitude. This provides a more nuanced measure</span>
<span id="cb7-542"><a href="#cb7-542"></a>compared to discrete scales. For example, participants might indicate</span>
<span id="cb7-543"><a href="#cb7-543"></a>their satisfaction on a line ranging from "Very Unsatisfied" to "Very</span>
<span id="cb7-544"><a href="#cb7-544"></a>Satisfied" <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>. This model is used in detailed feedback</span>
<span id="cb7-545"><a href="#cb7-545"></a>mechanisms, user experience studies, and fine-grained preference</span>
<span id="cb7-546"><a href="#cb7-546"></a>measurements.</span>
<span id="cb7-547"><a href="#cb7-547"></a></span>
<span id="cb7-548"><a href="#cb7-548"></a>Rating-scale sampling offers several advantages. They are simple for</span>
<span id="cb7-549"><a href="#cb7-549"></a>participants to understand and use, provide rich data on the intensity</span>
<span id="cb7-550"><a href="#cb7-550"></a>of preferences, and are flexible enough for various types of</span>
<span id="cb7-551"><a href="#cb7-551"></a>measurements (e.g., agreement, satisfaction). Moreover, the data</span>
<span id="cb7-552"><a href="#cb7-552"></a>collected can be easily analyzed using standard statistical methods</span>
<span id="cb7-553"><a href="#cb7-553"></a><span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>.</span>
<span id="cb7-554"><a href="#cb7-554"></a></span>
<span id="cb7-555"><a href="#cb7-555"></a>Applications include data collection on opinions, attitudes, and</span>
<span id="cb7-556"><a href="#cb7-556"></a>behaviors; in product reviews to measure customer satisfaction and</span>
<span id="cb7-557"><a href="#cb7-557"></a>product quality; in psychological assessments to evaluate mental states,</span>
<span id="cb7-558"><a href="#cb7-558"></a>personality traits, and attitudes; and in user experience studies to</span>
<span id="cb7-559"><a href="#cb7-559"></a>understand user satisfaction and usability of products <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>.</span>
<span id="cb7-560"><a href="#cb7-560"></a>However, rating-scale sampling methods also have limitations. Ratings</span>
<span id="cb7-561"><a href="#cb7-561"></a>can be influenced by personal biases and interpretations of scales,</span>
<span id="cb7-562"><a href="#cb7-562"></a>leading to subjectivity. There is a central tendency bias, where</span>
<span id="cb7-563"><a href="#cb7-563"></a>participants may avoid extreme ratings, resulting in a clustering of</span>
<span id="cb7-564"><a href="#cb7-564"></a>responses around the middle. Different participants might interpret</span>
<span id="cb7-565"><a href="#cb7-565"></a>scale points differently, and fixed-point scales may not capture the</span>
<span id="cb7-566"><a href="#cb7-566"></a>full nuance of participants' preferences or attitudes <span class="co">[</span><span class="ot">@harpe2015</span><span class="co">]</span>.</span>
<span id="cb7-567"><a href="#cb7-567"></a></span>
<span id="cb7-568"><a href="#cb7-568"></a><span class="fu">##### Best-Worst Scaling {#best-worst-scaling .unnumbered}</span></span>
<span id="cb7-569"><a href="#cb7-569"></a></span>
<span id="cb7-570"><a href="#cb7-570"></a>Best-Worst Scaling (BWS) is a powerful method for understanding</span>
<span id="cb7-571"><a href="#cb7-571"></a>preferences and the relative importance of different items. In BWS,</span>
<span id="cb7-572"><a href="#cb7-572"></a>participants are presented with a set of items and are asked to identify</span>
<span id="cb7-573"><a href="#cb7-573"></a>the most and least preferred options. This method helps to gather</span>
<span id="cb7-574"><a href="#cb7-574"></a>detailed preference data, providing more nuanced insights than</span>
<span id="cb7-575"><a href="#cb7-575"></a>traditional ranking or rating systems. The primary objective of BWS is</span>
<span id="cb7-576"><a href="#cb7-576"></a>to discern the relative importance or preference of items within a set,</span>
<span id="cb7-577"><a href="#cb7-577"></a>making it widely applicable in various fields such as market research,</span>
<span id="cb7-578"><a href="#cb7-578"></a>health economics, and social sciences <span class="co">[</span><span class="ot">@campbell2015</span><span class="co">]</span>.</span>
<span id="cb7-579"><a href="#cb7-579"></a></span>
<span id="cb7-580"><a href="#cb7-580"></a>A key method within BWS is MaxDiff Analysis, which involves presenting</span>
<span id="cb7-581"><a href="#cb7-581"></a>participants with sets of items and asking them to select the best and</span>
<span id="cb7-582"><a href="#cb7-582"></a>worst options. This approach yields richer data by identifying extremes</span>
<span id="cb7-583"><a href="#cb7-583"></a>in preferences, offering a clearer picture of the relative importance of</span>
<span id="cb7-584"><a href="#cb7-584"></a>each item. For instance, in a product development context, MaxDiff</span>
<span id="cb7-585"><a href="#cb7-585"></a>Analysis can help identify the most and least important features</span>
<span id="cb7-586"><a href="#cb7-586"></a>according to consumer preferences <span class="co">[</span><span class="ot">@campbell2015</span><span class="co">]</span>.</span>
<span id="cb7-587"><a href="#cb7-587"></a></span>
<span id="cb7-588"><a href="#cb7-588"></a>The advantages of Best-Worst Scaling are significant. It provides rich</span>
<span id="cb7-589"><a href="#cb7-589"></a>data on the relative importance of items, helps clarify preferences,</span>
<span id="cb7-590"><a href="#cb7-590"></a>reduces biases found in traditional rating scales, and results in</span>
<span id="cb7-591"><a href="#cb7-591"></a>utility scores that are easy to interpret. BWS is particularly useful in</span>
<span id="cb7-592"><a href="#cb7-592"></a>market research for understanding consumer preferences, in health</span>
<span id="cb7-593"><a href="#cb7-593"></a>economics for evaluating patient treatment preferences, in social</span>
<span id="cb7-594"><a href="#cb7-594"></a>sciences for studying the importance of social issues, and in product</span>
<span id="cb7-595"><a href="#cb7-595"></a>development for identifying key features driving consumer choices</span>
<span id="cb7-596"><a href="#cb7-596"></a><span class="co">[</span><span class="ot">@campbell2015</span><span class="co">]</span>.</span>
<span id="cb7-597"><a href="#cb7-597"></a></span>
<span id="cb7-598"><a href="#cb7-598"></a>However, BWS also has limitations, including increased complexity and</span>
<span id="cb7-599"><a href="#cb7-599"></a>cognitive load for participants compared to simpler rating scales,</span>
<span id="cb7-600"><a href="#cb7-600"></a>potential scale interpretation differences among participants, and</span>
<span id="cb7-601"><a href="#cb7-601"></a>design challenges to avoid biases. Additionally, differences in how</span>
<span id="cb7-602"><a href="#cb7-602"></a>participants interpret the scale can introduce variability, and the</span>
<span id="cb7-603"><a href="#cb7-603"></a>design of BWS studies requires careful consideration to avoid biases,</span>
<span id="cb7-604"><a href="#cb7-604"></a>such as the order effect or the context in which items are presented.</span>
<span id="cb7-605"><a href="#cb7-605"></a></span>
<span id="cb7-606"><a href="#cb7-606"></a><span class="fu">##### Multiple-Choice Sampling {#multiple-choice-sampling .unnumbered}</span></span>
<span id="cb7-607"><a href="#cb7-607"></a></span>
<span id="cb7-608"><a href="#cb7-608"></a>Multiple-choice sampling models are widely used in various fields such</span>
<span id="cb7-609"><a href="#cb7-609"></a>as voting systems, surveys, and market research to understand the</span>
<span id="cb7-610"><a href="#cb7-610"></a>preferred choice among a set of alternatives. These models involve</span>
<span id="cb7-611"><a href="#cb7-611"></a>participants selecting one option from a set of alternatives, providing</span>
<span id="cb7-612"><a href="#cb7-612"></a>insights into the most favored options.</span>
<span id="cb7-613"><a href="#cb7-613"></a></span>
<span id="cb7-614"><a href="#cb7-614"></a>Multiple-choice sampling methods offer several advantages. They are</span>
<span id="cb7-615"><a href="#cb7-615"></a>simple for participants to understand and reflect on realistic</span>
<span id="cb7-616"><a href="#cb7-616"></a>decision-making scenarios where individuals choose one option from many.</span>
<span id="cb7-617"><a href="#cb7-617"></a>These models are versatile and can be applied in various applications,</span>
<span id="cb7-618"><a href="#cb7-618"></a>from voting to market research, providing clear preferences directly</span>
<span id="cb7-619"><a href="#cb7-619"></a>from the participants' choices. It is particularly useful in complex</span>
<span id="cb7-620"><a href="#cb7-620"></a>choice scenarios such as mode of transportation, where choices are not</span>
<span id="cb7-621"><a href="#cb7-621"></a>independent <span class="co">[</span><span class="ot">@bolt2009</span><span class="co">]</span>.</span>
<span id="cb7-622"><a href="#cb7-622"></a></span>
<span id="cb7-623"><a href="#cb7-623"></a>However, multiple-choice sampling also has limitations. It often relies</span>
<span id="cb7-624"><a href="#cb7-624"></a>on simplistic assumptions such as the independence of irrelevant</span>
<span id="cb7-625"><a href="#cb7-625"></a>alternatives (IIA), which may not always hold true. Additionally, these</span>
<span id="cb7-626"><a href="#cb7-626"></a>models can place a cognitive load on participants, especially if the</span>
<span id="cb7-627"><a href="#cb7-627"></a>number of choices is large, leading to decision fatigue. This method may</span>
<span id="cb7-628"><a href="#cb7-628"></a>also fail to capture the variation in preferences among different</span>
<span id="cb7-629"><a href="#cb7-629"></a>individuals, as it typically records only the most preferred choice</span>
<span id="cb7-630"><a href="#cb7-630"></a>without accounting for the relative importance of other options.</span>
<span id="cb7-631"><a href="#cb7-631"></a></span>
<span id="cb7-632"><a href="#cb7-632"></a><span class="fu">### Data Interpretation</span></span>
<span id="cb7-633"><a href="#cb7-633"></a></span>
<span id="cb7-634"><a href="#cb7-634"></a><span class="fu">##### Binary Choice Model {#binary-choice-model .unnumbered}</span></span>
<span id="cb7-635"><a href="#cb7-635"></a></span>
<span id="cb7-636"><a href="#cb7-636"></a>is centered around one specific user option. The model predicts, for</span>
<span id="cb7-637"><a href="#cb7-637"></a>that option, after observing user choices in the past, whether that</span>
<span id="cb7-638"><a href="#cb7-638"></a>option will be chosen or not. Specifically, if we are looking at a</span>
<span id="cb7-639"><a href="#cb7-639"></a>certain choice, we use binary variable $y \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ to represent</span>
<span id="cb7-640"><a href="#cb7-640"></a>whether that choice will be picked or not by the user in the next phase</span>
<span id="cb7-641"><a href="#cb7-641"></a>of selection. Since $\mathbb{P}(y = 0) = 1 - \mathbb{P}(y = 1)$, we only need to</span>
<span id="cb7-642"><a href="#cb7-642"></a>model $\mathbb{P}(y = 1)$ which we will denote as $P$.</span>
<span id="cb7-643"><a href="#cb7-643"></a></span>
<span id="cb7-644"><a href="#cb7-644"></a>We can use a linear model represented by the parameter $\beta$ we have</span>
<span id="cb7-645"><a href="#cb7-645"></a>already defined. Since utility is a stochastic function of the choice</span>
<span id="cb7-646"><a href="#cb7-646"></a>attributes, we will represent our utility as</span>
<span id="cb7-647"><a href="#cb7-647"></a>$U = \beta^\top z + \epsilon$. We can formally model $y$ as a function</span>
<span id="cb7-648"><a href="#cb7-648"></a>of the utility of the positive choice: $y = \mathbb{I}<span class="co">[</span><span class="ot">U&gt;0</span><span class="co">]</span>$.</span>
<span id="cb7-649"><a href="#cb7-649"></a></span>
<span id="cb7-650"><a href="#cb7-650"></a>We explore two cases based on the choice of distribution for the</span>
<span id="cb7-651"><a href="#cb7-651"></a>unobserved random variable $\epsilon$. If</span>
<span id="cb7-652"><a href="#cb7-652"></a>$\epsilon \sim \text{Logistic}$, then</span>
<span id="cb7-653"><a href="#cb7-653"></a>$\mathbb{P}(\epsilon &lt; a) = \frac{1}{1 + \exp^{-a}}$. The probability $P$ can</span>
<span id="cb7-654"><a href="#cb7-654"></a>be modeled as: $$\begin{aligned}</span>
<span id="cb7-655"><a href="#cb7-655"></a>    P &amp; = \mathbb{P}(U &gt; 0) = \mathbb{P}(\beta^\top z + \epsilon &gt; 0) = \mathbb{P}( \epsilon &gt; -\beta^\top z) = 1 - \mathbb{P}( \epsilon &lt; -\beta^\top z) = 1 - \frac{1}{1 + \exp^{\beta^\top z}} <span class="sc">\\</span></span>
<span id="cb7-656"><a href="#cb7-656"></a>    &amp; = \frac{1 + \exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} - \frac{1}{1 + \exp^{\beta^\top z}} = \frac{\exp^{\beta^\top z}}{1 + \exp^{\beta^\top z}} = \frac{1}{1 + \exp^{-\beta^\top z}}</span>
<span id="cb7-657"><a href="#cb7-657"></a>\end{aligned}$$</span>
<span id="cb7-658"><a href="#cb7-658"></a></span>
<span id="cb7-659"><a href="#cb7-659"></a>In the health coaching example, using this logistic model, we can</span>
<span id="cb7-660"><a href="#cb7-660"></a>compute the probability that an individual would choose this meal over</span>
<span id="cb7-661"><a href="#cb7-661"></a>no meal: $P = \frac{1}{1 + \exp^{-(4 + 15 - 9)}} = 0.99995$. Therefore,</span>
<span id="cb7-662"><a href="#cb7-662"></a>the model predicts a high probability that the user would choose the</span>
<span id="cb7-663"><a href="#cb7-663"></a>meal over the no-meal option.</span>
<span id="cb7-664"><a href="#cb7-664"></a></span>
<span id="cb7-665"><a href="#cb7-665"></a>On the other hand, if $\epsilon \sim \mathcal{N}(0, 1)$, then</span>
<span id="cb7-666"><a href="#cb7-666"></a>$\mathbb{P}(\epsilon &lt; a) = \Phi(a)$, where $\Phi(a)$ is the cumulative</span>
<span id="cb7-667"><a href="#cb7-667"></a>distribution function of the standard normal distribution. The</span>
<span id="cb7-668"><a href="#cb7-668"></a>probability $P$ is modeled as:</span>
<span id="cb7-669"><a href="#cb7-669"></a></span>
<span id="cb7-670"><a href="#cb7-670"></a>$$P = \mathbb{P}(U &gt; 0) = \mathbb{P}(\beta^\top z + \epsilon &gt; 0) = \mathbb{P}( \epsilon &gt; -\beta^\top z) = \mathbb{P}( \epsilon &lt; \beta^\top z) = \Phi(\beta^\top z)$$</span>
<span id="cb7-671"><a href="#cb7-671"></a></span>
<span id="cb7-672"><a href="#cb7-672"></a>In the same health coaching example, we can compute the probability that</span>
<span id="cb7-673"><a href="#cb7-673"></a>an individual would choose this meal over no meal:</span>
<span id="cb7-674"><a href="#cb7-674"></a>$\Phi(4 + 15 - 9) = 1$. This model also predicts that the user will most</span>
<span id="cb7-675"><a href="#cb7-675"></a>likely take the meal!</span>
<span id="cb7-676"><a href="#cb7-676"></a></span>
<span id="cb7-677"><a href="#cb7-677"></a><span class="fu">##### Bradley-Terry Model {#bradley-terry-model .unnumbered}</span></span>
<span id="cb7-678"><a href="#cb7-678"></a></span>
<span id="cb7-679"><a href="#cb7-679"></a>The Bradley-Terry (BT) model introduces a framework to model the utility</span>
<span id="cb7-680"><a href="#cb7-680"></a>of choice *over all others* (a multipronged prediction of overall</span>
<span id="cb7-681"><a href="#cb7-681"></a>choices, not just a binary prediction over one choice), given their</span>
<span id="cb7-682"><a href="#cb7-682"></a>attribute vectors <span class="co">[</span><span class="ot">@bradley-terry-model</span><span class="co">]</span>. Given information about all</span>
<span id="cb7-683"><a href="#cb7-683"></a>available operations, this is a general yet powerful method for modeling</span>
<span id="cb7-684"><a href="#cb7-684"></a>human preferences. The core idea in this model is to compare utilities</span>
<span id="cb7-685"><a href="#cb7-685"></a>of all items at once to model the probability of a user's actions and,</span>
<span id="cb7-686"><a href="#cb7-686"></a>therefore, their preferences. In the BT model, we have a discrete set of</span>
<span id="cb7-687"><a href="#cb7-687"></a>$J$ choices $i \in <span class="sc">\{</span>1, 2, \dots, J<span class="sc">\}</span>$, each with an attribute</span>
<span id="cb7-688"><a href="#cb7-688"></a>representation $z_i \in \mathbb{Z}^n$ (where $n$ is the dimensionality of the</span>
<span id="cb7-689"><a href="#cb7-689"></a>representation). Each choice can also have its unique random noise</span>
<span id="cb7-690"><a href="#cb7-690"></a>variable representing the unobserved factor, although we can also choose</span>
<span id="cb7-691"><a href="#cb7-691"></a>to have all choices' unobserved factors follow the same distribution</span>
<span id="cb7-692"><a href="#cb7-692"></a>(e.g. independent and identically distributed, or iid).</span>
<span id="cb7-693"><a href="#cb7-693"></a></span>
<span id="cb7-694"><a href="#cb7-694"></a>We keep the assumption from previous sections that the utility $U_i$ of</span>
<span id="cb7-695"><a href="#cb7-695"></a>choice $i$ is also a linear stochastic function where the noise is</span>
<span id="cb7-696"><a href="#cb7-696"></a>sampled from the specified distribution:</span>
<span id="cb7-697"><a href="#cb7-697"></a>$U_i = \beta^\top z_i + \epsilon_i$. The noise is represented as an</span>
<span id="cb7-698"><a href="#cb7-698"></a>extreme value distribution, although we can choose alternatives such as</span>
<span id="cb7-699"><a href="#cb7-699"></a>a multivariate Gaussian distribution: $\epsilon \sim \mathcal{N}(0, \Sigma)$. If</span>
<span id="cb7-700"><a href="#cb7-700"></a>$\Sigma$ is not a diagonal matrix, we effectively model correlations in</span>
<span id="cb7-701"><a href="#cb7-701"></a>the noise across choices, enabling us to avoid the iid assumption if</span>
<span id="cb7-702"><a href="#cb7-702"></a>necessary. In the case of the extreme value distribution, we model the</span>
<span id="cb7-703"><a href="#cb7-703"></a>probability of a user preferring choice $i$, which we denote as $P_i$ as</span>
<span id="cb7-704"><a href="#cb7-704"></a>$P_i = \exp(\beta^\top z_i)/Z$ where</span>
<span id="cb7-705"><a href="#cb7-705"></a>$Z = \sum_{j = 1}^{J} \exp(\beta^\top z_j)$.</span>
<span id="cb7-706"><a href="#cb7-706"></a></span>
<span id="cb7-707"><a href="#cb7-707"></a>We revisit the health coaching example. Denote two choices, where</span>
<span id="cb7-708"><a href="#cb7-708"></a>$z_1 = <span class="co">[</span><span class="ot">20, 15, 3</span><span class="co">]</span>$ is the choice from the previous example. Still, we</span>
<span id="cb7-709"><a href="#cb7-709"></a>now have a second choice $z_2 = <span class="co">[</span><span class="ot">60, 20, 7</span><span class="co">]</span>$ (which seems to be a very</span>
<span id="cb7-710"><a href="#cb7-710"></a>carbohydrate-heavy meal and potentially a larger meal overall). We will</span>
<span id="cb7-711"><a href="#cb7-711"></a>also assume we choose an extreme value distribution to model the</span>
<span id="cb7-712"><a href="#cb7-712"></a>unobserved factors, which are sampled i.i.d. Then, we have</span>
<span id="cb7-713"><a href="#cb7-713"></a>$\beta^\top z_1 = 10$ and</span>
<span id="cb7-714"><a href="#cb7-714"></a>$\beta^\top z_2 = 11 \Rightarrow P_1 = \frac{1}{1 + \exp(1)} = 0.2689$.</span>
<span id="cb7-715"><a href="#cb7-715"></a>Since there are only two choices, the probabilities $P_1$ and $P_2$ must</span>
<span id="cb7-716"><a href="#cb7-716"></a>sum to $1$. Therefore, we can calculate $P_2$ as</span>
<span id="cb7-717"><a href="#cb7-717"></a>$P_2 = 1 - P_1 = 1 - 0.2689 \approx 0.7311$. Our model predicts that</span>
<span id="cb7-718"><a href="#cb7-718"></a>choice 2 is more favorable between these two options.</span>
<span id="cb7-719"><a href="#cb7-719"></a></span>
<span id="cb7-720"><a href="#cb7-720"></a><span class="fu">##### Ordered Preferences Model {#ordered-preferences-model .unnumbered}</span></span>
<span id="cb7-721"><a href="#cb7-721"></a></span>
<span id="cb7-722"><a href="#cb7-722"></a>In all previous examples, we have assumed that we have no information on</span>
<span id="cb7-723"><a href="#cb7-723"></a>any explicit ordering of the available options a human can choose from:</span>
<span id="cb7-724"><a href="#cb7-724"></a>all choices were treated as independent by the model. The model aims to</span>
<span id="cb7-725"><a href="#cb7-725"></a>capture how an individual chooses between them. However, in many cases,</span>
<span id="cb7-726"><a href="#cb7-726"></a>we may introduce an inductive bias based on information about the</span>
<span id="cb7-727"><a href="#cb7-727"></a>options. For example, in a study for stated preferences, a user may be</span>
<span id="cb7-728"><a href="#cb7-728"></a>able to choose from intricately dependent options such as very poor,</span>
<span id="cb7-729"><a href="#cb7-729"></a>poor, fair, good, and great. In this case, it can be useful to include</span>
<span id="cb7-730"><a href="#cb7-730"></a>this bias in our model to represent a human's decision-making process</span>
<span id="cb7-731"><a href="#cb7-731"></a>better. For such cases, instead of comparing choices against</span>
<span id="cb7-732"><a href="#cb7-732"></a>alternatives, we can focus on a single example and use additional</span>
<span id="cb7-733"><a href="#cb7-733"></a>parameters to define classification criteria based on the utility</span>
<span id="cb7-734"><a href="#cb7-734"></a>determined by the model. Formally, let us suppose we have a single</span>
<span id="cb7-735"><a href="#cb7-735"></a>example with attributes $z_i$, and wish to know which of $J$ predefined</span>
<span id="cb7-736"><a href="#cb7-736"></a>options an individual will choose from. We can define $J - 1$</span>
<span id="cb7-737"><a href="#cb7-737"></a>parameters, which act as thresholds on the utility computed by</span>
<span id="cb7-738"><a href="#cb7-738"></a>$U_i = H(z_i)$ to classify the predicted choice between these options.</span>
<span id="cb7-739"><a href="#cb7-739"></a>For example, if there are 3 predefined options, we can define parameters</span>
<span id="cb7-740"><a href="#cb7-740"></a>$a, b \in \mathbb{R}$ such that $$y_i = \begin{cases} </span>
<span id="cb7-741"><a href="#cb7-741"></a>      1 &amp; U &lt; a <span class="sc">\\</span></span>
<span id="cb7-742"><a href="#cb7-742"></a>      2 &amp; a \le U &lt; b <span class="sc">\\</span></span>
<span id="cb7-743"><a href="#cb7-743"></a>      3 &amp; \text{else}</span>
<span id="cb7-744"><a href="#cb7-744"></a>   \end{cases}$$</span>
<span id="cb7-745"><a href="#cb7-745"></a></span>
<span id="cb7-746"><a href="#cb7-746"></a>**1. Logistic Distribution**</span>
<span id="cb7-747"><a href="#cb7-747"></a></span>
<span id="cb7-748"><a href="#cb7-748"></a>From a probabilistic perspective, we can use our cumulative</span>
<span id="cb7-749"><a href="#cb7-749"></a>distributions as before to model the probability that a person will</span>
<span id="cb7-750"><a href="#cb7-750"></a>choose a given option. Continuing with our linear utility function</span>
<span id="cb7-751"><a href="#cb7-751"></a>$U_i = \beta^\top z_i + \epsilon_i$, we can start with the setting that</span>
<span id="cb7-752"><a href="#cb7-752"></a>we assume unobserved factors follow a logistic distribution and focus on</span>
<span id="cb7-753"><a href="#cb7-753"></a>the first case:</span>
<span id="cb7-754"><a href="#cb7-754"></a>$$\mathbb{P}(y_i = 1) = \mathbb{P}(U &lt; a) = \mathbb{P}(\beta^\top z + \epsilon &lt; a )  = \mathbb{P}( \epsilon &lt; a - \beta^\top z)  = \frac{1}{1 + \exp(\beta^\top z - a)}$$</span>
<span id="cb7-755"><a href="#cb7-755"></a></span>
<span id="cb7-756"><a href="#cb7-756"></a>Extending this method to the second case, where we estimate the</span>
<span id="cb7-757"><a href="#cb7-757"></a>probability of the utility falling within a specific interval:</span>
<span id="cb7-758"><a href="#cb7-758"></a>$$\begin{aligned}</span>
<span id="cb7-759"><a href="#cb7-759"></a>    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le U &lt; b) = \mathbb{P}(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \mathbb{P}( \epsilon &lt; a - \beta^\top z) ) <span class="sc">\\</span></span>
<span id="cb7-760"><a href="#cb7-760"></a>    &amp; = \frac{1}{1 + \exp(\beta^\top z - b)}  - (1 - \frac{1}{1 + \exp(\beta^\top z - a)}  ) = \frac{1}{1 + \exp(\beta^\top z - b)}  - \frac{1}{1 + \exp(a - \beta^\top z)}  ) <span class="sc">\\</span></span>
<span id="cb7-761"><a href="#cb7-761"></a>\end{aligned}$$</span>
<span id="cb7-762"><a href="#cb7-762"></a></span>
<span id="cb7-763"><a href="#cb7-763"></a>The final case follows the form of the inverse of the first case:</span>
<span id="cb7-764"><a href="#cb7-764"></a></span>
<span id="cb7-765"><a href="#cb7-765"></a>$$\mathbb{P}(y_i = 3) = \mathbb{P}(U &gt; b) = \mathbb{P}(\beta^\top z + \epsilon &gt; b ) = \mathbb{P}( \epsilon &gt; b - \beta^\top z) = 1 - \mathbb{P}( \epsilon &lt; b - \beta^\top z) = \frac{1}{1 + \exp(\beta^\top z - b)}$$</span>
<span id="cb7-766"><a href="#cb7-766"></a></span>
<span id="cb7-767"><a href="#cb7-767"></a>**2. Normal Distribution**</span>
<span id="cb7-768"><a href="#cb7-768"></a></span>
<span id="cb7-769"><a href="#cb7-769"></a>In the case of modeling unobserved factors with a standard normal</span>
<span id="cb7-770"><a href="#cb7-770"></a>distribution, we have: $$\begin{split}</span>
<span id="cb7-771"><a href="#cb7-771"></a>    \mathbb{P}(y_i = 1) &amp; = \mathbb{P}(U &lt; a) = \mathbb{P}(\beta^\top z + \epsilon &lt; a ) = \mathbb{P}( \epsilon &lt; a - \beta^\top z) = \Phi(a - \beta^\top z) <span class="sc">\\</span></span>
<span id="cb7-772"><a href="#cb7-772"></a>    \mathbb{P}(y_i = 2) &amp; = \mathbb{P}(a \le U &lt; b) </span>
<span id="cb7-773"><a href="#cb7-773"></a>    = \mathbb{P}(a - \beta^\top z \le \epsilon &lt; b - \beta^\top z) = \Phi(b - \beta^\top z) - \Phi(a - \beta^\top z) <span class="sc">\\</span></span>
<span id="cb7-774"><a href="#cb7-774"></a>    \mathbb{P}(y_i = 3) &amp; = \mathbb{P}(U &gt; b) </span>
<span id="cb7-775"><a href="#cb7-775"></a>    = 1 - \Phi(b - \beta^\top z) </span>
<span id="cb7-776"><a href="#cb7-776"></a>\end{split}$$</span>
<span id="cb7-777"><a href="#cb7-777"></a></span>
<span id="cb7-778"><a href="#cb7-778"></a>In our health coaching example, the derivation above yields three exact</span>
<span id="cb7-779"><a href="#cb7-779"></a>expressions for computing the probability of choosing each of our meals.</span>
<span id="cb7-780"><a href="#cb7-780"></a>Each computation involves the normal cumulative distribution function as</span>
<span id="cb7-781"><a href="#cb7-781"></a>seen for the binary choice model with standard normal for $\epsilon$</span>
<span id="cb7-782"><a href="#cb7-782"></a>after parameters $a$ and $b$ are learned @sec-learning.</span>
<span id="cb7-783"><a href="#cb7-783"></a></span>
<span id="cb7-784"><a href="#cb7-784"></a><span class="fu">##### Plackett-Luce Model {#plackett-luce-model .unnumbered}</span></span>
<span id="cb7-785"><a href="#cb7-785"></a></span>
<span id="cb7-786"><a href="#cb7-786"></a>In other cases, we may need an even more general framework combining</span>
<span id="cb7-787"><a href="#cb7-787"></a>elements of the BT model and ordered preferences. Specifically, we can</span>
<span id="cb7-788"><a href="#cb7-788"></a>model an open-ended ranking of the available options in a similar</span>
<span id="cb7-789"><a href="#cb7-789"></a>probabilistic framework. To do so, we can leverage the Plackett-Luce</span>
<span id="cb7-790"><a href="#cb7-790"></a>(PL) Model, in which we jointly model the full sequence of choice</span>
<span id="cb7-791"><a href="#cb7-791"></a>ordering. <span class="co">[</span><span class="ot">@plackett_luce</span><span class="co">]</span></span>
<span id="cb7-792"><a href="#cb7-792"></a></span>
<span id="cb7-793"><a href="#cb7-793"></a>The general form models the joint distribution as the product of</span>
<span id="cb7-794"><a href="#cb7-794"></a>conditional probabilities, where each is conditioned on the preceding</span>
<span id="cb7-795"><a href="#cb7-795"></a>ranking terms. Given an ordering of $J$ choices</span>
<span id="cb7-796"><a href="#cb7-796"></a>$<span class="sc">\{</span>Y_1, Y_2, \dots, Y_J<span class="sc">\}</span>$ where $Y_1$ is the first selection, $Y_2$ is</span>
<span id="cb7-797"><a href="#cb7-797"></a>the second, and so on, we decompose the joint probability into its</span>
<span id="cb7-798"><a href="#cb7-798"></a>respective conditionals. To compute the conditional probabilities, we</span>
<span id="cb7-799"><a href="#cb7-799"></a>can use the same method as the BT model, using a softmax to produce</span>
<span id="cb7-800"><a href="#cb7-800"></a>valid conditional distributions for each element of the sequence:</span>
<span id="cb7-801"><a href="#cb7-801"></a>$$\mathbb{P}(Y_1, Y_2, \dots, Y_J) = \mathbb{P}(Y_1) \cdot \mathbb{P}(Y_2 | Y_1) \cdot \dots \cdot \mathbb{P}(Y_J | Y_1, Y_2, \dots Y_{J - 1}) = \prod_{i = 1}^J \frac{\exp(\beta^\top z_i)}{\sum_{j \ge i} \exp(\beta^\top z_j)}$$</span>
<span id="cb7-802"><a href="#cb7-802"></a></span>
<span id="cb7-803"><a href="#cb7-803"></a>An interesting property of the PL Model is that in the naive case of</span>
<span id="cb7-804"><a href="#cb7-804"></a>only ordering a single choice, it is equivalent to the pairwise</span>
<span id="cb7-805"><a href="#cb7-805"></a>preference formulation of the BT model.</span>
<span id="cb7-806"><a href="#cb7-806"></a></span>
<span id="cb7-807"><a href="#cb7-807"></a>**Exercise (Health coaching example)**: In our application, if we have</span>
<span id="cb7-808"><a href="#cb7-808"></a>$3$ choices (burrito (B), fries (F), rice bowl (R)), we can let</span>
<span id="cb7-809"><a href="#cb7-809"></a>$Y_1, Y_2, Y_3$ be variables to which we assign meals in a one-to-one</span>
<span id="cb7-810"><a href="#cb7-810"></a>manner to establish a ranking.</span>
<span id="cb7-811"><a href="#cb7-811"></a></span>
<span id="cb7-812"><a href="#cb7-812"></a><span class="ss">1.  </span>One of the possible ranking assignments is $Y_1=B, Y_2=F, Y_3=R$.</span>
<span id="cb7-813"><a href="#cb7-813"></a>    How many assignments are there in all, and what are they explicitly?</span>
<span id="cb7-814"><a href="#cb7-814"></a></span>
<span id="cb7-815"><a href="#cb7-815"></a><span class="ss">2.  </span>What would one expect the sign to be, out of $<span class="sc">\{</span>\leq, \geq, =<span class="sc">\}</span>$ in</span>
<span id="cb7-816"><a href="#cb7-816"></a>    the following expression? (Hint: healthier meals should be placed</span>
<span id="cb7-817"><a href="#cb7-817"></a>    earlier in the ranking.)</span>
<span id="cb7-818"><a href="#cb7-818"></a>    $$\mathbb{P}(Y_1=F, Y_2=R, Y_3=B) \ \ <span class="sc">\_\_</span>\ \ \mathbb{P}(Y_1=R, Y_2=B, Y_3=F)$$</span>
<span id="cb7-819"><a href="#cb7-819"></a></span>
<span id="cb7-820"><a href="#cb7-820"></a><span class="fu">##### Ideal Point Model {#ideal-point-model .unnumbered}</span></span>
<span id="cb7-821"><a href="#cb7-821"></a></span>
<span id="cb7-822"><a href="#cb7-822"></a>An observation one can make is that we have strictly used linear</span>
<span id="cb7-823"><a href="#cb7-823"></a>functions to represent the utility. However, in the case of vector</span>
<span id="cb7-824"><a href="#cb7-824"></a>representations of choice attributes and the individual, one can exploit</span>
<span id="cb7-825"><a href="#cb7-825"></a>vector geometry to compute this utility value. The Ideal Point Model</span>
<span id="cb7-826"><a href="#cb7-826"></a>does this by using distance functions to compute utility for</span>
<span id="cb7-827"><a href="#cb7-827"></a>individual-choice pairs <span class="co">[</span><span class="ot">@huber1976ideal</span><span class="co">]</span>. Formally, with our vector</span>
<span id="cb7-828"><a href="#cb7-828"></a>representation $z_i$ of choice $i$ and a vector $\textbf{v}_n$</span>
<span id="cb7-829"><a href="#cb7-829"></a>representing an individual $n$, we can use a distance function to model</span>
<span id="cb7-830"><a href="#cb7-830"></a>a stochastic utility function, keeping the notion of unobserved factors</span>
<span id="cb7-831"><a href="#cb7-831"></a>following a specified distribution:</span>
<span id="cb7-832"><a href="#cb7-832"></a>$U_{n, i} = \texttt{dist}(z_i, \textbf{v}_n) + \epsilon_{n, i}$. We</span>
<span id="cb7-833"><a href="#cb7-833"></a>continue with our framework of a human's preference following the choice</span>
<span id="cb7-834"><a href="#cb7-834"></a>corresponding to the maximum utility:</span>
<span id="cb7-835"><a href="#cb7-835"></a>$y_{n, i} = \mathbb{I}<span class="co">[</span><span class="ot">U_{n, i} &gt; U_{n, j}\ \forall i \ne j</span><span class="co">]</span>$. The</span>
<span id="cb7-836"><a href="#cb7-836"></a>intuition supporting this type of model is that vectors exist in a</span>
<span id="cb7-837"><a href="#cb7-837"></a>shared $n$-dimensional space, and as such we can use geometry to match</span>
<span id="cb7-838"><a href="#cb7-838"></a>choices whose representations are closest to that of a given individual.</span>
<span id="cb7-839"><a href="#cb7-839"></a></span>
<span id="cb7-840"><a href="#cb7-840"></a>An observation with this model type is that it can often result in</span>
<span id="cb7-841"><a href="#cb7-841"></a>faster learning compared to non-geometric approaches</span>
<span id="cb7-842"><a href="#cb7-842"></a><span class="co">[</span><span class="ot">@ideal_point; @tatli2022distancepreferences</span><span class="co">]</span>. However, it carries the</span>
<span id="cb7-843"><a href="#cb7-843"></a>added burden of having to specify a distance metric. Certain distance</span>
<span id="cb7-844"><a href="#cb7-844"></a>metrics, such as Euclidian distance or inner product, can easily be</span>
<span id="cb7-845"><a href="#cb7-845"></a>biased by the scale of vectors. A distance measure such as cosine</span>
<span id="cb7-846"><a href="#cb7-846"></a>similarity, which compensates for scale by normalizing the inner product</span>
<span id="cb7-847"><a href="#cb7-847"></a>of two vectors by the product of their magnitudes, can mitigate this</span>
<span id="cb7-848"><a href="#cb7-848"></a>bias yet may discard valuable information encoded by the length of the</span>
<span id="cb7-849"><a href="#cb7-849"></a>vectors. Beyond the distance metric alone, this model places a strong</span>
<span id="cb7-850"><a href="#cb7-850"></a>inductive bias that the individual and choice representations all share</span>
<span id="cb7-851"><a href="#cb7-851"></a>a common embedding space. In some contexts, this can be a robust bias to</span>
<span id="cb7-852"><a href="#cb7-852"></a>add to the model <span class="co">[</span><span class="ot">@idealpoints</span><span class="co">]</span>, but it is a key factor one must take</span>
<span id="cb7-853"><a href="#cb7-853"></a>into account before employing such a model, and is a key design choice</span>
<span id="cb7-854"><a href="#cb7-854"></a>for modeling.</span>
<span id="cb7-855"><a href="#cb7-855"></a></span>
<span id="cb7-856"><a href="#cb7-856"></a>**Health coaching example**: vector representations may indeed be useful</span>
<span id="cb7-857"><a href="#cb7-857"></a>as an individual's representation can capture the macronutrient</span>
<span id="cb7-858"><a href="#cb7-858"></a>proportions and volumes they wish to consume, enabling a distance metric</span>
<span id="cb7-859"><a href="#cb7-859"></a>such as inner product to be a powerful tool. This model also starts</span>
<span id="cb7-860"><a href="#cb7-860"></a>capturing user properties (e.g. a user may be more into working out,</span>
<span id="cb7-861"><a href="#cb7-861"></a>another into lowering anxiety and another into gaining weight) and</span>
<span id="cb7-862"><a href="#cb7-862"></a>implicitly the commonalities between user characteristics start being</span>
<span id="cb7-863"><a href="#cb7-863"></a>captured, akin to a recommendation system <span class="co">[</span><span class="ot">@recommender_systems</span><span class="co">]</span>.</span>
<span id="cb7-864"><a href="#cb7-864"></a>However, in other domains and formulations, where perhaps user profiles</span>
<span id="cb7-865"><a href="#cb7-865"></a>are not as explicit, this may certainly hinder performance and make</span>
<span id="cb7-866"><a href="#cb7-866"></a>learning human preferences difficult.</span>
<span id="cb7-867"><a href="#cb7-867"></a></span>
<span id="cb7-868"><a href="#cb7-868"></a><span class="fu">## Parameter Learning {#sec-learning}</span></span>
<span id="cb7-869"><a href="#cb7-869"></a></span>
<span id="cb7-870"><a href="#cb7-870"></a>With an understanding of the various techniques we can use to model</span>
<span id="cb7-871"><a href="#cb7-871"></a>human preferences, we can now create robust models which utilize context</span>
<span id="cb7-872"><a href="#cb7-872"></a>attributes about the options an individual has in front of them and</span>
<span id="cb7-873"><a href="#cb7-873"></a>model their choices. However, these models on their own are powerless;</span>
<span id="cb7-874"><a href="#cb7-874"></a>their parameters are initialized randomly and we must fit the models to</span>
<span id="cb7-875"><a href="#cb7-875"></a>the actual human choice data!</span>
<span id="cb7-876"><a href="#cb7-876"></a></span>
<span id="cb7-877"><a href="#cb7-877"></a>Each of the models we have studied contain distinct parameters which aim</span>
<span id="cb7-878"><a href="#cb7-878"></a>to capture human preferences; for example $\beta$ is a parameter vector</span>
<span id="cb7-879"><a href="#cb7-879"></a>containing variables which represent a linear function to compute</span>
<span id="cb7-880"><a href="#cb7-880"></a>utility given a choice's attributes. We can also choose to represent</span>
<span id="cb7-881"><a href="#cb7-881"></a>stochastic utility functions or embedding functions for Ideal Point</span>
<span id="cb7-882"><a href="#cb7-882"></a>Models as neural networks. But how can we compute the optimal values of</span>
<span id="cb7-883"><a href="#cb7-883"></a>these parameters?</span>
<span id="cb7-884"><a href="#cb7-884"></a></span>
<span id="cb7-885"><a href="#cb7-885"></a>In this section, we give the reader an overview of the different methods</span>
<span id="cb7-886"><a href="#cb7-886"></a>available to tune human preference model parameters using given data. We</span>
<span id="cb7-887"><a href="#cb7-887"></a>refer the reader to <span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span></span>
<span id="cb7-888"><a href="#cb7-888"></a>for first-principle derivations of these methods and a deeper dive into</span>
<span id="cb7-889"><a href="#cb7-889"></a>their theoretical properties (convergence, generalization,</span>
<span id="cb7-890"><a href="#cb7-890"></a>data-hungriness, etc.).</span>
<span id="cb7-891"><a href="#cb7-891"></a></span>
<span id="cb7-892"><a href="#cb7-892"></a>A common and powerful approach for computing the parameters of a model</span>
<span id="cb7-893"><a href="#cb7-893"></a>is maximum likelihood estimation</span>
<span id="cb7-894"><a href="#cb7-894"></a><span class="co">[</span><span class="ot">@book_estimation_casella; @book_estimation_bock</span><span class="co">]</span>. The likelihood of a</span>
<span id="cb7-895"><a href="#cb7-895"></a>model is the probability of the observed data given the model</span>
<span id="cb7-896"><a href="#cb7-896"></a>parameters; intuitively we wish to maximize this likelihood, as that</span>
<span id="cb7-897"><a href="#cb7-897"></a>would mean that our model associates observed human preferences in the</span>
<span id="cb7-898"><a href="#cb7-898"></a>data with high probability. We can formally define the likelihood for a</span>
<span id="cb7-899"><a href="#cb7-899"></a>model with parameters $\beta$ and a given data point $(z_i, y_i)$ as:</span>
<span id="cb7-900"><a href="#cb7-900"></a>$$\mathcal{L}(z_i, y_i; \beta) = \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb7-901"><a href="#cb7-901"></a></span>
<span id="cb7-902"><a href="#cb7-902"></a>Assuming our data is independent and identically distributed (iid), the</span>
<span id="cb7-903"><a href="#cb7-903"></a>likelihood over the entire dataset is the joint probability of all</span>
<span id="cb7-904"><a href="#cb7-904"></a>observed data as defined by the model:</span>
<span id="cb7-905"><a href="#cb7-905"></a>$$\mathcal{L}(z, Y; \beta) = \prod_{i = 1}^J \mathbb{P}(y = y_i | z_i; \beta)$$</span>
<span id="cb7-906"><a href="#cb7-906"></a></span>
<span id="cb7-907"><a href="#cb7-907"></a>In our very first example of binary choice with logistic noise, this was</span>
<span id="cb7-908"><a href="#cb7-908"></a>simply the model's probability of the observed preference value:</span>
<span id="cb7-909"><a href="#cb7-909"></a>$$\mathcal{L}(z_i, y_i; \beta) = \frac{1}{1 + \exp^{-\beta^\top z}}$$</span>
<span id="cb7-910"><a href="#cb7-910"></a></span>
<span id="cb7-911"><a href="#cb7-911"></a>In the same case with noise following a standard normal distribution,</span>
<span id="cb7-912"><a href="#cb7-912"></a>this took the form: $$\mathcal{L}(z_i, y_i; \beta) = \Phi(\beta^\top z)$$</span>
<span id="cb7-913"><a href="#cb7-913"></a></span>
<span id="cb7-914"><a href="#cb7-914"></a>Fortunately, in these cases, there are straightforward methods for</span>
<span id="cb7-915"><a href="#cb7-915"></a>parameter estimation: logistic regression and probit regression (binary</span>
<span id="cb7-916"><a href="#cb7-916"></a>or multinomial, depending on the model), respectively. We can use</span>
<span id="cb7-917"><a href="#cb7-917"></a>ordinal regression to estimate the model's parameters for our ordered</span>
<span id="cb7-918"><a href="#cb7-918"></a>preference model.</span>
<span id="cb7-919"><a href="#cb7-919"></a></span>
<span id="cb7-920"><a href="#cb7-920"></a>Generally, the objective function commonly found in parameter learning</span>
<span id="cb7-921"><a href="#cb7-921"></a>can be optimized with stochastic gradient descent (SGD)</span>
<span id="cb7-922"><a href="#cb7-922"></a><span class="co">[</span><span class="ot">@gradient_descent</span><span class="co">]</span>. We can define an objective function as the</span>
<span id="cb7-923"><a href="#cb7-923"></a>likelihood to maximize this objective. Since SGD minimizes a given</span>
<span id="cb7-924"><a href="#cb7-924"></a>objective, we must negate the likelihood, which ensures that a converged</span>
<span id="cb7-925"><a href="#cb7-925"></a>solution maximizes the likelihood. SGD operates by computing the</span>
<span id="cb7-926"><a href="#cb7-926"></a>gradient of the objective with respect to the parameters of the model,</span>
<span id="cb7-927"><a href="#cb7-927"></a>which provides a signal of the direction in which the parameters must</span>
<span id="cb7-928"><a href="#cb7-928"></a>move to *maximize* the objective. Then, SGD makes an update step by</span>
<span id="cb7-929"><a href="#cb7-929"></a>subtracting this gradient from the parameters (most often with a scale</span>
<span id="cb7-930"><a href="#cb7-930"></a>factor called a *learning rate*), to move the parameters in a direction</span>
<span id="cb7-931"><a href="#cb7-931"></a>which *minimizes* the objective. When the objective is the negative</span>
<span id="cb7-932"><a href="#cb7-932"></a>likelihood (or sometimes negative log-likelihood for convenience or</span>
<span id="cb7-933"><a href="#cb7-933"></a>tractability), the result is an increase in the overall likelihood.</span>
<span id="cb7-934"><a href="#cb7-934"></a></span>
<span id="cb7-935"><a href="#cb7-935"></a>In the case of logistic and Gaussian models, SGD may yield a challenging</span>
<span id="cb7-936"><a href="#cb7-936"></a>optimization problem as its stochasticity can lead to noisy updates, for</span>
<span id="cb7-937"><a href="#cb7-937"></a>example, if certain examples or batches of examples are biased.</span>
<span id="cb7-938"><a href="#cb7-938"></a>Mitigations include batched SGD, in which multiple samples are randomly</span>
<span id="cb7-939"><a href="#cb7-939"></a>sampled from the dataset at each iteration, learning rates, which reduce</span>
<span id="cb7-940"><a href="#cb7-940"></a>the impact of noisy gradient updates, and momentum and higher-order</span>
<span id="cb7-941"><a href="#cb7-941"></a>optimizers which reduce noise by using movering averages of gradients or</span>
<span id="cb7-942"><a href="#cb7-942"></a>provide better estimates of the best direction in which to update the</span>
<span id="cb7-943"><a href="#cb7-943"></a>gradients. Some models, such as those that use neural networks, may, in</span>
<span id="cb7-944"><a href="#cb7-944"></a>fact, be intractable to estimate without a method such as SGD (or its</span>
<span id="cb7-945"><a href="#cb7-945"></a>momentum-based derivatives). For example, neural networks with many</span>
<span id="cb7-946"><a href="#cb7-946"></a>layers, non-linearities, and parameters can only be efficiently computed</span>
<span id="cb7-947"><a href="#cb7-947"></a>with gradient-based methods.</span>
<span id="cb7-948"><a href="#cb7-948"></a></span>
<span id="cb7-949"><a href="#cb7-949"></a><span class="fu">### Reward Learning with Large Language Models</span></span>
<span id="cb7-950"><a href="#cb7-950"></a></span>
<span id="cb7-951"><a href="#cb7-951"></a>Taking a step away from explicitly modeling human bias and preference,</span>
<span id="cb7-952"><a href="#cb7-952"></a>we consider applying a deep learning approach to state-of-the-art</span>
<span id="cb7-953"><a href="#cb7-953"></a>language models. We begin by introducing the concepts of *foundation</span>
<span id="cb7-954"><a href="#cb7-954"></a>models* and *alignment*. A foundation model <span class="co">[</span><span class="ot">@Liang2021</span><span class="co">]</span> in machine</span>
<span id="cb7-955"><a href="#cb7-955"></a>learning typically refers to a large and pre-trained neural network</span>
<span id="cb7-956"><a href="#cb7-956"></a>model that serves as the basis for various downstream tasks. In natural</span>
<span id="cb7-957"><a href="#cb7-957"></a>language processing, models like GPT-3, Llama, and BERT are considered</span>
<span id="cb7-958"><a href="#cb7-958"></a>foundation models. They are pre-trained on a massive corpus of text</span>
<span id="cb7-959"><a href="#cb7-959"></a>data, learning to understand language and context, and are capable of</span>
<span id="cb7-960"><a href="#cb7-960"></a>various language-related tasks such as text classification, language</span>
<span id="cb7-961"><a href="#cb7-961"></a>generation, and question answering. Foundation models are important</span>
<span id="cb7-962"><a href="#cb7-962"></a>because they alleviate the need to train massive neural networks from</span>
<span id="cb7-963"><a href="#cb7-963"></a>scratch, a compute and data expensive endeavor. However, a raw</span>
<span id="cb7-964"><a href="#cb7-964"></a>foundation model, trained on a pretraining objective such as a language</span>
<span id="cb7-965"><a href="#cb7-965"></a>modeling objective, is not useful on its own. It must be aligned to</span>
<span id="cb7-966"><a href="#cb7-966"></a>respond correctly based on human preferences.</span>
<span id="cb7-967"><a href="#cb7-967"></a></span>
<span id="cb7-968"><a href="#cb7-968"></a>In short, alignment for foundation models is the process by which model</span>
<span id="cb7-969"><a href="#cb7-969"></a>behavior is aligned with human values, ethics, and societal norms. Large</span>
<span id="cb7-970"><a href="#cb7-970"></a>Language Models (LLMs) are a foundation model for natural language</span>
<span id="cb7-971"><a href="#cb7-971"></a>processing. They are trained using a next-word prediction objective,</span>
<span id="cb7-972"><a href="#cb7-972"></a>allowing them to generate coherent language. A simple way to align a</span>
<span id="cb7-973"><a href="#cb7-973"></a>Large Language Model is to train it to follow instructions in a</span>
<span id="cb7-974"><a href="#cb7-974"></a>supervised way, using instruction-response pairs curated by hand.</span>
<span id="cb7-975"><a href="#cb7-975"></a>However, this limits the upper limit of LLM performance to the</span>
<span id="cb7-976"><a href="#cb7-976"></a>performance of the annotators' writing abilities. This type of</span>
<span id="cb7-977"><a href="#cb7-977"></a>annotation is also expensive.</span>
<span id="cb7-978"><a href="#cb7-978"></a></span>
<span id="cb7-979"><a href="#cb7-979"></a>An alternative, more promising approach is to train LLMs using</span>
<span id="cb7-980"><a href="#cb7-980"></a>reinforcement learning, potentially enabling them to surpass human-level</span>
<span id="cb7-981"><a href="#cb7-981"></a>performance. The main challenge with this method lies in defining an</span>
<span id="cb7-982"><a href="#cb7-982"></a>explicit reward function for generating free-form text. To address this,</span>
<span id="cb7-983"><a href="#cb7-983"></a>a reward model (RM) can be trained based on human preferences, providing</span>
<span id="cb7-984"><a href="#cb7-984"></a>a mechanism to score the quality of the generated text. This approach,</span>
<span id="cb7-985"><a href="#cb7-985"></a>known as Reinforcement Learning from Human Feedback (RLHF), leverages</span>
<span id="cb7-986"><a href="#cb7-986"></a>human feedback to guide model training, allowing LLMs to better align</span>
<span id="cb7-987"><a href="#cb7-987"></a>with human expectations while continuously improving performance.</span>
<span id="cb7-988"><a href="#cb7-988"></a></span>
<span id="cb7-989"><a href="#cb7-989"></a><span class="al">![Overall architecture of a reward model based on LLM](Figures/arch.png)</span>{#fig-rm-arch}</span>
<span id="cb7-990"><a href="#cb7-990"></a></span>
<span id="cb7-991"><a href="#cb7-991"></a>The Llama2 reward model <span class="co">[</span><span class="ot">@2307.09288</span><span class="co">]</span> is initialized from the pretrained</span>
<span id="cb7-992"><a href="#cb7-992"></a>Llama2 LLM. In the LLM, the last layer is a mapping</span>
<span id="cb7-993"><a href="#cb7-993"></a>$L: \mathbb{R}^D \rightarrow \mathbb{R}^V$, where $D$ is the embedding dimension</span>
<span id="cb7-994"><a href="#cb7-994"></a>from the transformer decoder stack and $V$ is the vocabulary size. To</span>
<span id="cb7-995"><a href="#cb7-995"></a>get the RM, we replace that last layer with a randomly initialized</span>
<span id="cb7-996"><a href="#cb7-996"></a>scalar head that maps $L: \mathbb{R}^D \rightarrow \mathbb{R}^1$. It's important to</span>
<span id="cb7-997"><a href="#cb7-997"></a>initialize the RM from the LLM it's meant to evaluate. This is because:</span>
<span id="cb7-998"><a href="#cb7-998"></a></span>
<span id="cb7-999"><a href="#cb7-999"></a><span class="ss">1.  </span>The RM will have the same "knowledge" as the LLM. This is</span>
<span id="cb7-1000"><a href="#cb7-1000"></a>    particularly useful if evaluating things like "does the LLM know</span>
<span id="cb7-1001"><a href="#cb7-1001"></a>    when it doesn't know?". However, in cases where the RM is simply</span>
<span id="cb7-1002"><a href="#cb7-1002"></a>    evaluating helpfulness or factuality, it may be useful to have the</span>
<span id="cb7-1003"><a href="#cb7-1003"></a>    RM know more.</span>
<span id="cb7-1004"><a href="#cb7-1004"></a></span>
<span id="cb7-1005"><a href="#cb7-1005"></a><span class="ss">2.  </span>The RM is on distribution for the LLM - it is initialized in a way</span>
<span id="cb7-1006"><a href="#cb7-1006"></a>    where it semantically understands the LLM's outputs.</span>
<span id="cb7-1007"><a href="#cb7-1007"></a></span>
<span id="cb7-1008"><a href="#cb7-1008"></a>An RM is trained with paired preferences, following the format:</span>
<span id="cb7-1009"><a href="#cb7-1009"></a>$$\begin{aligned}</span>
<span id="cb7-1010"><a href="#cb7-1010"></a>    \langle prompt<span class="sc">\_</span>history, response<span class="sc">\_</span>accepted, response<span class="sc">\_</span>rejected \rangle</span>
<span id="cb7-1011"><a href="#cb7-1011"></a>\end{aligned}$$ Prompt_history is a multiturn history of user prompts</span>
<span id="cb7-1012"><a href="#cb7-1012"></a>and model generations, response_accepted is the preferred final model</span>
<span id="cb7-1013"><a href="#cb7-1013"></a>generation by an annotator, and response_rejected is the unpreferred</span>
<span id="cb7-1014"><a href="#cb7-1014"></a>response. The RM is trained with a binary ranking loss with an optional</span>
<span id="cb7-1015"><a href="#cb7-1015"></a>margin term m(r), shown in equation (7). There is also often a small</span>
<span id="cb7-1016"><a href="#cb7-1016"></a>regularization term added to center the score distribution on 0.</span>
<span id="cb7-1017"><a href="#cb7-1017"></a>$$\mathcal{L}_{\text{ranking}} = -\log(\sigma(r_\theta(x,y_c) - r_\theta(x,y_r) - m(r)))$$</span>
<span id="cb7-1018"><a href="#cb7-1018"></a>The margin term increases the distance in scores specifically for</span>
<span id="cb7-1019"><a href="#cb7-1019"></a>preference pairs annotators rate as easier to separate.</span>
<span id="cb7-1020"><a href="#cb7-1020"></a></span>
<span id="cb7-1021"><a href="#cb7-1021"></a>::: {#tbl-margin_nums}</span>
<span id="cb7-1022"><a href="#cb7-1022"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb7-1023"><a href="#cb7-1023"></a><span class="in">                  Significantly   Better   Slightly     Negligibly</span></span>
<span id="cb7-1024"><a href="#cb7-1024"></a><span class="in">                     Better                 Better    Better / Unsure</span></span>
<span id="cb7-1025"><a href="#cb7-1025"></a>  Margin Small          1          2/3       1/3             0</span>
<span id="cb7-1026"><a href="#cb7-1026"></a>  Margin Large          3           2         1              0</span>
<span id="cb7-1027"><a href="#cb7-1027"></a>  -------------- --------------- -------- ---------- -----------------</span>
<span id="cb7-1028"><a href="#cb7-1028"></a></span>
<span id="cb7-1029"><a href="#cb7-1029"></a>  : Two variants of preference rating based margin with different magnitude.</span>
<span id="cb7-1030"><a href="#cb7-1030"></a>:::</span>
<span id="cb7-1031"><a href="#cb7-1031"></a></span>
<span id="cb7-1032"><a href="#cb7-1032"></a>![Reward model score distribution shift caused by incorporating</span>
<span id="cb7-1033"><a href="#cb7-1033"></a>preference rating based margin in ranking loss. With the margin term,</span>
<span id="cb7-1034"><a href="#cb7-1034"></a>we observe a binary split pattern in reward distribution, especially for</span>
<span id="cb7-1035"><a href="#cb7-1035"></a>a larger margin.](Figures/margin-2.png){#fig-margin-2</span>
<span id="cb7-1036"><a href="#cb7-1036"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-1037"><a href="#cb7-1037"></a></span>
<span id="cb7-1038"><a href="#cb7-1038"></a>It may seem confusing how the margins were chosen. It's primarily</span>
<span id="cb7-1039"><a href="#cb7-1039"></a>because the sigmoid function, which is used to normalize the raw reward</span>
<span id="cb7-1040"><a href="#cb7-1040"></a>model score, flattens out beyond the range of $<span class="co">[</span><span class="ot">-4, 4</span><span class="co">]</span>$. Thus, the</span>
<span id="cb7-1041"><a href="#cb7-1041"></a>maximum possible margin is eight.</span>
<span id="cb7-1042"><a href="#cb7-1042"></a></span>
<span id="cb7-1043"><a href="#cb7-1043"></a>When training or using a reward model, watching for the following is</span>
<span id="cb7-1044"><a href="#cb7-1044"></a>important:</span>
<span id="cb7-1045"><a href="#cb7-1045"></a></span>
<span id="cb7-1046"><a href="#cb7-1046"></a><span class="ss">1.  </span>**LLM Distribution Shift**: With each finetune of the LLM, the RM</span>
<span id="cb7-1047"><a href="#cb7-1047"></a>    should be updated through a collection of fresh human preferences</span>
<span id="cb7-1048"><a href="#cb7-1048"></a>    using generations from the new LLM. This ensures that the RM stays</span>
<span id="cb7-1049"><a href="#cb7-1049"></a>    aligned with the current distribution of the LLM and avoids drifting</span>
<span id="cb7-1050"><a href="#cb7-1050"></a>    off-distribution.</span>
<span id="cb7-1051"><a href="#cb7-1051"></a></span>
<span id="cb7-1052"><a href="#cb7-1052"></a><span class="ss">2.  </span>**RM and LLM are coupled**: An RM is generally optimized to</span>
<span id="cb7-1053"><a href="#cb7-1053"></a>    distinguish human preferences more efficiently within the specific</span>
<span id="cb7-1054"><a href="#cb7-1054"></a>    distribution of the LLM to be optimized. However, this</span>
<span id="cb7-1055"><a href="#cb7-1055"></a>    specialization poses a challenge: such an RM will underperform when</span>
<span id="cb7-1056"><a href="#cb7-1056"></a>    dealing with generations not aligned with this specific LLM</span>
<span id="cb7-1057"><a href="#cb7-1057"></a>    distribution, such as generations from a completely different LLM.</span>
<span id="cb7-1058"><a href="#cb7-1058"></a></span>
<span id="cb7-1059"><a href="#cb7-1059"></a><span class="ss">3.  </span>**Training Sensitivities of RMs**: Training RMs can be unstable and</span>
<span id="cb7-1060"><a href="#cb7-1060"></a>    prone to overfitting, especially with multiple training epochs. It's</span>
<span id="cb7-1061"><a href="#cb7-1061"></a>    generally advisable to limit the number of epochs during RM training</span>
<span id="cb7-1062"><a href="#cb7-1062"></a>    to avoid this issue.</span>
<span id="cb7-1063"><a href="#cb7-1063"></a></span>
<span id="cb7-1064"><a href="#cb7-1064"></a>The industry has centered around optimizing for two primary qualities in</span>
<span id="cb7-1065"><a href="#cb7-1065"></a>LLMs: helpfulness and harmlessness (safety). There are also other axes</span>
<span id="cb7-1066"><a href="#cb7-1066"></a>such as factuality, reasoning, tool use, code, multilingual, and more,</span>
<span id="cb7-1067"><a href="#cb7-1067"></a>but these are out of scope for us. In the Llama2 paper, preference data</span>
<span id="cb7-1068"><a href="#cb7-1068"></a>was collected from humans for each quality, with separate guidelines.</span>
<span id="cb7-1069"><a href="#cb7-1069"></a>This presents a challenge for co-optimizing the final LLM towards both</span>
<span id="cb7-1070"><a href="#cb7-1070"></a>goals.</span>
<span id="cb7-1071"><a href="#cb7-1071"></a></span>
<span id="cb7-1072"><a href="#cb7-1072"></a>Two main approaches can be taken for Reinforcement Learning from Human</span>
<span id="cb7-1073"><a href="#cb7-1073"></a>Feedback (RLHF) in this context:</span>
<span id="cb7-1074"><a href="#cb7-1074"></a></span>
<span id="cb7-1075"><a href="#cb7-1075"></a><span class="ss">1.  </span>Train a unified reward model that integrates both datasets.</span>
<span id="cb7-1076"><a href="#cb7-1076"></a></span>
<span id="cb7-1077"><a href="#cb7-1077"></a><span class="ss">2.  </span>Train two separate reward models, one for each quality, and optimize</span>
<span id="cb7-1078"><a href="#cb7-1078"></a>    the LLM toward both.</span>
<span id="cb7-1079"><a href="#cb7-1079"></a></span>
<span id="cb7-1080"><a href="#cb7-1080"></a>Option 1 is difficult because of the tension between helpfulness and</span>
<span id="cb7-1081"><a href="#cb7-1081"></a>harmlessness. They trade off against each other, confusing an RM trained</span>
<span id="cb7-1082"><a href="#cb7-1082"></a>on both. The chosen solution was option 2, where two RMs are used to</span>
<span id="cb7-1083"><a href="#cb7-1083"></a>train the LLM in a piecewise fashion. The helpfulness RM is used as the</span>
<span id="cb7-1084"><a href="#cb7-1084"></a>primary optimization term, while the harmlessness RM acts as a penalty</span>
<span id="cb7-1085"><a href="#cb7-1085"></a>term, driving the behavior of the LLM away from unsafe territory only</span>
<span id="cb7-1086"><a href="#cb7-1086"></a>when the LLM veers beyond a certain threshold. This is formalized as</span>
<span id="cb7-1087"><a href="#cb7-1087"></a>follows, where $R_s$, $R_h$, and $R_c$ are the safety, helpfulness, and</span>
<span id="cb7-1088"><a href="#cb7-1088"></a>combined reward, respectively. $g$ and $p$ are the model generation and</span>
<span id="cb7-1089"><a href="#cb7-1089"></a>the user prompt: $$\begin{aligned}</span>
<span id="cb7-1090"><a href="#cb7-1090"></a>    R_c(g \mid p) =</span>
<span id="cb7-1091"><a href="#cb7-1091"></a>    \begin{cases}</span>
<span id="cb7-1092"><a href="#cb7-1092"></a>        R_s(g \mid p) &amp; \text{if } \text{is<span class="sc">\_</span>safety}(p) \text{ or } R_s(g \mid p) &lt; 0.15 <span class="sc">\\</span></span>
<span id="cb7-1093"><a href="#cb7-1093"></a>        R_h(g \mid p) &amp; \text{otherwise}</span>
<span id="cb7-1094"><a href="#cb7-1094"></a>    \end{cases}</span>
<span id="cb7-1095"><a href="#cb7-1095"></a>\end{aligned}$$</span>
<span id="cb7-1096"><a href="#cb7-1096"></a></span>
<span id="cb7-1097"><a href="#cb7-1097"></a>There are several open issues with reward models alluded to in the</span>
<span id="cb7-1098"><a href="#cb7-1098"></a>paper. For example, how best to collect human feedback? Training</span>
<span id="cb7-1099"><a href="#cb7-1099"></a>annotators and making sure they do the correct thing is hard. What</span>
<span id="cb7-1100"><a href="#cb7-1100"></a>should the guidelines be? Another question is whether RMs can be made</span>
<span id="cb7-1101"><a href="#cb7-1101"></a>robust to adversarial prompts. Last but not least, do RMs have</span>
<span id="cb7-1102"><a href="#cb7-1102"></a>well-calibrated scores? This matters for RLHF - pure preference accuracy</span>
<span id="cb7-1103"><a href="#cb7-1103"></a>isn't enough.</span>
<span id="cb7-1104"><a href="#cb7-1104"></a></span>
<span id="cb7-1105"><a href="#cb7-1105"></a><span class="fu">### Reward Learning in Robotics</span></span>
<span id="cb7-1106"><a href="#cb7-1106"></a></span>
<span id="cb7-1107"><a href="#cb7-1107"></a>To help set up our basic reward learning problem, consider a user and a</span>
<span id="cb7-1108"><a href="#cb7-1108"></a>robot. The user's preferences or goals can be represented by an internal</span>
<span id="cb7-1109"><a href="#cb7-1109"></a>reward function, R($\xi$), which the robot needs to learn. Since the</span>
<span id="cb7-1110"><a href="#cb7-1110"></a>reward function isn't explicit, there are a variety of ways that the</span>
<span id="cb7-1111"><a href="#cb7-1111"></a>robot can learn this reward function, which we will discuss in the next</span>
<span id="cb7-1112"><a href="#cb7-1112"></a>section. An example method of learning a reward function from human data</span>
<span id="cb7-1113"><a href="#cb7-1113"></a>is using pairwise comparison. Consider the robot example from section</span>
<span id="cb7-1114"><a href="#cb7-1114"></a>one, but now, the robot shows the human two possible trajectories</span>
<span id="cb7-1115"><a href="#cb7-1115"></a>$\xi_A$ and $\xi_B$ as depicted in the diagram below.</span>
<span id="cb7-1116"><a href="#cb7-1116"></a></span>
<span id="cb7-1117"><a href="#cb7-1117"></a>![Two different trajectories taken by a robot to prompt</span>
<span id="cb7-1118"><a href="#cb7-1118"></a>user ranking.](Figures/robots.png){#fig-reward-robot-1 width="70%"}</span>
<span id="cb7-1119"><a href="#cb7-1119"></a></span>
<span id="cb7-1120"><a href="#cb7-1120"></a>The user is show both the trajectories above and asked to rank which one</span>
<span id="cb7-1121"><a href="#cb7-1121"></a>is better. Based on iterations of multiple trajectories and ranking, the</span>
<span id="cb7-1122"><a href="#cb7-1122"></a>robot is able to learn the user's internal reward function. There quite</span>
<span id="cb7-1123"><a href="#cb7-1123"></a>a lot of ways that models can learn a reward function from human data.</span>
<span id="cb7-1124"><a href="#cb7-1124"></a>Here's a list <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> of some of them:</span>
<span id="cb7-1125"><a href="#cb7-1125"></a></span>
<span id="cb7-1126"><a href="#cb7-1126"></a><span class="ss">1.  </span>Pairwise comparison: This is the method that we saw illustrated in</span>
<span id="cb7-1127"><a href="#cb7-1127"></a>    the previous example. The robot is able to learn based on a</span>
<span id="cb7-1128"><a href="#cb7-1128"></a>    comparison ranking provided by the user.</span>
<span id="cb7-1129"><a href="#cb7-1129"></a></span>
<span id="cb7-1130"><a href="#cb7-1130"></a><span class="ss">2.  </span>Expert demonstrations: Experts perform the task and the robot learns</span>
<span id="cb7-1131"><a href="#cb7-1131"></a>    the optimal reward function from these demonstrations.</span>
<span id="cb7-1132"><a href="#cb7-1132"></a></span>
<span id="cb7-1133"><a href="#cb7-1133"></a><span class="ss">3.  </span>Sub-optimal demonstrations: The robot is provided with</span>
<span id="cb7-1134"><a href="#cb7-1134"></a>    demonstrations that are not quite as good as the expert</span>
<span id="cb7-1135"><a href="#cb7-1135"></a>    demonstrations but it is still able to learn a noisy reward function</span>
<span id="cb7-1136"><a href="#cb7-1136"></a>    from the demonstrations.</span>
<span id="cb7-1137"><a href="#cb7-1137"></a></span>
<span id="cb7-1138"><a href="#cb7-1138"></a><span class="ss">4.  </span>Physical Corrections: While the robot is performing the task, at</span>
<span id="cb7-1139"><a href="#cb7-1139"></a>    each point in its trajectory (or at an arbitrary point in its</span>
<span id="cb7-1140"><a href="#cb7-1140"></a>    trajectory) its arm is corrected to a more suitable position. Based</span>
<span id="cb7-1141"><a href="#cb7-1141"></a>    on these corrections, the robot is able to learn the reward</span>
<span id="cb7-1142"><a href="#cb7-1142"></a>    function.</span>
<span id="cb7-1143"><a href="#cb7-1143"></a></span>
<span id="cb7-1144"><a href="#cb7-1144"></a><span class="ss">5.  </span>Ranking: This method is similar to pairwise comparison but involves</span>
<span id="cb7-1145"><a href="#cb7-1145"></a>    more trajectories than 2. All the trajectories may have subtle</span>
<span id="cb7-1146"><a href="#cb7-1146"></a>    differences from each other, but these differences help provide</span>
<span id="cb7-1147"><a href="#cb7-1147"></a>    insight to the model.</span>
<span id="cb7-1148"><a href="#cb7-1148"></a></span>
<span id="cb7-1149"><a href="#cb7-1149"></a><span class="ss">6.  </span>Trajectory Assessment: Given a single trajectory, the user rates how</span>
<span id="cb7-1150"><a href="#cb7-1150"></a>    close it is to optimal, typically using a ranking scale.</span>
<span id="cb7-1151"><a href="#cb7-1151"></a></span>
<span id="cb7-1152"><a href="#cb7-1152"></a>    Each of these methods allows the robot to refine its understanding</span>
<span id="cb7-1153"><a href="#cb7-1153"></a>    of the user's reward function, but their effectiveness can vary</span>
<span id="cb7-1154"><a href="#cb7-1154"></a>    depending on the application. For instance, expert demonstrations</span>
<span id="cb7-1155"><a href="#cb7-1155"></a>    tend to produce more reliable results but may not always be feasible</span>
<span id="cb7-1156"><a href="#cb7-1156"></a>    in everyday tasks. Pairwise comparison and ranking methods offer</span>
<span id="cb7-1157"><a href="#cb7-1157"></a>    more flexibility but might require a higher number of iterations.</span>
<span id="cb7-1158"><a href="#cb7-1158"></a></span>
<span id="cb7-1159"><a href="#cb7-1159"></a><span class="fu">### Reward Learning with Meta Learning</span></span>
<span id="cb7-1160"><a href="#cb7-1160"></a></span>
<span id="cb7-1161"><a href="#cb7-1161"></a>Learning a reward function from human preferences is an intricate and</span>
<span id="cb7-1162"><a href="#cb7-1162"></a>complicated task. At its core, this task is about designing algorithms</span>
<span id="cb7-1163"><a href="#cb7-1163"></a>that can capture what humans value based on their elicited preferences.</span>
<span id="cb7-1164"><a href="#cb7-1164"></a>However, due to the nuanced and multifaceted nature of human desires,</span>
<span id="cb7-1165"><a href="#cb7-1165"></a>learning reward functions from human can be a difficult task. Therefore,</span>
<span id="cb7-1166"><a href="#cb7-1166"></a>meta-learning rewards may be considered to facilitate the reward</span>
<span id="cb7-1167"><a href="#cb7-1167"></a>learning processes. Meta-learning, often referred to as "learning to</span>
<span id="cb7-1168"><a href="#cb7-1168"></a>learn," aims to design models that can adapt to new tasks with minimal</span>
<span id="cb7-1169"><a href="#cb7-1169"></a>additional efforts. We discuss paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> in @sec-few-shot showing how meta-learning can be leveraged for</span>
<span id="cb7-1170"><a href="#cb7-1170"></a>few-shot preference learning, where a system can quickly adapt to a new</span>
<span id="cb7-1171"><a href="#cb7-1171"></a>task after only a few queries to pairwise preferences from human.</span>
<span id="cb7-1172"><a href="#cb7-1172"></a></span>
<span id="cb7-1173"><a href="#cb7-1173"></a>Moving beyond the concept of learning from pairwise preferences, in @sec-watch we discuss a different approach where</span>
<span id="cb7-1174"><a href="#cb7-1174"></a>meta-learning intersects with both demonstrations and</span>
<span id="cb7-1175"><a href="#cb7-1175"></a>rewards <span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span>. This paper considers the use of both</span>
<span id="cb7-1176"><a href="#cb7-1176"></a>demonstrations and rewards elicited from human that guide the learning</span>
<span id="cb7-1177"><a href="#cb7-1177"></a>process.</span>
<span id="cb7-1178"><a href="#cb7-1178"></a></span>
<span id="cb7-1179"><a href="#cb7-1179"></a>In the regular learning setting, a model is fitted to a dataset with</span>
<span id="cb7-1180"><a href="#cb7-1180"></a>certain learning algorithm. The learning algorithm, for example, can be</span>
<span id="cb7-1181"><a href="#cb7-1181"></a>the minimization of a loss function. To formulate the "regular" learning</span>
<span id="cb7-1182"><a href="#cb7-1182"></a>procedure, let's denote the training dataset as $D$, and the test</span>
<span id="cb7-1183"><a href="#cb7-1183"></a>dataset as $S$. Given a model parameterized by $\theta$; training loss</span>
<span id="cb7-1184"><a href="#cb7-1184"></a>function $L(\theta, D)$; and test loss function $L(\theta, S)$, we can</span>
<span id="cb7-1185"><a href="#cb7-1185"></a>formulate a process of "regular" machine learning process as</span>
<span id="cb7-1186"><a href="#cb7-1186"></a>$$\begin{aligned}</span>
<span id="cb7-1187"><a href="#cb7-1187"></a>    \theta^\star = \arg\min_\theta\quad L(\theta, D).</span>
<span id="cb7-1188"><a href="#cb7-1188"></a>\end{aligned}$$ Note that the minimization of the training loss function</span>
<span id="cb7-1189"><a href="#cb7-1189"></a>is essentially *one* possible learning algorithm. For example, instead</span>
<span id="cb7-1190"><a href="#cb7-1190"></a>of minimizing the loss function, one may do gradient descent with model</span>
<span id="cb7-1191"><a href="#cb7-1191"></a>regularization on the loss function, where the final solution may not be</span>
<span id="cb7-1192"><a href="#cb7-1192"></a>the one that actually minimizes the loss function. As a result, we may</span>
<span id="cb7-1193"><a href="#cb7-1193"></a>want to be more general and more abstract for the moment, and denote the</span>
<span id="cb7-1194"><a href="#cb7-1194"></a>learning algorithm as $\mathcal{A}$. Thus, we can write</span>
<span id="cb7-1195"><a href="#cb7-1195"></a>$$\begin{aligned}</span>
<span id="cb7-1196"><a href="#cb7-1196"></a>    \theta^\star = \mathcal{A}(D),</span>
<span id="cb7-1197"><a href="#cb7-1197"></a>\end{aligned}$$ i.e., the learning algorithm $\mathcal{A}$ takes in a</span>
<span id="cb7-1198"><a href="#cb7-1198"></a>training dataset and outputs a model parameter $\theta^\star$. Then, the</span>
<span id="cb7-1199"><a href="#cb7-1199"></a>performance of the model is evaluated by the test loss</span>
<span id="cb7-1200"><a href="#cb7-1200"></a>$L(\mathcal{A}(D), S)$. As we can see, in the regime of "regular"</span>
<span id="cb7-1201"><a href="#cb7-1201"></a>learning, the learning algorithm $\mathcal{A}$ is pre-defined and fixed.</span>
<span id="cb7-1202"><a href="#cb7-1202"></a></span>
<span id="cb7-1203"><a href="#cb7-1203"></a>Meta-learning, or learning-to-learn, essentially asks the question of</span>
<span id="cb7-1204"><a href="#cb7-1204"></a>whether one can *learn* the learning algorithm $\mathcal{A}$ from prior</span>
<span id="cb7-1205"><a href="#cb7-1205"></a>tasks, such that the modal can adapt to a new task more</span>
<span id="cb7-1206"><a href="#cb7-1206"></a>quickly/proficiently. For example, different human languages share</span>
<span id="cb7-1207"><a href="#cb7-1207"></a>similar ideas, and therefore a human expert who has learned many</span>
<span id="cb7-1208"><a href="#cb7-1208"></a>languages should be able to learn a new language easier than an average</span>
<span id="cb7-1209"><a href="#cb7-1209"></a>person. In other words, the human expert should have learned how to</span>
<span id="cb7-1210"><a href="#cb7-1210"></a>learn new languages more quickly based on their past experiences on</span>
<span id="cb7-1211"><a href="#cb7-1211"></a>learning languages.</span>
<span id="cb7-1212"><a href="#cb7-1212"></a></span>
<span id="cb7-1213"><a href="#cb7-1213"></a>To mathematically formulate meta-learning, we consider a family of</span>
<span id="cb7-1214"><a href="#cb7-1214"></a>learning algorithms $\mathcal{A}_\omega$ parameterized by $\omega$. The</span>
<span id="cb7-1215"><a href="#cb7-1215"></a>"prior" tasks are represented by a set of meta-training datasets</span>
<span id="cb7-1216"><a href="#cb7-1216"></a>$<span class="sc">\{</span>(D_i, S_i)<span class="sc">\}</span>_{i=1}^N$ consists of $N$ pairs of training dataset $D_i$</span>
<span id="cb7-1217"><a href="#cb7-1217"></a>and test dataset $S_i$. As we noted before, a learning algorithm</span>
<span id="cb7-1218"><a href="#cb7-1218"></a>$\mathcal{A}_\omega$ takes in a training dataset, and outputs a model,</span>
<span id="cb7-1219"><a href="#cb7-1219"></a>i.e., $$\begin{aligned}</span>
<span id="cb7-1220"><a href="#cb7-1220"></a>    \forall i: \quad \theta^\star_i=\mathcal{A}_\omega(D_i).</span>
<span id="cb7-1221"><a href="#cb7-1221"></a>\end{aligned}$$</span>
<span id="cb7-1222"><a href="#cb7-1222"></a></span>
<span id="cb7-1223"><a href="#cb7-1223"></a>Therefore, the **meta-learning objective** is $$\begin{aligned}</span>
<span id="cb7-1224"><a href="#cb7-1224"></a>    \min_\omega \quad \sum_{i}\ L(\mathcal{A}_\omega(D_i), S_i).</span>
<span id="cb7-1225"><a href="#cb7-1225"></a>\end{aligned}$$ The above optimization problem gives a solution</span>
<span id="cb7-1226"><a href="#cb7-1226"></a>$\omega^\star$ which we use as the meta-parameter. Then, when a new task</span>
<span id="cb7-1227"><a href="#cb7-1227"></a>comes with a new training dataset $D_{new}$, we can simply apply</span>
<span id="cb7-1228"><a href="#cb7-1228"></a>$\theta^\star_{new}=\mathcal{A}_{\omega^\star}(D_{new})$ to obtain the</span>
<span id="cb7-1229"><a href="#cb7-1229"></a>adapted model $\theta^\star_{new}$. Note that we usually assume the</span>
<span id="cb7-1230"><a href="#cb7-1230"></a>meta-training datasets $D_i, S_i$ and the new dataset $D_{new}$ share</span>
<span id="cb7-1231"><a href="#cb7-1231"></a>the same underlying structure, or they come from the same distribution</span>
<span id="cb7-1232"><a href="#cb7-1232"></a>of datasets.</span>
<span id="cb7-1233"><a href="#cb7-1233"></a></span>
<span id="cb7-1234"><a href="#cb7-1234"></a>One of the most popular meta-learning method is Model-Agnosic</span>
<span id="cb7-1235"><a href="#cb7-1235"></a>Meta-Learning (MAML) <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>. In MAML, the meta-parameter</span>
<span id="cb7-1236"><a href="#cb7-1236"></a>$\omega$ shares the same space as the model parameter $\theta$. At its</span>
<span id="cb7-1237"><a href="#cb7-1237"></a>core, in MAML the learning algorithm is defined to be $$\begin{aligned}</span>
<span id="cb7-1238"><a href="#cb7-1238"></a>    \mathcal{A}_\omega(D_i)=\omega-\alpha \nabla_\omega L(\omega, D_i),</span>
<span id="cb7-1239"><a href="#cb7-1239"></a>\end{aligned}$$ where $\alpha$ is the step size. As we can see, in fact</span>
<span id="cb7-1240"><a href="#cb7-1240"></a>$\omega$ is defined as the initialization of fine-tuning $\theta$. With</span>
<span id="cb7-1241"><a href="#cb7-1241"></a>a good $\omega$ learned, the model can adapt to a new task very quickly.</span>
<span id="cb7-1242"><a href="#cb7-1242"></a>In general, meta-learning can be summarized as follows: Given data from</span>
<span id="cb7-1243"><a href="#cb7-1243"></a>prior tasks, learn to solve a new task more quickly/proficiently. Given</span>
<span id="cb7-1244"><a href="#cb7-1244"></a>the general nature of meta-learning, one may be curious about whether</span>
<span id="cb7-1245"><a href="#cb7-1245"></a>preference learning can be benefited from meta-learning, which we</span>
<span id="cb7-1246"><a href="#cb7-1246"></a>discuss in the following section.</span>
<span id="cb7-1247"><a href="#cb7-1247"></a></span>
<span id="cb7-1248"><a href="#cb7-1248"></a><span class="fu">#### Few-Shot Preference Learning for Reinforcement Learning {#sec-few-shot}</span></span>
<span id="cb7-1249"><a href="#cb7-1249"></a></span>
<span id="cb7-1250"><a href="#cb7-1250"></a>Reinforcement learning (RL) in robotics often stumbles when it comes to</span>
<span id="cb7-1251"><a href="#cb7-1251"></a>devising reward functions aligning with human intentions.</span>
<span id="cb7-1252"><a href="#cb7-1252"></a>Preference-based RL algorithms aim to solve this by learning from human</span>
<span id="cb7-1253"><a href="#cb7-1253"></a>feedback, but this often demands a *highly impractical number of</span>
<span id="cb7-1254"><a href="#cb7-1254"></a>queries* or leads to oversimplified reward functions that don't hold up</span>
<span id="cb7-1255"><a href="#cb7-1255"></a>in real-world tasks.</span>
<span id="cb7-1256"><a href="#cb7-1256"></a></span>
<span id="cb7-1257"><a href="#cb7-1257"></a>To address the impractical requirement of human queries, as we discussed</span>
<span id="cb7-1258"><a href="#cb7-1258"></a>in the previous section, one may apply meta-learning so that the RL</span>
<span id="cb7-1259"><a href="#cb7-1259"></a>agent can adapt to new tasks with fewer human queries. <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span></span>
<span id="cb7-1260"><a href="#cb7-1260"></a>proposes to pre-training models on previous tasks with the meta-learning</span>
<span id="cb7-1261"><a href="#cb7-1261"></a>method MAML <span class="co">[</span><span class="ot">@finn2017model</span><span class="co">]</span>, and then the meta-trained model can adapt</span>
<span id="cb7-1262"><a href="#cb7-1262"></a>to new tasks with fewer queries.</span>
<span id="cb7-1263"><a href="#cb7-1263"></a></span>
<span id="cb7-1264"><a href="#cb7-1264"></a>We consider Reinforcement Learning (RL) settings where a state is</span>
<span id="cb7-1265"><a href="#cb7-1265"></a>denoted as $s\in S$, and action is denoted as $a\in A$, for state space</span>
<span id="cb7-1266"><a href="#cb7-1266"></a>$S$ and action space $A$. The reward function $r:S\times A \to \mathbb{R}$ is</span>
<span id="cb7-1267"><a href="#cb7-1267"></a>unknown and need to be learned from eliciting human preferences. There</span>
<span id="cb7-1268"><a href="#cb7-1268"></a>are multiple tasks, where each task has its own reward function and</span>
<span id="cb7-1269"><a href="#cb7-1269"></a>transition probabilities. The reward model is parameterized by $\psi$.</span>
<span id="cb7-1270"><a href="#cb7-1270"></a>We denote $\hat{r}_\psi(s,a)$ to be a learned estimate of an unknown</span>
<span id="cb7-1271"><a href="#cb7-1271"></a>ground-truth reward function $r(s,a)$, parameterized by $\psi$.</span>
<span id="cb7-1272"><a href="#cb7-1272"></a>Accordingly, a reward model determines a RL policy $\phi$ by maximizing</span>
<span id="cb7-1273"><a href="#cb7-1273"></a>the accumulated rewards. The preferences is learned via pairwise</span>
<span id="cb7-1274"><a href="#cb7-1274"></a>comparison of trajectory segments $$\begin{aligned}</span>
<span id="cb7-1275"><a href="#cb7-1275"></a>    \sigma = (s_t, a_t, s_{t+1}, a_{t+1}, ..., s_{t+k-1}, s_{t+k-1})</span>
<span id="cb7-1276"><a href="#cb7-1276"></a>\end{aligned}$$ of $k$ states and actions.</span>
<span id="cb7-1277"><a href="#cb7-1277"></a></span>
<span id="cb7-1278"><a href="#cb7-1278"></a>For each pre-training task, there is a dataset $D$ consists of labeled</span>
<span id="cb7-1279"><a href="#cb7-1279"></a>queries $(\sigma_1, \sigma_2, y)$ where $y\in <span class="sc">\{</span>0, 1<span class="sc">\}</span>$ is the label</span>
<span id="cb7-1280"><a href="#cb7-1280"></a>representing which trajectory is preferred. Therefore, a loss function</span>
<span id="cb7-1281"><a href="#cb7-1281"></a>$L(\psi, D)$ captures how well the reward model characterizes the</span>
<span id="cb7-1282"><a href="#cb7-1282"></a>preferences in dataset $D$. In <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> they the preference</span>
<span id="cb7-1283"><a href="#cb7-1283"></a>predictor over segments using the Bradley-Terry model of paired</span>
<span id="cb7-1284"><a href="#cb7-1284"></a>comparisons <span class="co">[</span><span class="ot">@bradley1952rank</span><span class="co">]</span>, i.e., $$\begin{aligned}</span>
<span id="cb7-1285"><a href="#cb7-1285"></a>    P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2 </span><span class="co">]</span> = \frac{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1})}{\exp \sum_t \hat{r}_\psi(s_t^{1}, a_t^{1}) + \exp \sum_t \hat{r}_\psi(s_t^{2}, a_t^{2})}.</span>
<span id="cb7-1286"><a href="#cb7-1286"></a>\end{aligned}$$ Then, the loss function is essentially a binary</span>
<span id="cb7-1287"><a href="#cb7-1287"></a>cross-entropy which the reward model $\psi$ aims to minimize, i.e.,</span>
<span id="cb7-1288"><a href="#cb7-1288"></a>$$\begin{aligned}</span>
<span id="cb7-1289"><a href="#cb7-1289"></a>    {L}(\psi,  {D}) = - \mathbb{E}_{(\sigma^1, \sigma^2, y) \sim {D}} \left<span class="co">[</span><span class="ot"> y(1) \log (P[\sigma_1 \succ \sigma_2 ]) + y(2)\log(1 - P[\sigma_1 \succ \sigma_2 ]) \right</span><span class="co">]</span>.</span>
<span id="cb7-1290"><a href="#cb7-1290"></a>\end{aligned}$$</span>
<span id="cb7-1291"><a href="#cb7-1291"></a></span>
<span id="cb7-1292"><a href="#cb7-1292"></a><span class="fu">##### Method Component 1: Pre-Training with Meta Learning {#method-component-1-pre-training-with-meta-learning .unnumbered}</span></span>
<span id="cb7-1293"><a href="#cb7-1293"></a></span>
<span id="cb7-1294"><a href="#cb7-1294"></a>To efficiently approximate the reward function $r_\text{new}$ for a new</span>
<span id="cb7-1295"><a href="#cb7-1295"></a>task with minimal queries, as described in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, we aim to</span>
<span id="cb7-1296"><a href="#cb7-1296"></a>utilize a pre-trained reward function $\hat{r}_\psi$ that can be quickly</span>
<span id="cb7-1297"><a href="#cb7-1297"></a>fine-tuned using just a few preference comparisons. By pre-training on</span>
<span id="cb7-1298"><a href="#cb7-1298"></a>data from prior tasks, we can leverage the common structure across tasks</span>
<span id="cb7-1299"><a href="#cb7-1299"></a>to speed up the adaptation process. Although any meta-learning method is</span>
<span id="cb7-1300"><a href="#cb7-1300"></a>compatible, <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> opt for Model Agnostic Meta-Learning (MAML)</span>
<span id="cb7-1301"><a href="#cb7-1301"></a>due to its simplicity. Therefore, the pre-training update for the reward</span>
<span id="cb7-1302"><a href="#cb7-1302"></a>model $\psi$ is $$\begin{aligned}</span>
<span id="cb7-1303"><a href="#cb7-1303"></a>    \psi \xleftarrow{} \psi - \beta \nabla_\psi \sum_{i = 1}^N {L} (\psi - \alpha \nabla_\psi {L}(\psi, {D}_i), {D}_i),</span>
<span id="cb7-1304"><a href="#cb7-1304"></a>\end{aligned}$$ where $\alpha, \beta$ are the inner and outer learning</span>
<span id="cb7-1305"><a href="#cb7-1305"></a>rate, respectively. We note that data $<span class="sc">\{</span>D_i<span class="sc">\}</span>_i$ of labeled preferences</span>
<span id="cb7-1306"><a href="#cb7-1306"></a>queries for prior tasks can come from offline datasets, simulated</span>
<span id="cb7-1307"><a href="#cb7-1307"></a>policies, or actual humans.</span>
<span id="cb7-1308"><a href="#cb7-1308"></a></span>
<span id="cb7-1309"><a href="#cb7-1309"></a><span class="fu">##### Method Component 2: Few-Shot Adaptation {#method-component-2-few-shot-adaptation .unnumbered}</span></span>
<span id="cb7-1310"><a href="#cb7-1310"></a></span>
<span id="cb7-1311"><a href="#cb7-1311"></a>With the aforementioned pre-training with meta learning, the</span>
<span id="cb7-1312"><a href="#cb7-1312"></a>meta-learned reward model can then be used for few-shot preference based</span>
<span id="cb7-1313"><a href="#cb7-1313"></a>RL during an online adaptation phase. The core procedure of the few-shot</span>
<span id="cb7-1314"><a href="#cb7-1314"></a>adaption is descibed as below</span>
<span id="cb7-1315"><a href="#cb7-1315"></a></span>
<span id="cb7-1316"><a href="#cb7-1316"></a><span class="ss">1.  </span>Given a pre-trained reward model $\psi$</span>
<span id="cb7-1317"><a href="#cb7-1317"></a></span>
<span id="cb7-1318"><a href="#cb7-1318"></a><span class="ss">2.  </span>For time step $t=1, 2, \dots$</span>
<span id="cb7-1319"><a href="#cb7-1319"></a></span>
<span id="cb7-1320"><a href="#cb7-1320"></a><span class="ss">    1.  </span>Find pairs of trajectories $(\sigma_1, \sigma_2)$ with</span>
<span id="cb7-1321"><a href="#cb7-1321"></a>        preference uncertainty based on $\psi$.</span>
<span id="cb7-1322"><a href="#cb7-1322"></a></span>
<span id="cb7-1323"><a href="#cb7-1323"></a><span class="ss">    2.  </span>Query human preference $y$ and forms a new dataset $D_{new}$</span>
<span id="cb7-1324"><a href="#cb7-1324"></a></span>
<span id="cb7-1325"><a href="#cb7-1325"></a><span class="ss">    3.  </span>Update the reward model by</span>
<span id="cb7-1326"><a href="#cb7-1326"></a>        $\psi'\leftarrow \psi - \alpha \nabla_\psi L(\psi, D_{new})$</span>
<span id="cb7-1327"><a href="#cb7-1327"></a></span>
<span id="cb7-1328"><a href="#cb7-1328"></a><span class="ss">    4.  </span>Update the policy with the new reward model $\psi'$</span>
<span id="cb7-1329"><a href="#cb7-1329"></a></span>
<span id="cb7-1330"><a href="#cb7-1330"></a>As mentioned in <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span>, uncertain queries are selected using</span>
<span id="cb7-1331"><a href="#cb7-1331"></a>the disagreement of an ensemble of reward functions over the preference</span>
<span id="cb7-1332"><a href="#cb7-1332"></a>predictors. Specifically, comparisons that maximize</span>
<span id="cb7-1333"><a href="#cb7-1333"></a>$\texttt{std}(P<span class="co">[</span><span class="ot">\sigma_1 \succ \sigma_2</span><span class="co">]</span>)$ are selected each time</span>
<span id="cb7-1334"><a href="#cb7-1334"></a>feedback is collected.</span>
<span id="cb7-1335"><a href="#cb7-1335"></a></span>
<span id="cb7-1336"><a href="#cb7-1336"></a>The whole pipeline of the method is outlined in @fig-few-1.</span>
<span id="cb7-1337"><a href="#cb7-1337"></a></span>
<span id="cb7-1338"><a href="#cb7-1338"></a>!<span class="co">[</span><span class="ot">An overview of the proposed method in [@hejna2023few</span><span class="co">]</span>. **Pre-training</span>
<span id="cb7-1339"><a href="#cb7-1339"></a>(left):** In the pre-training phase, trajectory segment comparisons are</span>
<span id="cb7-1340"><a href="#cb7-1340"></a>generated using data from previously learned tasks. Then, they are used</span>
<span id="cb7-1341"><a href="#cb7-1341"></a>to train a reward model. **Online-Adaptation (Right)**: After</span>
<span id="cb7-1342"><a href="#cb7-1342"></a>pre-training the reward model, it is adapted to new data from human</span>
<span id="cb7-1343"><a href="#cb7-1343"></a>feedback. The adapted reward model is then used to train a policy for a</span>
<span id="cb7-1344"><a href="#cb7-1344"></a>new task in a closed loop manner.](Figures/overview-few.png){#fig-few-1</span>
<span id="cb7-1345"><a href="#cb7-1345"></a>width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-1346"><a href="#cb7-1346"></a></span>
<span id="cb7-1347"><a href="#cb7-1347"></a>We present one set of experiment from the paper, as it illustrates the</span>
<span id="cb7-1348"><a href="#cb7-1348"></a>effectiveness of the proposed method in a straightforward way. The</span>
<span id="cb7-1349"><a href="#cb7-1349"></a>experiment test the propoesed method on the Meta-World</span>
<span id="cb7-1350"><a href="#cb7-1350"></a>benchmark <span class="co">[</span><span class="ot">@yu2020meta</span><span class="co">]</span>. Three baselines are compared with the proposed</span>
<span id="cb7-1351"><a href="#cb7-1351"></a>method:</span>
<span id="cb7-1352"><a href="#cb7-1352"></a></span>
<span id="cb7-1353"><a href="#cb7-1353"></a><span class="ss">1.  </span>SAC: The Soft-Actor Critic RL algorithm trained from ground truth</span>
<span id="cb7-1354"><a href="#cb7-1354"></a>    rewards. This represents the standard best possible method given the</span>
<span id="cb7-1355"><a href="#cb7-1355"></a>    ground-truth reward.</span>
<span id="cb7-1356"><a href="#cb7-1356"></a></span>
<span id="cb7-1357"><a href="#cb7-1357"></a><span class="ss">2.  </span>PEBBLE: The PEBBLE algorithm <span class="co">[</span><span class="ot">@lee2021pebble</span><span class="co">]</span>. It does not use</span>
<span id="cb7-1358"><a href="#cb7-1358"></a>    information from pripor tasks.</span>
<span id="cb7-1359"><a href="#cb7-1359"></a></span>
<span id="cb7-1360"><a href="#cb7-1360"></a><span class="ss">3.  </span>Init: This method initialize the reward model with the pretained</span>
<span id="cb7-1361"><a href="#cb7-1361"></a>    weights from meta learning. However, instead of adapting the reward</span>
<span id="cb7-1362"><a href="#cb7-1362"></a>    model to the new task, it performs standard updates as in PEBBLE.</span>
<span id="cb7-1363"><a href="#cb7-1363"></a></span>
<span id="cb7-1364"><a href="#cb7-1364"></a>The results are shown in @fig-few-exp, where we can see that the proposed methord</span>
<span id="cb7-1365"><a href="#cb7-1365"></a>outperforms all of the baselines.</span>
<span id="cb7-1366"><a href="#cb7-1366"></a></span>
<span id="cb7-1367"><a href="#cb7-1367"></a>![Results on MetaWorld tasks. The title of each subplot indicates the</span>
<span id="cb7-1368"><a href="#cb7-1368"></a>task and number of artificial feedback queries used in training. Results</span>
<span id="cb7-1369"><a href="#cb7-1369"></a>for each method are shown across five seeds.</span>
<span id="cb7-1370"><a href="#cb7-1370"></a>](Figures/few-exp.png){#fig-few-exp width="<span class="sc">\\</span>linewidth"}</span>
<span id="cb7-1371"><a href="#cb7-1371"></a></span>
<span id="cb7-1372"><a href="#cb7-1372"></a>This paper <span class="co">[</span><span class="ot">@hejna2023few</span><span class="co">]</span> shows that meta reward learning indeed reduce</span>
<span id="cb7-1373"><a href="#cb7-1373"></a>the number of queries of human preferences. However, as mentioned in the</span>
<span id="cb7-1374"><a href="#cb7-1374"></a>paper, there are still some drawbacks, as shown in the following.</span>
<span id="cb7-1375"><a href="#cb7-1375"></a></span>
<span id="cb7-1376"><a href="#cb7-1376"></a>Many of the queries the model pick for human preference elicitation are</span>
<span id="cb7-1377"><a href="#cb7-1377"></a>actually almost identical to human. After all, the model would pick the</span>
<span id="cb7-1378"><a href="#cb7-1378"></a>most uncertain pair of trajectories for human preference queries, and</span>
<span id="cb7-1379"><a href="#cb7-1379"></a>similar trajectories are for sure having high uncertainty in their</span>
<span id="cb7-1380"><a href="#cb7-1380"></a>preference. This suggest the need of new ways for designing the query</span>
<span id="cb7-1381"><a href="#cb7-1381"></a>selection strategy.</span>
<span id="cb7-1382"><a href="#cb7-1382"></a></span>
<span id="cb7-1383"><a href="#cb7-1383"></a>Moreover, despite the improved query complexity, it still needs an</span>
<span id="cb7-1384"><a href="#cb7-1384"></a>impractical amount of queries. As shown in @fig-few-exp, the "sweep into" task still needs 2500 human</span>
<span id="cb7-1385"><a href="#cb7-1385"></a>queries for it to work properly, which is still not ideal for what we</span>
<span id="cb7-1386"><a href="#cb7-1386"></a>want them to be.</span>
<span id="cb7-1387"><a href="#cb7-1387"></a></span>
<span id="cb7-1388"><a href="#cb7-1388"></a>In addition, it is mentioned in the paper that the proposed method may</span>
<span id="cb7-1389"><a href="#cb7-1389"></a>be even worse than training from scratch, if the new task is too</span>
<span id="cb7-1390"><a href="#cb7-1390"></a>out-of-distribution. Certainly, since meta-learning assumes</span>
<span id="cb7-1391"><a href="#cb7-1391"></a>in-distribution tasks, we cannot expect the proposed method to be good</span>
<span id="cb7-1392"><a href="#cb7-1392"></a>for out-of-distribution task. It is thus an interesting future direction</span>
<span id="cb7-1393"><a href="#cb7-1393"></a>to investigate whether one can design a method that automatically</span>
<span id="cb7-1394"><a href="#cb7-1394"></a>balance between using the prior information or training from scratch.</span>
<span id="cb7-1395"><a href="#cb7-1395"></a></span>
<span id="cb7-1396"><a href="#cb7-1396"></a><span class="fu">#### Watch Try Learn {#sec-watch}</span></span>
<span id="cb7-1397"><a href="#cb7-1397"></a></span>
<span id="cb7-1398"><a href="#cb7-1398"></a>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards</span>
<span id="cb7-1399"><a href="#cb7-1399"></a><span class="co">[</span><span class="ot">@zhou2019watch</span><span class="co">]</span> asks the question "How can we efficiently learn both</span>
<span id="cb7-1400"><a href="#cb7-1400"></a>from expert demonstrations and from trials where we only get **binary**</span>
<span id="cb7-1401"><a href="#cb7-1401"></a>feedback from a human\". Why do we care about this question? In the</span>
<span id="cb7-1402"><a href="#cb7-1402"></a>context of robotics, a very compelling answer is the *cost of</span>
<span id="cb7-1403"><a href="#cb7-1403"></a>data-collection*. In a hypothetical world in which we have a vast number</span>
<span id="cb7-1404"><a href="#cb7-1404"></a>of **expert demonstrations** of robots accomplishing a large number of</span>
<span id="cb7-1405"><a href="#cb7-1405"></a>diverse tasks, we don't necessarily need to worry about learning from</span>
<span id="cb7-1406"><a href="#cb7-1406"></a>trials or from humans. We could simply learn a very capable imitation</span>
<span id="cb7-1407"><a href="#cb7-1407"></a>agent to perform any task. Natural Language Processing could be seen as</span>
<span id="cb7-1408"><a href="#cb7-1408"></a>living in this world, because internet-scale data is available.</span>
<span id="cb7-1409"><a href="#cb7-1409"></a>**Robots, however, are expensive**, so people generally don't have</span>
<span id="cb7-1410"><a href="#cb7-1410"></a>access to them, and therefore cannot use them to produce information to</span>
<span id="cb7-1411"><a href="#cb7-1411"></a>imitate. Similarly, **human time is expensive**, so even for large</span>
<span id="cb7-1412"><a href="#cb7-1412"></a>organizations that do have access to a lot of robots, it's still hard to</span>
<span id="cb7-1413"><a href="#cb7-1413"></a>collect a lot of expert demonstrations.</span>
<span id="cb7-1414"><a href="#cb7-1414"></a></span>
<span id="cb7-1415"><a href="#cb7-1415"></a>The largest available collection of robotics datasets today is Open</span>
<span id="cb7-1416"><a href="#cb7-1416"></a>X-Embodiment (<span class="co">[</span><span class="ot">@padalkar2023open</span><span class="co">]</span>), which consists of around 1M episodes</span>
<span id="cb7-1417"><a href="#cb7-1417"></a>from more than 300 different scenes. Even such large datastes are not</span>
<span id="cb7-1418"><a href="#cb7-1418"></a>enough to learn generally-capable robotic policies from imitation</span>
<span id="cb7-1419"><a href="#cb7-1419"></a>learning alone.</span>
<span id="cb7-1420"><a href="#cb7-1420"></a></span>
<span id="cb7-1421"><a href="#cb7-1421"></a>![Visualization of the Open X-Embodiment dataset collection. Even this</span>
<span id="cb7-1422"><a href="#cb7-1422"></a>large-scale dataset for robot learning is not yet enough to learn</span>
<span id="cb7-1423"><a href="#cb7-1423"></a>generally-capable robotic</span>
<span id="cb7-1424"><a href="#cb7-1424"></a>policies.](Figures/open_x_embodiment.png){#fig-open-x-embodiment}</span>
<span id="cb7-1425"><a href="#cb7-1425"></a></span>
<span id="cb7-1426"><a href="#cb7-1426"></a>**Main insight:** binary feedback is much cheaper to obtain than expert</span>
<span id="cb7-1427"><a href="#cb7-1427"></a>demonstrations! Instead of hiring people to act as robot operators to</span>
<span id="cb7-1428"><a href="#cb7-1428"></a>tell the robot exactly what to do, if there was a way of having many</span>
<span id="cb7-1429"><a href="#cb7-1429"></a>robots trying things in parallel, we can have humans watch videos of</span>
<span id="cb7-1430"><a href="#cb7-1430"></a>what the robots did and then give a success classification of whether</span>
<span id="cb7-1431"><a href="#cb7-1431"></a>the robot accomplished the goal. This is a much cheaper form of human</span>
<span id="cb7-1432"><a href="#cb7-1432"></a>supervision because the human labels don't necessarily need to be given</span>
<span id="cb7-1433"><a href="#cb7-1433"></a>in real time, so one human labeler can label many trajectories in</span>
<span id="cb7-1434"><a href="#cb7-1434"></a>parallel, and the human doesn't need to be a skilled robot operator.</span>
<span id="cb7-1435"><a href="#cb7-1435"></a></span>
<span id="cb7-1436"><a href="#cb7-1436"></a>Concretely, this paper seeks to learn new tasks with the following</span>
<span id="cb7-1437"><a href="#cb7-1437"></a>general problem setting:</span>
<span id="cb7-1438"><a href="#cb7-1438"></a></span>
<span id="cb7-1439"><a href="#cb7-1439"></a><span class="ss">1.  </span>We only get 1 expert demonstration of the target task</span>
<span id="cb7-1440"><a href="#cb7-1440"></a></span>
<span id="cb7-1441"><a href="#cb7-1441"></a><span class="ss">2.  </span>After seeing the expert demonstration, we have robots try to solve</span>
<span id="cb7-1442"><a href="#cb7-1442"></a>    the task 1 or more times.</span>
<span id="cb7-1443"><a href="#cb7-1443"></a></span>
<span id="cb7-1444"><a href="#cb7-1444"></a><span class="ss">3.  </span>The user (or some pre-defined reward function) annotates each trial</span>
<span id="cb7-1445"><a href="#cb7-1445"></a>    as success/failure.</span>
<span id="cb7-1446"><a href="#cb7-1446"></a></span>
<span id="cb7-1447"><a href="#cb7-1447"></a><span class="ss">4.  </span>The agent learns from both the demos and the annotated trials to</span>
<span id="cb7-1448"><a href="#cb7-1448"></a>    perform well on the target task.</span>
<span id="cb7-1449"><a href="#cb7-1449"></a></span>
<span id="cb7-1450"><a href="#cb7-1450"></a>Note that this work falls under the **meta-learning** umbrella, because</span>
<span id="cb7-1451"><a href="#cb7-1451"></a>we are learning an algorithm for quickly learning new tasks given new</span>
<span id="cb7-1452"><a href="#cb7-1452"></a>observations (demos, trials, and success labels.)</span>
<span id="cb7-1453"><a href="#cb7-1453"></a></span>
<span id="cb7-1454"><a href="#cb7-1454"></a>The **main contribution** of this paper is a meta-learning algorithm for</span>
<span id="cb7-1455"><a href="#cb7-1455"></a>incorporating demonstrations and binary feedback from trials to solve</span>
<span id="cb7-1456"><a href="#cb7-1456"></a>new tasks.</span>
<span id="cb7-1457"><a href="#cb7-1457"></a></span>
<span id="cb7-1458"><a href="#cb7-1458"></a>Meta-Learning deals with efficient learning of new tasks. In the context</span>
<span id="cb7-1459"><a href="#cb7-1459"></a>of robotics or reinforcement learning in general, **how do we define</span>
<span id="cb7-1460"><a href="#cb7-1460"></a>tasks**? We will use the Markov decision process (**MDP**) formalism. A</span>
<span id="cb7-1461"><a href="#cb7-1461"></a>task $T_i$ is described with the tuple $<span class="sc">\{</span>S, A, r_i, P_i<span class="sc">\}</span>$.</span>
<span id="cb7-1462"><a href="#cb7-1462"></a></span>
<span id="cb7-1463"><a href="#cb7-1463"></a><span class="ss">1.  </span>$S$ represents the *state-space* of the task, or all possible states</span>
<span id="cb7-1464"><a href="#cb7-1464"></a>    the agent could find itself in. This work uses image-observations,</span>
<span id="cb7-1465"><a href="#cb7-1465"></a>    so $S$ is the space of all possible RGB images.</span>
<span id="cb7-1466"><a href="#cb7-1466"></a></span>
<span id="cb7-1467"><a href="#cb7-1467"></a><span class="ss">2.  </span>$A$ is the action space, meaning the set of all possible actions the</span>
<span id="cb7-1468"><a href="#cb7-1468"></a>    agent could take. In robotics there are many ways of representing</span>
<span id="cb7-1469"><a href="#cb7-1469"></a>    action spaces, and this work considers end-effector positions,</span>
<span id="cb7-1470"><a href="#cb7-1470"></a>    rotations, and opening.</span>
<span id="cb7-1471"><a href="#cb7-1471"></a></span>
<span id="cb7-1472"><a href="#cb7-1472"></a><span class="ss">3.  </span>$r_i$ is the reward function for the task, with function signature</span>
<span id="cb7-1473"><a href="#cb7-1473"></a>    $r_i : S \times A \to \mathbb{R}$. This work assumes all reward functions</span>
<span id="cb7-1474"><a href="#cb7-1474"></a>    are binary.</span>
<span id="cb7-1475"><a href="#cb7-1475"></a></span>
<span id="cb7-1476"><a href="#cb7-1476"></a><span class="ss">4.  </span>$P_i$ is the transition dynamics function. It's a function that maps</span>
<span id="cb7-1477"><a href="#cb7-1477"></a>    state-action pairs to probability distributions over next states.</span>
<span id="cb7-1478"><a href="#cb7-1478"></a></span>
<span id="cb7-1479"><a href="#cb7-1479"></a>Notice that $S$ and $A$ are shared across tasks. Transition dynamics</span>
<span id="cb7-1480"><a href="#cb7-1480"></a>functions are normally also shared between tasks because they represent</span>
<span id="cb7-1481"><a href="#cb7-1481"></a>the laws of physics. However, this work considers environments with</span>
<span id="cb7-1482"><a href="#cb7-1482"></a>different objects, so they don't share the dynamics function. Given this</span>
<span id="cb7-1483"><a href="#cb7-1483"></a>definition for tasks, they assume that the tasks from the data that they</span>
<span id="cb7-1484"><a href="#cb7-1484"></a>get come from some unknown task-generating distribution $p(T)$.</span>
<span id="cb7-1485"><a href="#cb7-1485"></a></span>
<span id="cb7-1486"><a href="#cb7-1486"></a>Let's give a more precise definition of the problem statement considered</span>
<span id="cb7-1487"><a href="#cb7-1487"></a>by **Watch, Try, Learn**. As the paper name suggests, there are 3 phases</span>
<span id="cb7-1488"><a href="#cb7-1488"></a>for the problem statement.</span>
<span id="cb7-1489"><a href="#cb7-1489"></a></span>
<span id="cb7-1490"><a href="#cb7-1490"></a>**Watch:** During the *watch* phase, we give the agent $K$</span>
<span id="cb7-1491"><a href="#cb7-1491"></a>demonstrations of the target tasks. This paper considers the case where</span>
<span id="cb7-1492"><a href="#cb7-1492"></a>$K$ always equals 1, and all demonstrations are successful. That is,</span>
<span id="cb7-1493"><a href="#cb7-1493"></a>each demonstration consists of a trajectory</span>
<span id="cb7-1494"><a href="#cb7-1494"></a>$<span class="sc">\{</span>(s_0, a_0), \ldots, (s_H, a_H)<span class="sc">\}</span>$ where $H$ is the task horizon, and</span>
<span id="cb7-1495"><a href="#cb7-1495"></a>the final state is always successful, that is</span>
<span id="cb7-1496"><a href="#cb7-1496"></a>$r_i(s_H, a_H) = 1, r_i(s_j, a_j) = 0$ for every $j \neq H$.</span>
<span id="cb7-1497"><a href="#cb7-1497"></a></span>
<span id="cb7-1498"><a href="#cb7-1498"></a>Importantly, these demonstrations alone might not be sufficient for</span>
<span id="cb7-1499"><a href="#cb7-1499"></a>**full task specification**. As an example, consider a demonstration in</span>
<span id="cb7-1500"><a href="#cb7-1500"></a>which an apple is moved to the right, next to a pan. Seeing this</span>
<span id="cb7-1501"><a href="#cb7-1501"></a>demonstration alone, the task could be always moving the apple to the</span>
<span id="cb7-1502"><a href="#cb7-1502"></a>right, or it could be always moving the apple next to the pan,</span>
<span id="cb7-1503"><a href="#cb7-1503"></a>irrespective of where the pan is. The expected output after the Watch</span>
<span id="cb7-1504"><a href="#cb7-1504"></a>phase is a policy capable of gathering information about a task, given</span>
<span id="cb7-1505"><a href="#cb7-1505"></a>demonstrations.</span>
<span id="cb7-1506"><a href="#cb7-1506"></a></span>
<span id="cb7-1507"><a href="#cb7-1507"></a>**Try:** In the Try phase, we use the agent learned during the Watch</span>
<span id="cb7-1508"><a href="#cb7-1508"></a>phase to attempt the task for $L$ trials. As specified earlier, this</span>
<span id="cb7-1509"><a href="#cb7-1509"></a>paper considers the casae where $L$ always equals 1. After the agent</span>
<span id="cb7-1510"><a href="#cb7-1510"></a>completes the trials, humans (or pre-programmed reward functions)</span>
<span id="cb7-1511"><a href="#cb7-1511"></a>provide one binary reward for each trial, indicating whether the trial</span>
<span id="cb7-1512"><a href="#cb7-1512"></a>was successful. The expected output of this phase is $L$ trajectories</span>
<span id="cb7-1513"><a href="#cb7-1513"></a>and corresponding feedback that hopefully *disambiguate* the task.</span>
<span id="cb7-1514"><a href="#cb7-1514"></a></span>
<span id="cb7-1515"><a href="#cb7-1515"></a>**Learn:** After completing the trials, the agent must learn from both</span>
<span id="cb7-1516"><a href="#cb7-1516"></a>the original expert demonstrations and the trials, and become capable of</span>
<span id="cb7-1517"><a href="#cb7-1517"></a>solving the target task.</span>
<span id="cb7-1518"><a href="#cb7-1518"></a></span>
<span id="cb7-1519"><a href="#cb7-1519"></a>**Given Data:** To train agents that can Watch, Try, and Learn, we are</span>
<span id="cb7-1520"><a href="#cb7-1520"></a>given a dataset of expert demonstrations containing multiple demos for</span>
<span id="cb7-1521"><a href="#cb7-1521"></a>each task, and the dataset contains hundreds of tasks. Importantly, **no</span>
<span id="cb7-1522"><a href="#cb7-1522"></a>online interaction** is needed for training, and this method trains only</span>
<span id="cb7-1523"><a href="#cb7-1523"></a>with **supervised learning** and no reinforcement learning.</span>
<span id="cb7-1524"><a href="#cb7-1524"></a></span>
<span id="cb7-1525"><a href="#cb7-1525"></a>This section describes exactly how this paper trains an agent from the</span>
<span id="cb7-1526"><a href="#cb7-1526"></a>given expert demonstrations, and how to incorporate the trials and human</span>
<span id="cb7-1527"><a href="#cb7-1527"></a>feedback into the loop.</span>
<span id="cb7-1528"><a href="#cb7-1528"></a></span>
<span id="cb7-1529"><a href="#cb7-1529"></a>**Training to Watch:** We now describe the algorithm to obtain an agent</span>
<span id="cb7-1530"><a href="#cb7-1530"></a>conditioned on the given expert demonstration. In particular, what we</span>
<span id="cb7-1531"><a href="#cb7-1531"></a>want to obtain out of the Watch phase is a policy conditioned on a set</span>
<span id="cb7-1532"><a href="#cb7-1532"></a>of expert demonstrations. Formally, we want to obtain</span>
<span id="cb7-1533"><a href="#cb7-1533"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$.</span>
<span id="cb7-1534"><a href="#cb7-1534"></a></span>
<span id="cb7-1535"><a href="#cb7-1535"></a>The way we can obtain this policy is through **meta-imitation</span>
<span id="cb7-1536"><a href="#cb7-1536"></a>learning**. Given the demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$ for task</span>
<span id="cb7-1537"><a href="#cb7-1537"></a>$i$, we sample another *different* demonstration coming from the same</span>
<span id="cb7-1538"><a href="#cb7-1538"></a>task $\textbf{d}_i^{\text{test}}$. The key insight here is that</span>
<span id="cb7-1539"><a href="#cb7-1539"></a>$\textbf{d}_i^{\text{test}}$ is an example of **optimal behavior** given</span>
<span id="cb7-1540"><a href="#cb7-1540"></a>the demonstrations. Therefore, to obtain</span>
<span id="cb7-1541"><a href="#cb7-1541"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>d_{i,k}<span class="sc">\}</span>)$, we simply regress the</span>
<span id="cb7-1542"><a href="#cb7-1542"></a>policy to imitate actions taken on $\textbf{d}_i^{\text{test}}$.</span>
<span id="cb7-1543"><a href="#cb7-1543"></a>Concretely, we train policy parameters $\theta$ to minimize the</span>
<span id="cb7-1544"><a href="#cb7-1544"></a>following loss:</span>
<span id="cb7-1545"><a href="#cb7-1545"></a></span>
<span id="cb7-1546"><a href="#cb7-1546"></a>$\mathcal{L}^\text{watch}(\theta, \mathcal{D}_i^*) = \mathbb{E}_{\{d_{i,k}\} \sim \mathcal{D}_i^*} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^*  \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb7-1547"><a href="#cb7-1547"></a><span class="ss">- </span>\log \pi_\theta^{\text{watch}} (a_t | s_t, \{d_{i,k}<span class="sc">\}</span>) \big]$</span>
<span id="cb7-1548"><a href="#cb7-1548"></a></span>
<span id="cb7-1549"><a href="#cb7-1549"></a>This corresponds to doing imitation learning by minimizing the negative</span>
<span id="cb7-1550"><a href="#cb7-1550"></a>log-likelihood of the test trajectory actions, conditioning the policy</span>
<span id="cb7-1551"><a href="#cb7-1551"></a>on the entire demo set. However, how is the conditioning on the demo set</span>
<span id="cb7-1552"><a href="#cb7-1552"></a>achieved?</span>
<span id="cb7-1553"><a href="#cb7-1553"></a></span>
<span id="cb7-1554"><a href="#cb7-1554"></a>![Vision-based policy architecture that conditions on a set of</span>
<span id="cb7-1555"><a href="#cb7-1555"></a>demonstrations.](Figures/watch-try-learn-architecture.png){#fig-watch-try-learn-arch}</span>
<span id="cb7-1556"><a href="#cb7-1556"></a></span>
<span id="cb7-1557"><a href="#cb7-1557"></a>@fig-watch-try-learn-arch visualizes how Watch Try Learn</span>
<span id="cb7-1558"><a href="#cb7-1558"></a>deals with conditioning on demonstrations. In addition to using features</span>
<span id="cb7-1559"><a href="#cb7-1559"></a>obtained from the images of the current state, the architecture uses</span>
<span id="cb7-1560"><a href="#cb7-1560"></a>features from frames sampled (in order) from the demonstration episodes,</span>
<span id="cb7-1561"><a href="#cb7-1561"></a>which are concatenated together.</span>
<span id="cb7-1562"><a href="#cb7-1562"></a></span>
<span id="cb7-1563"><a href="#cb7-1563"></a>**Trying:** On the **Try** phase, when the agent is given a set of</span>
<span id="cb7-1564"><a href="#cb7-1564"></a>demonstrations $<span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>$, we deploy the policy</span>
<span id="cb7-1565"><a href="#cb7-1565"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$ to collect $L$</span>
<span id="cb7-1566"><a href="#cb7-1566"></a>trials. There is no training involved in the Try phase, we simply</span>
<span id="cb7-1567"><a href="#cb7-1567"></a>condition the policy on the given demonstrations</span>
<span id="cb7-1568"><a href="#cb7-1568"></a></span>
<span id="cb7-1569"><a href="#cb7-1569"></a>**Training to Learn:** During the Watch phase the objective was to train</span>
<span id="cb7-1570"><a href="#cb7-1570"></a>a policy conditioned on demonstrations</span>
<span id="cb7-1571"><a href="#cb7-1571"></a>$\pi_\theta^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}<span class="sc">\}</span>)$. The authors of</span>
<span id="cb7-1572"><a href="#cb7-1572"></a>Watch, Try, Learn use a similar strategy as the Watch phase for the</span>
<span id="cb7-1573"><a href="#cb7-1573"></a>Learn phase. We now want to train a policy that is conditioned on the</span>
<span id="cb7-1574"><a href="#cb7-1574"></a>demonstrations, as well as the trials and binary feedback. That is, we</span>
<span id="cb7-1575"><a href="#cb7-1575"></a>want to learn</span>
<span id="cb7-1576"><a href="#cb7-1576"></a>$\pi_\phi^{\text{watch}}(a | s, <span class="sc">\{</span>\textbf{d}_{i,k}\}, \{\mathbf{\tau}_{i, l}<span class="sc">\}</span>)$.</span>
<span id="cb7-1577"><a href="#cb7-1577"></a>To train the policy, we again use meta-imitation learning where we</span>
<span id="cb7-1578"><a href="#cb7-1578"></a>additionally sample yet another trajectory from the same task.</span>
<span id="cb7-1579"><a href="#cb7-1579"></a>Concretely, we train policy parameters $\phi$ to minimize the following</span>
<span id="cb7-1580"><a href="#cb7-1580"></a>loss:</span>
<span id="cb7-1581"><a href="#cb7-1581"></a></span>
<span id="cb7-1582"><a href="#cb7-1582"></a>$\mathcal{L}^{\text{learn}}(\phi, \mathcal{D}_i, \mathcal{D}_i^*) = \mathbb{E}_{(\{d_{i,k}\}, \{\mathbf{\tau}_{i,l}\}) \sim \mathcal{D}_i} \mathbb{E}_{\{d_{i,k}^{\text{test}}\} \sim \mathcal{D}_i^* \{d_{i,k}\}} \mathbb{E}_{(s_t, a_t) \sim d_i^{\text{test}}} \big[ </span>
<span id="cb7-1583"><a href="#cb7-1583"></a><span class="ss">- </span>\log \pi_\theta^{\text{learn}} (a_t | s_t, \{d_{i,k}\}, \{\tau_{i,l}<span class="sc">\}</span>) \big]$</span>
<span id="cb7-1584"><a href="#cb7-1584"></a></span>
<span id="cb7-1585"><a href="#cb7-1585"></a>The conditioning on both the demo episodes and the trial episodes is</span>
<span id="cb7-1586"><a href="#cb7-1586"></a>achieved in the exact same way as in the Watch phase, and is visualized</span>
<span id="cb7-1587"><a href="#cb7-1587"></a>in @fig-watch-try-learn-arch. The architecture is simply</span>
<span id="cb7-1588"><a href="#cb7-1588"></a>adjusted to be able to take in more images fro mthe trial episodes.</span>
<span id="cb7-1589"><a href="#cb7-1589"></a></span>
<span id="cb7-1590"><a href="#cb7-1590"></a>In this section, we describe the evaluation suite for the paper,</span>
<span id="cb7-1591"><a href="#cb7-1591"></a>including the simulation benchmark used, the baselines considered, and</span>
<span id="cb7-1592"><a href="#cb7-1592"></a>the results.</span>
<span id="cb7-1593"><a href="#cb7-1593"></a></span>
<span id="cb7-1594"><a href="#cb7-1594"></a>**Gripper environment setup:**</span>
<span id="cb7-1595"><a href="#cb7-1595"></a></span>
<span id="cb7-1596"><a href="#cb7-1596"></a>![Visualization of different tasks from the simulated benchmark for</span>
<span id="cb7-1597"><a href="#cb7-1597"></a>Watch Try Learn.](Figures/watch-try-learn-envs.png){#fig-envs}</span>
<span id="cb7-1598"><a href="#cb7-1598"></a></span>
<span id="cb7-1599"><a href="#cb7-1599"></a>@fig-envs illustrates the different task families considered in the simulated</span>
<span id="cb7-1600"><a href="#cb7-1600"></a>Gripper environment. Button Pressing, Grasping, Pushing, and Pick and</span>
<span id="cb7-1601"><a href="#cb7-1601"></a>Place. For each task family, the environment supports hundreds of</span>
<span id="cb7-1602"><a href="#cb7-1602"></a>different tasks by changing the objects in the scene and the objectives</span>
<span id="cb7-1603"><a href="#cb7-1603"></a>(e.g. which object to pick and where to place). For each task in each</span>
<span id="cb7-1604"><a href="#cb7-1604"></a>task family, a handful of expert demonstrations are given in a</span>
<span id="cb7-1605"><a href="#cb7-1605"></a>demonstrations dataset. As mentioned previously, the environment gives</span>
<span id="cb7-1606"><a href="#cb7-1606"></a>the agent image observations, and take in actions as end-effector</span>
<span id="cb7-1607"><a href="#cb7-1607"></a>(gripper) positions, angles, and opening.</span>
<span id="cb7-1608"><a href="#cb7-1608"></a></span>
<span id="cb7-1609"><a href="#cb7-1609"></a>**Baselines:** The following three baselines are considered:</span>
<span id="cb7-1610"><a href="#cb7-1610"></a></span>
<span id="cb7-1611"><a href="#cb7-1611"></a><span class="ss">1.  </span>**Behavior Cloning**: simple imitation learning based on maximum</span>
<span id="cb7-1612"><a href="#cb7-1612"></a>    log-likelihood training using data from all tasks.</span>
<span id="cb7-1613"><a href="#cb7-1613"></a></span>
<span id="cb7-1614"><a href="#cb7-1614"></a><span class="ss">2.  </span>**Meta-imitation learning**: This baseline corresponds to simply</span>
<span id="cb7-1615"><a href="#cb7-1615"></a>    running the policy from the Watch step, without using any trial</span>
<span id="cb7-1616"><a href="#cb7-1616"></a>    data. That is, we only condition on the set of expert</span>
<span id="cb7-1617"><a href="#cb7-1617"></a>    demonstrations, but no online trials.</span>
<span id="cb7-1618"><a href="#cb7-1618"></a></span>
<span id="cb7-1619"><a href="#cb7-1619"></a><span class="ss">3.  </span>**Behavior Cloning + SAC**: Pre-train a policy with Behavior Cloning</span>
<span id="cb7-1620"><a href="#cb7-1620"></a>    on all data, and follow that with Reinforcement Learning fine-tuning</span>
<span id="cb7-1621"><a href="#cb7-1621"></a>    for the specific target task, using the maximum-entropy algorithm</span>
<span id="cb7-1622"><a href="#cb7-1622"></a>    SAC (<span class="co">[</span><span class="ot">@haarnoja2018soft</span><span class="co">]</span>).</span>
<span id="cb7-1623"><a href="#cb7-1623"></a></span>
<span id="cb7-1624"><a href="#cb7-1624"></a>![Results for Watch Try Learn on the gripper control environment, and</span>
<span id="cb7-1625"><a href="#cb7-1625"></a>comparisons with</span>
<span id="cb7-1626"><a href="#cb7-1626"></a>baselines.](Figures/watch-try-learn-results.png){#fig-watch-try-learn-results</span>
<span id="cb7-1627"><a href="#cb7-1627"></a>width="50%"}</span>
<span id="cb7-1628"><a href="#cb7-1628"></a></span>
<span id="cb7-1629"><a href="#cb7-1629"></a>::: {#tbl-watch-try-learn-table}</span>
<span id="cb7-1630"><a href="#cb7-1630"></a>  **METHOD**                     **SUCCESS RATE**</span>
<span id="cb7-1631"><a href="#cb7-1631"></a>  ----------------------------- ------------------</span>
<span id="cb7-1632"><a href="#cb7-1632"></a>  BC                              .09 $\pm$ .01</span>
<span id="cb7-1633"><a href="#cb7-1633"></a>  MIL                             .30 $\pm$ .02</span>
<span id="cb7-1634"><a href="#cb7-1634"></a>  WTL, 1 TRIAL (OURS)             .42 $\pm$ .02</span>
<span id="cb7-1635"><a href="#cb7-1635"></a>  **RL FINE-TUNING WITH SAC**   </span>
<span id="cb7-1636"><a href="#cb7-1636"></a>  BC + SAC, 1500 TRIALS           .11 $\pm$ .07</span>
<span id="cb7-1637"><a href="#cb7-1637"></a>  BC + SAC, 2000 TRIALS           .29 $\pm$ .10</span>
<span id="cb7-1638"><a href="#cb7-1638"></a>  BC + SAC, 2500 TRIALS           .39 $\pm$ .11</span>
<span id="cb7-1639"><a href="#cb7-1639"></a></span>
<span id="cb7-1640"><a href="#cb7-1640"></a>  : Average success rates over all tasks.</span>
<span id="cb7-1641"><a href="#cb7-1641"></a>:::</span>
<span id="cb7-1642"><a href="#cb7-1642"></a></span>
<span id="cb7-1643"><a href="#cb7-1643"></a>@fig-watch-try-learn-results shows average success rates for</span>
<span id="cb7-1644"><a href="#cb7-1644"></a>Watch Try Learn compared to baselines. Watch Try Learn significantly</span>
<span id="cb7-1645"><a href="#cb7-1645"></a>outperforms baselines on every task family. In particular, it is far</span>
<span id="cb7-1646"><a href="#cb7-1646"></a>superior to Behavior Cloning, which is a very weak baseline, and it</span>
<span id="cb7-1647"><a href="#cb7-1647"></a>significantly surpasses Meta-Imitation Learning on 3 out of 4 task</span>
<span id="cb7-1648"><a href="#cb7-1648"></a>families. @tbl-watch-try-learn-table includes comparison with BC</span>
<span id="cb7-1649"><a href="#cb7-1649"></a>fine-tuned with Reinforcement Learning. Even after 2500 online trials,</span>
<span id="cb7-1650"><a href="#cb7-1650"></a>SAC is not able to obtain the success rate that Watch Try Learn achieves</span>
<span id="cb7-1651"><a href="#cb7-1651"></a>after only 1 trial. Overall, Watch Try Learn exhibits very significant</span>
<span id="cb7-1652"><a href="#cb7-1652"></a>performance gains over prior methods.</span>
<span id="cb7-1653"><a href="#cb7-1653"></a></span>
<span id="cb7-1654"><a href="#cb7-1654"></a><span class="fu">### Direct Preference Optimization</span></span>
<span id="cb7-1655"><a href="#cb7-1655"></a></span>
<span id="cb7-1656"><a href="#cb7-1656"></a>A modern method for estimating the parameters of a human preference</span>
<span id="cb7-1657"><a href="#cb7-1657"></a>model is direct preference optimization <span class="co">[</span><span class="ot">@rafailov2023direct</span><span class="co">]</span>, which is</span>
<span id="cb7-1658"><a href="#cb7-1658"></a>used in the context of aligning language models to human preferences. A</span>
<span id="cb7-1659"><a href="#cb7-1659"></a>recent approach <span class="co">[</span><span class="ot">@christiano2023deep</span><span class="co">]</span> first trains a reward model that</span>
<span id="cb7-1660"><a href="#cb7-1660"></a>captures human preferences and then uses proximal policy optimization to</span>
<span id="cb7-1661"><a href="#cb7-1661"></a>train a language model-based policy to reflect those learned</span>
<span id="cb7-1662"><a href="#cb7-1662"></a>preferences. Direct Preference Optimization (DPO), on the other hand,</span>
<span id="cb7-1663"><a href="#cb7-1663"></a>removes the need for a reward model by directly using the model</span>
<span id="cb7-1664"><a href="#cb7-1664"></a>likelihood of two outcomes (a preferred or highly-ranked sequence and an</span>
<span id="cb7-1665"><a href="#cb7-1665"></a>unpreferred or low-ranked sequence) to capture the preference</span>
<span id="cb7-1666"><a href="#cb7-1666"></a>represented in the data. DPO provides a simpler framework than its</span>
<span id="cb7-1667"><a href="#cb7-1667"></a>reinforcement learning approach and results in comparable performance</span>
<span id="cb7-1668"><a href="#cb7-1668"></a>with improved stability. Furthermore, it obviates the need to train a</span>
<span id="cb7-1669"><a href="#cb7-1669"></a>reward model, instead using a language model policy and human preference</span>
<span id="cb7-1670"><a href="#cb7-1670"></a>dataset to align the policy directly to human preferences.</span>
<span id="cb7-1671"><a href="#cb7-1671"></a></span>
<span id="cb7-1672"><a href="#cb7-1672"></a><span class="fu">### Model Design Consideration</span></span>
<span id="cb7-1673"><a href="#cb7-1673"></a></span>
<span id="cb7-1674"><a href="#cb7-1674"></a>When designing models and learning their parameters, one must account</span>
<span id="cb7-1675"><a href="#cb7-1675"></a>for important tradeoffs when designing and optimizing a model to learn</span>
<span id="cb7-1676"><a href="#cb7-1676"></a>human preferences.</span>
<span id="cb7-1677"><a href="#cb7-1677"></a></span>
<span id="cb7-1678"><a href="#cb7-1678"></a>**Bias vs. Variance Trade-off.** In modeling human preferences, we aim</span>
<span id="cb7-1679"><a href="#cb7-1679"></a>to ensure that predicted utilities accurately reflect overall human</span>
<span id="cb7-1680"><a href="#cb7-1680"></a>preferences. One key challenge is managing the bias and variance</span>
<span id="cb7-1681"><a href="#cb7-1681"></a>trade-off.</span>
<span id="cb7-1682"><a href="#cb7-1682"></a></span>
<span id="cb7-1683"><a href="#cb7-1683"></a>Bias refers to assumptions made during model design and training that</span>
<span id="cb7-1684"><a href="#cb7-1684"></a>can skew predictions. For example, in Ideal Point Models, we make the</span>
<span id="cb7-1685"><a href="#cb7-1685"></a>assumption that the representations we use for individuals and choices</span>
<span id="cb7-1686"><a href="#cb7-1686"></a>are aligned in the embedding space, and that this representation is</span>
<span id="cb7-1687"><a href="#cb7-1687"></a>sufficient to capture human preferences using distance metrics. However,</span>
<span id="cb7-1688"><a href="#cb7-1688"></a>there are myriad cases in which this may break down, for example if the</span>
<span id="cb7-1689"><a href="#cb7-1689"></a>two sets of vectors follow different distributions each with their own</span>
<span id="cb7-1690"><a href="#cb7-1690"></a>unique biases. If the representations do not come from the same domain,</span>
<span id="cb7-1691"><a href="#cb7-1691"></a>one may have little visibility into how a distance metric computes the</span>
<span id="cb7-1692"><a href="#cb7-1692"></a>final utility value for a choice for a given individual. Some ways to</span>
<span id="cb7-1693"><a href="#cb7-1693"></a>mitigate bias in human preference models include increasing the number</span>
<span id="cb7-1694"><a href="#cb7-1694"></a>of parameters in a model (allowing for better learning of patterns in</span>
<span id="cb7-1695"><a href="#cb7-1695"></a>the data) or removing inductive biases based on our assumptions of the</span>
<span id="cb7-1696"><a href="#cb7-1696"></a>underlying data.</span>
<span id="cb7-1697"><a href="#cb7-1697"></a></span>
<span id="cb7-1698"><a href="#cb7-1698"></a>On the other hand, variance refers to the model's sensitivity to small</span>
<span id="cb7-1699"><a href="#cb7-1699"></a>changes in the input, which leads to significant changes in the outp ut.</span>
<span id="cb7-1700"><a href="#cb7-1700"></a>This phenomenon is often termed 'overfitting' or 'overparameterization.'</span>
<span id="cb7-1701"><a href="#cb7-1701"></a>This behavior can occur in models that have many parameters, and learn</span>
<span id="cb7-1702"><a href="#cb7-1702"></a>correlations in the data that do not contribute to learning human</span>
<span id="cb7-1703"><a href="#cb7-1703"></a>preferences, but are artifacts of noise in the dataset that one should</span>
<span id="cb7-1704"><a href="#cb7-1704"></a>ultimately ignore. One can address variance in models by reducing the</span>
<span id="cb7-1705"><a href="#cb7-1705"></a>number of parameters or incorporating biases in the model based on</span>
<span id="cb7-1706"><a href="#cb7-1706"></a>factors we can assume about the data.</span>
<span id="cb7-1707"><a href="#cb7-1707"></a></span>
<span id="cb7-1708"><a href="#cb7-1708"></a>**Model Scope.** One important consideration unique to human preference</span>
<span id="cb7-1709"><a href="#cb7-1709"></a>models is that we wish to model individual preferences, and we may</span>
<span id="cb7-1710"><a href="#cb7-1710"></a>choose to do so at arbitrary granularity. For example, we can fit models</span>
<span id="cb7-1711"><a href="#cb7-1711"></a>to a specific individual or even multiple models for an individual, each</span>
<span id="cb7-1712"><a href="#cb7-1712"></a>for different purposes or contexts. On the other end of the spectrum, we</span>
<span id="cb7-1713"><a href="#cb7-1713"></a>may create a model to capture human preferences across large populations</span>
<span id="cb7-1714"><a href="#cb7-1714"></a>or the world.</span>
<span id="cb7-1715"><a href="#cb7-1715"></a></span>
<span id="cb7-1716"><a href="#cb7-1716"></a>Individual models may certainly prove to be more powerful, as they do</span>
<span id="cb7-1717"><a href="#cb7-1717"></a>not need to generalize across multiple individuals and can dedicate all</span>
<span id="cb7-1718"><a href="#cb7-1718"></a>of their parameters to learning the preferences of a single user. In the</span>
<span id="cb7-1719"><a href="#cb7-1719"></a>context of human behavior, this can be a significant advantage as any</span>
<span id="cb7-1720"><a href="#cb7-1720"></a>two individuals can be arbitrarily different or even opposite in their</span>
<span id="cb7-1721"><a href="#cb7-1721"></a>preferences. On the other hand, models fit only one person can</span>
<span id="cb7-1722"><a href="#cb7-1722"></a>tremendously overfit to the training distribution and capture noise in</span>
<span id="cb7-1723"><a href="#cb7-1723"></a>the data, which is not truly representative of human preferences.</span>
<span id="cb7-1724"><a href="#cb7-1724"></a></span>
<span id="cb7-1725"><a href="#cb7-1725"></a>On the end of the spectrum, models fit to the entire world may be</span>
<span id="cb7-1726"><a href="#cb7-1726"></a>inadequate to model human preferences for arbitrary individuals,</span>
<span id="cb7-1727"><a href="#cb7-1727"></a>especially those whose data it has not been fit to. As such, models may</span>
<span id="cb7-1728"><a href="#cb7-1728"></a>underfit the given training distribution. These models aim to generalize</span>
<span id="cb7-1729"><a href="#cb7-1729"></a>to many people but may fail to capture the nuances of individual</span>
<span id="cb7-1730"><a href="#cb7-1730"></a>preferences, especially for those whose data is not represented in the</span>
<span id="cb7-1731"><a href="#cb7-1731"></a>training set. As a result, they may not perform well for arbitrary</span>
<span id="cb7-1732"><a href="#cb7-1732"></a>individuals within the target population</span>
<span id="cb7-1733"><a href="#cb7-1733"></a></span>
<span id="cb7-1734"><a href="#cb7-1734"></a>Choosing the appropriate scope for a model is crucial. ne must balance</span>
<span id="cb7-1735"><a href="#cb7-1735"></a>the trade-off between overfitting to noise in highly granular models and</span>
<span id="cb7-1736"><a href="#cb7-1736"></a>underfitting in broader models that may not capture individual nuances.</span>
<span id="cb7-1737"><a href="#cb7-1737"></a></span>
<span id="cb7-1738"><a href="#cb7-1738"></a><span class="fu">## Multimodal Preferences</span></span>
<span id="cb7-1739"><a href="#cb7-1739"></a></span>
<span id="cb7-1740"><a href="#cb7-1740"></a>One of the core assumptions about learning a reward function is that it</span>
<span id="cb7-1741"><a href="#cb7-1741"></a>is unimodal, meaning that it consists of data from one person with a</span>
<span id="cb7-1742"><a href="#cb7-1742"></a>certain set of preferences or a group of people with similar</span>
<span id="cb7-1743"><a href="#cb7-1743"></a>preferences. However, the model of unimodality often oversimplifies</span>
<span id="cb7-1744"><a href="#cb7-1744"></a>human preferences and their often conflicting nature. To accurately</span>
<span id="cb7-1745"><a href="#cb7-1745"></a>capture all the nuances of human preference, we examine a multi-modal</span>
<span id="cb7-1746"><a href="#cb7-1746"></a>distribution with some baseline assumptions. Consider a scenario where</span>
<span id="cb7-1747"><a href="#cb7-1747"></a>we, as regular drivers, make a left-hand turn at an intersection</span>
<span id="cb7-1748"><a href="#cb7-1748"></a><span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. What would we do if we saw a car speeding down the</span>
<span id="cb7-1749"><a href="#cb7-1749"></a>road approaching us? The figure below describes some options. Following</span>
<span id="cb7-1750"><a href="#cb7-1750"></a>a timid driving pattern, some vehicles would stop to let the other car</span>
<span id="cb7-1751"><a href="#cb7-1751"></a>go, preventing a collision. Other vehicles would be more aggressive and</span>
<span id="cb7-1752"><a href="#cb7-1752"></a>try to make the turn before colliding with the oncoming vehicle. Given</span>
<span id="cb7-1753"><a href="#cb7-1753"></a>the data of one of these driving patterns, our model (our autonomous</span>
<span id="cb7-1754"><a href="#cb7-1754"></a>vehicle) can make an appropriate decision. However, what if our model</span>
<span id="cb7-1755"><a href="#cb7-1755"></a>was given data from both aggressive and timid drivers, and we don't know</span>
<span id="cb7-1756"><a href="#cb7-1756"></a>which data corresponds to which type of driver? If we applied standard</span>
<span id="cb7-1757"><a href="#cb7-1757"></a>learning based on comparison techniques, we see, as illustrated by the</span>
<span id="cb7-1758"><a href="#cb7-1758"></a>figure below, that the car would have an accident trying to find a</span>
<span id="cb7-1759"><a href="#cb7-1759"></a>policy close enough to both driving patterns.</span>
<span id="cb7-1760"><a href="#cb7-1760"></a></span>
<span id="cb7-1761"><a href="#cb7-1761"></a>![<span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span> shows the possibilities of 2 different driving</span>
<span id="cb7-1762"><a href="#cb7-1762"></a>patterns when a car is taking a left-hand turn at an intersection and</span>
<span id="cb7-1763"><a href="#cb7-1763"></a>sees another car approaching</span>
<span id="cb7-1764"><a href="#cb7-1764"></a>head-on.](Figures/driving-patt.png){#fig-driving-patt}</span>
<span id="cb7-1765"><a href="#cb7-1765"></a></span>
<span id="cb7-1766"><a href="#cb7-1766"></a>!<span class="co">[</span><span class="ot">The figure [@myers2021learning</span><span class="co">]</span> depicts the resultant collision when we</span>
<span id="cb7-1767"><a href="#cb7-1767"></a>try to find a policy close enough to both the driving</span>
<span id="cb7-1768"><a href="#cb7-1768"></a>patterns.](Figures/driving-coll.png){#fig-driving-coll}</span>
<span id="cb7-1769"><a href="#cb7-1769"></a></span>
<span id="cb7-1770"><a href="#cb7-1770"></a>As illustrated by the driving example, we see that multi-modality for</span>
<span id="cb7-1771"><a href="#cb7-1771"></a>our reward function is extremely important and, in some cases, if it is</span>
<span id="cb7-1772"><a href="#cb7-1772"></a>not considered, can lead to fatal decisions <span class="co">[</span><span class="ot">@myers2021learning</span><span class="co">]</span>. But why</span>
<span id="cb7-1773"><a href="#cb7-1773"></a>can't we label the groups, which would be the timid and aggressive</span>
<span id="cb7-1774"><a href="#cb7-1774"></a>drivers in the driving case, and then learn separate reward functions</span>
<span id="cb7-1775"><a href="#cb7-1775"></a>for each driver? The first problem with this approach is that it is</span>
<span id="cb7-1776"><a href="#cb7-1776"></a>inefficient and time-consuming to separate the data into groups because</span>
<span id="cb7-1777"><a href="#cb7-1777"></a>we would have to cluster and label the data. Secondly, it would not be</span>
<span id="cb7-1778"><a href="#cb7-1778"></a>accurate just to split the data because a more timid driver can be</span>
<span id="cb7-1779"><a href="#cb7-1779"></a>aggressive when they are in a hurry.</span>
<span id="cb7-1780"><a href="#cb7-1780"></a></span>
<span id="cb7-1781"><a href="#cb7-1781"></a>To formulate this problem of learning reward functions and mixing</span>
<span id="cb7-1782"><a href="#cb7-1782"></a>coefficients from ranking queries in a fully observable deterministic</span>
<span id="cb7-1783"><a href="#cb7-1783"></a>dynamical system, we begin by describing the system as a trajectory</span>
<span id="cb7-1784"><a href="#cb7-1784"></a>$\xi = (s_0, a_0, ..., s_T, a_T)$, where the sequence of states and</span>
<span id="cb7-1785"><a href="#cb7-1785"></a>actions represents the system's evolution over time. Assume there are</span>
<span id="cb7-1786"><a href="#cb7-1786"></a>$M$ different reward functions, each representing an expert's</span>
<span id="cb7-1787"><a href="#cb7-1787"></a>preferences. Using the linearity assumption in reward learning, we model</span>
<span id="cb7-1788"><a href="#cb7-1788"></a>each expert's reward function as a linear combination of features in a</span>
<span id="cb7-1789"><a href="#cb7-1789"></a>known, fixed feature space $\phi(\xi)$. The reward for the $m$-th expert</span>
<span id="cb7-1790"><a href="#cb7-1790"></a>is given by: $$R_m(\xi) = \omega^T_m \phi(\xi),$$ where $\omega_m$ is a</span>
<span id="cb7-1791"><a href="#cb7-1791"></a>vector of parameters corresponding to the $m$-th expert's preferences.</span>
<span id="cb7-1792"><a href="#cb7-1792"></a>There exists an unknown distribution over the reward parameters and we</span>
<span id="cb7-1793"><a href="#cb7-1793"></a>can represent this distribution with mixing coefficients $\alpha_m$ such</span>
<span id="cb7-1794"><a href="#cb7-1794"></a>that $\sum_M^{m = 1} \alpha_m = 1$. Our goal is to learn reward</span>
<span id="cb7-1795"><a href="#cb7-1795"></a>functions and mixing coefficients using ranking queries.</span>
<span id="cb7-1796"><a href="#cb7-1796"></a></span>
<span id="cb7-1797"><a href="#cb7-1797"></a>To define our problem, let's consider a robot who performs the following</span>
<span id="cb7-1798"><a href="#cb7-1798"></a>trajectories and asks a user to rank all the trajectories.</span>
<span id="cb7-1799"><a href="#cb7-1799"></a></span>
<span id="cb7-1800"><a href="#cb7-1800"></a>!<span class="co">[</span><span class="ot">The figure [@myers2022learning</span><span class="co">]</span> depicts a few different trajectories</span>
<span id="cb7-1801"><a href="#cb7-1801"></a>for an example multi-modal ranking</span>
<span id="cb7-1802"><a href="#cb7-1802"></a>scenario.](Figures/robot-traj.png){#fig-robot-traj}</span>
<span id="cb7-1803"><a href="#cb7-1803"></a></span>
<span id="cb7-1804"><a href="#cb7-1804"></a>The robot will be given back a set of trajectory rankings, coming from M</span>
<span id="cb7-1805"><a href="#cb7-1805"></a>humans and the objective is to learn the underlying reward function. We</span>
<span id="cb7-1806"><a href="#cb7-1806"></a>can represent the response of the ranking query as</span>
<span id="cb7-1807"><a href="#cb7-1807"></a>$x = (\xi_{a_1},\ ...\ ,\xi_{a_K})$ where $a_1$ is the index of the</span>
<span id="cb7-1808"><a href="#cb7-1808"></a>expert's top choice, $a_2$ is the index of the expert's second choice,</span>
<span id="cb7-1809"><a href="#cb7-1809"></a><span class="sc">\.</span>.. and so on. With the response $x$, we generate a probability</span>
<span id="cb7-1810"><a href="#cb7-1810"></a>distribution with the softmax rule <span class="co">[</span><span class="ot">@myers2022learning</span><span class="co">]</span>:</span>
<span id="cb7-1811"><a href="#cb7-1811"></a>$Pr(x_1 = \xi_{a_1} | R = R_m) = \frac{e^R_m(\xi_{a_1})}{\sum_{j=1}^Ke^R_m(\xi_{a_j})}$.</span>
<span id="cb7-1812"><a href="#cb7-1812"></a>where $R_m(\xi_{a_i})$ denotes the reward assigned by the $m$-th expert</span>
<span id="cb7-1813"><a href="#cb7-1813"></a>to trajectory $\xi_{a_i}$. Then, we randomly sample our probability</span>
<span id="cb7-1814"><a href="#cb7-1814"></a>distribution to pick our top choice. From the remaining trajectories, we</span>
<span id="cb7-1815"><a href="#cb7-1815"></a>noisily choose from our distribution to rank our second-best option. We</span>
<span id="cb7-1816"><a href="#cb7-1816"></a>repeat this process until we have ranked all our trajectories. This</span>
<span id="cb7-1817"><a href="#cb7-1817"></a>follows what is known as the Plackett-Luce Ranking Model.</span>
<span id="cb7-1818"><a href="#cb7-1818"></a></span>
<span id="cb7-1819"><a href="#cb7-1819"></a>Given knowledge of the true reward function weights $\omega_m$ and</span>
<span id="cb7-1820"><a href="#cb7-1820"></a>mixing coefficients $\alpha_m$, we have the following joint mass over</span>
<span id="cb7-1821"><a href="#cb7-1821"></a>observations x from a query Q:</span>
<span id="cb7-1822"><a href="#cb7-1822"></a>$Pr(x\ |\ Q) = \sum_{m = 1}^M \alpha_m\prod_{i = 1}^K\frac{e^{\omega_m^T \Phi(\xi_{a_i})}}{\sum_{j = i}^K e^{\omega_m^T \Phi(\xi_{a_j})}}$.</span>
<span id="cb7-1823"><a href="#cb7-1823"></a></span>
<span id="cb7-1824"><a href="#cb7-1824"></a>With the above formulation of the joint mass distribution over</span>
<span id="cb7-1825"><a href="#cb7-1825"></a>observation and queries, we can now formulate an objective.</span>
<span id="cb7-1826"><a href="#cb7-1826"></a>Specifically, it is to present users with the best set of queries that</span>
<span id="cb7-1827"><a href="#cb7-1827"></a>learn reward weights, $\omega$, and mixing coefficient, $\alpha$, based</span>
<span id="cb7-1828"><a href="#cb7-1828"></a>upon user rankings of preferred query responses. By learning these</span>
<span id="cb7-1829"><a href="#cb7-1829"></a>parameters, we can have an accurate estimation of the joint mass</span>
<span id="cb7-1830"><a href="#cb7-1830"></a>distribution of the observations.</span>
<span id="cb7-1831"><a href="#cb7-1831"></a></span>
<span id="cb7-1832"><a href="#cb7-1832"></a>To learn these parameters, we use a Bayesian learning framework. The</span>
<span id="cb7-1833"><a href="#cb7-1833"></a>goal will be to learn the reward weights, $\omega_m$, and all mixing</span>
<span id="cb7-1834"><a href="#cb7-1834"></a>coefficients $\alpha_m$. Thus, define the parameters to be</span>
<span id="cb7-1835"><a href="#cb7-1835"></a>$\theta = <span class="sc">\{</span>\omega, \alpha<span class="sc">\}</span>$. We start by simplifying the posterior</span>
<span id="cb7-1836"><a href="#cb7-1836"></a>over the parameters.</span>
<span id="cb7-1837"><a href="#cb7-1837"></a></span>
<span id="cb7-1838"><a href="#cb7-1838"></a>$$\begin{aligned}</span>
<span id="cb7-1839"><a href="#cb7-1839"></a>\Pr(\Theta | Q^{(1)}, x^{(1)}, Q^{(2)}, x^{(2)}, \ldots) &amp; \propto \Pr(\Theta) \Pr(Q^{(1)} | x^{(1)}, Q^{(2)}, x^{(2)}, \ldots | \Theta) <span class="sc">\\</span></span>
<span id="cb7-1840"><a href="#cb7-1840"></a>&amp; = \Pr(\Theta) \prod_t \Pr(x^{(t)} | Q^{(t)}, \Theta, Q^{(1)}, x^{(1)}, \ldots, Q^{(t-1)}, x^{(t-1)}) <span class="sc">\\</span></span>
<span id="cb7-1841"><a href="#cb7-1841"></a>&amp; \propto \Pr(\Theta) \prod_t \Pr(x^{(t)} | \Theta, Q^{(t)})</span>
<span id="cb7-1842"><a href="#cb7-1842"></a>\end{aligned}$$</span>
<span id="cb7-1843"><a href="#cb7-1843"></a></span>
<span id="cb7-1844"><a href="#cb7-1844"></a>Note that the first proportionality term is directly from Bayes rule</span>
<span id="cb7-1845"><a href="#cb7-1845"></a>(removing normalization constant). The first equation comes directly</span>
<span id="cb7-1846"><a href="#cb7-1846"></a>from the assumption that the queries at timestamp $t$ are conditionally</span>
<span id="cb7-1847"><a href="#cb7-1847"></a>independent of the parameters given previous queries &amp; rankings. This</span>
<span id="cb7-1848"><a href="#cb7-1848"></a>assumption is reasonable because the previous queries &amp; rankings ideally</span>
<span id="cb7-1849"><a href="#cb7-1849"></a>give all the information to inform the choice of the next set of. The</span>
<span id="cb7-1850"><a href="#cb7-1850"></a>last proportionality term comes from the assumption that the ranked</span>
<span id="cb7-1851"><a href="#cb7-1851"></a>queries are conditionally independent given the parameters</span>
<span id="cb7-1852"><a href="#cb7-1852"></a></span>
<span id="cb7-1853"><a href="#cb7-1853"></a>The prior distribution is dependent on use case. For example, in the</span>
<span id="cb7-1854"><a href="#cb7-1854"></a>user studies conducted by the authors to verify this method, they use a</span>
<span id="cb7-1855"><a href="#cb7-1855"></a>standard Gaussian for the reward weights and the mixing coefficients to</span>
<span id="cb7-1856"><a href="#cb7-1856"></a>be uniform on a $M - 1$ simplex to ensure that they add up to 1. Then we</span>
<span id="cb7-1857"><a href="#cb7-1857"></a>can use maximum likelihood estimation to compute the parameters with the</span>
<span id="cb7-1858"><a href="#cb7-1858"></a>simplified posterior.</span>
<span id="cb7-1859"><a href="#cb7-1859"></a></span>
<span id="cb7-1860"><a href="#cb7-1860"></a><span class="fu">## Social Choices</span></span>
<span id="cb7-1861"><a href="#cb7-1861"></a></span>
<span id="cb7-1862"><a href="#cb7-1862"></a>Game theory provides a mathematical framework for analyzing strategic</span>
<span id="cb7-1863"><a href="#cb7-1863"></a>interactions among rational agents. These models help in understanding</span>
<span id="cb7-1864"><a href="#cb7-1864"></a>and predicting human behavior by considering multiple criteria and the</span>
<span id="cb7-1865"><a href="#cb7-1865"></a>associated trade-offs. They enhance the understanding of preferences</span>
<span id="cb7-1866"><a href="#cb7-1866"></a>across multiple criteria and allow for richer and more accurate feedback</span>
<span id="cb7-1867"><a href="#cb7-1867"></a>through structured comparisons. Game-theory framings capture the</span>
<span id="cb7-1868"><a href="#cb7-1868"></a>complexity of preferences and interactions in decision-making processes</span>
<span id="cb7-1869"><a href="#cb7-1869"></a><span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1870"><a href="#cb7-1870"></a></span>
<span id="cb7-1871"><a href="#cb7-1871"></a>The most popular form of preference elicitation involves pairwise</span>
<span id="cb7-1872"><a href="#cb7-1872"></a>comparisons. Users are asked to choose between two options, such as</span>
<span id="cb7-1873"><a href="#cb7-1873"></a>product A or product B. This method is used in various applications like</span>
<span id="cb7-1874"><a href="#cb7-1874"></a>search engines, recommender systems, and interactive robotics. Key</span>
<span id="cb7-1875"><a href="#cb7-1875"></a>concepts include the Von Neumann Winner and the Blackwell Winner. The</span>
<span id="cb7-1876"><a href="#cb7-1876"></a>Von Neumann Winner refers to a distribution over objects that beats or</span>
<span id="cb7-1877"><a href="#cb7-1877"></a>ties every other object in the collection under the expected utility</span>
<span id="cb7-1878"><a href="#cb7-1878"></a>assumption. The Blackwell Winner generalizes the Von Neumann Winner for</span>
<span id="cb7-1879"><a href="#cb7-1879"></a>multi-criteria problems using a target set for acceptable payoff vectors</span>
<span id="cb7-1880"><a href="#cb7-1880"></a><span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1881"><a href="#cb7-1881"></a></span>
<span id="cb7-1882"><a href="#cb7-1882"></a>Game-theory framings provide a framework for preference learning along</span>
<span id="cb7-1883"><a href="#cb7-1883"></a>multiple criteria. These models use tools from vector-valued payoffs in</span>
<span id="cb7-1884"><a href="#cb7-1884"></a>game theory, with Blackwell's approach being a key concept. This</span>
<span id="cb7-1885"><a href="#cb7-1885"></a>approach allows for a more comprehensive understanding of preferences by</span>
<span id="cb7-1886"><a href="#cb7-1886"></a>considering multiple criteria simultaneously <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1887"><a href="#cb7-1887"></a></span>
<span id="cb7-1888"><a href="#cb7-1888"></a>In game-theory framings, pairwise preferences are modeled as random</span>
<span id="cb7-1889"><a href="#cb7-1889"></a>variables. Comparisons between objects along different criteria are</span>
<span id="cb7-1890"><a href="#cb7-1890"></a>captured in a preference tensor $P$. This tensor models the probability</span>
<span id="cb7-1891"><a href="#cb7-1891"></a>that one object is preferred over another along a specific criterion,</span>
<span id="cb7-1892"><a href="#cb7-1892"></a>allowing for a detailed understanding of preferences across multiple</span>
<span id="cb7-1893"><a href="#cb7-1893"></a>dimensions <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1894"><a href="#cb7-1894"></a></span>
<span id="cb7-1895"><a href="#cb7-1895"></a>The preference tensor $P$ captures object comparisons along different</span>
<span id="cb7-1896"><a href="#cb7-1896"></a>criteria. It is defined as:</span>
<span id="cb7-1897"><a href="#cb7-1897"></a>$$P(i_1, i_2; j) = P(i_1 \succ i_2 \text{ along criterion } j)$$ where</span>
<span id="cb7-1898"><a href="#cb7-1898"></a>$P(i_2, i_1; j) = 1 - P(i_1, i_2; j)$. These values are aggregated to</span>
<span id="cb7-1899"><a href="#cb7-1899"></a>form an overall preference matrix $P_{ov}$ <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1900"><a href="#cb7-1900"></a></span>
<span id="cb7-1901"><a href="#cb7-1901"></a>The Blackwell Winner is defined using a target set $S$ of acceptable</span>
<span id="cb7-1902"><a href="#cb7-1902"></a>score vectors. The goal is to find a distribution $\pi^*$ such that</span>
<span id="cb7-1903"><a href="#cb7-1903"></a>$P(\pi^*, \pi) \in S$ for all $\pi$. This method minimizes the maximum</span>
<span id="cb7-1904"><a href="#cb7-1904"></a>distance to the target set, providing a robust solution to</span>
<span id="cb7-1905"><a href="#cb7-1905"></a>multi-criteria preference problems <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1906"><a href="#cb7-1906"></a></span>
<span id="cb7-1907"><a href="#cb7-1907"></a>The optimization problem for finding the Blackwell Winner is defined as:</span>
<span id="cb7-1908"><a href="#cb7-1908"></a>$$\pi(P, S, \|\cdot\|) = \arg \min_{\pi \in \Delta_d} \left<span class="co">[</span><span class="ot"> \max_{\pi' \in \Delta_d} \rho(P(\pi, \pi'), S) \right</span><span class="co">]</span>$$</span>
<span id="cb7-1909"><a href="#cb7-1909"></a>where $\rho(u, v) = \|u - v\|$. This measures the distance to the target</span>
<span id="cb7-1910"><a href="#cb7-1910"></a>set, ensuring that the selected distribution is as close as possible to</span>
<span id="cb7-1911"><a href="#cb7-1911"></a>the ideal preference vector <span class="co">[</span><span class="ot">@bhatia2020preference</span><span class="co">]</span>.</span>
<span id="cb7-1912"><a href="#cb7-1912"></a></span>
<span id="cb7-1913"><a href="#cb7-1913"></a></span>
<span id="cb7-1914"><a href="#cb7-1914"></a>{{&lt; include psets/pset1.qmd &gt;}}</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/sangttruong/mlhp/blob/main/src/002-reward_model.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/sangttruong/mlhp/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer><script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>